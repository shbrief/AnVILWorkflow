"idx","workspace_namespace","workspace_name","accessLevel","workspace_url","public","workspace_is_locked","indication","study_design","cohort_country","description","dataset_owner","dataset_custodian","dataset_description","number_of_subjects","primary_disease_site","project_name","cell_type","data_use_restriction","reference","dataset_name","data_type","workspace_key"
1,"broad-firecloud-tcga","TCGA_LAML_hg38_OpenAccess_GDCDR-12-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_LAML_hg38_OpenAccess_GDCDR-12-0_DATA",TRUE,TRUE,"Acute Myeloid Leukemia","Tumor/Normal","USA","TCGA Acute Myeloid Leukemia Open-Access Data Workspace

*hg38 TCGA and TARGET workspaces reference files by their GDC UUIDs.  In order to run analyses on the referenced data files, you will need to run workflows that retrieve the files from the GDC and copy them to your workspace bucket.  See   [this forum post](https://gatkforums.broadinstitute.org/firecloud/discussion/10382/populating-hg38-tcga-and-target-workspaces-with-data-files#latest) for instructions on the running of these workflows.*","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients' clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","200","Bone Marrow","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh38/hg38","TCGA_LAML_hg38_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_LAML_hg38_OpenAccess_GDCDR-12-0_DATA"
2,"getz-lab-publications","REBC_methods_only","READER","https://app.terra.bio/#workspaces/getz-lab-publications/REBC_methods_only",TRUE,FALSE,NA,NA,NA,"mutation pipeline:

1. stewart/wgs_pip_m1_fragcounter_oxoq_1 : stewart/wgs_pip_m2_64core : stewart/strelka : erictdawson/strelka2 : stewart/SvABA_xtramem
2. stewart/merge_maflite2_workflow.  (stewart/maflite_merge_maf_workflow until 1/1/2021)
3. stewart/contest_oxoG_PoN_blat_Filter_Workflow
4. stewart/contEstFile2value
5. stewart/postFilter_contEst
6. stewart/rebc_consensus_maf
...
7. stewart/absolute_forcecall_maf_fpc (clonality & CCFs ~ ABSOLUTE)

SV pipeline

1. stewart/pipette_wgs_SV : stewart/manta : stewart/SvABA_xtramem
2. stewart/svaba_snowmanvcf2dRangerForBP : stewart/mantavcf2dRangerForBP : stewart/extract_dRanger_intermediates : stewart/pcawg_snowmanvcf2dRangerForBP
3. stewart/SV_cluster_forBP (or  cchu/SV_cluster_forBP_emptySVFiles if the former pipeline fails because one of the input files are empty)
4. stewart/breakpointer
5. Breakpointer_fix_sample
6. stewart/REBC_SV_consensus_filter_v3

SCNA pipeline

1-CNV_Somatic_Panel : to make CNV PoN

2-CNV_Somatic_Pair_gatk414  -> Model_Segments_PostProcessing_canonical_gatk414  -> absolute -> Aggregate_Model_Segments_PostProcessing_canonical -> CopyNumber_Gistic2_hg19

Germline mutation pipeline

1. gatk/haplotypecaller-gvcf-gatk4
2. find some way to run joint-discovery-gatk4 ?  (blows up with >10 samples, sometimes 1)",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","getz-lab-publications/REBC_methods_only"
3,"broad-firecloud-tcga","TARGET_CCSK_hg38_OpenAccess_GDCDR-12-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TARGET_CCSK_hg38_OpenAccess_GDCDR-12-0_DATA",TRUE,TRUE,"kidney clear cell sarcoma","Tumor/Normal",NA,"[TARGET Kidney Tumors Project](https://ocg.cancer.gov/programs/target/projects/kidney-tumors): Clear Cell Sarcoma of the Kidney

*hg38 TCGA and TARGET workspaces reference files by their GDC UUIDs.  In order to run analyses on the referenced data files, you will need to run workflows that retrieve the files from the GDC and copy them to your workspace bucket.  See   [this forum post](https://gatkforums.broadinstitute.org/firecloud/discussion/10382/populating-hg38-tcga-and-target-workspaces-with-data-files#latest) for instructions on the running of these workflows.*","NCI","Chet Birger","Clear Cell Sarcoma of the Kidney","13",NA,"TARGET",NA,"Open","GRCh38/hg38","TARGET Kidney Tumors Project Clear Cell Sarcoma of the Kidney","Whole Exome","broad-firecloud-tcga/TARGET_CCSK_hg38_OpenAccess_GDCDR-12-0_DATA"
4,"broad-firecloud-tcga","TCGA_DLBC_OpenAccess_V1-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_DLBC_OpenAccess_V1-0_DATA",TRUE,TRUE,"Lymphoid Neoplasm Diffuse Large B-cell Lymphoma","Tumor/Normal","USA","TCGA Lymphoid Neoplasm Diffuse Large B-cell Lymphoma Open-Access Data Workspace","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients’ clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","50","Lymph Nodes","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh37/hg19","TCGA_DLBC_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_DLBC_OpenAccess_V1-0_DATA"
5,"tidal-waves","Peat-Demo","READER","https://app.terra.bio/#workspaces/tidal-waves/Peat-Demo",TRUE,FALSE,NA,NA,NA,"Demo of how to use [Peat (external link)](https://broad.io/peat) to save overhead by grouping jobs into fewer WDL scatter branches. To compare scatter with and without Peat, this workspace has two simple demo workflows using WDL scatter, one with and one without using Peat.


## Overview
WDL scatter with lots of branches can incur a large overhead, especially when each branch only has a small payload task. Each branch will involves starting up a VM, pulling a docker image and typically copying needed data to local disk.

The solution is to group jobs and run multiple jobs in the same WDL branch. Making sure that each job ends up being run once and only once in an ad hoc manner is cumbersome and error-prone, especially when trying to do another run with different numbers of jobs or groups.

Peat is designed to make this as easy as possible, especially when used from within a WDL.

## Data
No data in this workspace. The workflows run without input data.
## Workflows
There are two workflows to compare, ScatterWithoutPeat and ScatterWithPeat. They produce the same end result, but as the name suggests, the first uses WDL scatter without Peat, and the second wit Peat.

To make it as simple as possible, the workflows need no input data, just parameters. A ""job"" consists of a simple echo command that writes a line to a file, and then there is an extra reduce task that concatenates all the files into a single output file.

Workflow ScatterWithoutPeat does a scatter over all jobs, so each scatter branch is one job.

Workflow ScatterWithPeat groups jobs and does a scatter where each scatter branch runs a group of jobs managed by Peat.

The end result is the same.

Inputs for both workflows are the number of jobs and the name of the final output file. ScatterWithPeat in addition also takes the number of groups as input.

## Runtimes and costs

Both workflows run the same payload jobs and produce the same final output.

### ScatterWithoutPeat

Performs a simple job (writing a line to a file) many times via simple WDL scatter, then additionally concatenates all files into a single output file.

|   n_jobs  |  time  |  cost $  |  link  |
|-------------|---------|----------|-------|
| 1000 |  0:30  |  2.85  | [link](https://app.terra.bio/#workspaces/tidal-waves/Peat-Demo/job_history/a78dd0f9-e315-445b-8ab3-90be7893bf32) |
| 1200 |  0:57  |  3.63  | [link](https://app.terra.bio/#workspaces/tidal-waves/Peat-Demo/job_history/8430bcb8-550a-4db2-9548-f66e79cf0fbf) |
| 1500  |   1:00  |  4.84  | [link](https://app.terra.bio/#workspaces/tidal-waves/Peat-Demo/job_history/c59adf92-de7d-4df0-aed4-259858b9547f)  |
|  2000  |  1:14  |  6.20  | [link](https://app.terra.bio/#workspaces/tidal-waves/Peat-Demo/job_history/41f35b0d-66f3-4df3-a624-1e96e01ba9ed)  |

### ScatterWithPeat

Performs the same job, but using Peat to run multiple jobs on each WDL scatter branch, then additionally concatenates all files into a single output file.

|   n_jobs  | n_groups  |  time  |  cost $  |  job  |
|-------------|--------------|---------|----------|-------|
|  1000  |   50  |  0:11  |  0.15  |  [link](https://app.terra.bio/#workspaces/tidal-waves/Peat-Demo/job_history/7e0f8ed1-cc98-46f9-9ef1-1468c04a5839)  |
|  1200  |   50  |  0:32  |  0.23  |  [link](https://app.terra.bio/#workspaces/tidal-waves/Peat-Demo/job_history/0604ce54-807f-44d0-a988-7765665a024d)  |
|  1500  |  50  |  0:33  |  0.15  |  [link](https://app.terra.bio/#workspaces/tidal-waves/Peat-Demo/job_history/20cbf945-5c23-46a9-9776-bb2dea3ec495)  |
|  2000  |  50  |  0:29  |  0.15  |  [link](https://app.terra.bio/#workspaces/tidal-waves/Peat-Demo/job_history/c4aaf3ef-0536-45ab-8194-0210d1f1d0a4)  |

## Source and License

The WDL code for both workflows is also available in the [Peat GitHub repository (external link)](https://github.com/broadinstitute/peat/tree/main/wdl). License is BSD-3-clause.

## Contact
Oliver Ruebenacker <oliverr@broadinstitute.org>

## Workflow change log
| Date | Change | Author|
| --- | --- | --- |
| 2021-05-19  |  Adding costs  |  Oliver Ruebenacker  |
| 2021-05-18  |  Creating this clone  |  Oliver Ruebenacker  |


![Peat logo](https://github.com/broadinstitute/peat/raw/main/logo/peat.png)
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","tidal-waves/Peat-Demo"
6,"broad-firecloud-tcga","TCGA_THYM_hg38_OpenAccess_GDCDR-12-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_THYM_hg38_OpenAccess_GDCDR-12-0_DATA",TRUE,TRUE,"Thymoma","Tumor/Normal","USA","TCGA Thymoma Open-Access Data Workspace

*hg38 TCGA and TARGET workspaces reference files by their GDC UUIDs.  In order to run analyses on the referenced data files, you will need to run workflows that retrieve the files from the GDC and copy them to your workspace bucket.  See   [this forum post](https://gatkforums.broadinstitute.org/firecloud/discussion/10382/populating-hg38-tcga-and-target-workspaces-with-data-files#latest) for instructions on the running of these workflows.*","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients' clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","124","Thymus","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh38/hg38","TCGA_THYM_hg38_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_THYM_hg38_OpenAccess_GDCDR-12-0_DATA"
8,"broad-firecloud-tcga","TCGA_BRCA_OpenAccess_V1-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_BRCA_OpenAccess_V1-0_DATA",TRUE,TRUE,"Breast Invasive Carcinoma","Tumor/Normal","USA","TCGA Breast invasive carcinoma Open-Access Data Workspace","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients’ clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","1098","Breast","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh37/hg19","TCGA_BRCA_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_BRCA_OpenAccess_V1-0_DATA"
9,"help-gatk","Introduction-to-TCGA-Dataset","READER","https://app.terra.bio/#workspaces/help-gatk/Introduction-to-TCGA-Dataset",TRUE,FALSE,NA,NA,NA,"Practice accessing controlled-access TCGA data and run a simple MD5sum workflow on the data. 


## What is TCGA?

> The Cancer Genome Atlas (TCGA), a landmark [cancer genomics](https://www.cancer.gov/about-nci/organization/ccg/cancer-genomics-overview) program, molecularly characterized over 20,000 primary cancer and matched normal samples spanning 33 cancer types. This joint effort between the National Cancer Institute and the National Human Genome Research Institute began in 2006, bringing together researchers from diverse disciplines and multiple institutions.
> 
> Over the next dozen years, TCGA generated over 2.5 petabytes of genomic, epigenomic, transcriptomic, and proteomic data. The data, which has already lead to improvements in our ability to diagnose, treat, and prevent cancer, will remain [publicly available](https://portal.gdc.cancer.gov/) for anyone in the research community to use.
> 
(From TCGA project site on the [National Cancer Institute site](https://www.cancer.gov/about-nci/organization/ccg/research/structural-genomics/tcga))

The NCI's TCGA program involved a collaboration of scientists from 16 nations that has discovered nearly 10 million cancer-related mutations before the sequencing aspect ended after its decade long run (Ledford 2015). The program set out to characterize the molecular changes that occur within cancer cells and types. Additionally, it built a comprehensive model demonstrating a better understanding of  protein pathways as well as their importance in the treatment of cancer. Their findings have improved the consolidation of data that helps diagnose, treat, and prevent cancer. This comprehensive understanding across cancer types has allowed researchers to use mutation, gene expression, and methylation data to create a sub-classification of individual cancers that respond differently to treatments (Hoadley, K. 2018). Though the sequencing aspect of the program has come to a close, researchers continue to reap the rewards of the data generated by the program producing  yearly valuable publications. Terra users interested in working with the dataset can access it through Broads Data Library linked under Terra's [DataSets Library](https://app.terra.bio/#library/datasets) currently tittled as ""The Cancer Genome Atlas Presented by the National Cancer Institute"".


## Data Access

There are two forms of TCGA data: **open-access** are unidentifiable data open to the research community at large, while **controlled-access**  contain information that could potentially re-identify patients. Because of this possibility re-identification, controlled-access data require researchers have an eRA Commons or NIH account with dbGaP authorization for access. 

Users interested in obtaining approval for access-controlled data from they Broad Data Library must 
1. [Have an eRA Commons/NIH account with dbGaP authorization](https://gdc.cancer.gov/access-data/obtaining-access-controlled-data)
2. [Request access to TCGA dataset through dbGaP account](https://dbgap.ncbi.nlm.nih.gov/aa/wga.cgi?page=login)
3. [Establish a link between your Terra and eRA Commons / NIH accounts](https://support.terra.bio/hc/en-us/articles/360038086332)

Please visit the read the [Accessing-TCGA-Controlled-Access-workspaces-in-Terra](https://support.terra.bio/hc/en-us/articles/360037648172) article for further details.

Once you have access, you will be able to browse **controlled-access** TCGA workspace datasets in the [Broad Data Libary](https://portal.firecloud.org/?return=terra&project=TCGA#library). **Open-access** TCGA workspaces do not require you to follow the steps above, they are accessible to all Terra users. 



## Data   
### TCGA Sample DATA
For hands-on experience accessing controlled-access TCGA data follow the steps below.   

1. First make a [clone](https://broadinstitute.zendesk.com/hc/en-us/articles/360026130851-How-to-share-or-clone-a-workspace) of this workspace. Be sure to give your workspace a name that will help you easily identify it and make sure the workspace is under the `TCGA-dbGaP-Authorized` domain name.    
2.  Go to [Broad’s Data Libary](https://portal.firecloud.org/?return=terra#library) and search for ""TCGA"" in the top left corner search box or check the “TCGA” box under the ""Project Name"" filter option.    
3.  You should see a list of TCGA workspace in the browser. For this example we will use the [TCGA_CHOL_ControlledAccess](https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_CHOL_ControlledAccess_V1-0_DATA) data workspace but you should be able to perform these steps with other TCGA workspaces. Open the TCGA_CHOL_ControlledAccess data workspace by clicking on the cohort name within the TCGA library. If you are having trouble viewing or opening this workspace, review the Data Access portion above and confirm you have registered to view controlled access data. 
4. Open the Data tab in the data workspace.
5. On the left hand side select the `pair` table.    
6. Select all of the pairs in the table which will enable you to see 3 dot icon along with the words ""36 rows selected"". Click on this 3 dot icon and select ""Export to Workspace"".      
8. A window will appear asking for the destination workspace. Select your cloned workspace and click ""Copy."" If you do not see your workspace under the choices of the destination workspaces, try confirming that your destination workspace is under the `TCGA-dbGaP-Authorized` domain name.

Congratulations! Now that you have uploaded the TCGA data table to your cloned workspace, you’ve linked the data (in the cloud) to you workspace and you will be able to access the data for analysis. 

Note: You've copied the paired table and any entities it references. Follow steps 5 to 6 for other tables you would like to copy.   

### Workspace Data   
Required and optional references and resources for the methods are included in the Workspace Data table. The reference genome for this workspace is hg19.


## Workflows

In the Workflows tab you will find an example workflow you can use to practice running your copied TCGA sample data table. The workflow has been pre-configured to use the data table copied from the TCGA_CHOL_ControlleadAccess workspace.
To run the workflow simply:
1. Set the workflow to ""Run workflow(s) with inputs defined by data table""
2. Set the root entity to “Sample”
3. Click on “Select Data” and select one of the samples
4. Run the workflow

### Get-MD5-Sum

What does this workflow do?
- Gets the md5 sum of a given file

Root Entity Type
- Sample

Requirements/expectations
- File

Outputs
- Md5 sum value

#### Example Time and Cost to Run Workflow   

| Sample Name | Sample Size | Time | Cost $ |
| ---  | :---: | :---: | :---: |
| CHOL-3X-AAV9-NB/TB | 19.91 GB | 00:16:00 | 0.01 |

Cost and Time will vary, view the [Controlling-Cloud-costs-sample-use-cases](https://support.terra.bio/hc/en-us/articles/360029772212) article for cost details.  

Users can also use [Google's cost calculator](https://cloud.google.com/products/calculator/) for cost estimates.   

---

### Contact information  
For questions about this workspace please visit the Featured Workspaces Topic topic on the [Terra Community Forum](https://broadinstitute.zendesk.com/hc/en-us/community/topics). Use the search box to see if other users have asked the same question previously. If not, post and tag **@ Beri Shifaw** so that we get notified.

This material is provided by the GATK Team. Please post any questions or concerns regarding the GATK tool to the [GATK forum](https://gatk.broadinstitute.org/hc/en-us/community/topics)

### Reference
Ledford, H. (2015). [End of cancer-genome project prompts rethink](https://www.nature.com/news/end-of-cancer-genome-project-prompts-rethink-1.16662). Nature 517, 128–129.

Hoadley, K. (2018).  [Cell-of-Origin Patterns Dominate the Molecular Classification of 10,000 Tumors from 33 Types of Cancer](https://www.ncbi.nlm.nih.gov/pubmed/29625048). Cell 173(2), P291-304.E6. doi: https://doi.org/10.1016/j.cell.2018.03.022

### Workspace Citation 
Details on citing Terra workspaces can be found here: [How to cite Terra](https://support.terra.bio/hc/en-us/articles/360035343652)

Data Sciences Platform, Broad Institute (*Year, Month Day that this workspace was last modified*) help-gatk/Introduction-to-TCGA-Dataset [workspace] Retrieved *Month Day, Year that workspace was retrieved*,  https://app.terra.bio/#workspaces/help-gatk/Introduction-to-TCGA-Dataset

### Workspace Change Log
| Date | Change | Author |
| --- | --- | --- |
|  2020-10-22 | Minor Changes to Dashboard, Updated workflow to point to Dockstore GATK workflows| Beri Shifaw |
|  2021-05-10 | Replaced GATK workflows with MD5Sum workflow| Beri Shifaw |
|  2021-09-15 | Addition of Workspace Citation section | Beri Shifaw |",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/Introduction-to-TCGA-Dataset"
10,"broad-firecloud-tcga","TCGA_LIHC_OpenAccess_V1-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_LIHC_OpenAccess_V1-0_DATA",TRUE,TRUE,"Liver Hepatocellular Carcinoma","Tumor/Normal","USA","TCGA Liver hepatocellular carcinoma Open-Access Data Workspace","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients’ clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","377","Liver","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh37/hg19","TCGA_LIHC_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_LIHC_OpenAccess_V1-0_DATA"
11,"ctat-firecloud","ctat-mutations","READER","https://app.terra.bio/#workspaces/ctat-firecloud/ctat-mutations",TRUE,FALSE,NA,NA,NA,"## CTAT-Mutations

A workflow for detecting variants from RNA sequencing data from short (Illumina) or long (PacBio HiFi ISO-seq) read RNA-seq.


Complete documentation for CTAT-Mutations is available on the [CTAT-Mutations Wiki](https://github.com/NCIP/ctat-mutations/wiki).

### Workflow Description

CTAT-Mutations, a component of the [Trinity Cancer Transcriptome Analysis Toolkit (CTAT)](https://github.com/NCIP/Trinity_CTAT/wiki), integrates GATK Best Practices along with downstream steps to annotate and filter variants, and to additionally prioritize variants that may be relevant to cancer biology such as likely somatic mutations. Our variant annotation includes leveraging the [RADAR](https://www.ncbi.nlm.nih.gov/pubmed/24163250) and [RediPortal](https://www.ncbi.nlm.nih.gov/pubmed/27587585) databases for identifying likely RNA-editing events, [dbSNP](https://www.ncbi.nlm.nih.gov/pubmed/10447503) and [gnomAD](https://gnomad.broadinstitute.org/) for annotating common variants, and [COSMIC](https://www.ncbi.nlm.nih.gov/pubmed/27899578) to highlight known cancer mutations.  Finally, [CRAVAT](https://www.ncbi.nlm.nih.gov/pubmed/29092935) is leveraged to annotate and prioritize variants according to likely biological impact and relevance to cancer.

### Input

- Sequencing data in FASTQ format, or a previously aligned BAM, or a previously aligned BAM and VCF file. 
- A reference genome and corresponding annotations.

>when using with long reads, set 'is_long_reads' to true and provide the fastq file as input to the 'left' fastq reads input parameter (leave 'right' blank).


### Output
The primary output files generated by the pipeline include the following:

-  Initially predicted variants 
- Variants after applying hard cutoffs to remove likely false positives or after applying machine learning algorithms to improve prediction accuracy
- Subset of variants that are considered most relevant to cancer biology.  These are selected based on the variant annotations requiring: gnomad AF < 0.01 and (CHASM or VEST pVal < 0.05, FATHMM in [""CANCER"", ""PATHOGENIC""], or clinvar =~ /pathogenic/i )
- Self-contained web-application for interactively navigating the cancer variants.
- Aligned BAM file



### Example data

The workflow in this workspace is preconfgured with a small set of of reads used for testing purposes . Additionally, the workflow is configured to use the GRCh38 reference, and to filter variants using hard filters instead of boosting due to the small number of reads in the testing data. 


### Time and cost estimates
Below is an example of the time and cost for running the workflow.

| Sample Name        | Cost    | Time |       
| ------------- |:-------------:| ----------:| 
| test      | $0.17 | 3 hours, 17 minutes


Note: Cost and time will vary with the use of preemptible instances.

### Example output
The cancer variants output file which lists all the cancer variants discovered with the testing data:

| CHROM | POS       | REF | ALT | GENE    | DP  | QUAL    | MQ | clinvar_sig                                 |
|-------|-----------|-----|-----|---------|-----|---------|----|---------------------------------------------|
| chr1  | 11130632  | G   | T   | MTOR    | 59  | 745.64  | 60 | NA                                          |
| chr1  | 11130740  | A   | C   | MTOR    | 62  | 202.64  | 60 | NA                                          |
| chr1  | 26774594  | C   | T   | ARID1A  | 23  | 303.64  | 60 | NA                                          |
| chr1  | 114716123 | C   | T   | NRAS    | 259 | 6233.06 | 60 | Pathogenic                                  |
| chr2  | 177234087 | T   | A   | NFE2L2  | 81  | 1812.06 | 60 | NA                                          |
| chr2  | 201286575 | T   | G   | CASP8   | 19  | 360.98  | 60 | NA                                          |
| chr2  | 208248388 | C   | T   | IDH1    | 51  | 357.64  | 60 | Pathogenic                                  |
| chr3  | 49375540  | C   | T   | RHOA    | 646 | 5082.64 | 60 | NA                                          |
| chr3  | 179218303 | G   | A   | PIK3CA  | 19  | 145.64  | 60 | Pathogenic/Likely-pathogenic                |
| chr5  | 68293360  | C   | T   | PIK3R1  | 26  | 216.64  | 60 | Benign/Likely-benign                        |
| chr5  | 112780896 | G   | A   | APC     | 7   | 70.64   | 60 | Uncertain-significance                      |
| chr7  | 55191831  | T   | A   | EGFR    | 190 | 675.64  | 60 | Pathogenic/Likely-pathogenic,-drug-response |
| chr12 | 25245350  | C   | A   | KRAS    | 24  | 261.64  | 60 | Pathogenic                                  |
| chr12 | 112450394 | G   | T   | PTPN11  | 66  | 276.64  | 60 | Pathogenic                                  |
| chr14 | 95103424  | C   | T   | DICER1  | 10  | 252.06  | 60 | Benign/Likely-benign                        |
| chr16 | 3781283   | C   | T   | CREBBP  | 23  | 79.64   | 60 | NA                                          |
| chr16 | 68813455  | C   | A   | CDH1    | 96  | 617.64  | 60 | NA                                          |
| chr17 | 7674221   | G   | A   | TP53    | 26  | 737.06  | 60 | Pathogenic                                  |
| chr17 | 39723967  | T   | C   | ERBB2   | 78  | 1020.64 | 60 | Likely-pathogenic                           |
| chr19 | 12690014  | T   | G   | FBXW9   | 42  | 156.64  | 60 | NA                                          |
| chr19 | 52216574  | T   | C   | PPP2R1A | 3   | 54.84   | 60 | NA                                          |
| chr22 | 41170455  | T   | A   | EP300   | 19  | 134.64  | 60 | NA                                          |

### Contact Information
Questions can be directed to the Trinity CTAT email list: trinity_ctat_users@googlegroups.com

### License
Copyright Broad Institute, 2021 | BSD-3
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at https://raw.githubusercontent.com/NCIP/ctat-mutations/master/LICENSE.txt).
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","ctat-firecloud/ctat-mutations"
13,"broad-firecloud-tcga","TCGA_LAML_OpenAccess_V1-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_LAML_OpenAccess_V1-0_DATA",TRUE,TRUE,"Acute Myeloid Leukemia","Tumor/Normal","USA","TCGA Acute Myeloid Leukemia Open-Access Data Workspace","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients’ clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","200","Bone Marrow","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh37/hg19","TCGA_LAML_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_LAML_OpenAccess_V1-0_DATA"
14,"warp-pipelines","DRAGEN-GATK-Whole-Genome-Germline-Pipeline","READER","https://app.terra.bio/#workspaces/warp-pipelines/DRAGEN-GATK-Whole-Genome-Germline-Pipeline",TRUE,FALSE,NA,NA,NA,"### DRAGEN-GATK  whole genome germline pipeline for variant discovery

This workspace contains a fully reproducible example workflow for whole-genome  germline sequence data pre-processing using the DRAGEN-GATK mode of the Whole Genome Germline Single Sample (WGS) Pipeline. 

This mode makes the pipeline functionally equivalent to DRAGEN’s analysis pipeline (read more in this [DRAGEN-GATK blog](https://gatk.broadinstitute.org/hc/en-us/articles/360039984151)). The workflow follows GATK's Best Practices for [preprocessing](https://gatk.broadinstitute.org/hc/en-us/articles/360035535912) and [Germline Short Variant Discovery](https://gatk.broadinstitute.org/hc/en-us/articles/360035535932).  

![](https://storage.googleapis.com/terra-featured-workspaces/DRAGEN-GATK-Germline-Whole-Genome-Pipeline/image_reszie.png)


Scroll down for an overview of the workflow, instructions, example data, cost estimates, and additional resources. 

*The materials in this workspace were developed by the Data Sciences Platform at the Broad Institute.* 

## DRAGEN-GATK workflow overview and instructions
### View a demo of this workspace
View a demo of this workspace in this [video](https://www.youtube.com/watch?v=bEFiA5n5JMY). 

### Workflow overview

**What does it do?**     
This WDL pipeline implements the DRAGEN-GATK mode for data pre-processing and initial variant calling (VCF generation) according to the GATK Best Practices for germline SNP and Indel discovery in human whole-genome sequencing data. 

The workflow takes as input an array of unmapped BAM files (all belonging to the same sample) to perform preprocessing tasks such as mapping, marking duplicates, and then uses Haplotypecaller in `--dragen-mode` to generate a VCF. 

- For the latest version of the Whole Germline Single Sample workflow, visit the WARP repository [release page](https://github.com/broadinstitute/warp/releases). 

- Read more about the workflow modes, inputs, and tasks in the [WGS Overview](https://broadinstitute.github.io/warp/docs/Pipelines/Whole_Genome_Germline_Single_Sample_Pipeline/README/). 

**What data does it require as input?**    
- Human whole-genome sequencing data in unmapped BAM (uBAM) format
    - If your sequence files are not in unmapped BAM format please review the [Sequence-Format-Conversion](https://app.terra.bio/#workspaces/help-gatk/Sequence-Format-Conversion) workspace for file conversion workflows. 
- One or more read groups, one per uBAM file, all belonging to a single sample (SM)
- Input uBAM files must additionally comply with the following requirements:
- Filenames all have the same suffix (we use "".unmapped.bam"")
- Files must pass validation by ValidateSamFile
- Reads are provided in query-sorted order
- All reads must have an RG tag
- GVCF output names must end in ""rb.g.vcf.gz""
- Reference genome must be Hg38 with ALT contigs


**What does it return as output?**     
The following files are stored in the workspace Google bucket and links to the files are written to the `read_group_set` data table:    
- CRAM file, CRAM index, and CRAM md5 
- [Reblocked GVCF](https://gatk.broadinstitute.org/hc/en-us/articles/4405443600667) and its GVCF index 
- Several summary metrics       


## Running the workflow    

This workspace provides **two example workflow configurations** to test the WGS pipeline in DRAGEN mode:
1. `WGS_Functional_Equivalence` mode: produces outputs functionally equivalent to the DRAGEN hardware.
2. `WGS_Maximum_Quality` mode: produces outputs with maximum quality.

Learn more about each workflow configuration and parameters in the Dashboard section **“Configuring DRAGEN-GATK modes?”**.

**Example data**

Both workflow configurations are pre-configured to use the `NA12878` sample listed in the `read_group_set` data table on the workspace Data page. This example is a single sample with 24 readgroups. The unmapped BAM for each `NA12878` readgroup is listed in the `read_group` data table. 

**Steps**

The workflows are configured to call sample inputs from the data table. To run:

1. Select the desired Whole Genome Germline Single Sample workflow configuration from the Workflows page. 
2. On the workflow setup page, select the `read_group_set` as the root entity in Step 1.
3.  Select the `NA12878` dataset in Step 2. 
4.  Select `Run Analysis`.
5.  Select `Launch`.

**Customizing the workflow for your data**

If you want to use this workflow on your own samples, you can download the Input JSON file from the workflow configuration page and use it as a template for setting up your own data.

- Optional inputs, like the `fingerprint_genotypes_file`, will need to match your samples. This workspace is set up to optionally check fingerprints for the `NA12878` sample.
- For the CheckFingerprint task, the sample name specified in the `sample_and_unmapped_bams` variable must match the sample name in the `fingerprint_genoptyes_file`(VCF format).
   
**Important configuration notes** 

* The workflow is written in WDL1.0 and imports structs to organize and use inputs. 
* If you run the workflow on a sample that only has one uBAM (i.e. one read group), you need to update the config attributes for  `sample_and_unmapped_bams` to include `[]` around the `flowcell_unmapped_bams` as shown below:

`{ ""sample_name"": this.read_group_set_id, ""base_file_name"": this.read_group_set_id, ""flowcell_unmapped_bams"": [this.read_groups.flowcell_unmapped_bams], ""final_gvcf_base_name"": this.read_group_set_id, ""unmapped_bam_suffix"": "".bam"" }`


**Reference data description and location**     

The required and optional references and resources for the workflows are set in the workflow configurations. The reference genome is hg38 (aka GRCh38).

**Time and cost estimates**         
Below is an example of the time and cost for running the workflow.

| Workflow Configuration | Sample Name | Number of Entities | Sample Size | Time | Cost $ |
| ---  | --- | --- | --- | --- | --- |
| Functional Equivalence | NA12878 | 24 | ~3.00 GB | 4 h 57 min | 1.03 |
| Maximum Quality | NA12878 | 24 | ~3.00 GB | 7 h 20 min | 1.17 |

**Note:** Cost and time will vary with the use of [Preemptibles](https://cloud.google.com/compute/docs/instances/preemptible).  

For more information about controlling Cloud costs, see [Overview: Controlling Cloud costs on Terra](https://support.terra.bio/hc/en-us/articles/360029748111).


## Configuring the DRAGEN-GATK modes

Currently, the default WGS pipeline must be configured to use the DRAGEN software. It has two input parameters (booleans) that allow you to run in the two different DRAGEN modes:
1. `dragen_functional_equivalence_mode`: when true, runs the pipeline in functional equivalence mode.
2.  `dragen_maximum_quality_mode`: when true, runs the pipeline in maximum quality mode.

* These two inputs are mutually exclusive; *if both are true, the pipeline will provide an error.* 
* When the desired mode is set to true, the pipeline will automatically select additional DRAGEN-related WGS parameters as listed in the table below. 
* These individual DRAGEN-related parameters can also be mixed and matched to meet different analysis requirements.

| Parameter | Description | Setting for functional equivalence mode | Setting for maximum quality mode |
| --- | --- | --- | --- | 
| `use_bwa_mem` | When true, uses BWA mem as the aligner; when false, uses the DRAGMAP aligner.  |  false | false |
| `run_dragen_mode_variant_calling` | When true, runs GATK HaplotypCaller in --dragen mode. | true | true |
| `perform_bqsr` | When true, performs BQSR; not necessary for DRAGEN modes because base errors are corrected during variant calling | false | false |
|  `use_dragen_hard_filtering` | When true, turns on hard filtering. | true | true |
| `use_spanning_event_genotyping` | When true, evaluates spanning deletions. | false | true |
| `unmap_contaminant_reads` | When true, identifies extremely short alignments (with clipping on both sides) as cross-species contamination and unmaps the reads. | false | true |

### Reference data for DRAGEN mode
References: the workflow uses DRAGEN-specific genomic references required for the DRAGEN aligner which are listed in the workspace table. These references are hosted in two public Google buckets: one for [hg38](https://console.cloud.google.com/storage/browser/gcp-public-data--broad-references/hg38/v0) and one for the [DRAGEN references](https://console.cloud.google.com/storage/browser/gcp-public-data--broad-references/hg38/v0/dragen_reference;tab=objects?prefix=&forceOnObjectsSortingFiltering=false&pageState=%22StorageObjectListTable%22:%22f%22:%22%255B%255D%22).

## Additional resources

- For the latest updates on the DRAGEN-GATK pipeline, see [DRAGEN-GATK landing page](https://gatk.broadinstitute.org/hc/en-us/articles/360045944831) on the [GATK Support](https://gatk.broadinstitute.org/hc/en-us). 

- Learn more about the WGS pipeline in the [WGS Overview](https://broadinstitute.github.io/warp/docs/Pipelines/Whole_Genome_Germline_Single_Sample_Pipeline/README).

- Learn more about functional equivalence and try the function equivalence pipeline in the [Functional Equivalence workspace](https://app.terra.bio/#workspaces/broad-firecloud-dsde-methods/FunctionalEquivalence).

- To try the WGS pipeline with joint calling, see the [Whole-Genome-Analysis-Pipeline workspace](https://app.terra.bio/#workspaces/warp-pipelines/Whole-Genome-Analysis-Pipeline).

  
## Questions and contact information  
* For workspace questions and feedback, email the Broad pipelines team at warp-pipelines-help@broadinstitute.org or create a post on the [Featured Workspaces community forum](https://support.terra.bio/hc/en-us/community/topics/360001603491) (login required). Tag @Alexander Baumann in the **Details** section of your post.

* You can also Contact the Terra team for questions from the Terra main menu. When submitting a request, it can be helpful to include:
    * Your Project ID
    * Your workspace name
    * Your Bucket ID, Submission ID, and Workflow ID
    * Any relevant log information
 
Please post any questions or concerns regarding the GATK tool to the [GATK forum](https://gatk.broadinstitute.org/hc/en-us/community/topics).


## License  
**Copyright Broad Institute, 2023 | BSD-3**  
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at https://github.com/broadinstitute/warp/blob/develop/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.

## Workspace changelog
| Date | Change | Author |
| --- | --- | --- |
| 08/25/2023 | Updated workflows to v3.1.11. | Kaylee Mathews |
| 09/12/2022 | Updated workflows to v3.1.6. | Liz Kiernan |
| 08/29/2022 | Updated contact information. | Kaylee Mathews |
| 04/22/2022 | Updated to Picard version 2.26.10 and GATK version 4.2.6.1 to address log4j vulnerabilities.  | Nikelle Petrillo |
| 10/19/2021 | First release of the workspace. | Liz Kiernan |

---",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","warp-pipelines/DRAGEN-GATK-Whole-Genome-Germline-Pipeline"
15,"broad-firecloud-tcga","TCGA_UCEC_OpenAccess_V1-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_UCEC_OpenAccess_V1-0_DATA",TRUE,TRUE,"Uterine Corpus Endometrial Carcinoma","Tumor/Normal","USA","TCGA Uterine Corpus Endometrial Carcinoma Open-Access Data Workspace","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients’ clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","560","Uterus","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh37/hg19","TCGA_UCEC_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_UCEC_OpenAccess_V1-0_DATA"
16,"pathogen-genomic-surveillance","COVID-19_Broad_Viral_NGS","READER","https://app.terra.bio/#workspaces/pathogen-genomic-surveillance/COVID-19_Broad_Viral_NGS",TRUE,FALSE,NA,NA,NA,"Massachusetts has been severely impacted by the COVID-19 pandemic, with over 475,000 cases and 13,844 deaths as of January 25, 2021 . Seventy percent of the state’s 6.9 M population lives in the city of Boston and its surrounding communities. To understand the introduction and spread of SARS-CoV-2 in this region, the Broad Institute is sequencing viral genomes from COVID-19 cases from the Boston area for genomic epidemiological analyses.

This dataset provides the first high resolution view of the introductions and spread of SARS-CoV-2 in the greater Boston area based on viral genomic data. All genomes were obtained from nasopharyngeal swabs from individuals with confirmed SARS-CoV-2 infection from March 3rd and May 9th, 2020. These cases represent a non-random sample from a single tertiary care center whose clinical catchment area primarily involves eastern Massachusetts. 


To view the related blog post on COVID-19 efforts by the Viral Genomics group at Broad, please see [here](https://www.broadinstitute.org/node/630361)

To view the Terra blog post related to this COVID-19 viral NGS workspace, please see[ here](https://support.terra.bio/hc/en-us/articles/360044440971)

Epidemiological analysis results are available for review as a pre-print on Virological and can be found [here](https://virological.org/t/introduction-and-spread-of-sars-cov-2-in-the-greater-boston-area/503)

Laboratory protocols used by the Broad Viral Genomics group can be found [here](https://tinyurl.com/y8qxb9mk)

## **The Data**

In this workspace we've provided tools and data, so that labs can go from raw reads (uBAM), through to producing a phylogenetic tree with their private and publicly available data.

The data in this workspace includes:

* High-quality viral genomes sequenced from nasopharyngeal swabs from individuals with confirmed SARS-CoV-2 infection from MGH and the MA DPH
* Over 5,000 viral genomes from [Genbank](https://www.ncbi.nlm.nih.gov/labs/virus/vssi/#/virus?SeqType_s=Nucleotide&VirusLineage_ss=SARS-CoV-2,%20taxid:2697049) that can be used to build NextStrain phylogenetic trees with your data



## **From .BAM to NextStrain Tree and GenBank Data Submission**


![Overview](https://drive.google.com/uc?id=1gqyH1CGbAw_FVpGNs0oH5N2XQlKzmEMN)





## **Working with SARS-CoV-2 Sequences**

In recent months, the scientific and public health community tackling SARS-CoV-2 genomics has increasingly been favoring simplified approaches for both data generation and data analysis, many of which are documented by the CDC. Simple align-to-reference based approaches for consensus calling (similar to those used in the study of non-diverse genomes, such as humans), provide more efficient analysis processes and ease of interpretation. Additionally, the popularity of PCR tiled amplicon-based data generation approaches (such as ARTIC) frequently necessitates specialized filtration steps to remove the primer artifacts during analysis (the iVar trimming tool from Scripps being one of the more popular tools for ARTIC+Illumina data).

We have provided our reference based viral assembly tool (assemble_refbased.wdl) that has been updated to reflect these best practices and is appropriate for use on any Illumina data generated from SARS-CoV-2. In particular, there is an optional input parameter, a BED file, to describe any PCR amplicon primers used in the process of data generation (this can be omitted if no such primers were used). The original de novo assembly workflow is still provided in this workspace—although it is not necessary for SARS-CoV-2, it is applicable to a much broader range of viruses than the reference based workflow.

We also provide in this workspace an “assisted de novo” viral assembly pipeline that has been refined over the past decade of use and validated on metagenomic Illumina data from diverse viral taxa including Lassa, Ebola, Zika, Mumps, Influenza A, HIV, Rabies, Hepatitis A, and several herpes viruses (HHV 1, 2, 3, and 5). It is designed to assemble contigs, scaffold, and polish assemblies for viruses that may exhibit up to 30% nucleotide divergence from available reference databases. This approach, while robust to a wide range of viral taxa, may be more computationally intensive and complex than necessary for viruses that exhibit very limited diversity—such as those involved in single-origin disease outbreaks.


## **Running Assembly Workflows**

Provided in this workspace is a reference based assembly workflow. The details of the workflow are outlined below:

#### assemble_refbased

*What does it do?*

This takes an unaligned read file (uBAM) and assembles against a viral genome reference.

*What does it require for input?*

|Input | Example |
|---|---|
| reads_unmapped_bams | sample1.bam |
| reference_fasta | reference_genome.fasta |


*What does it return as output?*

The full list of outputs can be seen in the “Outputs” tab of the Workflow Method Configuration. Below are a few important outputs:

|Output |
|---|
| assembly_fasta |
| assembly_length_unambiguous |
| assembly_mean_coverage |

## **GenBank Data Submission**

Labs can submit their batch sequencing data to GenBank by following the steps below to run a convenient workflow that will package all required files into a single zip bundle that can be emailed directly to NCBI.


![](https://drive.google.com/uc?id=1blG9VWM2WZHmRHMFAJDAtInX7UdrZhXE)


### **Submitting Viral Sequences to NCBI**

### **Register your BioProject**

*If you want to add samples to an existing BioProject, skip to Step 2.*

1. Go to: https://submit.ncbi.nlm.nih.gov and login (new users - create new login).
2. Go to the Submissions tab and select BioProject - click on New Submission.
3. Follow the onscreen instructions and then click submit - you will receive a BioProject ID (PRJNA###) via email almost immediately.


### **Register your BioSamples**

1. Go to: https://submit.ncbi.nlm.nih.gov and login.
2. Go to the Submissions tab and select BioSample - click on New Submission.
3. Follow instructions, selecting ""batch submission type"" where applicable.
4. The metadata template to use is likely: ""Pathogen affecting public health"" (Pathogen.cl.1.0.xlsx).
5. Follow template instructions to fill in the sheet. Pay particular attention to the Excel comments that are attached to each column header: they describe the intended content for these columns, the valid formatting, and controlled vocabulary.
* For example, ""organism"" should always match the long name that is given by the NCBI Taxonomy database for that species.
* Date fields seem to have multiple acceptable formats, but we prefer ISO8601 (YYYY-MM-DD) just to reduce ambiguity. Note that this format will trigger a warning when uploading, if you don't have HH:MM time values as well (it will suggest an edit for you).
* You will likely need to duplicate your sample_name to the host_subject_id column (or something like it)--if you do not, then any samples that happen to have the same attribute values will trigger an error when trying to register new BioSamples because they look like duplicates. Assuming that your sample_names are one-to-one corresponding to a human patient, host_subject_id is probably the most appropriate place to duplicate the value in order to make all entries unique.
* Populate the isolate column using the naming convention you want to apply to this organism (most viral species have a specific, structured naming convention you should follow). Our workflow will re-use this value for the GenBank record name.
6. Export to text and submit as .txt file. You will receive BioSamples IDs (SAMN####) via email.
7. After NCBI accepts your submission and registers your samples, retrieve the text-formatted ""attributes table"" associated with this submission from the portal at https://submit.ncbi.nlm.nih.gov/subs/ and clicking on ""Download attributes file with BioSample accessions"". You will need this file later. Do not use the file that was attached to the NCBI response email--it does not contain the full record and is formatted differently.
8. If you wish to amend/correct any metadata in your submissions, you can always do so at a future time -- however, you will need BioSample IDs before any of the following steps, so it's best to register as soon as you have collection_date and sample_name for everything. This can be a super-set of anything you submit to NCBI in the future (GenBank or SRA), so we typically register BioSamples for every viral sample we attempt to sequence, regardless of whether we successfully sequenced it or not.

### **Set up an NCBI author template**

1. If different author lists are used for different sets of samples, create a new .sbt file for each list
2. Go to: https://submit.ncbi.nlm.nih.gov/genbank/template/submission/
3. Fill out the form including all authors and submitter information (if unpublished, the reference title can be just a general description of the project).
4. At the end of the form, include the BioProject number from Step 1 but NOT the BioSample number'
5. Click ""create template"" which will download an .sbt file to your computer
6. Save file as ""authors.sbt"" or similar. If you have multiple author files, give each file a different name and prep your submissions as separate batches, one for each authors.sbt file.

### **Prepare requisite input files for your submission batches**

1. Stage the above files you've prepared and other requisite inputs into the environment you plan to execute the Genbank workflow. If that is Terra, push these files into the appropriate GCS bucket, if DNAnexus, drop your files there. If you plan to execute locally (e.g. with miniwdl run), move the files to an appropriate directory on your machine. The files you will need are the following:

* The files you prepared above: the submission template (authors.sbt) and the biosample attributes table (attributes.tsv).
* All of the assemblies you want to submit. These should be in fasta files, one per genome. Multi-segment/multi-chromosome genomes (such as Lassa virus, Influenza A, etc) should contain all segments within one fasta file.
* Your reference genome, as a set of fasta files, one per segment/chromosome. The fasta sequence headers should be GenBank accession numbers. This can come directly from GenBank.
* Your reference gene annotations, as a set of TBL files, one per segment/chromosome. These must correspond to the accessions in your reference genome. These must be presented in the same order as the reference genome fasta files, which must also be in the same order as all the sequences in all of your assembled fasta files.
* A genome coverage table as a two-column tabular text file (optional, but helpful).
* The organism name (which should match what NCBI taxonomy calls the species you are submitting for). This is a string input to the workflow, not a file.
* The sequencing technology used. This is a string input, not a file.
* The NCBI Taxonomy taxid. This is a numeric input, not a file.

2. The reference genome you provide should be annotated in the way you want your genomes annotated on NCBI. 
3. Note that you will have to run the pipeline separately for each virus you are submitting AND separately for each author list.

### **Running the genbank submission WDL**

1. Run the genbank.wdl. Most of the metadata files described above (BioSample map, source modifier table, genome coverage table) are allowed to be a super-set of the samples you are submitting--the extra metadata will be ignored by the workflow. The samples that are included in this batch are the ones you provide to the assemblies_fasta input field. Any missing samples in the metadata inputs should not cause failures, but will produce less descriptive submission files. Viral genomes should set molType to cRNA.
2. The GenBank workflow performs the following steps: it aligns your assemblies against a GenBank reference sequence, transfers gene annotation from that GenBank reference into your assemblies' coordinate spaces, and then takes your genomes, the transferred annotations, and all of the sample metadata prepared above, and produces a zipped bundle that you send to NCBI. There are two zip bundles: sequins_only.zip is the file to email to NCBI. all_files.zip contains a full set of files for your inspection prior to submission.
3. In the all_files.zip output, for each sample, you will see a .sqn, .gbf, .val, and .tbl file. You should also see an error summary.val file that you can use to check for annotation errors (or you can check the .val file for each sample individually). Ideally, your samples should be error-free before you submit them to NCBI unless you're confident enough in the genomic evidence for unusual coding effects and frameshifts. For an explanation of the cryptic error messages, see: https://www.ncbi.nlm.nih.gov/genbank/genome_validation/.
4. We currently use a bioconda wrapper of NCBI's tbl2asn tool called tbl2asn-forever. This works around some deficiencies in NCBI's tool but has the side effect of setting the submission date to Jan 1, 2019 for all submission, regardless of today's date. Unfortunately, until NCBI releases a fixed tool, you will need to search-replace the date in the SQN files in a text editor prior to submission.
5. Check your .gbf files for a preview of what your GenBank entries will look like. Once you are happy with your files email the sequins_only.zip file to gb-sub@ncbi.nlm.nih.gov.
6. NCBI is processing SARS-CoV-2 data submissions faster than it typically takes, given the circumstances. It often takes 2-8 weeks otherwise to receive a response and accession numbers for your samples. Do follow up if you haven’t heard anything for a few weeks!




## **NextStrain Tree Building Tools**

NextStrain is a collection of open source tools that help scientists, epidemiologists and public health officials in their understanding of pathogen spread and evolution, especially in outbreak scenarios. One of these tools is [Augur](https://github.com/nextstrain/augur), which was developed in order to track pathogen evolution from sequencing data. With this tool, scientists can build trees to analyze the phylogeny and evolutionary relationships of Sars-CoV-2, including the initial emergence and sustained transmission.


### **Running the augur_from_assemblies Workflow**

The nextstrain_augur_viral-pipelines workflow by default takes as input the collection of assembled .FASTA files generated as output from the assemble_refbased workflow. The reference .FASTA and genbank (.gb) files are Sars-CoV-2 specific references. The default auspice_config.json file represents the metadata we have modeled and was curated to include the metadata from over 5,000 sequences available from GenBank.

*What does it require for input?*

This workflow requires a set of assembled .FASTA files, a metadata file, reference files specific to the virus and a configuration file to determine the tree visuals. The metadata file requires specific formatting that must be strictly followed for the tool to successfully complete. In addition to the provided example that can be utilized as a template, full details of the format can be found [here](https://github.com/nextstrain/augur/blob/master/docs/faq/metadata.md). 

|Input | Example |
|---|---|
| assembly_fastas | samples.fasta |
| genbank_gb| genbank.db |
| metadata | metadata.tsv |
| ref_fasta | reference_genome.fasta |
| virus | Sars-CoV-2 |
| auspice_config | auspice_config.json |

The augur workflow also has numerous optional attributes for filtering your data and inference.


### **Using augur_from_assemblies.wdl to re-create the tree in this workspace**

The MGH and DPH samples used to create our Nextstrain tree correspond to samples that had >98% assembly_length_unambiguous.

If you would like to re-create the tree that we have provided in this workspace, here is a step-by-step guide:

1. Click on the **augur_from_assemblies** workflow, inputs should be populated as outlined below. 


|Input | File |
|---|---|
| assembly_fastas | ""gs://fc-061a81bb-6bbb-4906-8f07-fdb50b1f13b5/nextstrain/MGH_DPH_98percent_772samples.fasta"", ""gs://fc-061a81bb-6bbb-4906-8f07-fdb50b1f13b5/nextstrain/sc2-genbank-2020.06.17-highq-5923.fasta"" |
| genbank_gb| ""gs://fc-061a81bb-6bbb-4906-8f07-fdb50b1f13b5/nextstrain/reference_nextstrain_modified_MN908947-3_2020-05-06.gb"" |
| metadata | ""gs://fc-061a81bb-6bbb-4906-8f07-fdb50b1f13b5/nextstrain/MGH_DPH_Genbank_metadata.tsv"" |
| ref_fasta | ""gs://fc-061a81bb-6bbb-4906-8f07-fdb50b1f13b5/nextstrain/updatedNC_045512_ref_sars-cov-2.fasta"" |
| virus | ""Sars-CoV-2"" |
| auspice_config | ""gs://fc-061a81bb-6bbb-4906-8f07-fdb50b1f13b5/nextstrain/auspice_config_Genbank.json"" |
| ancestral_traits_to_infer | ""country"", ""geoloc_cat""  |
| colors_tsv | ""gs://fc-061a81bb-6bbb-4906-8f07-fdb50b1f13b5/nextstrain/newcolors_geoloc_2020-08-23.tsv"" |
| lat_longs_tsv | ""gs://fc-061a81bb-6bbb-4906-8f07-fdb50b1f13b5/nextstrain/lat-longs_2020-06-02.tsv"" |
| clades | ""gs://fc-061a81bb-6bbb-4906-8f07-fdb50b1f13b5/nextstrain/clades.tsv"" |
| ancestral_traits.sampling_bias_correction | 2.5 |
| ancestral_tree.infer_ambiguous | fasle |
| ancestral_tree.inference | ""joint"" |
| ancestral_tree.keep_ambiguous | false |
| ancestral_tree.keep_overhangs | false |
| augur_mafft_align.fill_gaps | true |
| augur_mafft_align.remove_reference | false |
| draft_augur_tree.exclude_sites | ""gs://fc-061a81bb-6bbb-4906-8f07-fdb50b1f13b5/nextstrain/nextstrain_masking.bed"" |
| draft_augur_tree.method | ""iqtree"" |
| draft_augur_tree.substitution_model | ""GTR"" |
| filter_subsample_sequences.include_where | ""host=human"" |
| filter_subsample_sequences.non_nucleotide | true |
| refine_augur_tree.clock_filter_iqd | 4 |
| refine_augur_tree.clock_rate | 0.0008 |
| refine_augur_tree.clock_std_dev | 0.0004 |
| refine_augur_tree.coalescent | ""skyline"" |
| refine_augur_tree.date_inference | ""marginal"" |
| refine_augur_tree.divergence_units | ""mutations"" |
| refine_augur_tree.keep_polytomies | false |
| refine_augur_tree.keep_root | false |
| refine_augur_tree.root | ""NC_045512"" |




All augur-related files can be found in this workspace bucket (gs://fc-061a81bb-6bbb-4906-8f07-fdb50b1f13b5/nextstrain)

3. Once you have entered the attributes (note: you can enter whatever values you wish for run-time memory and preemptibles), run the analysis. We have noted that the average run time for ~3,000 samples is anywhere from 4-5 hours. 

4. The output of the workflow will produce a .JSON file, that can be dropped into  http://auspice-us.herokuapp.com/ to visualize. This .JSON file(gs://fc-061a81bb-6bbb-4906-8f07-fdb50b1f13b5/nextstrain/sars-cov-2_MGH_DPH_772_Genbank.json) is an example and corresponds to the data described above.

The augur tree that is generated will look like this:

![The augur tree that is generated will look like this](http://drive.google.com/uc?export=view&id=1og69xV_0MuFHOUaUV1C-XdTwyx-mWVJG)



### **Using augur_from_assemblies.wdl to create a tree with your data and GenBank data**

If you would like to add your own data to ours and the data we've provided here from GenBank, you will need to prepare the following files:

1. A concatenated fasta that contains your fasta files, along with ours and GenBank's 
2. An updated metadata.tsv file. We suggest taking the one provided in this workspace and modifying it to include metadata for your samples 

If you wish to customize the visualization or configuration to your liking, you will need to update the following files as well:
* the augur_config file
* the colors.tsv file
* the lat_longs.tsv file

* Note: This document outlines the requirements for the metadata.tsv file: https://github.com/nextstrain/augur/blob/master/docs/faq/metadata.md#parsing-from-the-header. The metadata.tsv file has to be curated manually with any metadata that you want to use for your tree. The strain column HAS to match with the fasta headers or else it wont work.

* Note: There are several ways that you can filter and configure how you view your data. For example, the following parameters are available for filtering data (please see https://nextstrain-augur.readthedocs.io/en/stable/usage/cli/cli.html for details on each parameter)


3. Once the workflow is complete, download the auspice_input_json file to your local machine
4. Simply drop your .json file that was outputted into http://auspice-us.herokuapp.com/

* This tutorial has detailed instructions on how to set up your computer with Docker and Nextstrain (https://nextstrain.org/docs/getting-started/quickstart)
* If you would like to use GitHub to visualize, this tutorial explains the steps: https://nextstrain.org/docs/contributing/community-builds



Overview of the steps involved in the WDL:

![Overview of NextStrain tree building workflow](https://drive.google.com/uc?id=1_jWTaLb1NrFoQnimlaGuG8_pPXmTtEHZ)




",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","pathogen-genomic-surveillance/COVID-19_Broad_Viral_NGS"
17,"broad-firecloud-testing","deTiN_release_data","READER","https://app.terra.bio/#workspaces/broad-firecloud-testing/deTiN_release_data",TRUE,FALSE,"Cell line","Tumor/Normal",NA,"This workspace includes a titration of tumor-normal mixes of cell lines used as reference data to evaluate tumor-in-normal heterogeneity in cancer biopsies. 

This dataset was applied to evaluate deTiN [https://github.com/broadinstitute/deTiN] and experimentally confirm the results of deTiN in silico simulations. DeTiN estimates tumor in normal (TiN) based on tumor and matched normal sequencing data. The estimate is based on both candidate SSNVs and aSCNAs. DeTiN then applies the joint TiN estimate to reclassify SSNVs and InDels as somatic or germline. Install and run time on standard exome data is about 5 mins.

To validate the efficacy of the deTiN method on sequencing data for estimating TiN contamination and improving SSNV/InDel detection we artificially generated tumor-normal mixes. 15 tumor and normal mixtures were created using control (EBV-transformed lymphoblastoid, CRL-2362D) and matched tumor (breast invasive ductal carcinoma, CRL-2321D) cell lines,.  These samples were mixed in equal proportions to generate a .5 TiN pool with total mass of 500ng. We then mixed pure tumor and pure normal with this pool to generate the other mixtures with TiN ranging from 0.007 to 0.9. Samples were volume checked using nanodrop to ensure we achieved the desired mixtures.

We then performed library preparation. Briefly, dsDNA was quantified by Picogreen fluorescence assay using provided DNA standards, 100ng of DNA were fragmented to obtain 150bp pieces by sonication using a Covaris E210 instrument. Solid phase reversible immobilization purification and library construc- tion were performed using AMPure XP Beads, KAPA Library Preparation and KAPA Library Amplification Kits. Library preparation was performed in 96-well plates on an Agilent Bravo Liquid Handler.

Finally we performed hybrid selection, capture and sequencing. DNA was processed through two hybridization events using the Illumina Content Exome Rapid Capture Kit. Samples were normalized to 2ng/uL and pooled. Quantitative PCR (qPCR) was then performed on the pool in order to normalize it to 2nM, before using 0.1N NaOH to denature. Samples were sequenced on Illumina HiSeq2500 machines in Rapid Run mode using 76 base-pair, paired-end reads. The bam files generated by these experiments are publicly available in this workspace. 

","Gad Getz","Amaro Taylor-Weiner","Coriel cell lines CRL-2321D / CRL-2362D mixed to simulate Tumor in Normal. The set includes a titration of tumor-normal mixes of cell lines used as reference data to evaluate tumor-in-normal heterogeneity in cancer biopsies.   This dataset was applied to evaluate deTiN [https://github.com/broadinstitute/deTiN] and experimentally confirm the results of deTiN in silico simulations. DeTiN estimates tumor in normal (TiN) based on tumor and matched normal sequencing data. The estimate is based on both candidate SSNVs and aSCNAs. DeTiN then applies the joint TiN estimate to reclassify SSNVs and InDels as somatic or germline. The dataset includes 15 tumor and normal mixtures using control (EBV-transformed lymphoblastoid, CRL-2362D) and matched tumor (breast invasive ductal carcinoma, CRL-2321D) cell lines, with TiN ranging from 0.007 to 0.9. These were sequenced using exome capture to 210-fold coverage.","1",NA,"deTiN",NA,"General Research Use","GRCh37/hg19","deTiN cell line validation experiment","Whole Exome","broad-firecloud-testing/deTiN_release_data"
18,"biodata-catalyst","BioData Catalyst GWAS 1000 Genomes Tutorial","READER","https://app.terra.bio/#workspaces/biodata-catalyst/BioData%20Catalyst%20GWAS%201000%20Genomes%20Tutorial",TRUE,FALSE,NA,NA,NA,"# GWAS Tutorial in NHLBI's BioData Catalyst

This tutorial workspace offers example tools for conducting mixed-models GWAS from start to finish using the [NHLBI BioData Catalyst](https://biodatacatalyst.nhlbi.nih.gov/) ecosystem. We've created a set of documents [to get you started in the BioData Catalyst system](https://bdcatalyst.gitbook.io/biodata-catalyst-documentation/analyze-data/terra). If you're ready to conduct an analysis, proceed with this dashboard: 

## Data Model
This template was set up to work with the NHLBI BioData Catalyst Gen3 data model. In this dashboard, you'll learn how to import open access data from the Gen3 platform into this Terra template and conduct an association test. If you have never used the Gen3 data model before, we suggest you start with the tutorial [Getting Started with Gen3 Data in Terra](https://terra.biodatacatalyst.nhlbi.nih.gov/#workspaces/fc-product-demo/BioDataCatalyst-Gen3-data-on-Terra-Tutorial).

For this tutorial, we are using synthetic phenotypic data coupled with downsampled 1000 Genomes data that has been ingested into NHLBI BioData Catalyst Powered by Gen3. This data model is likely new to most users and may take some time to become accustomed to. First, the data model is based on a graph structure that is more complex than a single columns x rows data table that you may be familiar with. Second, genomic data is accessible through DRS URLs that point to Google Cloud Buckets that hold the data. 

After this tutorial, you can employ the BioData Catalyst ecosystem for more analyses. Currently, BioData Catalyst's Gen3 hosts the [TOPMed](https://www.nhlbi.nih.gov/science/trans-omics-precision-medicine-topmed-program) program, which is controlled access, in addition to the public data used in this tutorial. To apply for access to TOPMed, submit an application through [dbGAP](https://www.nhlbiwgs.org/topmed-data-access-scientific-community). If you already have access to a TOPMed project and have been onboarded to the BioData Catalyst platform, you should be able to access your data through BioData Catalyst Powered By Gen3 and use your data with another GWAS resource we have created that you can access [here](https://app.terra.bio/#workspaces/biodata-catalyst/BioData%20Catalyst%20GWAS%20blood%20pressure%20trait). 

## About the data
To demonstrate an analysis that could be run on typical whole genome sequence data, this workspace provides mock phenotype data generated from publicly available 1000 Genomes phase 3 genotypes. Phenotypes have been simulated based on individual genotypes and known associated loci for multiple complex traits. The GCTA software was used with lists of causal variants and an estimate of narrow sense heritability for each phenotype.

Traits and sources for causal variants  
a. BMI: Giant-UKBB meta-analysis  
b. Fasting glucose: MAGIC  
c. Fasting insulin: MAGIC  
d. Waist-to-hip ratio: GIANT-UKBB meta-analysis  
e. Height: GIANT-UKBB meta-analysis  
f. HDL: MVP  
g. LDL: MVP  
h. Total cholesterol: MVP  
i. Triglycerides: MVP  

**Generating the synthetic data**
The scripts used to create the phenotype data, as well as intermediate data files and a readme file, are in a public Google bucket (gs://terra-featured-workspaces/GWAS/data_processing/). To browse the Google bucket, click [here](https://console.cloud.google.com/storage/browser/terra-featured-workspaces/GWAS/data_processing).

## Tutorial outline
***Part 1: Navigate the BioData Catalyst environment***
Learn how to search and export data from Gen3 and worfklows from Dockstore into a Terra workspace. 

***Part 2: Reformat Gen3 phenotypic data for use in downstream analysis***
Review the data you imported in Terra and use the interactive Jupyter notebook **1-Prepare-Gen3-data-for-exploration** to consolidate several clinical data tables into a single data table that can be used in the next notebook. This notebook calls functions in the companion notebook **terra_data_table_util**. 

***Part 3: Data exploration and preparation***
The **2-GWAS-preliminary-analysis** notebook will lead you through a series of steps to explore the phenotypic data. The final notebook **3-GWAS-genomic-data-preparation** focuses on genotypic data and finalizes the preparation of files for import into association workflows. This third notebook is the most time and resource consuming analysis in the GWAS.

***Part 4: Perform mixed-model association tests using workflows***
Next, perform mixed models genetic association tests (run as a series of batch workflows using GCP Compute engine). The workflows are also publicly available in [Dockstore](https://dockstore.org/) in this [collection](https://dockstore.org/organizations/bdcatalyst/collections/GWAS).       

# Part 1: Navigate the NHLBI BioData Catalyst ecosystem

## 1a. Link your Terra account to Gen3 using external services
Before you're able to access genomic data from Gen3 in the Terra data table, you need to link your Terra account to external services. If you have not already done so, link your profile [by following these instructions](https://support.terra.bio/hc/en-us/articles/360038086332).

## 1b. Create an Authorization Domain to protect your controlled-access data
If you bring controlled access data into Terra, it should be registered under an Authorization Domain that limits its access to only researchers with the appropriate approvals. Learn how to set up an Authorization Domain [here](https://support.terra.bio/hc/en-us/articles/360039415171).

## 1c. Export the training dataset from Gen3 to Terra
1. Start by learning about Gen3's graph-structured data model for NHLBI's BioData Catalyst using this [orientation document](https://support.terra.bio/hc/en-us/articles/360038087312).
2. Once you better understand the graph, log into [Gen3](https://gen3.biodatacatalyst.nhlbi.nih.gov/) through the NIH portal using your eRA Commons username and password. 
3. Navigate to the [Gen3 Explorer](https://gen3.biodatacatalyst.nhlbi.nih.gov/explorer) view to see what datasets you currently have and do not have access to. On the left hand side, you can use the faceted search tool to narrow your results to specific projects. 
4. First, under ""Files"" and ""Access"", select ""Data with Access"" to filter through projects that you currently have access to.
5. Next, under ""Filters"", select the ""Subject"" tab. 
6. In the ""Project Id"" filter, there is a small search bar, you can type in ""tutorial"".  Select the project ""tutorial-synthetic_data_set_1"". 
7.  Once selected, click the button ""Export all to Terra"", and then proceed to section 1d below. 

## 1d. Select a Terra workspace for your data
Once the export window in Gen3 transitions to Terra, you are given a few options for where to place your data:

1) ""Start with a template""
This feature allows you to import data directly into a template workspace that has everything set up for you to do an analysis but does not contain any data. Once you select a workspace, you will need to enter:
- Workspace name: Enter a name a name that is meaningful for your records.
- Billing Project: Select the billing projects available to you. If you are a new user, you can use the [$300 of free credits offered](https://support.terra.bio/hc/en-us/articles/360046295092).
- Authorization Domain: Assign the authorization domain that you generated above to protect your data. This is important for working with controlled access. data. 

2) ""Start with an existing workspace"" 
If you have already created a workspace, you can import your data directly to this workspace. 

3) ""Start a new workspace""
This will create an empty workspace. You can individually copy notebooks and workflows from other workspaces, import workflows from Dockstore, or start fresh.

# Part 2: Reformat Gen3 phenotypic data for use in downstream analysis
If you have not used the Gen3 data model before, we recommend you first start with this tutorial [Getting Started with Gen3 Data in Terra](https://terra.biodatacatalyst.nhlbi.nih.gov/#workspaces/fc-product-demo/BioDataCatalyst-Gen3-data-on-Terra-Tutorial) and then come back to this step in the GWAS.

1. Once data has been successfully uploaded to your Terra account, review the Data tab of your Terra workspace. The tutorial dataset should include the following clinical data tables that you will use in the GWAS: Subject, Demographic, Lab Result, Sample. 
2. Navigate to the Notebooks tab and open **1-Prepare-Gen3-data-for-exploration**. This notebook consolidates the clinical data tables mentioned in step 1 into a single data table that can be used in the GWAS. This notebook calls functions in the companion notebook **terra_data_table_util**. You don't have to open terra_data_table_util notebook, it is available for users to edit for other use cases.
3. Once this notebook completes, you should go back to the Data tab in your workspace and check out the new data table ""consolidated_metadata"". This data table has all of the phenotypic data merged into one table. Now you are ready to import your data into the first GWAS analysis notebook. 

Note: This consolidated_metadata table is closer to the data model you use in our ""Featured Workspaces"".  We hope these two notebooks (1-Prepare-Gen3-data-for-exploration, terra_data_table_util) help you use Gen3 data with more of our training resources in Terra.       

# Part 3: Explore TOPMed data in Jupyter Notebooks   
Now that you can interact with the Gen3 structured data more easily, you will use an interactive notebook to explore your phenotypic and environmental data and performs several analyses to prepare the data for use in batch association workflows. 

1. [Learn how to customize your interactive analysis compute](https://support.terra.bio/hc/en-us/articles/360038125912) to work with the data you imported. Your computing needs will vary depending on the size of your VCF file. 
2. Open the **2-GWAS-preliminary-analysis** notebook. You will notice that it uses a similar runtime configuration to what Terra the previous notebook used.
3. Call functions from the terra_data_util notebook to reformat multiple data tables into a single data table that can be loaded as a dataframe in the notebook.
4. Subset the dataframe to include only your traits of interest and remove any individuals that lack data for these traits.
5. Visualize phenotype and environmental variable distributions in a series of plots.
6. Save your resulting data to your bucket, and open the **3-GWAS-genomic-data-preparation** notebook and set your runtime configuration. We have given a suggested configuration within the notebook for a downsampled VCF (representing chromosomes 10 and 11) that we include for training purposes.
7. Filter your VCF to only common variants to increase statistical power. Genetic analyses in this notebook utilize the [Hail software](https://hail.is/). Hail is a framework for distributed computing with a focus on genetics. Particularly relevant for whole genome sequence ([WGS](https://en.wikipedia.org/wiki/Whole_genome_sequencing)) analysis, Hail allows for efficient, nearly boundless computing (in terms of variant and sample size).    
8. Perform a principal component analysis ([PCA](https://en.wikipedia.org/wiki/Principal_component_analysis)) to assess population stratification. Genetic stratification can strongly affect association tests and should be accounted for.
9. Generate a genetic relatedness matrix ([GRM](https://hail.is/docs/0.2/methods/genetics.html?highlight=pc_rel#hail.methods.genetic_relatedness_matrix)) to account for closely related individuals in your association testing workflows.
10. Generate a new ""sample_set"" data table that holds the derived files we created in the steps above using the [FireCloud Service Selector (FISS) package](https://github.com/broadinstitute/fiss).  The files in this data table will be used in the workflows we run in Part 4.     

Note: VCF files you import from Gen3 are in the Reference_File node and are accessible via DRS URLs. For TOPMed Freeze 5b datasets, these are also tar compressed. We have this GWAS tutorial that has notebooks and examples for interacting with these VCFs [here](https://terra.biodatacatalyst.nhlbi.nih.gov/#workspaces/biodata-catalyst/BioData%20Catalyst%20GWAS%20blood%20pressure%20trait).

### Time and cost estimate 
Time to execute all the commands is ~28 minutes which currently costs ~$0.50 to complete (with the recommended cluster configuration available inside the notebook).       

# Part 4: Perform mixed-model association tests using workflows
In Part 3, we explored the data we imported from Gen3 and performed a few important steps for preparing our data for association testing. We generated a new ""sample_set"" data table that holds the files we created in the interactive notebook. These files will be used in our batch workflows that will perform the association tests. Below, we describe the four workflows in this workspace and their cost estimates for running on the sample set we create in this tutorial.  

The workflows used in this template were imported from [Dockstore](www.dockstore.org) and their parameters were configured to work with Terra's data model.  If you're interested in searching other docker-based workflows, [learn more about how they can easily be launched in Terra](https://support.terra.bio/hc/en-us/articles/360038137292).

## Notes on how attributes are set in workflows
We have set the input and output attributes for each workflow in this template. Before running the first workflow, you can look through the inputs and outputs of each workflow and see that outputs from the first workflow feed into the second workflow, and so on. 

In the 2-GWAS-preliminary-analysis notebook, we created a Sample Set data table that holds a row called ""tutorial-analysis"" which contains the input files for the following workflows. You can check this data table out in the Data tab of this workspace. When you open a workflow, make sure that ""Sample Set"" is set and the ""tutorial-analysis"" (or whatever you named your run) is selected before running a workflow. 


#### [1-vcfToGds](https://dockstore.org/workflows/github.com/manning-lab/vcfToGds)

This workflow converts genotype files from Variant Call Format ([VCF](https://en.wikipedia.org/wiki/Variant_Call_Format)) to Genomic Data Structure ([GDS](http://si.biostat.washington.edu/sites/default/files/modules/GDS_intro.pdf)), the input format required by the R package GENESIS.       

##### Time and cost estimates    

| Sample Set Name | Sample Size | Time | Cost |
| -------  | --------  | -------- | -------- |
| tutorial-analysis | 2,504 samples | 6m | $0.07

Inputs:
* VCF  genotype file (or chunks of VCF files)

Outputs:
* GDS genotype file

#### [2-genesis_GWAS](https://dockstore.org/workflows/github.com/AnalysisCommons/genesis_wdl/genesis_GWAS)

This workflow creates a null model from phenotype data with the [GENESIS biostatistical package](http://bioconductor.org/packages/devel/bioc/html/GENESIS.html). This null model can then be used for association testing. This workflow also runs single variant and aggregate test for genetic data. Implements Single-variant, Burden, SKAT, SKAT-O and SMMAT tests for Continuous or Dichotomous outcomes. All tests account for familiar relatedness through kinship matrixes. Underlying functions adapted from:
	Conomos MP and Thornton T (2016). *GENESIS: GENetic EStimation and Inference in Structured samples (GENESIS): Statistical methods for analyzing genetic data from samples with population structure and/or relatedness.* R package version 2.2.7.

##### Time and cost estimates    

| Sample Set Name | Sample Size | Time | Cost |
| -------  | -------- | -------- | ---------- |
|  tutorial-analysis | 2,504 samples | 6m | $0.07

Inputs:
* GDS genotype file
* Genetic Relatedness Matrix
* Trait outcome name
* Trait outcome type
* CSV file of covariate traits
* Sample ID list

Outputs:
* A null model as an RData file
* Compressed csv file(s) containing raw results
* CSV file containing all associations
* CSV file containing top associations
* PNG file of Quantile-Quantile and Manhattan plots


# Optional: Bring your own data
Both the notebook and workflows can be adapted to other genetic datasets. The steps for adapting these tools to another dataset are outlined below:

***Update the data tables***  
Learn more about uploading data to Terra [here](https://support.terra.bio/hc/en-us/articles/360025758392).  

***Update the notebook***
Accommodating other datasets may require modifying many parts of this notebook. Inherently, the notebook is an interactive analysis where decisions are made as you go. It is not recommended that the notebook be applied to another dataset without careful thought. 

***Run an additional workflow***
You can search [Dockstore](https://www.dockstore.org) for available workflows and export them to Terra following [this method](https://docs.dockstore.org/en/develop/launch-with/terra-launch-with.html). 


### Authors, contact information, and funding
This template was created for the [NHLBI's BioData Catalyst](https://biodatacatalyst.nhlbi.nih.gov/) project in collaboration with the [Computational Genomics Platform](https://cgpgenomics.ucsc.edu/) at [UCSC Genomics Institute](https://ucscgenomics.soe.ucsc.edu/) and the [Data Sciences Platform](https://www.broadinstitute.org/data-sciences-platform) at [The Broad Institute](https://www.broadinstitute.org/). The association analysis tools were contributed by the [Manning Lab](https://manning-lab.github.io/).

Contributing authors include:
* [Beth Sheets](mailto:esheets@ucsc.edu) (UC Santa Cruz Genomics Institute)
* Michael Baumann (Broad Institute, Data Sciences Platform)
* Brian Hannafious (UC Santa Cruz Genomics Institute)
* [Tim Majarian](mailto:tmajaria@broadinsitute.org) (Manning Lab)
* Alisa Manning (Manning Lab)


----

### Workspace change log 
| Date | Change | Author | 
| -------  | -------- | -------- |
| March 13, 2020 | Created notebook | Beth |
| April 7, 2020 | Reviewed dashboard and tested notebooks and workflows | Beth |
| June 26, 2020 | terra_data_table_util & codefolding updates | Beth |
| December 9, 2020 | Updated notebooks | Ash |
| January 7, 2021 | Update workspace text, hotfix second workflow, split second notebook into three | Ash |",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","biodata-catalyst/BioData Catalyst GWAS 1000 Genomes Tutorial"
20,"broad-firecloud-tcga","TCGA_LUSC_OpenAccess_V1-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_LUSC_OpenAccess_V1-0_DATA",TRUE,TRUE,"Lung squamous cell carcinoma","Tumor/Normal","USA","TCGA Lung squamous cell carcinoma Open-Access Data Workspace","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients’ clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","504","Lung","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh37/hg19","TCGA_LUSC_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_LUSC_OpenAccess_V1-0_DATA"
22,"techinno","cSplotch Workflow","READER","https://app.terra.bio/#workspaces/techinno/cSplotch%20Workflow",TRUE,TRUE,NA,NA,NA,"# cSplotch in Terra

This workspace contains the Terra workflows to run cSplotch, a hierarchical generative probabilistic model for analyzing Spatial Transcriptomics (ST) [[1]](#references) and 10x Genomics' Visium data. 


## cSplotch Model Features
- Supports complex hierarchical experimental designs and model-based analysis of replicates
- Full Bayesian inference with Hamiltonian Monte Carlo (HMC) using the adaptive HMC sampler as implemented in Stan [[2]](#references)
- Analysis of expression differences between anatomical regions and conditions using posterior samples
- Different anatomical annotated regions are modelled using a linear model
- Zero-inflated Poisson or Poisson likelihood for counts
- Conditional autoregressive (CAR) prior for spatial random effect
- Ability to deconvolve gene expression into cell type-specific signatures using compositional data gathered from histology images
- Use single-cell/single-nuclear expression data to calculate priors over expression in each cell type

We support the original ST array design (1007 spots, a diameter of 100 μm, and a center-to-center distance of 200 μm) by [Spatial Transcriptomics AB](https://spatialtranscriptomics.com), as well as [Visium Spatial Gene Expression Solution](https://www.10xgenomics.com/spatial-transcriptomics/) by [10x Genomics, Inc.](https://www.10xgenomics.com), interfacing directly with file formats output by [Spaceranger and Loupe Browser](https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview).


For more details on the cSplotch probablistic model, or to run cSplotch locally, visit the GitHub page https://github.com/adaly/cSplotch. A diagram of the cSplotch model from the GitHub is illustrated below:

![Hierarchical models](https://github.com/adaly/cSplotch/blob/bd7f77dfc7414f432f6e05ab9789b7aa593b12b4/cSplotch_Model.png?raw=true)

## cSplotch Worfklows

There are three workflows that must be run to get the cSplotch output and there is one workflow to assist in identifying differential expression in genes. 

The following must run successfully in order:
1. **[Prepare_Count_Files](https://app.terra.bio/#workspaces/techinno/cSplotch/workflows/techinno/Prepare_Count_Files)** - creates unified count files for each sample and filters lowly expressed genes
2. **[Generate_Input_files](https://app.terra.bio/#workspaces/techinno/cSplotch/workflows/techinno/Generate_Input_Files)** - generates an individual input file for each gene and specifies the model parameters
3. **[Run_cSplotch](https://app.terra.bio/#workspaces/techinno/cSplotch/workflows/techinno/Run_Splotch)** - runs the cSplotch model and outputs the posterior likelihoods for a given gene
4. **[Gene_Diff_Exp](https://app.terra.bio/#workspaces/techinno/cSplotch/workflows/techinno/Gene_Diff_Exp)** - calculates Bayes Factor and log2 fold change between a given pair of conditions/regions for all genes

# Workflow descriptions and inputs

## Global runtime input parameters
The following input parameters are present in all of the cSplotch Terra workflows. For a more in-depth explanation of each parameter, see https://cromwell.readthedocs.io/en/stable/RuntimeAttributes/.

_disk\_size\_gb_ - Size in GB that the VM should allocate for storage.

_boot\_disk\_size\_gb_ (default 3) - Size of the disk where the docker image is booted.

_docker_ (default ""us-central1-docker.pkg.dev/techinno/images/csplotch_img:latest"") - Docker image to use for the VM (it must have cSplotch and CmdStan installed and use google-cloud-cli as a base, see the [csplotch_img](https://github.com/matthewse19/cSplotch-workflow/blob/main/Dockerfile) default Dockerfile). 

_memory_ (e.g. ""16G"" for 16GB) - Amount of RAM to use.

_preemptible_ (default 2) - Number of times to preempt a VM before switching to an on-demand machine.

_zones_ (default ""us-central1-a us-central1-b... us-east1-a us-east1-b... us-west1-a us-west1-b..."") - Ordered list of zone preference. 


Many of the workflows also have inputs which supposed to be the Google Cloud GS URIs pointing to folders (e.g. _root_dir_, _spaceranger_dir_, _st_cout_dir_, etc).
These strings must start with _gs://_ and are then followed by the URI of the folder. To find the URI of specific directory, nagivate to https://console.cloud.google.com/storage/browser and find the directory within the bucket. The full URI (without the prefix _gs://_) of the folder can be copied as shown below:

![GS URI](https://github.com/matthewse19/cSplotch-workflow/blob/main/documentation/copy_gs_uri.png?raw=true)

## Prepare_Count_Files

This workflow creates a .unified.tsv file for each sample, which ensures that the gene indexing across all samples is consistent and also filters out lowly expressed genes. If Visium was used, only fill in the _spaceranger_dir_ input, otherwise for ST data, fill in _st_count_dir_.

### Inputs
_root_dir_ - GS URI (e.g. ""gs://[bucket_uri]/[parent_dir]"") to the root directory containing the cSplotch metadata file and the Spaceranger/ST directory. A _Prepare_Count_Files.log_ file will be placed in this directory after the Workflow is completed successfully. 

_memory_ (default ""16G"") - Amount of RAM.

_min_detection_rate_ (default 0.02) - Minimum expression rate over every labeled spot in all samples that a gene must have in order to be kept.

_spaceranger_dir_ - The GS URI of the parent directory containing each sample's Spaceranger folder (not used for ST data).

_st_count_dir_ - The GS URI of the parent directory containing each sample's count file (not used for Visium data).


### Outputs
Before running the workflow, create a data table with a blank row so the outputs get stored. This can be done in the workspace by going to the Data tab > IMPORT DATA > Upload TSV > TEXT IMPORT > Copy and paste the following:
```
entity:cSplotch_run_id
1
```
Now back in the **Prepare_Count_Files** workflow, ensure that ""Run workflow(s) with inputs defined by data table"" is chosen and click 'SELECT DATA' to choose the cSplotch run ID. In the outputs tab, enter the following for each output variable (or choose any other descriptive column name):

_genes_detected_ - this.genes_detected

_genes_kept_ - this.genes_kept

_median_seq_depth_ - this.median_seq_depth

After **Prepare_Count_Files** is completed successfully, the workflow will automatically populate these fields  in the selected row in the data table.

## Generate_Input_Files

This workflow generates an R dump formatted file for each gene, grouping the genes into subdirectories of 100 files each. The model structure must be specified in this step with the metadata file that is provided. 

The metadata file is a .TSV with the following columns: library_sample_id, Level 1, Level 2, Level 3, Count file, Annotation file, Spaceranger output (only for Visium data).
""Level 2"" and ""Level 3"" may be left out if they are not applicable to the data.

See https://github.com/adaly/cSplotch/tree/master#annotation-of-st-spots on how to create and structure the samples' annotation files.

To determine the expression estimates of individual cell-types (and not just annotomical regions), a compositional file may be provided, and listed in a column titled ""Composition file"". See https://github.com/adaly/cSplotch/tree/master#annotation-of-cell-types on how to generate and structure the compositional files. 


Since all referenced files must be localized within a VM, the naming conventions of the metadata file is strict when using cSplotch in Terra. Paths in the Count file column must start with ""./st_counts"" for ST data and ""./spaceranger_output"" for Visium data. The Annotation file column must start with ""./annotation"", Composition file must start with ""./composition"", and the Spaceranger output column must start with ""./spaceranger_output"". 

An example row in a metadata file for a three level Visium data run would look like:

| library_sample_id | Level 1 | Level 2 | Level 3 | Count file                               | Annotation file      | Spaceranger output       |
|-------------------|---------|---------|---------|------------------------------------------|----------------------|--------------------------|
| 001               | Ctrl    | No EFI  | Male    | ./spaceranger_output/001/001.unified.tsv | ./annotation/001.csv | ./spaceranger_output/001 |

### Inputs

_annotation_dir_ - The GS URI of the annotation directory.

_csplotch_input_dir_ - The GS URI of an empty directory in the _root_dir_ where the gene input files will be placed and a file named _information.p_ which is a pickled Python dictionary that has useful metadata and model parameters about the run.

_metadata_file_ - The metadata file that is in the above structure.

_n_levels_ - The number of condition levels present in the metadata (1, 2, or 3).

_root_dir_ - The GS URI of the root directory containing the directories _annotation_dir_, _spaceranger_dir_/_st_count_dir_, _csplotch_input_dir_, and optionally _composition_dir_. A _Generate_Input_Files.log_ file will be placed in this directory after the workflow is completed successfully. 

_scaling_factor_ - The median sequencing depth over all spots which is found during **Prepare_Count_Files**. Select ""Run workflow(s) with inputs defined by data table"" and enter the value ""this.median_seq_depth"" to reference the value stored in data table's selected row. This value can also be found in _Prepare_Count_Files.log_ in the _root_dir_.

_composition_dir_ (optional) - The GS URI to the directory containing the composition files, referenced in the metadata file.

_empirical_priors_ (optional) - An AnnData (HDF5 format) file with single cell gene expression to inform the model's priors of the expression in each cell type (only for compositional data). 

_maximum_spots_per_tissue_ (default 4992) - Number of spots threshold for identifying overlapping tissue sections. 4992 covers an entire Visium array.

_memory_ (default ""16G"") - Amount of RAM.

_minimum_sequencing_depth_ (default 100) - Minimum number of UMIs per spot.

_no_car_ (default false) - Disable the conditional autoregressive prior.

_no_zip_ (default false) - Use the Poisson likelihood instead of the default zero-inflated Poisson likelihood.

_sc_gene_symbols_ (optional) - Key in `AnnData.var` of the _empirical_priors_ file corresponding to gene names in ST count/Visium data (required when _empirical_priors_ is given; must match gene names in count files).

_sc_group_key_ (optional) - Key in `AnnData.obs` of the _empirical_priors_ file corresponding to cell type annotations (required when _empirical_priors_ is given; must match cell type naming in annotation files).

_spaceranger_dir_ - The GS URI of the parent directory containing each sample's Spaceranger folder (not used for ST data).

_st_count_dir_ - The GS URI of the parent directory containing each sample's count file (not used for Visium data).

### Outputs

Enter the following in the output fields to easily access the ""genes_indexes.csv"" file that gets generated:

_gene_indexes_ - this.gene_indexes

This file has the columns ""gene_index"", ""ensembl"", ""type"" and ""gene"". It is a useful reference when wanting to run individual genes at a time in **Run_cSplotch** and is also needed in the **plotting_and_analysis.ipynb** notebook.

## Run_cSplotch

### Inputs

_compositional_data_ - A boolean set to _true_ or _false_, indicating whether to run the compositional cSplotch model or the non-compositional Splotch model.

_csplotch_input_dir_ - The GS URI of the directory within the _root_dir_ where the gene input files exist.

_csplotch_output_dir_ - The GS URI of the directory within the _root_dir_ where the summarized output files will be placed (if a gene's output file already exists, then the workflow will skip over the gene).

_max_concurrent_VMs_ - The number of VMs to distribute the set of genes across. 

_gene_timeout_hrs_ (default 20) - The number of hours the cSplotch model will run on a single gene before restarting or moving on to the next gene.

_num_chains_ (default 4) - The number of independent MCMC chains to run.

_num_cpu_ (default 4) - The number of CPUs each VM will request. Should match the number of chains for optimal performance.

_num_samples_ (default 500) - The number of times each chain will draw a sample in the MCMC process.

_splotch_gene_idxs_ (optional) - The integer indexes of the genes to run cSplotch on. If left blank, the first _total_genes_ number of genes will be ran.

_total_genes_ (optional) - The number of genes to run cSplotch on. Defaults to the length of _splotch_gene_idxs_ if the list is defined, otherwise 0 if also left blank. 

_tries_per_gene_ (default 1) - The number of timeouts alloted per gene before skipping to the next.

_vm_total_retries_ (default 3) - The total number of allowed timeouts across all genes ran on the VM plus the number of preemptions.

### Outputs

## Gene_Diff_Exp

This workflow calculates the [Bayes Factor](https://en.wikipedia.org/wiki/Bayes_factor) and log<sub>2</sub> fold change between two experimental groups. The groups can be specified by any level condition, anatomical annotation regions (AAR), or single cell types for compositional data.

The group corresponding with _test_type_ (""aars"", ""conditions"" or ""cells"") must have a length of one (for one group vs the rest) or a length of two (to directly compare the two groups). The other groups that aren't of type _test_type_ are then used to subset the data.

For example, if _test_type_ = ""aars"", _condition_level_ = 1, _conditions_=[""ALS""], and _aars_ = [""Layer_1"", ""Layer_2""], then the differential gene expression is calculated between the two groups of spots labeled as ""Layer_1"" and ""Layer_2"" for ALS samples.

### Inputs

_aars_ - When _test_type_ = ""aars"", these must be the two AARs to test against each other or one to test against the rest. Otherwise, the list is the specified AARs to subset the data by.

_condition_level_ - The condition/beta level that _conditions_ specifies.

_conditions_ - When _test_type_ = ""conditions"", these must be the two conditions to test against each other or one to test against the rest. Otherwise, the list is the specified conditions to subset the data by. These conditions must also be valid conditions for the specified _condition_level_ of the data.

_csplotch_output_dir_ - The GS URI of the directory where the summarized output files will be placed (if a gene's output file already exists, then the workflow will skip the gene).

_gene_indexes_ - The file named ""gene_indexes.csv"" that is produced by the **Generate_Input_Files** workflow. This file is used as a lookup for the gene and ensembl names of each cSplotch gene index.

_results_csv_name_ - Name of the results file that will be placed in the Google cloud _results_dir_ directory.

_results_dir_ - The GS URI of the directory where the results CSV file will be placed.

_splotch_information_p_ -  The pickled Python dictionary with cSplotch metadata that is output during the **Generate_Input_Files** workflow and typically located in the cSplotch input directory.

_test_type_ - Must be ""conditions"", ""aars"", or ""cell_types"" for compositional data. The corresponding variable list be must 

_cell_types_ (optional) - When _test_type_ = ""cell_types"", these must be the two cell types to test against each other or one to test against the rest. Otherwise, the list is the specified cell types to subset the data by. This is only applicable for compositional data.

_memory_ (default ""32G"") - Amount of RAM.

_num_cpu_ (default 1) - Number of CPUs to allocate to the VM. The workflow takes advantage of multi-processing and will drastically speed up with more CPUs.

### Outputs

_de_results_ - The string of the GS URI pointing to the results file. Can be set to the column of a Terra data table for easy retrieval.

# Downstream analysis

There is an example notebook named **plotting_and_analysis.ipynb** found in the ""ANALYSIS"" tab of this workspace. This notebook demonstrates how to find the most differentially expressed genes from the CSVs produced by **Diff_Gene_Exp** and also provides example uses of each visualization function in `splotch.utils_plotting`. 

The following plotting/analysis tasks are included:

- Visualize KDEs of the posterior distributions grouped by combinations of conditions, AARs, and cell-types
- Plot a gene's raw expression levels and cSplotch's lambda values on a specified array
- Identify gene-gene coexpression modules using the cSplotch lambda values (as done in [[1]](#references))
- Plot the scaled gene expression of genes in each module on a specified array
- Visualize the standardized gene expression of genes in a module grouped by AAR or condition
- Identify coexpression submodules using an scRNA AnnData file


# References
[1] Ståhl, Patrik L., et al. [""Visualization and analysis of gene expression in tissue sections by spatial transcriptomics.""](https://science.sciencemag.org/content/353/6294/78) *Science* 353.6294 (2016): 78-82.

[2] Carpenter, Bob, et al. [""Stan: A probabilistic programming language.""](https://www.jstatsoft.org/article/view/v076i01) *Journal of Statistical Software* 76(1) (2017).

[3] Maniatis, Silas, et al. [""Spatiotemporal dynamics of molecular pathology in amyotrophic lateral sclerosis.""](https://science.sciencemag.org/content/364/6435/89) *Science* 364.6435 (2019): 89-93.",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","techinno/cSplotch Workflow"
23,"warp-pipelines","Smart-seq2_Single_Nucleus_Muti-Sample","READER","https://app.terra.bio/#workspaces/warp-pipelines/Smart-seq2_Single_Nucleus_Muti-Sample",TRUE,FALSE,NA,NA,NA,"## Smart-seq2 Single Nucleus Multi-Sample Overview
The Smart-seq2 Single Nucleus Multi-Sample (Multi-snSS2) Pipeline is developed in collaboration with the BRAIN Initiative Cell Census Network (BICCN) to process single-nucleus RNAseq (snRNAseq) data generated by Smart-seq2 assays.  

* This workspace currently describes `v1.2.25` of the Multi_snSS2 Pipeline and provides a fully reproducible example of the workflow.  

* The pipeline's [workflow](https://github.com/broadinstitute/warp/blob/master/pipelines/skylab/smartseq2_single_nucleus_multisample/MultiSampleSmartSeq2SingleNucleus.wdl) is written in WDL, is freely available in the WARP repository on GitHub, and can be run by any compliant WDL runner (e.g. Crowmell).

* The pipeline is designed for single-nucleus datasets; for the **single-cell** Smart-seq2 Pipeline, see the [Smart-seq2 Multi-Sample Pipeline workspace](https://app.terra.bio/#workspaces/featured-workspaces-hca/HCA%20Smart-seq2%20Multi%20Sample%20Pipeline).

* The pipeline counts **both** exons and introns; if you want to count exonic reads only, see the [Smart-seq2 Multi-Sample Pipeline workspace](https://app.terra.bio/#workspaces/featured-workspaces-hca/HCA%20Smart-seq2%20Multi%20Sample%20Pipeline).

Scroll down for workflow details, including required input and output descriptions, estimated run times and costs, and downsampled example data information.       

**Note that cost and time estimates will vary** with the use of [Preemptibles](https://cloud.google.com/preemptible-vms/).  
You can also use [Google's cost calculator](https://cloud.google.com/products/calculator/) to estimate cost to run. 

**For helpful hints on controlling Cloud costs**, see [Overview: Controlling Cloud costs on Terra](https://support.terra.bio/hc/en-us/articles/360029748111).  

------
##  Multi-snSS2 

### What does it do?   

This Multi-snSS2 workflow processes multiple cells by importing and running the Smart-seq2 Single Nucleus workflow for each cell. 

Overall, the snSS2 workflow:
* Trims adapters from paired-end FASTQ files.
* Aligns reads to the genome using a modified GTF.
* Counts intronic and exonic reads.
* Calculates quality control metrics.

The pipeline then merges the resulting count matrices (Loom format) into a single matrix that includes the exonic and intronic counts, as well as quality metrics.

For more details about the pipeline, see the [Multi-snSS2 Overview](https://broadinstitute.github.io/warp/docs/Pipelines/Smart-seq2_Single_Nucleus_Multi_Sample_Pipeline/README) in the [WARP documentation](https://broadinstitute.github.io/warp/).  

---
### How to run the workflow

This workspace is preloaded with an **example mouse brain dataset (3 cells) and workflow configuration** designed to demo the Multi-snSS2 workflow. 

The example samples are publicly available from the [NeMO Data Portal](https://portal.nemoarchive.org/), but if you wish to run your own data, the pipeline is validated for mouse paired-end reads derived from single-nucleus Smart-seq2 library preparations.

The workspace workflow is configured to leverage the `sample` and `sample_set`  data tables in the workspace’s Data page. 

To run the workflow: 

1. Navigate to the Workflows page and choose the Multi-snSS2 workflow configuration.
2. On the configuration page, make sure the `root entity` type is `sample_set` (selected by default).
3. Select the Mouse_brain dataset. 
4. Select the checkbox next to `Use reference disks`.
5. Run the analysis.

---
### What does Multi-snSS2 require as input?

The Multi-snSS2 workflow requires the following input:

| Input name | Input Description | Input Type |
| --- | --- | --- |
| batch_id | Identifier for the batch of multiple samples. | String |
| fastq1_input_files | Cloud path to FASTQ files containing forward paired-end sequencing reads for each cell (sample); order must match the order in input_id. | Array of strings | 
| fastq2_input_files | Cloud path to FASTQ files containing reverse paired-end sequencing reads for each cell (sample); order must match the order in input_id. | Array of strings |
| input_ids | Unique identifiers or names for each cell; can be a UUID or human-readable name. | Array of strings |

#### Reference data description and location  
The required mouse reference genome and additional resources for the tools in this workspace are included in the `Workspace` data table. The references are preconfigured in the workspace example workflow configurations.

| Reference Name | Input Description | Input Type |
| --- | --- | --- |
| adapter_list | FASTA file listing adapter sequences used in the library preparation (i.e. Illumina adapters for Illumina sequencing). | File | 
| annotations_gtf | Custom GTF file containing annotations for exon and intron tagging; must match `tar_star_reference`. | File | 
| genome_ref_fasta | FASTA file used for metric collection by [Picard](https://broadinstitute.github.io/picard/) | File | 
| tar_star_reference | TAR file containing genome indices used for the STAR aligner. | File |

##### Reference genomes: 
The reference for mouse is GRCm38, the [GENCODE M23](https://www.gencodegenes.org/mouse/release_M23.html).

* Reference inputs are created using the [BuildIndices Pipeline](https://github.com/broadinstitute/warp/tree/master/pipelines/skylab/build_indices).
* The workflow uses a modified version of the 10x Genomic's code for building mouse ([GRCm38-2020-A](https://support.10xgenomics.com/single-cell-gene-expression/software/release-notes/build#mm10_2020A)) and human ([GRCh38-2020-A](https://support.10xgenomics.com/single-cell-gene-expression/software/release-notes/build#GRCh38_2020A)) reference packages. 
* To enable intron counting, the workflow calls a [shell script](https://github.com/broadinstitute/warp-tools/blob/develop/3rd-party-tools/build-indices/add-introns-to-gtf.py) to create a custom GTF with intron annotations. Introns are considered any part of a contig that is not exonic nor intergenic. 

##### Enabling reference disks
The suggested workflow configuration (see ""How to run the workflow"" above) uses [reference disks](https://support.terra.bio/hc/en-us/articles/360056384631). That means when Terra kicks off the workflow on a virtual computer, it attaches a portable disk (kind of like a flash-drive) that is preloaded with the reference files needed for the workflow. This saves time and cost when running the workflow. 

### Optional parameters 

The Multi-snSS2 workflow offers optional inputs, including metadata related to the sample. These inputs are detailed in the [Multi-snSS2 Overview](https://broadinstitute.github.io/warp/docs/Pipelines/Smart-seq2_Single_Nucleus_Multi_Sample_Pipeline/README#sample-data-and-reference-inputs).

---
### What does Multi-snSS2 return as output?

In this workspace, the metadata for all outputs (including the optional Loom file) are written to the Workspace Data table. 

| Output variable name | Description | Type |
| --- | --- | --- |
| bam_files | Array of genome-aligned BAM files (one for each cell) generated with STAR.  | Array [BAM] |
| exon_intron_count_files | Array of TXT files (one per cell) that contain intronic and exonic counts. | Array [TXT]| 
| loom_output | Cell-by-gene matrix in Loom format containing intronic and exonic counts for every cell. | Loom |
| pipeline_version_out | Version of the processing pipeline run on this data. | String |

The final Loom-formatted cell-by-gene matrix contains the Picard metrics (alignment_summary_metrics, deduplication metrics, and the G/C bias summary metrics) and the featureCount exon and intron counts. 

Exonic counts are stored in the main Loom matrix which is unnamed by default. The exonic counts are the default return value of `loompy.connect()`, but can also be accessed using Loompy's `layers()` method. For example, `loompy.connect.layers[“”]` will return the exonic counts from the output Loom file. Intronic counts are stored in the Loom file as an additional layer which is named `intron_counts`. Intronic counts can be accessed in a similar manner, where `loompy.connect.layers[“intron_counts”]` will return the intronic counts from the output Loom file. Whole gene counts (which include both intronic and exonic counts) can be accessed by adding the intronic and exonic counts together. 

For more information about accessing these counts, Loom file format, and the use of layers, see the [Multi-snSS2 pipeline](https://broadinstitute.github.io/warp/docs/Pipelines/Smart-seq2_Single_Nucleus_Multi_Sample_Pipeline/README#creating-the-cell-by-gene-matrix-loom) and [Loompy](https://linnarssonlab.org/loompy/index.html) documentation.

---
### Estimated time and cost to run on sample data 

The following estimates are based on the Mouse_brain example dataset with reference disks enabled.
 
| Sample Set Name | Set Size | Sample Set R1.fastq Size | Sample Set R2.fastq Size | Time | Cost $ |
| :---:  | :---: | :---: | :---: | :---: | :---: |
| Mouse_brain | 3 cells | ~ 135 MB | ~ 135 MB | 1 hr 4 min | 0.10 | 
 
**For helpful hints on controlling Cloud costs, see [Overview: Controlling Cloud costs on Terra](https://support.terra.bio/hc/en-us/articles/360029748111).**   

---

---
### Versions

You can access previous versions of this pipeline by cloning the workspace and choosing a version in the version dropdown. 

 For a complete version list, see the [Multi-snSS2 changelog](https://github.com/broadinstitute/warp/blob/master/pipelines/skylab/smartseq2_single_nucleus_multisample/MultiSampleSmartSeq2SingleNucleus.changelog.md) in GitHub.

| Multi-snSS2 Release Version | Date | Release Note | 
| ---  | :---: | :---: | 
| MultiSampleSmartSeq2SingleNucleus_v1.2.25 | 08/16/2023 | Updated several task dockers. Added a new task to the workflow that reads the tar_star_reference file to obtain the genomic reference source, build version, and annotation version and outputs the information as a txt file. |
| MultiSampleSmartSeq2SingleNucleus_v1.2.7 | 09/09/2022 | Updated several task dockers. |
| MultiSampleSmartSeq2SingleNucleus_v1.2.3 | 04/20/2022 | Updated outputs in workflow configuration to write to the workspace data table. Updated LoomUtils.wdl for a task in the Optimus pipeline. This change does not affect the MultiSampleSmartSeq2SingleNucleus pipeline. |
| MultiSampleSmartSeq2SingleNucleus_v1.2.0 | 01/24/2022 | Fixed missing metadata in loom file and updated FeatureCounts.wdl to use Python 3. |
| MultiSampleSmartSeq2SingleNucleus_v1.1.0 | 10/12/2021 | Removed the Smart-seq2 Single Nucleus workflow (SmartSeq2SingleNucleus.wdl) and changed the workflow tasks to run multiple samples in the same VM. This change is expected to make the pipeline faster and cheaper. Renamed the StarAlignFastq.StarAlignFastqPairedEnd task to StarAlign.StarAlignFastqMultisample. | 
| MultiSampleSmartSeq2SingleNucleus_v1.0.2 | 08/05/2021 | Updated SmartSeq2 to accommodate spaces in input_name. | 
| MultiSampleSmartSeq2SingleNucleus_v1.0.0 | 06/25/2021 | This is the first release of the pipeline. | 

---


### Contact Information

* For workspace questions and feedback, email the Broad pipelines team at warp-pipelines-help@broadinstitute.org or create a post on the [Featured Workspaces community forum](https://support.terra.bio/hc/en-us/community/topics/360001603491) (login required). Tag @Alexander Baumann in the **Details** section of your post.

* You can also Contact the Terra team for questions from the Terra main menu. When submitting a request, it can be helpful to include:
    * Your Project ID
    * Your workspace name
    * Your Bucket ID, Submission ID, and Workflow ID
    * Any relevant log information

---


### License
**Copyright WARP,  2023 | BSD-3**

All rights reserved. Full license text at https://github.com/broadinstitute/warp/blob/master/LICENSE. Note that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.

---",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","warp-pipelines/Smart-seq2_Single_Nucleus_Muti-Sample"
24,"broad-firecloud-tcga","TARGET_OS_hg38_OpenAccess_GDCDR-12-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TARGET_OS_hg38_OpenAccess_GDCDR-12-0_DATA",TRUE,TRUE,"osteosarcoma","Tumor/Normal",NA,"[TARGET Osteosarcoma (OS) Project](https://ocg.cancer.gov/programs/target/projects/osteosarcoma)

*hg38 TCGA and TARGET workspaces reference files by their GDC UUIDs.  In order to run analyses on the referenced data files, you will need to run workflows that retrieve the files from the GDC and copy them to your workspace bucket.  See   [this forum post](https://gatkforums.broadinstitute.org/firecloud/discussion/10382/populating-hg38-tcga-and-target-workspaces-with-data-files#latest) for instructions on the running of these workflows.*","NCI","Chet Birger","Osteosarcoma","90",NA,"TARGET",NA,"Open","GRCh38/hg38","TARGET Osteosarcoma (OS) Project","Whole Exome","broad-firecloud-tcga/TARGET_OS_hg38_OpenAccess_GDCDR-12-0_DATA"
25,"aryee-lab","DNA-methylation-preprocessing","READER","https://app.terra.bio/#workspaces/aryee-lab/DNA-methylation-preprocessing",TRUE,FALSE,NA,NA,NA,"### DNA-methylation-preprocessing

Suite of tools to conduct methylation data analysis. Methods from this workspace can be used for alignment and quality control analysis for various protocols including Whole Genom Bisulfite Sequencing (WGBS), Reduced Representation Bisulfite Sequencing (RRBS) and Hybrid Selection Bisulfite Sequencing (HSBS).

More information could be find in the github repo here: https://github.com/aryeelab/dna-methylation-tools

---

### Data

**Sample Data**  
All preprocessing methods require forward and reverse  fastq or fastq.gz files entered into the data participants table.  The tables below outline the format and content of the **Participant** data model tables (tab separated files with “tsv” extension).

   **Example 1:** Participants.tsv for RRBS and WGBS

| entity:participant_id 	| bs_fastq1                          	| bs_fastq2                          	|
|-----------------------	|------------------------------------	|------------------------------------	|
| Sample1               	| gs://location/sample_1_R1.fastq.gz 	| gs://location/sample_1_R2.fastq.gz 	|
| Sample2               	| gs://location/sample_2_R1.fastq.gz 	| gs://location/sample_2_R2.fastq.gz 	||

   **Example 2:** Participants.tsv for HSBS requires an additional input, the target_region.

| entity:participant_id 	| bs_fastq1                          	| bs_fastq2                          	| target_region                         	|
|-----------------------	|------------------------------------	|------------------------------------	|---------------------------------------	|
| Sample1               	| gs://location/sample_1_R1.fastq.gz 	| gs://location/sample_1_R2.fastq.gz 	| gs://location/hg38_targetCoverage.bed 	|
| Sample2               	| gs://location/sample_2_R1.fastq.gz 	| gs://location/sample_2_R2.fastq.gz 	| gs://location/hg38_targetCoverage.bed 	|


   **Example 3:** Participant_set_membership.tsv can be created from participant_id by creating a table in this format:

| membership:participant_set_id 	| participant 	|
|-------------------------------	|-------------	|
| Human_single_cell_Expt_1      	| Sample1     	|
| Human_single_cell_Expt_1      	| Sample2     	|


This workspace includes test sets to help you  become familiar with the workflows

| participant_set_id 	| participants 	| cell type                                             	|
|--------------------	|--------------	|-------------------------------------------------------	|
| HES                	| 3            	| Human single cell                                     	|
| HES_set2           	| 2            	| Human single cell                                     	|
| Human_wgbs         	| 1            	| Human whole genome                                    	|
| MES                	| 4            	| Mouse single cell                                     	|
| Mouse_wgbs         	| 1            	| Mouse whole genome                                    	|
| test_set_mouse_sc  	| 3            	| Small mouse single cell data set(for program testing) 	|

**Workspace Data**  
This workspace has no set attributes.  To change reference genomes and target regions,  you will need to modify the input json before running the program.

---

### Tools

This workspace contains the following preset method configurations, already set up for grch38, hg19 and mm10.  Other genomes can be loaded by changing the json inputs.  

* Preprocessing tools are run as ""participant"", selecting one sample or a set of samples.  
* The aggregation tool is run as a ""participant set"" on a set of preprocessed samples aligned to the same genome.  
* Aggregation can only be done after preprocessing.      
* Both preprocessing and aggregation generate html reports (see links to examples below).  

**Preprocessing:**

* bismark_wgbs: Preprocess Whole Genome Bisulfite Sequencing (WGBS) data
* bismark_rrbs: Preprocess Reduced Representation Bisulfite Sequencing (RRBS) data
* bismark_hsbs: Preprocess Hybrid Selection Bisulfite Sequencing (HSBS) data

[Example bismark processing report](https://storage.googleapis.com/terra-featured-workspaces/methylation-preprocessing/example_output/test_HES_sample_1_R1.fastq.gz_bismark_report.html)

**Aggregation:**

* aggregate_bismark_output: Aggregates outputs from preprocessing pipelines and produces an aggregated data structure for further downstream analysis

[Example output from scmeth R package](https://storage.googleapis.com/terra-featured-workspaces/methylation-preprocessing/example_output/qcReport.html)

Based on the reference genome different method configurations could be selected. So far, we have hg38, hg19 and mm10 reference genome versions of all preprocessing and aggregation tools.

---

### Example Time and Cost to Run Workflow

| Sample size |     Per-sample preprocessing |     Aggregation and QC | Total |
| :---:    | :---: | :---: | :---: |
|     | (Hours / $)                  | (Hours / $)            | (Hours / $)        |
| ..................................... | ............................................................................. | .......................................................... | ......................... |
| 10              | 0.98 ($0.93)                 | 0.97 ($0.28)           | 1.95 ($1.21)       |
| 100             | 1.47 ($8.99)                 | 6.00 ($0.86)           | 7.47 ($9.85)       |
| 1000            | 4.48 ($52.48)                | 58.01 ($13.74)         | 62.49 ($66.22)     |


To obtain these estimates, we ran the workflows in FireCloud on the default n1-highmem-4 compute nodes (26 GB RAM with 4 CPUs). Test-run samples were 1000 single-cell RRBS samples with a median of 872,223 reads.

---

### Contact information  

Divy Kangyen, 
Department of Biostatistics
Harvard T.H. Chan School of Public Health
Email address: divyswar01@g.harvard.edu

Issues and feature requests can be submitted to issue tracker in the [dna-methylation-tools github repo](https://github.com/aryeelab/dna-methylation-tools/issues)

Paper associated with the workspace can be found here: https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-2750-4

---

### License  

**Copyright Broad Institute, 2019 | BSD-3**
All code provided in this workspace is released under the WDL open source code license (BSD-3) [full license text here](https://github.com/openwdl/wdl/blob/master/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.

",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","aryee-lab/DNA-methylation-preprocessing"
26,"broad-firecloud-dsde-methods","sr-malaria-public","READER","https://app.terra.bio/#workspaces/broad-firecloud-dsde-methods/sr-malaria-public",TRUE,TRUE,NA,NA,NA,"# _P. falciparum_ Short Read Whole Genome Workspace
This is the workspace for short read whole genome variant discovery and analysis in _Plasmodium falciparum_.   This workspace can call variants in a single-sample, joint call cohorts of samples, and perform various tertiary analyses (e.g. drug resistance screening, rapid diagnostic test evasion screening, etc.).

While the current focus of this workspace is _P. falciparum_,  the processing steps here are generalized and can be adapted to other _Plasmodium_ species.

## Variant Calling Pipeline

As part of this workspace there are workflows to call variants on both single samples, and for joint calling across cohorts of samples.

The main variant calling pipeline has has the following high-level structure:

![LRMA SP Malaria Variant Calling](https://github.com/broadinstitute/long-read-pipelines/raw/jts_kvg_sp_malaria/extra_documentation/sp_malaria/lrma_sr_malaria_pipeline_diagram_high_level.png)

## Data

### Datasets

The following datasets are currently in this workspace:
- [PF7](https://www.malariagen.net/apps/pf7/)
- The MalariaGEN [crosses](https://www.malariagen.net/parasite/p-falciparum-genetic-crosses)
- 2019 data collected in Senegal

### Data Structure

The data processing is broken down into three levels (similar to other LRMA projects) in the following Terra data tables:
*  Sample (flowcell data)
*  Sample Set (sample data / single-sample calling)
*  Sample Set Set (cohort data for joint calling)

_Sample / Flowcell_ data consists of reads from a single flowcell.  The sample from which these reads have been processed may or may not be represented in other flowcells.

_Sample Set_ data consists of all data from a specific sample.  This may include data from multiple flowcells that belong to the same ""participant"" (i.e. same strain / clone).

_Sample Set Set / Cohort_ data consists of data from multiple samples.  ",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","broad-firecloud-dsde-methods/sr-malaria-public"
27,"DSI_Africa_eLwazi_training2023","eLwazi ODSP Containers and Workflows Workshop 2023","READER","https://app.terra.bio/#workspaces/DSI_Africa_eLwazi_training2023/eLwazi%20ODSP%20Containers%20and%20Workflows%20Workshop%202023",TRUE,FALSE,NA,NA,NA,"This is a tutorial workspace for the eLwazi ODSP Containers and Workflows Workshop held in Johannesburg in October 2023. The workspace demonstrates how to reuse standardized workflows from the pathogen surveillance community and review results in an interactive Jupyter notebook. 

Additionally, we will showcase a workflow that leverages a population subsampling method for phylogenetic inference showcased in this paper:

Houriiyah Tegally, James E. San, Matthew Cotten, Monika Moir, Bryan Tegomoh,  et. al. The evolving SARS-CoV-2 epidemic in Africa: Insights from rapidly expanding genomic surveillance https://doi.org/10.1126/science.abq5358


# Tutorial 
[Step-by-step instructions](https://drive.google.com/file/d/1arNl310rhwmIlUGRjW0A5yEkTk1bffI_/view?usp=drive_link)

#### Data
We pulled SARS-CoV-2 data collected in sites in Africa that were published in the [NCBI Virus](https://www.ncbi.nlm.nih.gov/labs/virus/vssi/#/virus?SeqType_s=Nucleotide&VirusLineage_ss=Wuhan%20seafood%20market%20pneumonia%20virus,%20taxid:2697049&Region_s=Africa) and  the Sequence Read Archive. 



# Analysis 1
### Getting Data into your workspace

**Fetch_sra_to_bam**: This workflow downloads .fastq files from SRA, given an SRA_ID as input. Specifically, the output of the workflow will produce the unaligned BAM file that is needed for viral assembly.

Required Inputs:
- An SRA Accession number

Required Outputs: 
- reads_uBAM


### Sequence Data processing and assembly

**Assemble_refbased**: This takes a raw read file (uBAM) and assembles a viral genome by aligning to a reference genome.

Required Inputs:
- reads_uBAM (an output from fetch_sra_to_bam or your own data)
- reference_fasta (we provide a reusable reference fasta in our open access storage container)
- sample_name (the sra_id or your own sample's id)

Outputs: Viral genome assemblies for all input files and quality metadata that can be used to build a Nextstrain analysis. See the COVID-19 Featured Workspaces for more resources.


### Notebook
We also include a Jupyter Notebook that pulls a data table from your workspace into an interactive session, where you can explore the outputs of workflows. 


# Analysis 2
### Subsampling for phylogenetic inference
**augur_from_msa_with_subsampler**

Inputs:
- aligned_msa_sequences: open source aligned sequences available from the Nextstrain community
- sample_metadata: open source metadata available from the Nextstrain community
- auspice_config: configuration file to determine tree visuals
- case_data: an example case data file originally created by Johns Hopkins University for SARS-CoV-2 cases
- geo_column: the name of the subsampling method
- id_column: strain identifier

Outputs:
- auspice_input_json: a file you can load into a nextstrain visualization tool

Below is the Methods section pulled from https://doi.org/10.1126/science.abq5358: 

To address the potential oversampling of African sequences relative to global reference in the above-mentioned analyses, we performed an-other phylogeographic inference on subsamples based on global case counts to try to eliminate oversampling bias in our inference. To this end, we considered all high-quality sequences for each of the VOCs (Alpha, Beta, Delta, and Omicron BA.1 and BA.2) globally over the same sampling period (until 31 March 2022). We used subsampler (https://github.com/andersonbrito/subsampler) to generate subsamples for each variant based on globally reported cases. In short, subsampler uses a case-count matrix of daily cases, along with the fasta sequence sand GISAID associated metadata, to sample a user-defined number of sequences. For each VOC and for BA.1 and BA.2, we performed 10 samplings using different number seeds to sample datasets of ~20,000. Once again, sampled sequences were screened for viral recombination as described above and sequences with signs of recombination were removed. Sub-sampler has the added advantage that it dis-regards poor quality sequences (e.g., <90%coverage) and sequences with missing meta-data (e.g., exact date of sampling). Each data-set was then subjected to the same analytical pipeline as mentioned above to infer the viral transitions between Africa and the rest of the world.


#### After running the subsampling method
1. Download the auspice_input_json from your data table.
2. Open the visualization browser http://auspice-us.herokuapp.com/
3. Simply drop your auspice_input_json file into the browser to create a visualization",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","DSI_Africa_eLwazi_training2023/eLwazi ODSP Containers and Workflows Workshop 2023"
28,"anvil-outreach","galaxy-demo-message-in-a-genome-assembly","READER","https://app.terra.bio/#workspaces/anvil-outreach/galaxy-demo-message-in-a-genome-assembly",TRUE,FALSE,NA,NA,NA,"### Galaxy Demo -- Message in a Genome Assembly

- Exercise Worksheet ([link](https://docs.google.com/document/d/1ngYJ0zcw2tuv5LwgMzrf-UFRSxFJ91dCyBetEX501lw))

	- Launch Galaxy on AnVIL
	- Import data from Terra Workspace
	- Configure and run Galaxy Tools

- Adapted from JHU EN.601.749: Computational Genomics: Applied Comparative Genomics [Assignment 2](https://github.com/schatzlab/appliedgenomics2021/tree/master/assignments/assignment2)",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","anvil-outreach/galaxy-demo-message-in-a-genome-assembly"
30,"help-gatk","Bioconductor","READER","https://app.terra.bio/#workspaces/help-gatk/Bioconductor",TRUE,FALSE,NA,NA,NA,"Explore common Bioconductor packages that can be used to perform bulk RNA differential expression analyses or manipulate single-cell RNA-seq data

# Table of Contents
- Getting started with Bioconductor in Terra
- Using the Bioconductor workspace
  * Cloud Environment configuration and cost
  * Testing for differential gene expression using the ""edgeR"" Notebook
    + Sample data
    + Required tools (R packages)
    + Analyses
  * Exploring single-cell RNAseq data using the 'SingleCellExperiment' Notebook
    + Sample data
    + Required tools (R packages)
    + Analyses

# Getting started with Bioconductor in Terra
Bioconductor is a suite of open source tools, primarily written as R packages, designed for the statistical analysis of high-throughput genomic data. The Terra Bioconductor image is an extension of the Terra R image and comes preloaded with commonly used Bioconductor packages. These include tools for:

1.  Analyzing single cell sequencing data (SingleCellExperiment)
2.  Annotating genes (GenomicFeatures) 
3.  Storing and manipulating short genomic alignments (GenomicAlignments)
4.  Manipulating and assessing the quality of FASTQ files (ShortRead)
5.  Performing RNA-seq differential gene expression analyses (DESeq2). 

Don’t see the package you need? No problem! You can download ANY Bioconductor package using the ```BiocManager::install()``` command.  View additional pre-installed Bioconductor packages and Docker image details in the [Terra Bioconductor ReadMe](https://github.com/DataBiosphere/terra-docker/tree/master/terra-jupyter-bioconductor). 

You can also view all the contents of the Bioconductor image by selecting the Cloud Environment and choosing ""What's installed on this environment?"" (shown below).  To identify Bioconductor-relevant packages, choose R in the Installed packages dropdown.

| Viewing Bioconductor packages | Image |
| --- | --- |
| Select ""What's installed on this environment"" | ![](https://storage.cloud.google.com/terra-featured-workspaces/Bioconductor/Bioconductor_whats_in_image.png) |
| Select ""R "" in dropdown | ![](https://storage.cloud.google.com/terra-featured-workspaces/Bioconductor/Bioconductor_r_packages.png) |


# Using the Bioconductor workspace 
This workspace uses Jupyter Notebooks to showcase two commonly used Bioconductor packages designed for RNAseq analyses: 'edgeR' and 'SingleCellExperiment'. 

**To get started with these notebooks, make a clone of this workspace** using the three button icon in the upper right of the dashboard. Then read through the following sections to learn more about the workspace configurations, Notebooks, example data and analyses.

## Cloud environment configuration and cost
To use the preset Bioconductor image in a new workspace, first **click on the Cloud Environment icon (top right of the new workspace's dashboard)** and set the environment and compute power as follows:      

| Environment | Screenshot |    
|----|--------|  
| Select the Bioconductor option from the Environment drop-down. | ![](https://storage.cloud.google.com/terra-featured-workspaces/Bioconductor/Bioconductor_Runtime_image.png) |    

| Compute power | Screenshot |    
|-----|------|  
| Use the default standard VM settings, which will cost approximately $0.19 per hour. | ![](https://storage.cloud.google.com/terra-featured-workspaces/Bioconductor/Bioconductor_compute_settings.png) |

--------
--------
## Testing for differential gene expression using the ""edgeR"" Notebook
The ""edgeR"" Notebook uses the Bioconductor edgeR package to analyze  publically available bulk RNAseq  gene count data. 

### Sample data

The sample data used for this Notebook are read counts derived from [Sato et al. 2020](https://cancerres.aacrjournals.org/content/80/6/1279.long) ([GEO #GSE124407](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE124407)). The counts are uploaded to a public Google Bucket as a tab-delimited text file (""counts.txt"") that contains a gene ID column followed read count columns for six samples. Instructions and sample code for accessing Google Bucket files from a Jupyter Notebook are listed in this edgeR Notebook.

### Required tools (R packages) 

This Notebook uses the following R packages:

* 'methods': common methods and classes for R objects
* 'edgeR': statistical tools for RNAseq differential expression analyses
* 'RColorBrewer': color schemes for graphics
* 'ggplot2': a data visualization package


### Analyses

This Notebook performs the following:

*  Creates an edgeR DGEList object
*  Filters and normalizes count data
*  Estimates a common negative binomial dispersion parameter
*  Performs a likelihood ratio test (LRT) for differential expression
*  Creates smear plot of differential expression 

--------
--------
## Exploring single-cell RNAseq data using the 'SingleCellExperiment' Notebook
The ""SingleCellExperiment"" Notebook uses Bioconductor's  [SingleCellExperiment](https://www.bioconductor.org/packages/release/bioc/html/SingleCellExperiment.html) package, which defines a container for storing single-cell RNAseq data. This includes common gene and cell  metadata, such as size factors, control RNA transcript spike-ins, and antibody/CRISPR tags. 

### Sample data

The sample data for this Notebook is the Allen Brain Single-cell RNAseq subset stored in the Bioconductor package 'scRNAseq'. Details about using this dataset can be found in the Notebook. 
### Required tools (R packages)

This Notebook uses the following pre-installed R packages:

* 'SingleCellExperiment': S4 class for storing single-cell data
* 'scRNAseq': package of gene-level counts for publically available RNAseq datasets
* 'Rtsne': R wrapper around the T-distributed Stochastic Neighbor Embedding implementation
* 'magrittr': package that provides a handy pipe-like function
* 'ggplot2': a data visualization package

### Analyses
This Notebook performs the following:
* Creates a SingleCellExperiment object
* Labels spike-in transcripts
* Accounts for sequencing depth
* Makes a SingleCellExperiment subset
* Plots a SingleCellExperiment subset in a t-SNE 

----------
----------
 ## Additional Resources
 You can find additional resources  for using Terra in the [Terra Support](https://support.terra.bio/hc/en-us) and read more about the Bioconductor Image in  [""Using the Bioconductor Docker image in Terra""](https://support.terra.bio/hc/en-us/articles/360037749912).

## License
**Copyright Broad Institute, 2019 | BSD-3**  
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at https://github.com/openwdl/wdl/blob/master/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.
 

 ## Questions and Feedback 

Please post workspace questions and feedback to the [Featured Workspaces community forum](https://support.terra.bio/hc/en-us/community/topics/360001603491) (login required). Tag @Liz Kiernan and @Anton Kovalsky in the “Details” section of your post.




",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/Bioconductor"
31,"broad-firecloud-tcga","TARGET_WT_hg38_OpenAccess_GDCDR-12-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TARGET_WT_hg38_OpenAccess_GDCDR-12-0_DATA",TRUE,TRUE,"metachronous kidney Wilms' tumor","Tumor/Normal",NA,"[TARGET Kidney Tumors Project](https://ocg.cancer.gov/programs/target/projects/kidney-tumors): Wilms Tumor

*hg38 TCGA and TARGET workspaces reference files by their GDC UUIDs.  In order to run analyses on the referenced data files, you will need to run workflows that retrieve the files from the GDC and copy them to your workspace bucket.  See   [this forum post](https://gatkforums.broadinstitute.org/firecloud/discussion/10382/populating-hg38-tcga-and-target-workspaces-with-data-files#latest) for instructions on the running of these workflows.*","NCI","Chet Birger","Wilms Tumor","130",NA,"TARGET",NA,"Open","GRCh38/hg38","TARGET Kidney Tumors Project: Wilms Tumor","Whole Exome","broad-firecloud-tcga/TARGET_WT_hg38_OpenAccess_GDCDR-12-0_DATA"
33,"broad-firecloud-dsde-methods","lr-malaria-public","READER","https://app.terra.bio/#workspaces/broad-firecloud-dsde-methods/lr-malaria-public",TRUE,TRUE,NA,NA,NA,"## Long-read whole genome sequencing of malaria parasites (*P. falciparum* and *P. vivax*)

### About this workspace

This workspace contains processed Oxford Nanopore data from Dr. Daniels's, Dr. Wirth's, and Dr. Garimella's investigations of handheld Nanopore-based sequencing of malaria field parasites and reference isolates.  It also includes the cloud workflows used to process the data as well as notebooks to QC and analyze the data in-place.

### Changelog

| Date | Notes |
|---------------------|---|
| 04/01/2021 | Initial data release of workspace processing two exemplar samples. |
| 04/15/2021 | Improved basecalling (updated to Guppy 4.5.2, supports multiplexed data). Initial implementation of Canu-based genome assembly. |

### Navigating this workspace


### Contact

For questions regarding long read genomics analysis methods, please contact Kiran V Garimella (kiran@broadinstitute.org).

",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","broad-firecloud-dsde-methods/lr-malaria-public"
34,"terra-outreach","DEMO-Working-with-gnomAD","READER","https://app.terra.bio/#workspaces/terra-outreach/DEMO-Working-with-gnomAD",TRUE,FALSE,NA,NA,NA,"This workspace demonstrates several options for working with cloud-hosted gnomAD data from within Terra, intended as a companion to [this blog post](https://terra.bio/a-demo-workspace-for-working-with-gnomad-data-in-terra/).

*Disclaimer: The code examples in this workspace are provided as a proof of concept and may not remain functional over time.*


# Working with gnomAD in Terra

As announced on the [gnomAD blog](https://gnomad.broadinstitute.org/blog/2020-10-open-access-to-gnomad-data-on-multiple-cloud-providers/) in October 2020, the entire gnomAD dataset is now available for direct use or download from Google Cloud as well as Amazon Web Services and Microsoft Azure. (If you're not familiar with gnomAD, see the appendix at the bottom of this page).

There are several ways that you could interact with the [gnomAD data hosted by Google Cloud](https://console.cloud.google.com/storage/browser/gcp-public-data--gnomad) from within Terra, so we developed this workspace to illustrate some approaches that we thought might be useful:

- Running a workflow on the callset VCFs
- Exploring the callset using Hail in a notebook
- Exploring the callset using BigQuery in a notebook

We focused on the variant callsets included in the v3.1 release, which is the latest available at the time we write this. For a full inventory of the dataset across all releases, see the [Downloads page](https://gnomad.broadinstitute.org/downloads) of the gnomAD website, which provides item-by-item links to the data on each of the participating cloud platforms.


## The gnomAD v3.1 callsets: contents and available formats

The main resource that we typically see people derive from gnomAD (outside of the excellent [gnomAD browser](https://gnomad.broadinstitute.org/)) is the **sites-only form of the overall callset**, which contains all variant calls made across all the project samples, but without any individual-level genotypes. 

As [described on the gnomAD blog](https://gnomad.broadinstitute.org/blog/2020-10-gnomad-v3-1-new-content-methods-annotations-and-data-availability/#the-gnomad-hgdp-and-1000-genomes-callset), the v3.1 release also includes a **callset of individual-level genotypes for a subset of samples** (2,435 from the 1000 Genomes Project and 780 from the Human Genome Diversity Project) that represent some of the most genetically diverse populations present in gnomAD.

Both callsets (sites-only of all samples and subset with genotypes) are provided in three different formats:

- A set of [per-chromosome VCF files](https://console.cloud.google.com/storage/browser/gcp-public-data--gnomad/release/3.1/vcf) (block gzipped, with their respective tabix index files) 
- A [Hail table (`.ht`)](https://console.cloud.google.com/storage/browser/gcp-public-data--gnomad/release/3.1/ht)) for the sites-only callset and a [Hail Sparse Matrix Table (`.mt`)](https://console.cloud.google.com/storage/browser/gcp-public-data--gnomad/release/3.1/mt)) for the subset with genotypes, for working with [Hail](https://hail.is/) (the gnomAD team's preferred toolkit for variant manipulation operations, most popular with analysts who are comfortable with Python)
- A [BigQuery dataset](https://console.cloud.google.com/marketplace/product/broad-institute/gnomad) for working with Google Cloud's [BigQuery](https://cloud.google.com/bigquery) compute service (most popular with people who are used to working with SQL databases)


## Contents of this workspace

### Data table of the gnomAD v3.1 VCF files -- For running workflows

Outside of Terra, there are three ways of interacting with the data files: getting links from the [Downloads page](https://gnomad.broadinstitute.org/downloads) of the gnomAD website, or browsing the storage bucket, either through [the GCP console](https://console.cloud.google.com/storage/browser/gcp-public-data--gnomad/release/3.1) or with gsutil.

We found that all of these options were rather tedious, so we set up a Data table (called `chr_callset`) for the per-chromosome VCF files and their index files, organized by chromosome. You can use that table to easily launch workflows against the per-chromosome VCF files. 

**ACTION:** Check out the simple [variant format validation workflow](https://app.terra.bio/#workspaces/terra-outreach/DEMO-Working-with-gnomAD/workflows/dsp-comms-dev/gatk4-basic-variant-validation) that we preconfigured to run on the sites-only VCF files to show how that works. We already ran it on the chr1 sites-only VCF so you can see what the completed job looks like (see additional workflow notes in the resources section below).

To run the workflow, clone this workspace, select which rows you want to run it on and launch the analysis.

We also set up key-value pairs in the Workspace Data table for the other files in the v3.1 release, i.e. the Hail tables and some coverage files also provided by the project. You can use those in a workflow configuration too, though we expect most people will want to load them into a notebook for interactive analysis instead. See the next section for an example of how that works.

### Notebook with the Hail tables -- For exploring the callset with Hail

If you're interested in exploring the gnomAD dataset interactively, one option is to use the [Hail](https://hail.is) toolkit from within a Python 3 notebook. 

**ACTION:** Check out the [gnomAD-with-Hail notebook](https://app.terra.bio/#workspaces/terra-outreach/DEMO-Working-with-gnomAD/notebooks/launch/gnomAD-with-Hail.ipynb), which shows how to load the data (using the Workspace Data table definitions we set up earlier) and get the analysis started. 

To run this notebook, clone this workspace and be sure to select the **Hail environment** in the Cloud Environment configuration menu. 

We refer you to the Hail documentation for details on how to use the many built-in functions in the Hail package for genetic analysis. You may also be interested in [this Reproducible GWAS notebook](https://app.terra.bio/#workspaces/amp-t2d-op/2019_ASHG_Reproducible_GWAS-V2/notebooks/launch/GWAS_initial_analysis_completed.ipynb) as a source of Hail usage examples, from the [Reproducible GWAS workspace](https://app.terra.bio/#workspaces/amp-t2d-op/2019_ASHG_Reproducible_GWAS-V2) that is featured in the [Terra showcase](https://app.terra.bio/#library/showcase).

### Notebook with BigQuery example queries -- For exploring the callset with BigQuery

Another option for interactive analysis of the gnomAD dataset is to use the BigQuery database version of the dataset, which was prepared by the Google Cloud Life Sciences team as described [here](https://console.cloud.google.com/marketplace/product/broad-institute/gnomad). In this case, we no longer use files; instead we send queries to the database, which will return dataframes that we can then manipulate using our preferred data toolkit. 

**ACTION:** Check out the [gnomAD-with-BigQuery notebook](https://app.terra.bio/#workspaces/terra-outreach/DEMO-Working-with-gnomAD/notebooks/launch/gnomAD-with-BigQuery.ipynb)
that the Google Cloud team to demonstrate how this works, which includes various types of queries depending on what you're trying to do with the data. 

To run this notebook, clone this workspace and select either the **Hail environment** or the default **Python 3 environment** in the Cloud Environment configuration menu. 

The advantage of this option is that you don't need to copy over the full data files so it can be a faster way to get to the answers you're looking for. The downside is that Google Cloud charges you a fee for each query. The fee is based on the amount of data you're querying, so if you're composing queries efficiently the amount is usually very small. This option is usually most popular with people who have experience working with databases and are familiar with SQL syntax. 


## Resources / getting help


### New to Terra?

If you are entirely new to Terra, you may want to start [here](https://support.terra.bio/hc/en-us/sections/360006866192-New-users-overview). 

Otherwise, here are some resources for getting started with workflows and notebooks in Terra if you haven't used one of those before. 

### Workflows

The [Workflows Quickstart video](https://www.youtube.com/watch?v=rUBrJNqLyfU) on YouTube provides an end-to-end walkthrough of how to use the Workflows feature in Terra. The video covers how to run a preconfigured workflow on example data in a workspace. 

You can also find complete documentation on this topic [here](https://support.terra.bio/hc/en-us/sections/360004147011-Pipelining).

_Notes about the example workflow included in this workspace:_

- The workflow is designed to be parallelized over genomic intervals to reduce the time it takes to get results. For convenience, we use a set of consolidated intervals that are used for variant calling at the Broad Institute. However, the length of the intervals varies greatly, leading to uneven processing times. If you care about improving this, you could modify the intervals to be more balanced.

- The workflow is designed to fail if the validation tool encounters a formatting issue in the data; therefore the success/failure of the workflow is the best indicator of whether a file is valid or not. The workflow does produce an output file called `stdout` that captures the standard output of the validation tool, including the relevant error message that describes the validation issue. That file will be empty if the validation was successful. If you encounter a failed workflow, you will need to look for the `stdout` file in the execution directory. See the Terra User Guide or contact the helpdesk if you need help finding this. 

### Notebooks

The [Notebooks Quickstart video](https://www.youtube.com/watch?v=Hn5xlnHyE-4) on YouTube provides an end-to-end walkthrough of how to use the Notebooks feature in Terra. If you're short on time, you can skip the first half, which covers finding data and importing it into a workspace. Starting around 8:00, the video will then cover finding notebooks in the workspace, starting the Jupyter environment, launching the notebook and using it to analyze some example data. 

You can also find complete documentation on this topic [here](https://support.terra.bio/hc/en-us/sections/360004143932-Visualization-and-Statistics-notebooks-).

### Support

Keep in mind that we make this workspace available for demonstration purposes, not as a fully-supported tutorial. In particular, we are not able to provide support for the BigQuery notebook (see [here](https://github.com/googlegenomics/gcp-variant-transforms/blob/master/docs/sample_queries/gnomad/gnomad.ipynb) for help with that) or for any advanced Hail features you may choose to use. However, if you run into any trouble using Terra functionality, you are always welcome to reach out to the Frontline Support team, either through the in-app helpdesk or through the [Community Forum](https://support.terra.bio/hc/en-us/community/topics).


## Appendix: Learn more about gnomAD

""gnomAD"" stands for Genome Aggregation Database.

From the gnomAD website: 

> The Genome Aggregation Database (gnomAD), is a coalition of investigators seeking to aggregate and harmonize exome and genome sequencing data from a variety of large-scale sequencing projects, and to make summary data available for the wider scientific community. The project is overseen by co-directors Heidi Rehm and Mark Daly, and steering committee members Daniel MacArthur, Benjamin Neale, Michael Talkowski, Anne O'Donnell-Luria, Konrad Karczewski, Grace Tiao, Matthew Solomonson, and Kat Tarasova. In its first release, which contained exclusively exome data, it was known as the Exome Aggregation Consortium (ExAC).

To learn more about this project, see the [About gnomAD](https://gnomad.broadinstitute.org/about) webpage.

*Note that while there are no publication restrictions or embargoes on the gnomAD data, the project maintainers request that you please cite [the flagship gnomAD paper](https://www.nature.com/articles/s41586-020-2308-7#citeas) if you use any of this data in your work.*





",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","terra-outreach/DEMO-Working-with-gnomAD"
36,"broad-firecloud-tcga","TCGA_CESC_hg38_OpenAccess_GDCDR-12-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_CESC_hg38_OpenAccess_GDCDR-12-0_DATA",TRUE,TRUE,"Cervical Squamous Cell Carcinoma and Endocervical Adenocarcinoma","Tumor/Normal","USA","TCGA Cervical squamous cell carcinoma and endocervical adenocarcinoma Open-Access Data Workspace

*hg38 TCGA and TARGET workspaces reference files by their GDC UUIDs.  In order to run analyses on the referenced data files, you will need to run workflows that retrieve the files from the GDC and copy them to your workspace bucket.  See   [this forum post](https://gatkforums.broadinstitute.org/firecloud/discussion/10382/populating-hg38-tcga-and-target-workspaces-with-data-files#latest) for instructions on the running of these workflows.*","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients' clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","308","Cervix","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh38/hg38","TCGA_CESC_hg38_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_CESC_hg38_OpenAccess_GDCDR-12-0_DATA"
37,"fc-product-demo","Terra_Quickstart_Workspace","READER","https://app.terra.bio/#workspaces/fc-product-demo/Terra_Quickstart_Workspace",TRUE,FALSE,NA,NA,NA,"This QuickStart workspace has been deprecated. For updated QuickStart tutorials with hands-on practice running notebooks or workflows analyses, or generating and using workspace data tables, see the links below.            


### [Terra-Notebooks-QuickStart](https://app.terra.bio/#workspaces/fc-product-demo/Terra-Notebooks-Quickstart)
This [tutorial](https://app.terra.bio/#workspaces/fc-product-demo/Terra-Notebooks-Quickstart) walks through using the Data Explorer to select a subset of the 1,000 Genomes project data (custom cohort), importing the data to a Terra workspace, and running an interactive analysis in an R notebook. 

### [Terra-Workflows-QuickStart](https://app.terra.bio/#workspaces/fc-product-demo/Terra-Workflows-Quickstart)
Learn how to import data to a data table and configure and run workflows in [this tutorial](https://app.terra.bio/#workspaces/fc-product-demo/Terra-Workflows-Quickstart). Three exercises of increasing complexity enable you to start your own batch analysis on Terra.      

### [Terra-Data-QuickStart](https://app.terra.bio/#workspaces/fc-product-demo/Terra-Data-Quickstart)     
Learn first-hand how to use workspace tables to help organize, access, and analyze data - including sets of data - in the cloud.


## Contact information  
This material is provided by the Terra Team. Please post any questions or concerns to our forum site : [Terra](https://broadinstitute.zendesk.com/hc/en-us/community/topics/360000500432-General-Discussion) 

------

## License  
**Copyright Broad Institute, 2019 | BSD-3**  
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at https://github.com/openwdl/wdl/blob/master/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","fc-product-demo/Terra_Quickstart_Workspace"
40,"broad-firecloud-dsde","COVID-19-cross-tissue-analysis","READER","https://app.terra.bio/#workspaces/broad-firecloud-dsde/COVID-19-cross-tissue-analysis",TRUE,FALSE,NA,NA,NA,"The COVID-19 pandemic, caused by the novel coronavirus SARS-CoV-2, underscores the urgent need to identify molecular mechanisms that mediate viral entry, propagation, and tissue pathology in distinct cell types across organs. The surface receptor angiotensin-converting enzyme 2 (ACE2) and the associated proteases, transmembrane protease serine 2 (TMPRSS2) and Cathepsin L (CTSL), were previously identified mediators of SARS-CoV cellular entry.

### Experimental Overview
We use single-cell RNA-seq (scRNA-seq) across diverse tissues to assess the cell-type-specific expression of ACE2, TMPRSS2, and CTSL. We identify specific subsets of respiratory epithelial cells as putative targets of viral infection, including subsets of epithelial cells in the nasal passages, lung and airways. Additionally, we detect expression in other tissues that may serve as routes of viral transmission, including the gut and corneal epithelia, and in cells potentially associated with COVID-19 clinical pathology including cardiomyocytes, olfactory sustentacular cells, and renal epithelial cells.",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","broad-firecloud-dsde/COVID-19-cross-tissue-analysis"
41,"terra-outreach","MIT-Milk-Study","READER","https://app.terra.bio/#workspaces/terra-outreach/MIT-Milk-Study",TRUE,TRUE,NA,NA,NA,"### Introduction

This workspace demonstrates the power and simplicity of using Jupyter Notebooks in Terra for interactive analysis to analyze MIT Milk Study sc-RNA sequencing data.

**What is the MIT Milk Study?**

Brittany A. Goods and Sarah Nyquist, colleagues in the Shalek Lab, used massively parallel single-cell RNA sequencing with the (Seq-Well platform ) to chart the transcriptional landscape of cells isolated from human breast milk over the course of lactation as part of the MIT Milk Study. The first of its kind, this ongoing study is providing deep insight into the immune state of macrophages and promises to answer some long-unanswered questions about the protective benefits of breast milk for infants.

*The authors request that those seeking to use the data for any scientific publication please contact Alex Shalek prior.

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

#### Get Started**

For first time Terra users, the following Quickstart videos, specifically to navigate this workspace, are recommended for a high-level introduction:

1. [Introduction to Jupyter Notebooks/Interactive Analysis](https://www.youtube.com/watch?v=DbakAsk4-5c)

For a full list of videos to learn more about the platform, please refer to this [playlist](https://www.youtube.com/playlist?list=PLh_zJaZ9uQ7P0w6bMLWgL8oDul2EiNlv6).


To actively participate in the workspace, make edits, run notebooks, etc, a few prerequisite tasks are required:

1. [Set up your billing](https://support.terra.bio/hc/en-us/articles/360026182251-How-to-set-up-billing-in-Terra)
2. [Clone the workspace](https://support.terra.bio/hc/en-us/articles/360026130851-How-to-clone-a-workspace)

Cloning a workspace (1) creates a copy that can be edited and therefore allows for analysis actions (such as running the `Macrophage_MITMilkStudy.ipynb` notebook) which will be billed to the above designated source (2). Continue to the Dashboard in the cloned workspace and follow the steps below to run the `Macrophage_MITMilkStudy.ipynb` notebook.

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

#### Data

**What is the data?**

The data provided in the workspace represents the macrophages that were isolated and analyzed as part of the study. The data available in this workspace is not yet published*, but is an exciting resource that the authors felt was important to provide to the scientific community access to. As more datasets are generated, they will be added to this space.

The input data for the described analysis is pulled from a [Google bucket](https://console.cloud.google.com/storage/browser/fc-943d6020-b7d7-4091-b0d4-51cc6cb7617e/macrophage_input_data;tab=objects?prefix=&forceOnObjectsSortingFiltering=false) within the code of the notebook.

*As mentioned prior, please contact Alex Shalek (shaleklab@shaleklab.com) prior to using any data in scientific publications. 

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

#### Notebooks

This workspace contains two Notebooks, named `Analysis_Setup.ipynb`** and  `Macrophage_MITMilkStudy.ipynb`*** - accessible from the Notebooks tab. Clicking into each notebook will open a Preview page and cues to “Create a Runtime Environment” from which various required and optional definitions can be set up prior to starting interactive analyses in the notebook. 

Outlined below are general steps to start a notebook runtime environment with default settings - full details on customizing can be found here:
Click on the notebook to reveal the Preview page.
Click on “Cloud Environment” from the top right-hand corner widget to view default and custom options for creating a runtime environment.
Click “Create”.
Once the “Creating” status changes to “Running”, press “Edit” on the notebook.

Once your runtime is created and the notebook has been launched, execute the cells of code with `Shift+Enter`. This website offers more documentation on understanding Notebooks architecture in the cloud as well as additional information on navigating Terra specific features (detachable disk, custom and project environments, etc).

** `Analysis_Setup.ipynb`

This Notebook installs all packages needed for performing a variety of analyses with this data, including the R packages Seurat and PROGENy.

*** `Macrophage_MITMilkStudy.ipynb`

This Notebook imports a portion of the MIT Milk Study data comprised of macrophages. The Notebook performs all analyses, including sub-clustering and marker selection, for reproduction of a figure summarizing our macrophage results.

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

#### License + Contact Information**

Copyright Broad Institute, 2020 | BSD-3
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at https://github.com/openwdl/wdl/blob/master/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.

To cite this workspace/notebooks:

			Goods, B. A. (2020, December 16) MIT-MILK-Study, https://app.terra.bio/#workspaces/terra-outreach/MIT-Milk-Study

For general questions about getting started and working in Terra, please contact:

support@terra.bio, post on our [Community Forum](https://support.terra.bio/hc/en-us/community/topics), or use our Contact Us in-app feature (accessible from the main menu).

For questions that are specific to the algorithm or science, please contact:

			Alex Shalek (shaleklab@shaleklab.com)",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","terra-outreach/MIT-Milk-Study"
42,"landmarkanvil2","MRC GWAS Ecosystem Explorations","WRITER","https://app.terra.bio/#workspaces/landmarkanvil2/MRC%20GWAS%20Ecosystem%20Explorations",TRUE,FALSE,NA,NA,NA,"This workspace is devised to start some explorations of a large collection of
GWAS summary statistics.  The following schematic is from the [gwasglue repo](https://github.com/MRCIEU/gwasglue).  See below for
considerations on how to use AnVIL to explore this ecosystem; file questions at the [gwaslake issues repo](https://github.com/vjcitn/gwaslake/issues).

![gwasglue schematic](https://storage.googleapis.com/bioc-anvil-images/mrcieuglue.png)

The custom cloud environment based on the dockerhub container image `vjcitn/gwlakeanvil:0.0.3` provides most of the packages
required to explore this infrastructure; we are still working on the `gwasglue` and associated packages for Mendelian randomization
and colocalization (as of 2021-01-25).

Here are some illustrations of workspace functionality:

### Setup; elementary checks

After
```
library(gwaslake)
library(ieugwasr)
```
we can make the fundamental query to learn about contents of the ecosystem:
```
API: public: http://gwas-api.mrcieu.ac.uk/
> gwi = gwasinfo()
> gwi
# A tibble: 34,513 x 28
   id    trait coverage imputation_panel group_name  year    mr author consortium sex   qc_prior_to_upl…    pmid population sample_size
   <chr> <chr> <chr>    <chr>            <chr>      <int> <int> <chr>  <chr>      <chr> <chr>              <int> <chr>            <int>
 1 ieu-… Alco… whole g… HRC              public      2019     1 Liu, M GWAS and … Male… ""mapped rsids t…  3.06e7 European        335394
 2 ieu-… Urin… whole g… NA               public      2020     1 Zanet… NA         Male…  NA               3.20e7 European        326938
 3 eqtl… ENSG… NA       NA               public      2018     1 Vosa U NA         Male…  NA              NA      European          4677
 4 eqtl… ENSG… NA       NA               public      2018     1 Vosa U NA         Male…  NA              NA      European         30935
 5 eqtl… ENSG… NA       NA               public      2018     1 Vosa U NA         Male…  NA              NA      European         31684
 6 eqtl… ENSG… NA       NA               public      2018     1 Vosa U NA         Male…  NA              NA      European         31470
 7 eqtl… ENSG… NA       NA               public      2018     1 Vosa U NA         Male…  NA              NA      European          4169
 8 prot… PDGF… NA       NA               public      2019     1 Suhre… NA         Male…  NA               2.82e7 European            NA
 9 ubm-… NET1… NA       NA               public      2018     1 Ellio… NA         Males  NA               3.03e7 European          7916
10 prot… PDZ … NA       NA               public      2018     1 Sun BB NA         Male…  NA               2.99e7 European          3301
# … with 34,503 more rows, and 14 more variables: nsnp <int>, build <chr>, study_design <chr>, covariates <chr>, category <chr>,
#   subcategory <chr>, doi <chr>, note <chr>, priority <int>, unit <chr>, ontology <chr>, sd <dbl>, ncase <int>, ncontrol <int>
```
Additional demonstrations and interactive tables that show how to query about diseases and eQTLs, and obtain PheWAS results,
are available at the [gwaslake pkgdown site](https://vjcitn.github.io/gwaslake/articles/gwaslake.html).

### Running the gwaslake vignette

The easiest way to obtain, run, and expand upon the gwaslake vignette is to create an RStudio project, cloning the
github repository `vjcitn/gwaslake` through the `version control` project creation option.

### Related papers from MRC

Publications related to the MRC infrastructure include their [OpenGWAS Biorxiv paper](https://doi.org/10.1101/2020.08.10.244293) and a
recent [Genome Biology paper on GWAS-VCF](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-02248-0).

### Caveats

The R packages mentioned in the figure above are all available for installation.  However, many of them are not in regulated
repository collections like CRAN or Bioconductor, and some include obsolete dependency specifications.  Thus forks
of some of these packages were necessary to establish the container that could function as an AnVIL Cloud Environment.",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","landmarkanvil2/MRC GWAS Ecosystem Explorations"
43,"broad-firecloud-tcga","TCGA_SKCM_OpenAccess_V1-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_SKCM_OpenAccess_V1-0_DATA",TRUE,TRUE,"Skin Cutaneous Melanoma","Tumor/Normal","USA","TCGA Skin Cutaneous Melanoma Open-Access Data Workspace","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients’ clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","470","Skin","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh37/hg19","TCGA_SKCM_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_SKCM_OpenAccess_V1-0_DATA"
44,"broad-firecloud-tcga","TCGA_PRAD_hg38_OpenAccess_GDCDR-12-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_PRAD_hg38_OpenAccess_GDCDR-12-0_DATA",TRUE,TRUE,"Prostate adenocarcinoma","Tumor/Normal","USA","TCGA Prostate adenocarcinoma Open-Access Data Workspace

*hg38 TCGA and TARGET workspaces reference files by their GDC UUIDs.  In order to run analyses on the referenced data files, you will need to run workflows that retrieve the files from the GDC and copy them to your workspace bucket.  See   [this forum post](https://gatkforums.broadinstitute.org/firecloud/discussion/10382/populating-hg38-tcga-and-target-workspaces-with-data-files#latest) for instructions on the running of these workflows.*","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients' clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","500","Prostate","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh38/hg38","TCGA_PRAD_hg38_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_PRAD_hg38_OpenAccess_GDCDR-12-0_DATA"
45,"biodata-catalyst","BDC-PIC-SURE API RStudio Examples","READER","https://app.terra.bio/#workspaces/biodata-catalyst/BDC-PIC-SURE%20API%20RStudio%20Examples",TRUE,FALSE,NA,NA,NA,"# NHLBI BioData Catalyst® (BDC) Powered by PIC-SURE RStudio API examples

This workspace contains Jupyter Notebook examples of BDC-PIC-SURE API use cases, using BDC studies. The PIC-SURE API is available in two languages: R and python. This workspace features the RStudio PIC-SURE API and requires R 3.4 or higher.


## PIC-SURE API Overview
The main goal of the PICSURE API is to provide a simple and reliable way to work with data from studies that are part of BDC. Each individual study is accessible in a unique, easy to use, tabular format directly in an R or python environment. The API allows users to filter studies based on user-specified criteria, as well as to retrieve a cohort that has been created using the [PIC-SURE interface](https://picsure.biodatacatalyst.nhlbi.nih.gov). In addition to many heart, lung, blood, and sleep related datasets, there are 43 specific phenotype variables that have been harmonized across multiple TOPMed studies that are also accessible directly through the PIC-SURE API. 


## Workspace information
- Requirement : R 3.4 or higher. To select the appropriate runtime environment for your Terra Workspace, click on the gear wheel beside 'Cloud Environment' in the top right corner, and under Application Configuration select “Default: (GATK 4.1.4.1, Python 3.7.10, R 4.0.5)” or another appropriate configuration.
- Notebooks update information: the central repository for these notebooks is available on the [Access to Data using PIC-SURE API GitHub](https://github.com/hms-dbmi/Access-to-Data-using-PIC-SURE-API/tree/master/NHLBI_BioData_Catalyst).  Currently under active development, the repository is updated on a regular basis. Although the Terra public Workspace will be kept up-to-date as much as possible, there might be a difference between the version of the notebook you're using and the most recent one. So if you ran into an unexpected issue when running one of these example notebooks, it may be worth checking for a potential more up-to-date version available on GitHub.


## Available notebooks
The following example notebooks are available: 
  - Workspace_setup.ipynb: a notebook that helps to set up your workspace to use the PIC-SURE API. This includes saving your personal access token from PIC-SURE.
  - 0_Export_from_UI.ipynb: an interactive tutorial on how to export a dataframe built in the UI into your Terra workspace.
  - 1_PICSURE-API_101.ipynb: an illustration of the main functionalities of the PIC-SURE API.
  - 2_TOPMed_DCC_Harmonized_Variables_analysis.ipynb: an example of how to access and work with the ""harmonized variables"" across the TOPMed studies.
  - 3_PheWAS.ipynb: a straightforward PIC-SURE API use-case, using a PheWAS (Phenome-Wide Association Study) analysis as an illustration example.
  - 4_Genomic_Queries.ipynb: an illustration of how to use genomic variables to build queries.
  - 5_LongitudinalData.ipynb: an example of how to select longitudinal variables from PIC-SURE.
  - 6_Sickle_Cell.ipynb: an example illustrating how to select and view data related to Sickle Cell Disease (specifically the HCT for SCD dataset).
  - 7_Harmonization_with_PICSURE.ipynb: examples of harmonization across studies using (1) sex and BMI variables (including TOPMed Harmonized dataset and others) and (2) orthopnea and pneumonia variables.
  - ORCHID_COVID19_python.ipynb: the code accompanying the JAMA publication on November 7th 2021: ""[Effect of Hydroxychloroquine on Clinical Status at 14 Days in Hospitalized Patients With COVID-19](https://jamanetwork.com/journals/jama/fullarticle/2772922)"" 


## Data information
The data accessible through the PIC-SURE API are controlled-access data. Hence, if you're saving any data from the Jupyter notebooks directly into the Google Bucket associated with this workspace, you will need to set an Authorization Domain to protect this data. Instructions about how to proceed can be found here:

https://support.terra.bio/hc/en-us/articles/360039415171-Authorization-Domain-overview-for-BioData-Catalyst-users

## Contact
For bug report or additional information, please contact us using the [BioData Catalyst contact form](https://biodatacatalyst.nhlbi.nih.gov/contact/).

",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","biodata-catalyst/BDC-PIC-SURE API RStudio Examples"
46,"Stevenslab","public_iMGLdatasets","READER","https://app.terra.bio/#workspaces/Stevenslab/public_iMGLdatasets",TRUE,FALSE,NA,NA,NA,"Included in this workspace are the new iMGL  datasets generated and described in the manuscript:  ""A resource for generating and manipulating human microglial states in vitro"",
doi: https://doi.org/10.1101/2022.05.02.490100. Detailed descriptions of the experimental and computational methods used can be found in the methods section of this paper. 

To view data, go to DATA>Files. Files can be downloaded for free using Terminal download command.

**Note**: This has been updated in July 2024 with Seurat objects and csv files containing metadata for Figure 1 and Figure 6. These are detailed in the manifest below. 


**Data Manifest:**

**Single cell RNA-sequencing iMGL datasets**

iMGLs differentiated from H1 and exposed to multiple substrates and lentivirus are in the following files: ""H1-matrix.mtx"", ""H1-genes.tsv"" and ""H1-barcodes.tsv"". See Figures 1 and 4 of paper for more details. The final version of this data is available as a LIGER object file ""H1_liger_revision.rds"" (Welch et al. 2019, PMID: 31178122, version 0.5) and a Seurat object (Version 4) and iMGLs_depthnorm_liger_seurat_2024.rds. The metadata used in Figure 1 are contained in iMGL_Figure1_metadata2024.csv. 

For iMGLs differentiated from iPSC, Figure 6, a list of expression matrices for each line and condition is provided in file: ""iPSC_all_revision.rds"". This also includes H1 data above and is a LIGER object file (Welch et al. 2019, PMID: 31178122, version 0.5). We also include this data as a Seurat object (Version 4) called iPSC_H1_imgl_LIGER_seurat_2024.rds. The metadata used in Figure 6 is iPSC_H1_Integration_metadata2024.xlsx. 

**Single cell RNA-sequencing human datasets**

File ""finmg_cellbender_annot_revision.rds"" contains summary gene expression matrix for microglia from Gazestani*, Kamath* et al. in preparation. Note that this file is a LIGER object (Welch et al. 2019, PMID: 31178122, 0.5).

Further details about this dataset: We performed single-nuclei RNA-sequencing on 51 high-quality frontal cortex biopsies (Brodmann Area 8 or 9) from individuals with suspected idiopathic normal pressure hydrocephalus. Using our optimized protocol for nuclei extraction (PMID: 34616064), we sampled approximately 1.5 million nuclei, including approximately 54,475 microglia profiles (based on expression of key marker genes including CX3CR1 and P2RY12), from the human brain identifying 91 cell types, unperturbed by death or agonal state. A Seurat object (Version 4) with the same data is called Fin.MG_Vahid_Final_seurat_2024.rds. 


**ATAC-seq iMGL datasets**

file ATAC_iMGL_apoptoticneurons contains information for all samples regarding ATAC-seq study including bam files, bai files and bed files information with exact conditions. In additional, all bam, bai and bed files can be found in this workspace.
				

		iMGL treated with PBS	   replicate 1	   segmentation_054228.bed	            aggregated_aln_029160.bam
		iMGL treated with PBS       replicate 2       segmentation_054230.bed	        aggregated_aln_029163.bam
		
		iMGL treated with Apoptotic neurons	  replicate 2  segmentation_054230.bed      aggregated_aln_029161.bam
		iMGL treated with Apoptotic neurons      replicate 1  segmentation_054222.bed           aggregated_aln_029157	  
		
	
**Bulk RNA-sequencing iMGL datasets**

file RNAseqiMGLMITFexperiment contains the gene expression matrix count for iMGL samples overexpressing MITF or mCherry and name. conditions and replicate information. ",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","Stevenslab/public_iMGLdatasets"
47,"mccarroll-genomestrip-terra","C4AB_Analysis","READER","https://app.terra.bio/#workspaces/mccarroll-genomestrip-terra/C4AB_Analysis",TRUE,FALSE,NA,NA,NA,"# Showcase workspace for GenomeSTRiP *C4* A/B analysis on the 1000 Genomes WGS data set

## Background

The complement component 4 (*C4*) gene is part of the complement pathway in the human innate immune system. Variation in *C4* has been associated with risk for schizophrenia¹, systemic lupus erythematosus (SLE or “lupus”)² and Sjögren’s syndrome².

In humans, the *C4* gene is copy-number variable and exists in a variety of allelic forms. Humans typically have between 2 and 8 total copies of the *C4* gene (as opposed to two copies for most genes). Each of these copies may be one of the two forms called *C4A* or *C4B* which are distinguished by a sequence of 6 amino acids at an active binding site. In addition, each copy of *C4* may be either “long” or “short” as determine by the presence or absence of a endogenous human retroviral DNA sequence (HERV) that is present within intron 9 of some copies of the gene. These different numbers and forms of *C4* exist on a large variety of different structural haplotypes at the *C4* locus on chromosome 6 in the HLA region¹.

Because of the complexity of this region and the high degree of variability in humans, standard automated analysis pipelines generally do not provide full, accurate characterization of the *C4* genotypes in each individual. This workflow contains a custom analysis pipeline, built on top of the Genome STRiP software³, to analyze *C4* from whole-genome sequencing data. This analysis pipeline is the basis for our continuing research on *C4* and its association with schizophrenia and other diseases.

This demonstration workspace is set up to be able to run the *C4* analysis workflow on the 1000 Genomes⁴ high-coverage WGS data, available from the NHGRI AnVIL⁵. The workflow is designed to run in batches of approximately 100 samples each. For this demonstration workspace, we have set up 26 batches (sample sets) corresponding to each of the 1000 Genomes populations. Running the workflow on one population takes less than an hour and typically costs one or two cents.

## Workspace Data

The data used in this demonstration workspace is high-coverage whole-genome sequencing from the 1000 Genomes project. The underlying cram files are stored in the AnVIL.

## Running the Workflow

The gs_analyze_C4AB_batch workflow operates on a sample set. To run the workflow, select one of the sample sets corresponding to a 1000 Genomes population from the Terra user interface on which to run. These sample sets are named md_batchN. The workflow is designed to run on sample sets of approximately 100 to 200 samples. The workflow will not run on the sample set that represents the entire cohort (“1000G-high-coverage-2019-all”) without modifications to increase the resources used by the workflow.

Typical runtime is around 45 minutes and the typical cost is one or two cents.

The workflow will analyze the copy-number variable segments at the *C4* locus, including the copy number of the *C4* genes and the HERV segment. The workflow will also analyze the reads mapping to the *C4A*/*C4B* active site and estimate the number of copies of *C4A* and *C4B* in each sample.

In this demonstration workspace, we use precomputed Genome STRiP metadata for the 1000 Genomes project (which we host in a bucket on Google cloud storage). To run this workflow on your own data set, you will first need to run standard Genome STRiP preprocessing and group your samples into analysis batches of approximately 100 samples each. See the Genome STRiP documentation for more details.

## Workflow Output

The workflow produces a .zip file with multiple output files. The output files produced are:

### C4_hg38.C4_table.txt.gz

This is the primary output file. It is a tab delimited summary file with the following columns:

| Column | Description |
| :----- | :---------- |
| SAMPLE |	Sample identifier (from the input cram file)
| C4 |		Total C4 copy number (integer value)
| CNF_C4 |	Internal fractional estimate of C4 copy number
| CNQ_C4 |	Phred-scaled quality value for the C4 copy number estimate
| HERV |	Total HERV copy number (integer value)
| CNF_HERV |	Internal fractional estimate of HERV copy number
| CNQ_HERV |	Phred-scaled quality value for the HERV copy number estimate
| C4A |    	Estimate of total number of C4A genes
| C4B |		Estimate of total number of C4B genes
| C4A1 |	Estimate of total number of non-classical C4AB gene of type C4A1
| C4A2 |	Estimate of total number of non-classical C4AB gene of type C4A2
| C4B1 |	Estimate of total number of non-classical C4AB gene of type C4B1
| C4R1 |	Estimate of total number of non-classical C4AB gene of type C4R1
| AB_QUAL |	Phred-scaled quality value for the C4A/C4B copy number estimates
| COUNTS |	The number of reads supporting each C4A/C4B estimate (comma separated).

### C4_hg38.genotypes.vcf.gz

This is a VCF file containing the copy number estimates for the *C4* gene and the HERV segments, the VCF is generated using Genome STRiP genotyping of two CNV segments: CNV_C4_SEGABC_M2 gives the total *C4* copy number (without the HERV) and CNV_HERV_M2 gives the total HERV copy number.

### C4_hg38.genotyping_plots.pdf

This PDF file contains plots of the copy number distributions of the *C4* and HERV segments, based on the information in the vcf file.

### C4_hg38.kmer_analysis_table.txt.gz

This tab-delimited file contains a list of the kmers observed at the *C4A*/*C4B* active site. If the kmer is present in the known_kmers file, it will be identified based on the information in that file. Novel kmers will also be included in this file, although many kmers may also be listed that are caused by occasional sequence errors, not true genetic variation. See the section on novel *C4* A/B alleles for more details.

### C4_hg38.known_kmers.txt
This tab-delimited file describes the known / expected sequence variation at the *C4A*/*C4B* active site. If you encounter a novel *C4* A/B allele in your data set, you can provide an updated known_kmers file to analyze the unique allele. See the section on novel *C4* A/B alleles for more details.

### C4_hg38.kmers.txt.gz
This tab-delimited file lists all of the kmers observed at the *C4A*/*C4B* active site and includes information about the sequencing read and sample where each kmer was observed. This file is mainly useful for detailed assessment or validation of novel sequence variation at the active site.

## Classical and Non-classical *C4* A/B Alleles

There are two main forms of the *C4* gene, *C4A* and *C4B*, which are distinguished by 4 amino acid substitutions are a 4 amino acid active site on exon 26. As we study the *C4* gene in larger and more diverse populations, however, we observe other (rare) DNA variation at the active site that creates copies of the *C4* gene that are neither the classical A or B alleles.

To date, we have characterized four novel alleles of *C4* that are segregating in the human population. Three of these alleles are present in the 1000 Genomes samples. In the 1000 Genomes cohort, each individual that carries a non-classical *C4* allele carries that allele in only one of their copies of *C4* and the individual has other copies of *C4* with the classical A/B alleles as well.

The DNA sequences for the (known) non-classical alleles are given in the file C4_hg38.known_kmers.txt (see the workflow output section). The alleles named C4A1 and C4A2 are closer to the classical *C4A* allele (one amino acid change) while the allele named C4B1 is closer to the classical *C4B* allele. The C4R1 allele is (at the sequence level) a combination of *C4A* and *C4B*.

## Novel *C4* A/B Alleles

This workflow does automated analysis of the known *C4* A/B alleles listed in the known_alleles input file. In addition, the workflow characterizes all observed kmers at the active site to facilitate the identification of novel alleles.

The output file C4_hg38.kmer_analysis_table.txt.gz lists all kmers observed at the active site. If the kmer is present in the known kmers file, it will be labeled with the identifier from the known kmers file. Other kmers are labeled as NA. Most of these unknown kmers will be due to occasional sequencing errors and do not represent true genetic variation.

The workflow runs an analysis of all observed kmers to determine whether they are likely to result from true genetic variation (as opposed to sequencing error). True genetic variation is modeled as a non-random occurrence of a kmer in a subset of samples compared to a null model where observations of a kmer are from a Poisson-distributed model with a sequencing error rate estimated from the observed data. The results of this analysis are listed in the kmer_analysis_table file, which has the following columns:

| Column | Description |
| :----- | :---------- |
| KMER |	The DNA sequence at the active site
| LABEL |	The label from the known_kmers file, or NA if this is a novel kmer
| COUNT |	The number of reads where this kmers was observed (across all samples)
| ERATE	|	The estimated sequencing error rate at this base position
| MAXLOD |	The LOD score from the sample with the highest LOD score for this kmer
| DIST |	The distribution of COUNT for the top 10 samples with the highest counts

The table is sorted in decreasing order of MAXLOD. The MAXLOD score is the estimate of whether this kmer represents a true novel allele. In most cases, the classical *C4A* and *C4B* alleles will appear at the top of the table as confidently assigned true alleles. A row with a negative MAXLOD (or a small positive MAXLOD) is most likely due to sequencing error. A true allele would be observed multiple times (depending on sequencing depth) in a small number of samples. If you discover a novel *C4* allele, please contact us so that we may add it to the known allele table.

If you discover a novel allele in your cohort, you can add this allele to the known_alleles file (it is an optional workflow input) and rerun the analysis. This will add the novel allele to the C4_table and generate genotypes based on the novel allele. The genotype calculations in the C4_table assume a fixed set of true alleles, defined by the known_alleles file.

## Pre-computed Output

If you just want pre-computed output from running the pipeline on the 1000 Genomes cohort, it is available in the Files/PipelineResults directory under the Data tab in the workspace.

## References

1. Sekar, A. et al. Schizophrenia risk from complex variation of complement component 4. Nature 530, 177–183 (2016)

2. Kamitaki, N. et al. Complement component 4 genes contribute sex-specific vulnerability in diverse illnesses, Biorxiv (2019)

3. Handsaker, R. E. et al. Large multiallelic copy number variations in humans. Nat Genet 47, 296–303, doi:10.1038/ng.3200 (2015)

4. The 1000 Genomes Project Consortium. A global reference for human genetic variation. Nature 526, 68-74, doi:10.1038/nature15393 (2015)

5. National Human Genome Research Institute, www.anvilproject.org

",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","mccarroll-genomestrip-terra/C4AB_Analysis"
48,"manning-lab-2021","Template_TOPMed_Freeze_8_rare_variant_analysis_with_in_BioData_Catalyst","READER","https://app.terra.bio/#workspaces/manning-lab-2021/Template_TOPMed_Freeze_8_rare_variant_analysis_with_in_BioData_Catalyst",TRUE,FALSE,NA,NA,NA,"# Template TOPMed Freeze 8 rare variant analysis with Blood Pressure Trait in NHLBI's BioData Catalyst

This template workspace was created to offer example tools for conducting a rare variant, mixed-models GWAS focusing on a blood pressure trait from start to finish using the [NHLBI BioData Catalyst](https://biodatacatalyst.nhlbi.nih.gov/) ecosystem.

## Data Model

This template was set up to work with the *NHLBI BioData Catalyst Powered by Gen3* data model. In this dashboard, you will learn how to import genetic data files from the Gen3 platform into this Terra template and conduct an association test.

### TOPMed Data

All the genetic files used in this tutorial are from the freeze 8 release available for TOPMed Investigators who have access to TOPMed data.

If you already have access to a TOPMEd project and have been onboarded to the BioData Catalyst platform, you should be able to use your data with this template workspace. We focused this template on analyzing a blood pressure trait, but not all TOPMed projects may contain blood pressure data. You will need to carefully consider how to update this analysis for the dataset you bring and how this may affect the scientific accuracy of the question you are asking. 

## Outline of this template

***Part 1: Navigate the BioData Catalyst environment and import analysis files***

Learn how to search and export data files from *BioData Catalyst Powered by Gen3* and workflows from Dockstore into a Terra workspace. Each cloud-based platform interoperates with one another for fast and secure research.  The template we have created here can be cloned for you to walk through as suggested, or you can use the basics you learn here to perform your own analysis. 

***Part 2: Set up data model for rare variant analysis using Jupiter Notebooks***

First, you will use the notebook **1_explore_reference_data_set_up_analysis.ipynb** to load  `R/BioConductor AnVIL ` and use the `avtables()` function to explore the workspace data model. You will use the `avtable()` function to create an R data frame containing the contents of the `reference_file` data table exported from *BioData Catalyst Powered by Gen3*. You will then create a new data table called `analysis_gds_file` with the columns: chromosome, file name, DRS URI [Documentation about Interoperable data (GA4GH DRS URIs) used in BioData Catalyst](https://support.terra.bio/hc/en-us/articles/360039330211-Overview-Interoperable-data-GA4GH-DRS-URIs-). Next, you will explore annotation files for rare variant analysis and add column to `analysis_gds_files`. To further prepare for analysis, we will add the Principal Component and Kinship Matrix files to the Workspace data attributes.

Documentation of these files can be found on the [TOPMed website](https://topmed.nhlbi.nih.gov/genetic)

Next, we will review the format of the trait data.


***Part 3: Perform mixed-model association tests using workflows***

Next, we will perform mixed models genetic association tests using the genetic, annotation, kinship, principal component and trait files set up in Part 2.). For details on the four workflows and what they do, scroll down to **Perform mixed-model association test workflows**.

The workflows are publicly available in [Dockstore](https://dockstore.org/) in this [collection](https://dockstore.org/organizations/bdcatalyst/collections/GWAS).       

Mixed models require two steps within the [GENESIS](https://bioconductor.org/packages/release/bioc/html/GENESIS.html) package in R:     

1) Fitting a null model assuming that each genetic variant has no effect on phenotype. For this, we use the `null-model-wdl` workflow.
2) Performing genetic association tests using the fitted null model. For this, we use the `assoc_agg_one_gds` workflow.
    
-----

# Part 1: Navigate the NHLBI BioData Catalyst multi-platform ecosystem

## 1a) Link your Terra account to external services
Before you're able to access genomic data from Gen3 in the Terra data table,  you need to link your Terra account to external services. Link your profile [by following these instructions](https://support.terra.bio/hc/en-us/articles/360038086332?flash_digest=2f492682b688b21da27c701af68656ac095d5803).

## 1b) Create an Authorization Domain to protect your controlled-access data

Because this workspace was created to be used with controlled access data, it should be registered under an Authorization Domain that limits its access to only researchers with the appropriate approvals. Learn how to set up an Authorization Domain [here](https://support.terra.bio/hc/en-us/articles/360039415171) before proceeding.

## 1c) Clone template workspace and prepare phenotype file

## 1d) Export genetic data files

The files we use in the GENESIS pipeline are:

 - Genetic data in gds format
 - a sparse kinship matrix to account for family structures within the cohorts
 - genetically derived principal components
 - annotation files for filtering variants used in the rare variant association tests.

Steps:

 1. Start by learning about Gen3's graph-structured data model for NHLBI's BioData Catalyst using this [orientation document](https://support.terra.bio/hc/en-us/articles/360038087312).
 2. Once you better understand the graph, log into [Gen3](https://gen3.biodatacatalyst.nhlbi.nih.gov/) through the NIH portal using your eRA Commons username and password. 
 3. Navigate to the [Exploration](https://gen3.biodatacatalyst.nhlbi.nih.gov/explorer) view to see what datasets you currently have and do not have access to. The focus of this tutorial is the `File` explorer.
 4. Genetic data in gds format
  - Use the filter tools to select this data:
    - Program: TOPMed_Common_Exchange_Area
    - Project ID: TOPMed_Common_Exchange_Area-Freeze_8
    - Data Format: GDS
    - Bucket Path: minDP0.gds

This results in 23 gds files, one file per chromosome.

Once selected, click the button ""Export all to Terra"", wait until the Terra window appears, and add your data to your copy of this template workspace. 

5. Kinship matrix and Principal Components
 - Use the filter tools to select this data:
   - Program: TOPMed_Common_Exchange_Area
   - Project ID: TOPMed_Common_Exchange_Area-Freeze_8
   - Data Type: Principal Component, Kinship Matrix

This results in 3 files.

Once selected, click the button ""Export all to Terra"", wait until the Terra window appears, and add your data to your copy of this template workspace. 

6. Annotation files
   - Program: TOPMed_Common_Exchange_Area
   - Project ID: TOPMed_Common_Exchange_Area-Freeze_8
   - Data Category: Other
   - Data Format: RData
   - Bucket Path: var_grouping
   
This will result in 299 files.
   
Once selected, click the button ""Export all to Terra"", wait until the Terra window appears, and add your data to your copy of this template workspace. 


-----

# Part 2: Set up data model for rare variant analysis using Jupyter Notebooks  

The files needed for the analysis are in the `reference_file` data table in the Data Model.


## 2a) Start a cloud environment

The default (GATK 4.2.4.0, Python 3.7.12, R 4.1.3) environment is sufficient.

## 2a) Explore data model and set up `analysis_gds_file` data table

The R Notebook *1_explore_reference_data_set_up_analysis.ipynb* contains these steps:

 1. Use the avtables() function to explore the workspace data model. 
 2. Use the avtable() function to create an R data frame containing the contents of the reference_file data table exported from BioData Catalyst Powered by Gen3. 
 3. Create a new data table called `analysis_gds_file` with the columns: chromosome, file name, DRS URI
 4. Explore annotation files for rare variant analysis
 5. Add the DRS URIs to some annotation files as a column to the `analysis_gds_files.`
 6. Add the DRS URIs to the Principal Component and Kinship Matrix files to the Workspace data.

We will use the DRS URIs in an interactive analysis. More information can be found [here](https://support.terra.bio/hc/en-us/articles/6635247495579-How-to-access-data-with-DRS-URIs), and [Documentation about Interoperable data (GA4GH DRS URIs) used in BioData Catalyst](https://support.terra.bio/hc/en-us/articles/360039330211-Overview-Interoperable-data-GA4GH-DRS-URIs-). Examples can be found in the 'Accessing GA4GH DRS URI data using terra-notebook-utils in R' notebook within the [BioData Catalyst Collection](https://terra.biodatacatalyst.nhlbi.nih.gov/#workspaces/biodata-catalyst/BioData%20Catalyst%20Collection) Workspace.

Documentation of the freeze 8 annotation files can be found on the [Genetic Data](WGSA variant annotation and related resources) page on the TOPMed web site.
 
## 2b) Prepare phenotype data

Outcome:
```
outcome <- ""diastolic.blood.pressure""
covariates <- c(""age"",""sex"",""antihypertensive.medication"",""race"",""Hispanic.or.Latino.ethnicity"")
```

Phenotype file in RData format:

```
library(Biobase)
library(AnVIL)

project <- gcloud_project()
workspace <- avworkspace_name()
bucket <- avbucket()

cat(paste(""Google project ID:"",project,""\nworkspace:"", workspace,""\nbucket:"",bucket))

tmp <- as(in.phe,""AnnotatedDataFrame"")
phenotypes.RData.file <- paste0(""phenotypes_for_analysis.RData"")

save(tmp,file=phenotypes.RData.file)
gsutil_cp(phenotypes.RData.file,paste0(bucket,""/""))
```

Update the workspace attributes with the analysis details:
```
workspace.attributes.to.upload <- data.frame(rbind(
    c(type=""other"",table=""workspace"",key=""phenotype_file"",value=paste0(bucket,""/"",phenotypes.RData.file)),
    c(type=""other"",table=""workspace"",key=""outcome"",value=""diastolic.blood.pressure""),
    c(type=""other"",table=""workspace"",key=""covariates"",value=paste(c(""age"",""sex"",""antihypertensive.medication"",""race"",""Hispanic.or.Latino.ethnicity""),collapse="",""))
    ))
workspace.attributes.to.upload

avdata_import(workspace.attributes.to.upload)

```



-----

# Part 3: Perform mixed-model association tests using workflows

In Part 2, we explored the data we imported from Gen3 and performed a few important steps for preparing our data for association testing. These files will be used in our batch workflows that will perform the association tests. Below, we describe the four workflows in this workspace and their cost estimates for running on the sample set we create in this tutorial.  

The workflows used in this template were imported from [Dockstore](https://www.dockstore.org) and their parameters were configured to work with Terra's data model.  If you're interested in searching other Docker-based workflows, [learn more about how they can easily be launched in Terra](https://support.terra.bio/hc/en-us/articles/360038137292).

Documentation is from the [TOPMed Analysis Pipeline](https://github.com/UW-GAC/analysis_pipeline/blob/master/README.md)

## Mixed model association tests with GENESIS

Association tests are done with a mixed model if a kinship matrix or GRM (`relatedness_matrix_file`) is used. If `relatedness_matrix_file` is `NA` or missing, testing is done with a fixed effects model.


## Workflow `null-model-wdl` performs the Null Model step
When combining samples from groups with different variances for a trait (e.g., study or ancestry group), it is recommended to allow the null model to fit heterogeneous variances by group using the parameter `group_var`. The default pipeline options will then result in the following procedure:

Workflow Steps:
1. Fit null mixed model with outcome variable
    - Allow heterogeneous variance by `group_var`
    - Include covariates and PCs as fixed effects
    - Include kinship as random effect
2. Inverse normal transform marginal residuals (if `inverse_normal = TRUE`)
3. Rescale variance to match original (if `rescale_variance = ""marginal""` or `""varcomp""`)
4. Fit null mixed model using transformed residuals as outcome
    - Allow heterogeneous variance by `group_var`
    - Include covariates and PCs as fixed effects
    - Include kinship as random effect

Required workflow parameters:

config parameter | default value | description
--- | --- | ---
`phenotype_file` | | RData file with AnnotatedDataFrame of phenotypes.
`outcome` | | Name of column in `phenotype_file` containing outcome variable.
`family` | `gaussian` | The error distribution to be used in the model. Allowed values are `gaussian` (continuous outcome), `binomial` (binary or case/control outcome), or `poisson` (count outcome).


Additional workflow parameters:

config parameter | default value | description
--- | --- | ---
`out_prefix` | | Prefix for files created by this script.
`pca_file` | `NA` | RData file with PCA results created by `pcair.py`. A matrix or data.frame is also accepted, as long as the rownames contain sample.id.
`relatedness_matrix_file` | `NA` | RData or GDS file with a kinship matrix or GRM.
`family` | `gaussian` | The error distribution to be used in the model. Allowed values are `gaussian` (continuous outcome), `binomial` (binary or case/control outcome), or `poisson` (count outcome).
`covars` | `NA` | Names of columns `phenotype_file` containing covariates, quoted and separated by spaces.
`group_var` | `NA` | Name of covariate to provide groupings for heterogeneous residual error variances in the mixed model.
`inverse_normal` | `TRUE` | `TRUE` if an inverse-normal transform should be applied to the outcome variable.
`norm_bygroup` | `FALSE` | If `TRUE` and `group_var` is provided, the inverse normal transform is done on each group separately.
`rescale_variance` | `marginal` | Applies only if `inverse_normal` is `TRUE`. Controls whether to rescale the variance after inverse-normal transform, restoring it to the original variance before the transform. Options are `marginal`, `varcomp`, or `none`.
`n_pcs` | `0` | Number of PCs to include as covariates.
`conditional_variant_file` | `NA` | RData file with data frame of of conditional variants. Columns should include `chromosome` (or `chr`) and `variant.id`. The alternate allele dosage of these variants will be included as covariates in the analysis.
`gds_file` | `NA` | GDS file. Include a space to insert chromosome. Required if `conditional_variant_file` is specified.
`sample_include_file` | `NA` | RData file with vector of sample.id to include.
`n_categories_boxplot` | `10` | If a covariate has fewer than the specified value, boxplots will be used instead of scatter plots for that covariate in the null model report.


## Workflow `assoc_agg_one_gds` performs the genetic association analysis

Parameter notes: The effect estimate is for the alternate alelle, and multiple alternate alelles for a single variant are treated separately.

Required Inputs:

config parameter | default value | description
--- | --- | ---
`gds_file` | | GDS file. Include a space to insert chromosome.
`null_model_file` | | RData file with null model from the null model step.
`phenotype_file` | | RData file with AnnotatedDataFrame of phenotypes. (Use the output phenotype file from the null model step)


Required Inputs specific to rare variant analysis:

config parameter | default value | description
--- | --- | ---
`aggregate_type` | `allele` | Type of aggregate grouping. Options are to select variants by `allele` (unique variants) or `position` (regions of interest).
`variant_group_file` | | RData file with data frame defining aggregate groups. If `aggregate_type` is `allele`, columns should be `group_id`, `chr`, `pos`, `ref`, `alt`. If `aggregate_type` is `position`, columns should be `group_id`, `chr`, `start`, `end`.
`group_id` | `group_id` | Alternate name for `group_id` column
`test` | `burden` | Test to perform. Options are `burden`, `skat`,  `smmat`, `fastskat`, or `skato`.


Other parameter inputs: 

config parameter | default value | description
--- | --- | ---
`alt_freq_max` | `1` | Maximum alternate allele frequency to consider.
`rho` | `0` | A numeric value (or quoted, space-delimited list of numeric values) in [0,1] specifying the rho parameter when `test` is `skat`. `0` is a standard SKAT test, `1` is a score burden test, and multiple values is a SKAT-O test.
`variant_weight_file` | `NA` | RData file with data frame defining variant weights. Columns should contain either `variant.id` or all of (`chr`, `pos`, `ref`, `alt`).
`weight_user` | `NA` | Name of column in `variant_weight_file` or `variant_group_file` (see aggregate test, below) containing the weight for each variant.
`weight_beta` | `""1 1""` | Parameters of the Beta distribution used to determine variant weights, quoted and space-delimited. `""1 1""` is flat weights, `""0.5 0.5""` is proportional to the Madsen-Browning weights, and `""1 25""` gives the Wu weights. This parameter is ignored if `weight_user` is provided.
`out_prefix` | | Prefix for files created by this script.
`pass_only` | `TRUE` | `TRUE` to select only variants with FILTER=PASS.
`genome_build` | `hg38` | Genome build for the genotypes in the GDS file (`hg19` or `hg38`). Used to divide the genome into segments for parallel processing and identify pseudoautosomal regions on the X chromosome.

Inputs:
* GDS genotype file
* Genetic Relatedness Matrix
* Trait outcome name
* Trait outcome type
* CSV file of covariate traits
* Sample ID list

Outputs:
* A null model as an RData file
* Compressed CSV file(s) containing raw results
* CSV file containing all associations
* CSV file containing top associations
* PNG file of Quantile-Quantile and Manhattan plots

-----

### Authors, contact information, and funding

This template was created for the [NHLBI's BioData Catalyst](https://biodatacatalyst.nhlbi.nih.gov/) project in collaboration with the [Computational Genomics Platform](https://cgpgenomics.ucsc.edu/) at [UCSC Genomics Institute](https://ucscgenomics.soe.ucsc.edu/) and the [Data Sciences Platform](https://www.broadinstitute.org/data-sciences-platform) at [The Broad Institute](https://www.broadinstitute.org/). The association analysis tools were contributed by the [Manning Lab](https://manning-lab.github.io/).

Contributing authors include:
* Alisa Manning (Manning Lab)
* [Beth Sheets](mailto:esheets@ucsc.edu) (UC Santa Cruz Genomics Institute)
* Michael Baumann (Broad Institute, Data Sciences Platform)
* Brian Hannafious (UC Santa Cruz Genomics Institute)
* Ash O'Farrell (UC Santa Cruz Genomics Institute)

----

### Workspace change log 

| Date | Change | Author | 
| -------  | -------- | -------- |
| August 10, 2022 | First release of workspace | Alisa |
|",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","manning-lab-2021/Template_TOPMed_Freeze_8_rare_variant_analysis_with_in_BioData_Catalyst"
49,"anvil-datastorage","AnVIL_Explorer_FSS_Tool","READER","https://app.terra.bio/#workspaces/anvil-datastorage/AnVIL_Explorer_FSS_Tool",TRUE,TRUE,NA,NA,NA,"## Background
Until recently, AnVIL data has been hosted and shared from Terra workspaces. Depending on the AnVIL dataset/study, the data tables in the workspace have varying schemas. In an effort to ingest all of the AnVIL datasets into the Terra Data Repository (TDR), the Broad's Data Sciences Platform created a common schema across all AnVIL datasets for the successful development of the AnVIL Explorer. The AnVIL Data Explorer will allow users to create a cohort of samples across multiple datasets/studies in a single location.

The current process of ingesting AnVIL workspaces into TDR starts with ingesting the workspace data, in its existing schema, into a TDR dataset. After the original workspace tables have been ingested into TDR, an additional step is performed to edit, modify, reorganize the data into a unified schema that we use across all the AnVIL datasets. Because of the large size of AnVIL data, the AnVIL Data Explorer is not able to store all information across all studies. Therefore, one notable change to the new tables is that they are a subset of columns and information from the original - values that are important for filtering and creating cohorts. The normalized subset of data that is accessible via the AnVIL Data Explorer is called the Findability Subset (FSS). 

When a user creates a cohort from the AnVIL Explorer, the data that is handed off to the workspace are those in the Findability Subset. Because the FSS is limited in what columns it includes, the data ingested into the notebook is incomplete, and a final step must be performed to recover the missing columns associated with the selected rows.

## What does the notebook do?
At point of hand-off, the user will see data tables with information that is representative of the Findability Subset. To retrieve the additional information, the non-Findability Subset (NFS), they must execute the notebook within the workspace to which they did their hand-off.

The notebook will loop through each of the hand-off tables, isolate pointers to the NFS data, query the original snapshot for full data, and re-create tables that are representative of the schema in the original Terra AnVIL workspaces.

The result of the notebook should be data tables that are organized the same as the original Terra AnVIL workspace as well as the normalized schema tables that were created for the purposes of the AnVIL Explorer - to allow for cross dataset/study search and filtering.

## How to get the notebook?
1. Navigate to the Analyses tab of this workspace.
2. Locate the get_non_findability_subset_data_v7.ipynb in the list and click the icon on the right.
3. From the menu that opens, select ""Copy to another workspace"" and select your destination workspace.
4. A copy of the notebook will appear in the selected destination workspace.

![whitespace](https://storage.cloud.google.com/terra-featured-workspaces/AnVIL/AnVIL_FSS_NB_Workspace_Dashboard_image8.png)

## How to run the notebook?
1. To start the notebook environment, click on the notebook in your workspace and press Open. When prompted with a pop-up window to select Environment Configuration options, leave all values as the default values and press Create.

![whitespace](https://storage.googleapis.com/terra-featured-workspaces/AnVIL/AnVIL_FSS_NB_Workspace_Dashboard_image9.png)

2. Once the environment is created, you will see a note along the top of your notebook and can press Open a second time to start the interactive environment.

![whitespace](https://storage.cloud.google.com/terra-featured-workspaces/AnVIL/AnVIL_FSS_NB_Workspace_Dashboard_image10.png)

3. Once the notebook environment is indicated to be running, click Cell → Run All (can be found along the top of the notebook).

![whitespace](https://storage.cloud.google.com/terra-featured-workspaces/AnVIL/AnVIL_FSS_NB_Workspace_Dashboard_image11.png)

4. The user should see stdout that displays the status of the notebook’s actions as it performs its NFS data extraction.

More detailed instructions on running the notebook are listed at the start of the Notebook.

## What are the current limitations of the notebook?
1. When a second cohort is exported, the notebook will do an extraction of all the data in the table - even if it has been extracted in a previous run of the notebook. This is a less efficient method and will be improved in future versions of the tool.
2. The resulting non Findability Subset tables will be a merge of different instances of the same table across different AnVIL datasets. For example, if your cohort contains data from different AnVIL datasets that all have a ""sample"" table but the with differring schema (less/more columns, column names differ, etc), your resulting ""sample"" table will be a superset of all the different schemas of the ""sample"" table.",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","anvil-datastorage/AnVIL_Explorer_FSS_Tool"
50,"terra-biom-mass","MLSC_Btb","READER","https://app.terra.bio/#workspaces/terra-biom-mass/MLSC_Btb",TRUE,FALSE,NA,NA,NA,"# MLSC Bits to Bytes

This is a workspace for the BIOM-Mass project containing the full set of multi'omic microbiome sequencing data from the MLSC Bits to Bytes project.

For access to the metadata, contact the [Nurses Health Study](https://nurseshealthstudy.org/contact).

**All data is hosted and access controlled by the [BIOM-Mass Portal](http://biom-mass.org/repository?searchTableTab=cases).  The BIOM-Mass Data Portal is provided by the <a href=""https://hcmph.sph.harvard.edu/"">Harvard Chan Microbiome in Public Health Center (HCMPH)</a> to manage and share microbiome profiles, sample, and population information from microbiome epidemiology studies carried out through the HCMPH <a href=""https://hcmph.sph.harvard.edu/resources/"">BIOM-Mass platform</a>.  Please <a href=""https://hcmph.sph.harvard.edu/contact/"">contact us</a> to request portal access, for data depositions, and with any other inquires.**

The BIOM-Mass (Biobank for Microbiome Research in Massachusetts) project is funded by the [Massachusetts Life Sciences Center (MLSC)](http://www.masslifesciences.com/) as a collaboration with the BWH/Harvard Cohorts Biorespository.

![MLSC](http://biom-mass.org/static/media/mlsc_logo.93b1eb31.png)


",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","terra-biom-mass/MLSC_Btb"
51,"help-gatk","GATK For Microbes","READER","https://app.terra.bio/#workspaces/help-gatk/GATK%20For%20Microbes",TRUE,FALSE,NA,NA,NA,"### GATK Best Practices for Variant Calling in Microbial Genomes

A reproducible pipeline for SNP and Indel variant calling in microbial whole-genome sequencing data.  

This pipeline is in the **alpha stage** of development. We have done our intial benchmarking and have seen promising results with sensitivity and precision. We are  sharing this pipeline with the community to get feedback and make further improvements where required. 

### Summary 

Genome Analysis Toolkit (GATK) is the industry standard for human variant discovery and genotyping. Due to its highly effective variant discovery methods, researchers are adopting GATK for microbes even though microbial genomes are radically different from human genomes and GATK has not been optimized for it. To optimize GATK for microbes and provide researchers with robust resources to study and combat life threatening pathogens, we developed a scalable, portable, and reproducible workflow that calls high-quality filtered variants using short reads sequencing data. We optimized for low allele frequencies, varying read depths, and sequencing and mapping errors typical of bacterial data, resulting in improved sensitivity and precision. For improved coverage across circular bacterial genomes, we developed a tool that calls variants on reads spanning the breakpoint at which they are linearized. We started these efforts with bacteria and will expand it to cover other microbial pathogens.

## Workflows
![pipeline](https://drive.google.com/uc?export=view&id=12RMqb-jkw6RDGgEhS1JoEr7kwQ5ycd5M)


### 1. MicrobialGenomePipeline

**What does it do?**     
This WDL workflow performs SNP/Indel variant calling on WGS input data.

**What data does it require as input?**  
This workflow accepts WGS FASTQ or BAM files as inputs and closest reference FASTA files. 

**What does it output?** .    
A filtered VCF file of SNP/Indel calls.         

**Sample data description and location**    
An example bam input of *Mycobacterium tuberculosis* is provided in the workspace data model for testing.    

**Reference data description and location**  
The required and optional references and resources for the Tools are included in the Workspace Data table.       

**Time and cost estimates**    

| Participant | Size | Time | Cost $ |
| :------------------: | :----------------: | :------: | :--------: |
| mtb | 675.12 MB | 0:16:00 | <0.01 |

**Software Version Notes**   
GATK 4.1.9.0  

### 2. MicrobialConcordancePipeline

**What does it do?**     
This WDL workflow performs concordance on one or more variant files (vcf) against a truth vcf.

**What data does it require as input?**  
This workflow accepts one or more samples and truth vcf files as inputs and closest reference FASTA files. 

**What does it output?** .    
Results of concordance comparison, summary metrics, and ROC curve data files.      

Cost and Time will vary, view [Controlling-Cloud-costs-sample-use-cases](https://support.terra.bio/hc/en-us/articles/360029772212) for further details.  
Users can also use [Google's BigQuery](https://software.broadinstitute.org/firecloud/documentation/article?id=11788) for task level calculation. 

### 3. DisplayConcordance Jupyter Notebook

We have also provided a notebook which takes as input the results of the MicrobialConcordancePipeline and plots the concordance comparison as shown in the image below. This notebook helps us streamline and quickly test new improvements we make to the pipeline against a) previous versions of the pipeline and/or b) other tools/pipelines.

![notebook](https://drive.google.com/uc?export=view&id=1Z_39Gv6LbvDqa1obqoTfR7PIYoKiqhFC)

---

### Contact Information  
For questions about this workspace please reach out to Bhanu Gandham at bgandham@broadinstitute.org

This material is provided by the GATK Team. Please post any questions or concerns regarding the GATK tool to the [GATK forum](https://gatk.broadinstitute.org/hc/en-us/community/topics)

### License  
**Copyright Broad Institute, 2019 | BSD-3**  
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at https://github.com/openwdl/wdl/blob/master/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.

### Workspace Change Log
| Date | Change | Author |
| --- | --- | --- |
|  2020-11-12 | Initial Setup | Bhanu Gandham and Andrea Haessly |


",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/GATK For Microbes"
52,"help-gatk","Germline-CNVs-GATK4","READER","https://app.terra.bio/#workspaces/help-gatk/Germline-CNVs-GATK4",TRUE,FALSE,NA,NA,NA,"### GATK Best Practices for Germline Copy Number Variation   

An analysis to detect germline copy number variants in exome sequence data using GATK's GermlineCNVCaller. 

- **`cnv_germline_cohort_workflow`** :    
Calls a cohort of samples and builds a model for denoising further case samples      

- **`cnv_germline_case_workflow`** :       
Calls case samples using a previously built model for denoising     

For tool documentation and parameters please visit the [GATK Tool Documentation](https://gatk.broadinstitute.org/hc/en-us/categories/360002369672-Tool-Index) site.      

For a step by step walkthrough of detecting germline copy number variants using GATK tools please review [How-to-Call-common-and-rare-germline-copy-number-variants](https://gatk.broadinstitute.org/hc/en-us/articles/360035531152).        


**Important note on Featured Workspace configuration limitations**      

The  workflows are configured to run the plumbing samples only. The configurations in this workspace do not represent the correct method of running the workflow for a germline CNV analysis. Please use the correct configurations from the  [GATK Tool Documentation](https://gatk.broadinstitute.org/hc/en-us/categories/360002310591) or use the default settings for optional parameters when running on real samples.

**Scroll down for detailed information on each workflow**

- Workflow description and function
- Input data description and requirements
- Time and cost estimates for running the workflows

Note that time and cost estimates may vary due to the use of preemptibles. 

## Workflows

### 1. cnv_germline_cohort_workflow
**What does it do?**     
Calls a cohort of samples and builds a model for denoising further case samples

**What input data does it take?**     
Required input data include a list of BAM and BAI files and an intervals file. There are also a number of required parameters (see the chart below for details. Note the reference used must be the same between PoN and case samples.


| Input Type | Input Name | Description |
|------------|-------------|-------------|
| String | cohort_entity_id | Name of the cohort.  Will be used as a prefix for output filenames. |
| File | contig_ploidy_priors | TSV file containing prior probabilities for the ploidy of each contig (e.g. 2 for human autosomes and 0.5 for chrY), with column headers: CONTIG_NAME, PLOIDY_PRIOR_0, PLOIDY_PRIOR_1, ... |
| String | gatk_docker | GATK Docker image (e.g., ``broadinstitute/gatk:4.1.5.0``). |
| File | intervals | Picard or GATK-style interval list. |
| Array[String]+ | normal_bais | List of BAI files.  This list must correspond to `normal_bams`.  For example, `[""Sample1.bai"", ""Sample2.bai""]`. |
| Array[String]+ | normal_bams | List of BAM files.  This list must correspond to `normal_bais`.  For example, `[""Sample1.bam"", ""Sample2.bam""]`. |
| Int | num_intervals_per_scatter | Number of intervals (i.e., targets or bins) in each scatter for GermlineCNVCaller.  If the total number of intervals is not divisible by the value provided, the last scatter will contain the remainder. |
| File | ref_fasta_dict | Path to reference dictionary file. |
| File | ref_fasta_fai | Path to reference fasta index file. |
| File | ref_fasta | Path to reference fasta file. |
| Int | maximum_number_events_per_sample | Maximum number of events threshold for doing sample QC (recommended for human WES with ~65Mb panel is ~100 ) |

#### Advanced user parameters
These optional workflow-level and task-level parameters may be set by advanced users:

| Input Type | Input Name | Description |
|------------|-------------|-------------|
| Boolean | do_explicit_gc_correction | (optional) If true, perform explicit GC-bias correction when creating PoN and in subsequent denoising of case samples.  If false, rely on PCA-based denoising to correct for GC bias. |
| Int | PreprocessIntervals.bin_length | Size of bins (in bp) for coverage collection.  *This must be the same value used for all case samples.*|
| Int | PreprocessIntervals.padding | Amount of padding (in bp) to add to both sides of targets for WES coverage collection.  *This must be the same value used for all case samples.* |


#### What does it output?
* annotated_intervals
* contig_ploidy_calls_tar
* contig_ploidy_model_tar
* denoised_copy_ratios
* filtered_intervals
* gcnv_calls_tars
* gcnv_model_tars
* gcnv_tracking_tars
* genotyped_intervals_vcfs
* genotyped_segments_vcfs
* read_counts
* read_counts_entity_ids
* QC metrics 



**Time and Cost Estimates**     

| Sample Name | Sample Total Size | Time | Cost $ |
| :-------:  | :--------: | :--------: | :----------: |
| cnv_germline_plumbing | 6.07MB | 00:58:00 | 0.10 |    

To learn more about cost-controlling options, see [this article](https://support.terra.bio/hc/en-us/articles/360029748111) . 


### 2. cnv_germline_case_workflow
**What does it do?**    
Calls case samples using a previously built model for denoising     



**What input data does it take?**   
Required input data include a list of normal BAM and BAI (index) files and an intervals list file. Find additional input parameters in the table below. The reference, number of intervals per scatter, and bins (if specified) must be the same between cohort and case samples.    

In addition, there are several task-level parameters that may be set by advanced users as above.

| Input Type | Input Name | Description |
|------------|-------------|-------------|
| Array[String]+ | normal_bais | List of BAI files.  This list must correspond to `normal_bams`.  For example, `[""Sample1.bai"", ""Sample2.bai""]`. |
| Array[String]+ | normal_bams | List of BAM files.  This list must correspond to `normal_bais`.  For example, `[""Sample1.bam"", ""Sample2.bam""]`. |
| File | contig_ploidy_model_tar | Path to tar of the contig-ploidy model directory generated by the DetermineGermlineContigPloidyCohortMode task.  |
| String | gatk_docker | GATK Docker image (e.g., ``broadinstitute/gatk:4.1.5.0``). |
| File | gcnv_model_tars | Array of paths to tars of the contig-ploidy model directories generated by the GermlineCNVCallerCohortMode tasks. |
| File | intervals | Picard or GATK-style interval list. |
| Int | num_intervals_per_scatter | Number of intervals (i.e., targets or bins) in each scatter for GermlineCNVCaller.  If the total number of intervals is not divisible by the value provided, the last scatter will contain the remainder. |
| File | ref_fasta_dict | Path to reference dictionary file. |
| File | ref_fasta_fai | Path to reference fasta index file. |
| File | ref_fasta | Path to reference fasta file. |
| File | maximum_number_events_per_sample | Maximum number of events threshold for doing sample QC (recommended for human WES with ~65Mb panel is ~100) |


#### What does it output?    
* contig_ploidy_calls_tar
* denoised_copy_ratios
* gcnv_calls_tars
* gcnv_tracking_tars
* genotyped_intervals_vcf
* genotyped_segments_vcf
* read_counts
* read_counts_entity_id
* Additional metrics     


**Time and Cost Estimates**     

| Sample Set Name | Sample Total Size | Duration | Cost $ |
| :-------:  | :--------: | :--------: | :----------: |
| cnv_germline_plumbing | 6.07MB | 00:43:00 | 0.05 |    


To learn more about cost-controlling options, see [this article](https://support.terra.bio/hc/en-us/articles/360029748111) . 


---

### Contact information  

For questions about this workspace please visit the [Featured Workspaces](https://broadinstitute.zendesk.com/hc/en-us/community/topics) topic on the Terra forum. Use the search box to see if other users have asked the same question previously. If not, post and tag **@Samantha** so that we get notified.

This material is provided by the GATK Team. Please post any questions or concerns regarding the GATK tool to the [GATK forum](https://gatk.broadinstitute.org/hc/en-us/community/topics)

### Workspace Citation 
Details on citing Terra workspaces can be found here: [How to cite Terra](https://support.terra.bio/hc/en-us/articles/360035343652)

Data Sciences Platform, Broad Institute (*Year, Month Day that this workspace was last modified*) help-gatk/Germline-CNVs-GATK4 [workspace] Retrieved *Month Day, Year that workspace was retrieved*,  https://app.terra.bio/#workspaces/help-gatk/Germline-CNVs-GATK4

### License  
**Copyright Broad Institute, 2020 | BSD-3**  
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at https://github.com/openwdl/wdl/blob/master/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.
 
 
 ### Workspace Change Log
| Date | Change | Author |
| --- | --- | --- |
|  2020-03-20 | Addition of Dashboard, Workflows, and Data| Beri Shifaw |      
| 2020-03-25 | Dashboard proofread | Allie Hajian |
| 2020-08-25 | Minor format edits to Dashboard | Beri Shifaw |
| 2021-03-08 | Updated GATK version to 4.2.0.0 | Beri Shifaw |
| 2021-09-15 | Addition of Workspace Citation section | Beri Shifaw |
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/Germline-CNVs-GATK4"
53,"broad-firecloud-dsde","SynapseCLR","READER","https://app.terra.bio/#workspaces/broad-firecloud-dsde/SynapseCLR",TRUE,FALSE,NA,NA,NA,"# SynapseCLR
SynapseCLR is a contrastive learning framework for navigating 3D electron microscopy data. A graphical overview of SynapseCLR pipeline and downstream applications is shown below:

![Alt text](https://github.com/broadinstitute/SynapseCLR/blob/main/docs/source/_static/synapseclr_graphical_abstract_1200x1200.png?raw=True ""SynapseCLR Overview"")

# Code Availability
SynapseCLR scripts for model pretraining, feature extraction, and downstream analysis notebooks can be obtained from [GitHub](https://github.com/broadinstitute/SynapseCLR).

# Data Availability
You can download SynapseCLR raw and preprocessed data, pretrained models, and analysis results from this public Terra workspace. This workspace contains the following folders:
```
gs://fc-212b2d2b-6b73-4461-87a0-62164cd9b59a/
├─ data/                  # Raw and processed 3D EM synapse image chunks from layer 2/3 mouse visual cortex (MICrONS Consortium)
├─ ext/                   # External resources (e.g. other pretrained models)
├─ output/                # SynapseCLR outputs (model weights, interactive analysis results)
└─ tables/                # Primary and derived resource tables
```

# Terms and Conditions
<a rel=""license"" href=""http://creativecommons.org/licenses/by/4.0/""><img alt=""Creative Commons License"" style=""border-width:0"" src=""https://i.creativecommons.org/l/by/4.0/88x31.png"" /></a><br />The data files made available in this Terra workspace are licensed under a <a rel=""license"" href=""http://creativecommons.org/licenses/by/4.0/"">Creative Commons Attribution 4.0 International License</a> with the following notes and exceptions. The original MICrONS Layer 2/3 structural EM data and segmentation were previously made available [here](https://www.microns-explorer.org/) under the CC BY 4.0 License. A part of that dataset is reproduced in this workspace under `/data`, in raw and processed forms. The pretrained MedicalNet model was obtained from [here](https://github.com/Tencent/MedicalNet) and was originally released under the MIT License (see [here](https://github.com/Tencent/MedicalNet/blob/master/LICENSE)). The MedicalNet model weights are reproduced in this workspace under `/ext` in the original format. Re-use of MedicalNet is subject to the terms and conditions of the [MIT License](https://github.com/Tencent/MedicalNet/blob/master/LICENSE).",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","broad-firecloud-dsde/SynapseCLR"
54,"kco-tech","Waddington-OT","READER","https://app.terra.bio/#workspaces/kco-tech/Waddington-OT",TRUE,FALSE,NA,NA,NA,"This tutorial provides a practical, hands-on introduction to inferring developmental trajectories with [Waddington-OT](https://broadinstitute.github.io/wot/).

Single cell RNA-sequencing allows us to profile the diversity of cells along a developmental time-course by recording static snapshots at different time points. However, we cannot directly observe the progression of any individual cell over time because the measurement process is destructive.

Waddington-OT is designed to infer the temporal couplings of a developmental stochastic process ℙt from samples collected independently at various time-points. We represent a developing population of cells with a time-varying distribution ℙt on gene expression space. The temporal couplings describe the flow of mass as the population develops and grows. For a pair of time-points (t&#7522;,t&#11388;), the coupling γt&#7522;,t&#11388; tells us: What descendants does cell x from time t&#7522; give rise to at time t&#11388;?

In this tutorial, we explore the Waddington-OT workflow, starting with inferring temporal couplings with optimal transport. We then go through numerous downstream analyses including visualizing cell fates, interpolating the distribution of cells at held-out time points, and inferring gene regulatory networks. The tutorial guides the reader through the concepts, each of which is illustrated with an interactive Jupyter notebook using data from a time-course of iPS reprogramming ([Schiebinger et al. 2019](https://www.cell.com/cell/fulltext/S0092-8674(19)30039-X)).  

![image1](https://broadinstitute.github.io/wot/images/Nbk1_fig.png)   

 *Figure above is an example  of single cell gene expression data visualization in Notebook 1.*

**Tutorial:** A step by step tutorial that accompanies this workspace is available at https://broadinstitute.github.io/wot/tutorial/.

---
---


### Data
The data used in the notebooks is single-cell RNA sequencing from a time-course of iPS reprogramming ([Schiebinger et al. 2019](https://www.cell.com/cell/fulltext/S0092-8674(19)30039-X)). It is loaded from the workspace bucket under the name notebooks(gs://fc-<bucket id>/notebooks/).

### Notebooks
Nine notebooks are provided:

* **Notebook 0-wot-setup**: Install wot and download data
* **Notebook 1 FLE-cell_sets-gene_sets**: Visualizing and exploring the data 
* **Notebook 2 Compute-transport-maps**: Computing transport matrices
* **Notebook 3 Long-range-couplings**: Inferring long-range temporal couplings
* **Notebook 4 Ancestors-descendants-trajectories**: Ancestors descendants, and trajectories
* **Notebook 5 Fate-matrices**: Fate Matrices
* **Notebook 6 Transition_table**: Transition tables
* **Notebook 7 OT-validation**: Validation by Geodesic Interpolation
* **Notebook 8 Predictive-TFs**: Predictive TFs 




### Time and cost
It takes ~60 minutes and costs ~19 cents to run the notebooks using the data provided in the tutorial.

| Runtime Environments | Value |
| --- | --- |
| CPU Minimum| 4|
| Disksize Minimum | 50 GB |
| Memory Minimum | 15 GB |
|	Environment | Custom Environment: klarmancellobservatory/wot-terra:1.0.8.post1 |


### Software versions
This workspace requires wot version 1 and Python 3 which are included in the klarmancellobservatory/wot-terra:1.0.8.post1 environment.

---
---

###  Contact information

* **Website:** https://broadinstitute.github.io/wot/
* **Email:** wot@broadinstitute.org
* **Github:** https://github.com/broadinstitute/wot

### License
[BSD 3-Clause](https://github.com/broadinstitute/wot/blob/master/LICENSE)
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","kco-tech/Waddington-OT"
55,"help-gatk","GATK4-Germline-Preprocessing-VariantCalling-JointCalling","READER","https://app.terra.bio/#workspaces/help-gatk/GATK4-Germline-Preprocessing-VariantCalling-JointCalling",TRUE,FALSE,NA,NA,NA,"### GATK Best Practices for Germline SNPs & Indels
This workspace contains tutorial notebooks and workflows that cover pre-processing, SNP and Indel variant calling. 

There are two ways to run the workflows, in the single sample case one sample is run through the first two workflows and a single VCF file for the sample is generated. In the cohort case, all four workflows are run on several samples to generate a multi-sample VCF. The workflows are properly configured in this workspace to run back to back, so that outputs from each step will automatically become the inputs for the next. 

Each workflow follows the GATK Best Practices on human whole-genome sequence data. Detailed description of the workflows is available in [Gatk's Best Practices Document](https://gatk.broadinstitute.org/hc/en-us/articles/360035535932).  

To learn more about how GATK workflows and Best Practices are used for production at the Broad Institute, you can also view the [Whole Genome Analysis Pipeline Workspace](https://app.terra.bio/#workspaces/warp-pipelines/Whole-Genome-Analysis-Pipeline). 

The workspace material is provided by the GATK team. Please post any questions or concerns to one of our forum sites : [GATK](https://gatk.broadinstitute.org/hc/en-us/community/topics) or [Terra](https://broadinstitute.zendesk.com/hc/en-us/community/topics/360000500432) 


## Notebooks
Here you will be running two different tutorial notebooks describing variant calling and different methods for variant filtering. This workspace is read-only, so clone your own unique copy to work with it.

You should find the documentation within the notebooks sufficient to run through them again on your own or to share with a friend later (and we encourage you to do so!).

The notebook(s) in this workspace are:
* 1-germline-variant-discovery-tutorial
* 2-gatk-hard-filtering-tutorial

All notebooks in this workspace can use the following runtime settings:

| Option | Value |
| --- | --- |
| Environment | Default (GATK 4.1.4.1) |
| Profile | Custom |
| CPU Minimum | 4|
| Disksize Minimum | 100 GB |
| Memory Minimum | 15 GB |

## Workflows overview 

1. **Preprocessing-For-Variant-Discovery**:  takes as input an unmapped BAM list file (text file containing paths to unmapped bam files) to perform preprocessing tasks such as mapping, marking duplicates, and base recalibration. It produces a single BAM file      

2.  **Haplotypecaller**:  takes as input a bam file and produces a file GVCF ( precursor to a VCF )    

3.  **Generate-Sample-Map**:  takes as input several GVCF files and generates a sample map file, which is text file where the first coloumn is the name of the sample and the second column contains the path to the samples GVCF file    

4.  **Joint-Genotyping**: takes as input a sample map file to perform variants calling all the provided GVCF files and filtering to produce a multi-sample VCF (minimum of 50 samples is required)   

Scroll down for details on each workflow, including input and output descriptions and requirements, estimated run times and costs, and information on sample data.   

### Workflow Naming Schema

There are two complete sets of workflows. Those with a ""1"" in front use hg38 reference data and those that begin with a ""2"" use b37. The table below lists the workflows and the corresponding reference.


|Workflow Name | Reference Data |
|---|---|
|1-1-Preprocessing-For-Variant-Discovery| hg38 |
|1-2-Haplotypecaller | hg38 |
|1-3-Generate-Sample-Map | hg38 |
|1-4-Joint-Genotyping | hg38 |
|2-1-Preprocessing-For-Variant-Discovery| b37 |
|2-2-Haplotypecaller | b37 |
|2-3-Generate-Sample-Map | b37 |
|2-4-Joint-Genotyping | b37 |


### Input and output data files overview


 **Single sample case**    
This will produce a VCF file for a single sample.

 ![drawing](https://storage.googleapis.com/terra-featured-workspaces/Germline-VariantCalling-JointCalling/images/GVCFdisabled.png)
 
 To run the workflows  in single sample mode:   

1.   Head to the workflows tab
2.   Click on 1-1-Preprocessing-For-Variant-Discovery-HG38 workflow  
		A.  Click on ""Select Data""  
		B.  Select ""Choose existing sets""  
		C.  Check the row with ""1kgp-50-wgs” as the sample_set  
		D.  Click ""OK""  
3. Click on Run analysis  
4. Once the workflow has completed head to the Workflows tab and on to the next workflow.   
5. Click on 1-2-Haplotypecaller-HG38 workflow   
	 A.  Click on ""Select Data""  
	 B.  Select ""Choose existing sets""  
	 C.  Check the row with ""1kgp-50-wgs” as the sample_set  
	 D.  Click ""OK""  
	 E. In the Input tab set the ""make_gvcf"" parameter to `False`
	 F. In the output tab set the output_vcf and output_vcf_index parmeter to `this.downsampled_hg38_vcf` and `this.downsampled_hg38_vcf_index`
6. Click on Run analysis  

**Cohort sample case**
 
 It's possible to generate a multi-sample VCF instead of a single sample VCF by setting the ""make_gvcf"" parameter  in the **Haplotypecaller** workflow to `True`. Haplotypecaller will then create a GVCF which can be used in the preceding workflows to create the multisample VCF. Running all four workflows takes an unmapped BAM (uBAM) input file and returns a multi-sample VCF. The first two workflows are run once for each sample. 

 ![drawing](https://storage.googleapis.com/terra-featured-workspaces/Germline-VariantCalling-JointCalling/images/GVCFenabled.png)


To run the workflows  in cohort mode:   

1.   Head to the workflows tab
2.   Click on 1-1-Preprocessing-For-Variant-Discovery-HG38 workflow  
		A.  Click on ""Select Data""  
		B.  Select ""Choose existing sets""  
		C.  Check the row with ""1kgp-50-wgs” as the sample_set  
		D.  Click ""OK""  
3. Click on Run analysis  
4. Once the workflow has completed head to the Workflows tab and on to the next workflow.   
5. Click on 1-2-Haplotypecaller-HG38 workflow   
	 A.  Click on ""Select Data""  
	 B.  Select ""Choose existing sets""  
	 C.  Check the row with ""1kgp-50-wgs” as the sample_set  
	 D.  Click ""OK""  
	 E. Set the ""make_gvcf"" parameter to `True`
	 F. In the output tab set the output_vcf and output_vcf_index parmeter to `this.downsampled_hg38_gvcf` and `this.downsampled_hg38_gvcf_index`
6. Click on Run analysis  
7. Once the workflow has completed head to the Workflows tab and on to the next workflow.  
8. Click on 1-3-Generate-Sample-Map-HG38 workflow  
  A.  Click on ""Select Data""  
  B.  Check the row with ""1kgp-50-wgs” as the sample_set  
  C.  Click ""OK""  
9. Click on Run analysis  
10. Once the workflow has completed head to the Workflows tab and on to the next workflow.  
11. Click on 1-4-Joint-Genotyping-HG38 workflow  
  A.  Click on ""Select Data""  
  B.  Check the row with ""1kgp-50-wgs” as the sample_set  
  C.  Click ""OK""  
12. Click on Run analysis  

If you would like to run the workflows again from scratch, delete the sample and sample_set table and re-upload both tables from this [google bucket](https://console.cloud.google.com/storage/browser/terra-featured-workspaces/Germline-VariantCalling-JointCalling/?project=broad-dsde-outreach&organizationId=548622027621). 

## Workspace Data 
There are two sets of sample tables in the DATA tab - one corresponding to template input files, and another with examples of processed data (i.e. example_sample_output).

**Template input data**    
These are the  tables you would use to run the workflow pipeline from the beginning. 

- participant - Individual per row. 
This won't be used directly in the workspace but it's best practice to include this in the DATA tab.
- sample - Sample from a participant  per row.      
This will be used by the preprocessing and haplotypecaller workflows (*-1 and *-2). 
- sample_set - A set of samples per row. 
This will be used by the Generate-sample-map and joint genotyping workflows (*-3 and *-4)

**Examples of processed data**    
Once all the workflows have been executed you will see several additional columns within the sample and sample_set tables. Examples of what you should see are in the following tables:      
- example_sample_output
- example_sample_set_output  

## Workflows

###  1-Preprocessing-For-Variant-Discovery  
**What does it do?**    
This WDL takes sequencing data in unmapped BAM (uBAM) format and outputs a clean BAM file and its index, suitable for variant discovery analysis. 

**What does it require as input?**    
The 1-Preprocessing-For-Variant-Discovery workflow accepts a file containing a list of unaligned BAMS. To learn more about how to generate a list file, see [this article](https://support.terra.bio/hc/en-us/articles/360033353952).     

The input data are samples:

* Pair-end sequencing data in unmapped BAM (uBAM) format
* One or more read groups, one per uBAM file, all belonging to a single sample (SM)

Input uBAM files must comply with the following requirements:

* Filenames all have the same suffix (we use "".unmapped.bam"")
* Files must pass validation by ValidateSamFile
* Reads are provided in query-sorted order
* All reads must have an RG tag
* Reference index files must be in the same directory as source (e.g. reference.fasta.fai in the same directory as reference.fasta)    

**If your sequencing data is not in uBAM format** (e.g. FASTQ), check out this file conversion workspace, [https://app.terra.bio/#workspaces/help-gatk/Sequence-Format-Conversion](https://app.terra.bio/#workspaces/help-gatk/Sequence-Format-Conversion) for workflows to convert:    

1. Interleaved FASTQ to paired FASTQ
2. Paired FASTQ to unmapped BAM
3. BAM to unmapped BAM
4. CRAM to BAM files from sequencer output for use in GATK analysis tools

**Sample data description and location**  
The workspace DATA tab contains downsampled 1000 Genome Project unaligned BAM list files in the `sample` table under the column `flowcell_unmapped_bams_list`.   

**What does it return as output?**  
The workflow generates a clean BAM file and its index, suitable for variant discovery analyses and stored in the workspace bucket. Metadata for all outputs are written to the `sample` table in the workspace DATA tab.           
   
**Reference data description and location**  
Required and optional references and resources for the workflows are included in the Workspace DATA tab in the Reference Data tables. The input unmapped BAM samples have yet to be aligned to a reference so they are not restricted to a particular reference. Only after the first workflow will the samples be restricted to working with a reference, because the unmapped BAM files will be mapped after that point. Again there are two sets of each workflow, one set configured with hg38 references and the other set configured with b37 references. 
 
 
**Estimated time and cost to run on sample data**    

| Sample Name | Sample Size | Time | Cost $ |
| :---:  | :---: | :---: | :---: |
| NA12878_24RG_small | 3.11 GB | 1:28:00 | 0.18 |
| NA12878 | 64.89 GB | 22:35:00 | 4.98 |  
| downsampled-1kgp-50-exomes | 32.13 GB | 02:07:00 | 7.29 |       



### 2-Haplotypecaller 
**What does it do?**    
The workflow scatters the HaplotypeCaller tool over a sample (clean BAM file and index, from the previous step) using an intervals list file. In particular, it runs the HaplotypeCaller tool from GATK4 on a single sample according to GATK Best Practices. The output file produced will be a single VCF or GVCF file depending on the mode in which it is run. If a GVCF is produced then it can be used by the joint genotyping workflow.   

**What does it require as input?**       
The workflow accepts:  In particular:     
- One analysis-ready BAM file for a single sample (as identified in RG:SM), pre-processed using GATK Best Practices. 
- A file containing a set of variant calling interval lists for the scatter

**What does it return as output?**        
One VCF or GVCF file and its index      

**Sample data description and location**  
Links to the expected input types are available in the `sample` data table (processed example) for testing. The `sample` data table lists analysis-ready BAM files under the `analysis_ready_bam` column.     

**Reference data description and location**  
Required and optional references and resources for the workflows are included in the Workspace DATA tab in the Reference Data tables. The **1-2_Haplotypecaller** workflow is configured with HG38 references and **2-2_Haplotypecaller** is configured with B37 references. 
          

**Estimated time and cost to run on sample data**      

| Sample Name | Sample Size | Time | Cost $ |
| :---:  | :---: | :---: | :---: |
| NA12878_24RG_small | 4.66 GB | 02:28:00 | 0.21 |
| NA12878  (CRAM)| 19.55 GB | 14:05:00 | 2.24 |    
| NA12878  (BAM)| 68.00 GB | 03:44:00 | 1.37 |    
| downsampled-1kgp-50-exomes | 23.00 GB | 01:12:00 | 24.25 |  
   

### 3-Generate-Sample-Map

**What does it do?**    
This WDL generates a sample_map file, which can be used for the Joint-Genotyping workflow. A sample map is a tab-delimited text file of 2 columns; 1. the name of the sample and 2. the file path (in this case the Google bucket path of the file).

**What does it require as input?**  
- An array of file names
- An array of file paths
- Name of output sample_map 

**What does it return as output?**    
- Sample map file

**Reference data description and location**  
Reference files are not used in this workflow. 


### 4-Joint-Genotyping    
**What does it do?**    
This WDL implements the joint calling and variant quality score recalibration (VQSR) filtering portion of the GATK Best Practices.

**What does it require as input?**  
- GVCFs produced by HaplotypeCaller in GVCF mode
- Bare minimum: 50 samples. Gene panels are not supported

**What does it return as output?**     
A VCF file and its index, filtered using VQSR, with genotypes for all samples present in the input VCF. All sites that are present in the input VCF are retained. Filtered sites are annotated as such in the FILTER field.      
	
**Sample data description and location**     
Links to the expected input types are available in the processed example data table for testing. The 4-Joint-Genotyping workflow accepts one or more GVCFs produced by haplotypecaller.  The `gvcf` column in the data table contains a full-sized GVCF of NA12878 that will be used for the Joint-Genotyping workflow.     

**Reference data description and location**  
Required and optional references and resources for the workflows are included in the Workspace DATA tab in the Reference Data tables. **1-4_JointGenotyping** is configured with hg38 references and **2-4_JointGenotyping** is configured with the b37 reference. 
 

**Estimated time and cost to run on sample data**     

| Sample Name | Sample Size | Time | Cost $ |
| :---:  | :---: | :---: | :---: | 
| downsampled-1kgp-50-exomes | 9.52 GB | 03:17:00 | 1.35 |  
 
### Optional Workflows
Additional workflows have been added for your convenience.   

**Optional-Paired-FASTQ-to-Unmapped-BAM**: 
This WDL converts paired FASTQ to uBAM and adds read group information.

Requirements/expectations
* Pair-end sequencing data in FASTQ format (one file per orientation)
* The following metadata descriptors per sample:
  * readgroup
  * sample_name
  * library_name
  * platform_unit
  * run_date
  * platform_name
  * sequecing_center
Outputs
  * Unmapped BAM

**Optional-Gatk-GatherVCFsCloud**:  
This tool combines together rows of variant calls from multiple VCFs, e.g. those produced by scattering calling across genomic intervals, into a single VCF. This tool enables scattering operations, e.g. in the cloud, and is preferred for such contexts over Picard MergeVcfs or Picard GatherVCfs. The input files need to have the same set of samples but completely different sets of loci. These input files must be supplied in genomic order and must not have events at overlapping positions. 

Input  
* A set (array) of VCF files, sorted by genomic position. Its also possible to provide a file containing a list of VCF files, each row being an individual VCF file path.  

Output  
* A single VCF file containing the variant call records from the multiple VCFs. 

**Optional-ReblockGVCF-gatk4_exomes_goodCompression**:  
Users working with large sample sets can invoke the GnarlyGenotyper task in the JointGenotyping.wdl workflow. However, the [ReblockGVCF](https://gatk.broadinstitute.org/hc/en-us/articles/360037593171-ReblockGVCF-BETA-) tool must be run for all GVCFs produced by HaplotypeCaller before they can be appropriately processed by GnarlyGenotyper. 

Input  
* A GVCF file 

Output  
* Reblocked GVCF file


### Important notes on workflow limitations 
**Small Cohorts**

We believe the results of this workflow run on a single WGS sample are equally accurate, but there may be some shortcomings when the workflow is modified and run on small cohorts.  Specifically, modifying the SNP ApplyRecalibration step for higher specificity may not be effective.  You can verify if this is an issue by consulting the gathered SNP tranches file.  If the listed `truthSensitivity` in the rightmost column is not well matched to the `targetTruthSensitivity` in the leftmost column, then requesting that `targetTruthSensitivity` from ApplyVQSR will not use an accurate filtering threshold.     

Additionally 
- No allele subsetting for the Joint-Genotyping workflow
  - For large cohorts, even exome callsets can have more than 1000 alleles at low complexity/STR sites
  - For sites with more than six alternate alleles (by default) called genotypes will be returned, but without the PLs since the PL arrays get enormous
  - Allele-specific filtering could be performed if AS annotations are present, but the data will still be in the VCF in one giant INFO field
- JointGenotyping output is divided into lots of shards
  - Desirable for use in [Hail](https://hail.is/), which supports parallel import
  - It's possible to use [GatherVcfs](https://app.terra.bio/#workspaces/help-gatk/GATK4-Germline-Preprocessing-VariantCalling-JointCalling/workflows/broad-firecloud-dsde/Optional-Gatk-GatherVCFsCloud) to combine shards.
- GnarlyGenotyper uses a QUAL score approximation
   - Dramatically improves performance compared with GenotypeGVCFs, but QUAL output (and thus the QD annotation) may be slightly discordant between the two tools.

**Exomes**

Currently the workflows are configured for WGS processing. 
The dynamic scatter interval creating task was optimized for genomes.  The scattered SNP VariantRecalibration may fail because of too few ""bad"" variants to build the negative model. Also, apologies that the logging for SNP recalibration is overly verbose.   

The provided tool configurations are meant to be a ready-to-use example of the workflows. It is the user’s responsibility to correctly set the reference and resource input variables using the [GATK Tool and Tutorial Documentations](https://software.broadinstitute.org/gatk/documentation/).

**GnarlyGenotyper**

Users working with large sample sets can invoke the GnarlyGenotyper task in the JointGenotyping.wdl workflow. However, the [ReblockGVCF](https://gatk.broadinstitute.org/hc/en-us/articles/360037593171-ReblockGVCF-BETA-) tool must be run for all GVCFs produced by HaplotypeCaller before they can be appropriately processed by GnarlyGenotyper. A workflow that applies the reblocking tool is provided here: [ReblockGVCF-gatk4_exomes_goodCompression](https://portal.firecloud.org/?return=terra#methods/methodsDev/ReblockGVCF-gatk4_exomes_goodCompression/4)


**Controlling cloud costs**

Note that cost and time estimates will vary with the use of [preemptibles](https://support.terra.bio/hc/en-us/articles/360029772212). Using preemptibles can save up to 80% on compute costs.  For further helpful hints on controlling cloud costs, see [this article](https://support.terra.bio/hc/en-us/articles/360029748111) and for additional ways to estimate cloud cost use [Google's cost calculator](https://cloud.google.com/products/calculator/).

### Software Versions  
- GATK 4.1.4.0
- BWA 0.7.15-r1140
- Picard 2.16.0-SNAPSHOT
- Samtools 1.3.1 (using htslib 1.3.1)
- Python 2.7

---

### Contact information  
For questions about this workspace please visit the Featured Workspaces Topic topic on the [Terra Community Forum](https://broadinstitute.zendesk.com/hc/en-us/community/topics). Use the search box to see if other users have asked the same question previously. If not, post and tag **@Samantha** so that we get notified.

This material is provided by the GATK Team. Please post any questions or concerns regarding the GATK tool to the [GATK forum](https://gatk.broadinstitute.org/hc/en-us/community/topics)


### Workspace Citation 
Details on citing Terra workspaces can be found here: [How to cite Terra](https://support.terra.bio/hc/en-us/articles/360035343652)

Data Sciences Platform, Broad Institute (*Year, Month Day that this workspace was last modified*) help-gatk/GATK4-Germline-Preprocessing-VariantCalling-JointCalling [workspace] Retrieved *Month Day, Year that workspace was retrieved*,  https://app.terra.bio/#workspaces/help-gatk/GATK4-Germline-Preprocessing-VariantCalling-JointCalling

### License  
**Copyright Broad Institute, 2019 | BSD-3**  
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at https://github.com/openwdl/wdl/blob/master/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.


### Workspace Change Log
| Date | Change | Author |
| --- | --- | --- |
|  2020-01-14 | Initial feature of workspace | Beri Shifaw |
|  2020-02-24 | Added Optional-ReblockGVCF-gatk4_exomes_goodCompression workflow | Beri Shifaw |
|  2020-04-02 | Updated Preprocessing-For-Variant-Discovery to v2.0.0, Reuploaded HC and Generate Map workflow  | Beri Shifaw |
|  2020-06-22 | Added workflow overview image to dashboard and additional description to workflows, Updated Joint Genotyping workflow, removed ""GVCF"" from Haplotypecaller name to indicate the workflow can be run to create VCF  | Beri Shifaw |
|  2020-09-28 | Updated Haplotypecaller and JointGenotype workflow to release 2.2.0  | Beri Shifaw |
|  2020-11-28 | Updated Haplotypecaller to release 2.3.0, updated JointGenotype workflow source to Warp repo, added GATK Notebook tutorials | Beri Shifaw |
|  2020-12-07 | Updated data tables to use downsample wgs  | Beri Shifaw |
|  2021-01-29 | Updated dashboard to refer to [Whole Genome Analysis Pipeline Workspace](https://app.terra.bio/#workspaces/warp-pipelines/Whole-Genome-Analysis-Pipeline) | Beri Shifaw |
|  2021-09-15 | Addition of Workspace Citation section | Beri Shifaw |",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/GATK4-Germline-Preprocessing-VariantCalling-JointCalling"
56,"help-gatk","BroadE-Day4-BigQuery","READER","https://app.terra.bio/#workspaces/help-gatk/BroadE-Day4-BigQuery",TRUE,FALSE,NA,NA,NA,"# Welcome to The GATK BroadE Day 4 BigQuery Tutorial Workspace

This workspace introduces BigQuery functionality with notebooks that demonstrate how to manipulate cohorts and two ways of querying BigQuery data (via SQL and R). 

It also includes a teaser notebook on Hail, an open-source, scalable framework for exploring and analyzing  large scale data sets. The notebooks runs through a demo of how to use Hail in a notebook to analysize public-access 1,000 genomes data.

## BigQuery Resources
* For more instructions on using BigQuery in a notebook, see this [Google Cloud Platform intro](https://cloud.google.com/blog/products/gcp/google-cloud-platform-for-data-scientists-using-jupyter-notebooks-with-apache-spark-on-google-cloud). 
* Intro to controlling BigQuery costs: [https://cloud.google.com/bigquery/docs/controlling-costs](https://cloud.google.com/bigquery/docs/controlling-costs)
* BigQuery best practices: [https://cloud.google.com/bigquery/docs/best-practices-costs](https://cloud.google.com/bigquery/docs/best-practices-costs)
* BigQuery cost calculator: [https://cloud.google.com/products/calculator/](https://cloud.google.com/products/calculator/)

## Variant Calling and BigQuery Resources  
There are a lot of built-in genomics tools already in BigQuery (including GATK). 
* BigQuery genomics: [https://cloud.google.com/genomics/](https://cloud.google.com/genomics/) 
* Analyzing variants: [https://cloud.google.com/genomics/docs/how-tos/analyze-variants](https://cloud.google.com/genomics/docs/how-tos/analyze-variants)
* On Github: [https://github.com/googlegenomics/](https://github.com/googlegenomics/)


## Hail Resources

The Hail Team at Broad gives regular workshops through Broad-E. Please defer to their coursework material.

* **Contact the Hail team** at hail@broadinstitute.org
* **Follow Hail on Twitter** @hailgenetics
* **Citing Hail** If you use Hail for published work, please cite the software:  Hail, https://github.com/hail-is/hail

Thank you to Tim Majarian from the Leonardo Team for providing the demo dataset and notebook, which was adjusted to work on Terra for this workshop.

-----
## Workspace Data  
The BigQuery notebooks in this workspace uses only notebooks (that pull data from BigQuery) and have no associated data of their own.

The Hail demo notebook features data in a Hail MatrixTable, which has been organized in a data model table and can be seen in the **Data** tab. The data is phase 3 genotypes from the 1,000 genomes with some mock phenotype measures for Type 2 Diabetes

## Workspace Tools
Likewise, there are no tools in this notebooks-based tutorial workspace. 

If you are interested in WDL based workflows you can use for  analyses, check out the [Showcase](https://app.terra.bio/#library/showcase) area in Terra, which features many of our popular GATK workflows in workspaces ready to run your data.

## Notebooks
Users can run code cells manually and view intermediate outputs. Markdown cells explain the function of the code cells. This is a great way to understand each step in the notebook and give you the chance to manipulate the parameters to see what happens with output.


**1_R_environment_setup**    
Run this notebook to set up the workspace for the rest of the BigQuery tutorial notebooks.

| Runtime Environments | Value |
| --- | --- |
| CPU Minimum| 2|
| Disksize Minimum | 100 GB |
| Memory Minimum | 13 GB |



**2_R_How_to_retrieve_a_cohort_using_BigQuery**     
This notebook provides examples of how to manipulate cohorts using standard SQL (via the R library bigrquery) to access BigQuery. 

| Runtime Environments | Value |
| --- | --- |
| CPU Minimum| 2|
| Disksize Minimum | 100 GB |
| Memory Minimum | 13 GB |



**3_R_How_to_read_data_from_BigQuery**     
This notebook shows how to retrieve a subset of data from the 1,000 genomes project public data in two different ways: standard SQL and R coding. 

| Runtime Environments | Value |
| --- | --- |
| CPU Minimum| 2|
| Disksize Minimum | 100 GB |
| Memory Minimum | 13 GB |


**4_Python_environment_setup**    
Run this notebook to set up the workspace before the Hail tutorial notebook.

| Runtime Environments | Value |
| --- | --- |
| CPU Minimum| 2|
| Disksize Minimum | 100 GB |
| Memory Minimum | 13 GB |


**5_Hail_Short_Demo_Mock_Data**     
This small demonstration of Hail code utilizes a mock data set where a diagnosis of Type 2 Diabetes has been randomly assigned to a set of samples from the 1000 genomes data set. The corresponding bmi values of the Type 2 Diabetes cohort have been altered to show the power of Hail tools for comparing phenotypes.  **Avoid storage costs:** Because this notebook outputs tsv, text, ipynb and html files into the general workspace bucket, you will incur additional storage costs if you don't remove these (or delete the workspace) after using the notebook.

| Runtime Environments | Value |
| --- | --- |
| CPU Minimum| 2|
| Disksize Minimum | 100 GB |
| Memory Minimum | 13 GB |
 
-----

## Time and cost 
This workspace focuses on the use of notebooks, thus the time and cost of completing the tutorial depends on the runtime environment of your notebook and the length of time you spend actively working in the notebook. With the runtime environments above, users can expect the cost to be much less than a dollar an hour for each notebook. The specific cost will be displayed on the top right corner of your screen, next to the notebook machine options. 

In addition to the cost of running a notebook, you will incur BigQuery costs when you query data. Querying the first TB of data every month is free. You can find [Google's cost calculator here](https://cloud.google.com/products/calculator/). 

-----

## Appendix

### GATK @ BroadE 2019 in the Terra Support Center

Get yourself oriented with the entire workshop in the [Terra Support Center](https://broadinstitute.zendesk.com/hc/en-us/community/topics). (more on Terra Support below).

### Terra documentation and support

Documents and helpful guides to support your journey through Terra are available in the Terra Support Center. Navigate there by following the “Learn About Terra” link in the left-side menu. At the time of this workshop, we are still actively writing and publishing documents but we’ve got a good set of docs in the Quick Start Guide to get you going.

Please post any questions or concerns to our forum site : [Terra Forum](https://broadinstitute.zendesk.com/hc/en-us/community/topics/360000500432-General-Discussion).",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/BroadE-Day4-BigQuery"
57,"warp-pipelines","Whole-Genome-Analysis-Pipeline","READER","https://app.terra.bio/#workspaces/warp-pipelines/Whole-Genome-Analysis-Pipeline",TRUE,FALSE,NA,NA,NA,"### Whole Genome Germline Variant Discovery used by Broad Genomics
This workspace contains fully reproducible example workflows for whole genome sequence data pre-processing, germline short variant discovery, and joint variant calling, as used for production by the Genomics Platform at the Broad Institute and recommended for research purposes.

Descriptions of the workflows are available in GATK's Best Practices Documents for [preprocessing](https://gatk.broadinstitute.org/hc/en-us/articles/360035535912) and [Germline short variant discovery](https://gatk.broadinstitute.org/hc/en-us/articles/360035535932).  

Scroll down for an overview of each workflow, example data,  cost estimates, and additional resources.

#### This workspace is an updated version of the previously featured [Whole-Genome-Analysis-Pipeline workspace](https://app.terra.bio/#workspaces/help-gatk/Whole-Genome-Analysis-Pipeline).

The materials in this workspace were developed by the Data Sciences Platform at the Broad Institute. 


## Workflows Overview


![](https://storage.googleapis.com/terra-featured-workspaces/Whole-Genome-Analysis-Pipeline/WGS_overview.png)

This workspace has three example workflows that you can run using the “Running the workflow” instructions below. The workflows are designed to run in the order presented, but we have provided sample inputs for each workflow so that you can try each one independently.  

1. **WholeGenomeGermlineSingleSample**: performs preprocessing on a set of unmapped BAM files (multiple read groups per single sample sequenced across multiple flowcell lanes) to produce single CRAM and reblocked GVCF files per sample.

2.  **Generate-Sample-Map**: takes as input several (reblocked) GVCF files (i.e., output from WholeGenomeGermlineSingleSample) and generates a sample map file, a text file where the first column is the name of the sample and the second column contains the path to the samples GVCF file.    

3.  **JointGenotyping**: takes a sample map file listing the (reblocked) GVCFs produced with WholeGenomeGermlineSingleSample and performs variant calling on all the provided GVCF files. It then filters to produce a multi-sample VCF (a minimum of 50 samples is required).  

## Workflows

### 1. Whole Genome Germline Single Sample

**What does it do?**     

This WDL pipeline implements data pre-processing and initial variant calling (GVCF generation) according to the GATK best practices for germline SNP and indel discovery in human whole genome sequencing data. 

The workflow takes as input an array of unmapped BAM files (all belonging to the same sample) to perform preprocessing tasks such as mapping, marking duplicates, and base recalibration, then uses Haplotypecaller and ReblockGVCF to generate a reblocked GVCF or VCF. 

For the latest version of the workflow, please visit the WARP repository [WholeGenomeGermlineSingleSample](https://github.com/broadinstitute/warp/blob/master/pipelines/broad/dna_seq/germline/single_sample/wgs/WholeGenomeGermlineSingleSample.wdl). You can also read the pipeline's [documentation](https://broadinstitute.github.io/warp/docs/Pipelines/Whole_Genome_Germline_Single_Sample_Pipeline/README). 

**What data does it require as input?**    
- Human whole genome sequencing data in unmapped BAM (uBAM) format
    - If your sequence files are not in unmapped BAM format, please review the [Sequence-Format-Conversion](https://app.terra.bio/#workspaces/help-gatk/Sequence-Format-Conversion) workspace for file conversion workflows. 
- One or more read groups, one per uBAM file, all belonging to a single sample (SM)
- Input uBAM files must additionally comply with the following requirements:
  - Filenames all have the same suffix (we use "".unmapped.bam"")
  - Files must pass validation by ValidateSamFile
  - Reads are provided in query-sorted order
  - All reads must have an RG tag
- Reblocked GVCF output names must end in "".rb.g.vcf.gz""
- Reference genome must be Hg38 with ALT contigs


**What does it return as output?**     

The following files are stored in the workspace Google bucket and links to the files are written to the `read_group_set` data table:    
- CRAM file, CRAM index, and CRAM md5 
- [Reblocked](https://gatk.broadinstitute.org/hc/en-us/articles/4414594365467) GVCF and its GVCF index (read more about reblocking in the [WGS pipeline documentation](https://broadinstitute.github.io/warp/docs/Pipelines/Whole_Genome_Germline_Single_Sample_Pipeline/README#reblocking) and [WARP blog](https://broadinstitute.github.io/warp/blog/Nov21_ReblockedGVCF))
- BQSR report
- Several summary metrics       

**Running the workflow**    

The workflow in this workspace is pre-configured to use the `NA12878` sample listed in the `read_group_set` data table in the workspace Data tab. This example is a single sample with 24 read groups. The unmapped BAM for each `NA12878` read group is listed in the `read_group` data table. 

The workflow is configured to call this input from the data table. To run:

1. Select the workflow from the Workflows tab. 
2. In the configuration page, select the `read_group_set` entity in Step 1. 
3. Select the `NA12878` dataset in Step 2.
4. Select the checkbox next to `Use reference disk`.
5. Run the workflow.

If you want to use this workflow on your samples, you can download the Input JSON file from the workflow configuration page and use it as a template for setting up your own data.

- Optional inputs, like the `fingerprint_genotypes_file`, will need to match your samples. This workspace is set up to check fingerprints for the `NA12878` sample optionally.
- For the CheckFingerprint task, the sample name specified in the `sample_and_unmapped_bams` variable must match the sample name in the `fingerprint_genoptyes_file`(VCF format).
   
**Important configuration notes** 
- The workflow is written in WDL1.0 and imports structs to organize and use inputs. 

- If you run the workflow on a sample that only has one uBAM (i.e., one read group), you need to update the config attributes for  `sample_and_unmapped_bams` to include `[]` around the `flowcell_unmapped_bams` as shown below:

`{ ""sample_name"": this.read_group_set_id, ""base_file_name"": this.read_group_set_id, ""flowcell_unmapped_bams"": [this.read_groups.flowcell_unmapped_bams], ""final_gvcf_base_name"": this.read_group_set_id, ""unmapped_bam_suffix"": "".bam"" }`

-By default, the workflow produces a reblocked GVCF to be used in [joint calling](https://gatk.broadinstitute.org/hc/en-us/articles/360035890431), but can be set to output a VCF instead of a GVCF. 

-The reblocking step is optional; to turn it off, change the `skip_reblocking`parameter  to `true`.

**Reference data description and location**     

The required and optional references and resources for the workflows are set in the workflow configurations. The reference genome is hg38 (aka GRCh38).

**Enabling reference disks**

The suggested workflow configuration (see ""Running the workflow"" above) uses [reference disks](https://support.terra.bio/hc/en-us/articles/360056384631). That means when Terra kicks off the workflow on a virtual computer, it attaches a portable disk (kind of like a flash drive) that is preloaded with the reference files needed for the workflow. This saves time and cost running the workflow. 


**Time and cost estimates**         
Below is an example of the time and cost for running the workflow with the `Use reference disk` option enabled.

| Sample Name | Number of Entities | Sample Size | Time | Cost $ |
| ---  | --- | --- | --- | --- |
| NA12878 | 24 | ~3.20 GB | 06:38:00 | ~$0.39 |

**Note:** 

For more information about controlling Cloud costs, see [this article](https://support.terra.bio/hc/en-us/articles/360029748111).

------------
### 2. Generate Sample Map

**What does it do?**    
This WDL generates a sample_map file, a tab-delimited text file of 2 columns; 1. the name of the sample and 2. the file path (in this case, the Google bucket path of the file). 

The sample map can be used for downstream tools, like the JointGenotyping workflow, that require the sample names in the GVCF header. It allows the downstream tools to determine the sample names without downloading the GVCF headers.

**What does it require as input?**  
- An array of filenames
- An array of file paths
- Name of output sample_map 

**What does it return as output?**    
- Sample map file

**Running the workflow**

Example paths to GVCF files are listed in the `samples` data table of the workspace Data tab. To run the workflow, select the workflow from the Workflows tab. Next, select the `sample_set` data table as the root entity and choose the `WGS_JointGenotyping` set. 

**Reference data description and location**  
Reference files are not used in this workflow. 

**Time and cost estimates**         
Below is an example of the time and cost for running the workflow.

| Sample Name | Number of Entities | Sample Size | Time | Cost $ |
| -------  | -------- | -------- | ---------- | --- |
| WGS_JointGenotyping | 66 | NA | 00:03:00 | ~$0.01 |


------------
### 3. Joint Genotyping    
**What does it do?**    
This WDL implements the joint calling and variant quality score recalibration (VQSR) filtering portion of the GATK Best Practices.

**What does it require as input?**  
- a sample_map file listing the GVCFs produced by HaplotypeCaller in GVCF mode
- Bare minimum: 50 samples. Gene panels are not supported.

By default, the Whole Genome Germline Single Sample Pipeline performs variant calling with GATK3.5, producing VCFs with older mapping quality annotations.

To account for these older annotations, we've set the Joint Genotyping pipeline's `allow_old_rms_mapping_quality_annotation_data` parameter to `true`.

**What does it return as output?**     
A VCF file and its index, filtered using VQSR, with genotypes for all samples present in the input VCF. All sites that are present in the input VCF are retained. Filtered sites are annotated as such in the FILTER field.      
	
**Running the workflow**     
The JointGenotyping workflow accepts a sample_map file that lists GVCFs produced by haplotypecaller.  The map file is provided in the `sample_map` column of the `sample_set` data table. 

To run, select the workflow from the Workflows tab and choose the `sample_set` table as the root entity. Then, select the `WGS_JointGenotyping` data and launch the workflow.

**Reference data description and location**  
Required and optional references and resources for the workflows are included in the workflow configuration. **JointGenotyping** is configured with hg38 references.
 

**Estimated time and cost to run on sample data**     

| Sample Name | Time | Cost $ |
| :---:  | :---: | :---: |
| WGS_JointGenotyping | 04:43:00 | $9.48 |  
 
---

### Additional Resources
As the workflows highlighted here are used for production, they contain unique quality measures that differ from other GATK-related workflows for short variant discovery. 
- For additional examples of applying the GATK Best Practices, including Jupyter Notebooks with step-by-step GATK workflow details, try the [GATK4-Germline-Preprocessing-VariantCalling-JointCalling Workspace](https://app.terra.bio/#workspaces/help-gatk/GATK4-Germline-Preprocessing-VariantCalling-JointCalling).
- This workspace showcases whole genome analysis, but the Generate-Sample-Map and  JointGenotyping workflows can similarly be applied to whole exome analysis, which only uses sequencing data from the protein-coding regions of the genome (exomes). See the [Exome-Anaysis-Pipeline workspace](https://app.terra.bio/#workspaces/warp-pipelines/Exome-Analysis-Pipeline) for an example exome sequencing workflow.


* For questions regarding GATK-related tools and Best Practices, see the [GATK website](https://gatk.broadinstitute.org/hc/en-us).
* For Terra-specific documentation and support, see the [Terra Support](https://support.terra.bio/hc/en-us).

  
### Contact Information  
* For workspace questions and feedback, email the Broad pipelines team at warp-pipelines-help@broadinstitute.org or create a post on the [Featured Workspaces community forum](https://support.terra.bio/hc/en-us/community/topics/360001603491) (login required). Tag @Alexander Baumann in the **Details** section of your post.

* You can also Contact the Terra team for questions from the Terra main menu. When submitting a request, it can be helpful to include:
    * Your Project ID
    * Your workspace name
    * Your Bucket ID, Submission ID, and Workflow ID
    * Any relevant log information

Please post any questions or concerns regarding the GATK tool to the [GATK forum](https://gatk.broadinstitute.org/hc/en-us/community/topics)

### Workspace Citation 
Details on citing Terra workspaces can be found here: [How to cite Terra](https://support.terra.bio/hc/en-us/articles/360035343652)

Data Sciences Platform, Broad Institute (*Year, Month Day that this workspace was last modified*) warp-pipelines/Whole-Genome-Analysis-Pipeline [workspace] Retrieved *Month Day, Year that workspace was retrieved*,  https://app.terra.bio/#workspaces/warp-pipelines/Whole-Genome-Analysis-Pipeline

### License  
**Copyright Broad Institute, 2023 | BSD-3**  
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at https://github.com/broadinstitute/warp/blob/develop/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.

### Workspace Change Log
| Date | Change | Author |
| --- | --- | --- |
| 2023-08-28 | Updated workflows to latest versions. | Kaylee Mathews |
| 2023-05-01 | Updated WGS workflow to latest version. | Kaylee Mathews |
| 2022-09-01 | Updated workflows to latest versions. | Kaylee Mathews |
| 2022-08-26 | Updated contact information. | Kaylee Mathews |
| 2022-04-22 | Updated to Picard version 2.26.10 and GATK version 4.2.6.1 to address log4j vulnerabilities. | Nikelle Petrillo |
| 2022-02-09 | Updated workflows to latest version and added reblocked GVCF outputs. | Kaylee Mathews |
| 2021-01-15 | Initial feature of workspace. | Nikelle Petrillo |
| 2021-09-15 | Addition of Workspace Citation section. | Beri Shifaw |",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","warp-pipelines/Whole-Genome-Analysis-Pipeline"
58,"aryee-lab","dna-methylation","READER","https://app.terra.bio/#workspaces/aryee-lab/dna-methylation",TRUE,FALSE,NA,NA,NA,"**Summary**

Suite of tools to conduct methylation data analysis. Methods from this workspace can be used for alignment and quality control analysis for various protocols including Whole Genom Bisulfite Sequencing (WGBS), Reduced Representation Bisulfite Sequencing (RRBS) and Hybrid Selection Bisulfite Sequencing (HSBS).

More information could be find in the github repo here: https://github.com/aryeelab/dna-methylation-tools

**Workspace Attributes** 

This workspace has no set attributes.  Reference genomes and target regions are changed by altering the input json before running the program.

**Data**

All preprocessing methods require paired end fastq or fastq.gz files entered into the data methods table.

| entity:participant_id 	| bs_fastq1                          	| bs_fastq2                          	|
|-----------------------	|------------------------------------	|------------------------------------	|
| Sample1               	| gs://location/sample_1_R1.fastq.gz 	| gs://location/sample_1_R2.fastq.gz 	|
| Sample2               	| gs://location/sample_2_R1.fastq.gz 	| gs://location/sample_2_R2.fastq.gz 	||

Hybrid Selection Bisulfite Sequencing (HSBS) requies an additional input, the target_region.

| entity:participant_id 	| bs_fastq1                          	| bs_fastq2                          	| target_region                         	|
|-----------------------	|------------------------------------	|------------------------------------	|---------------------------------------	|
| Sample1               	| gs://location/sample_1_R1.fastq.gz 	| gs://location/sample_1_R2.fastq.gz 	| gs://location/hg38_targetCoverage.bed 	|
| Sample2               	| gs://location/sample_2_R1.fastq.gz 	| gs://location/sample_2_R2.fastq.gz 	| gs://location/hg38_targetCoverage.bed 	|

Participant sets can be created from participant_id by creating a table in this format:

| membership:participant_set_id 	| participant 	|
|-------------------------------	|-------------	|
| Human_single_cell_Expt_1      	| Sample1     	|
| Human_single_cell_Expt_1      	| Sample2     	|

A few test sets have been provided for becoming familiar with the workflow.


| participant_set_id 	| participants 	| cell type                                             	|
|--------------------	|--------------	|-------------------------------------------------------	|
| HES                	| 3            	| Human single cell                                     	|
| HES_set2           	| 2            	| Human single cell                                     	|
| Human_wgbs         	| 1            	| Human whole genome                                    	|
| MES                	| 4            	| Mouse single cell                                     	|
| Mouse_wgbs         	| 1            	| Mouse whole genome                                    	|
| test_set_mouse_sc  	| 3            	| Small mouse single cell data set(for program testing) 	|


**Tools**

This workspace contains the following preset method configurations, already set up for grch38, hg19 and mm10.  Other genomes can be loaded by changing the json inputs.

* Preprocessing steps are run as ""participant"", then selecting one sample or a set of samples  
* The aggregation method is run on a ""participant set"" and is run on a set of preprocessed samples aligned to the same genome.
* Aggregation can only be done after preprocessing.
* Both preprocessing and aggregation generate html reports.  Samples are provided below.

**Preprocessing:**

* bismark_wgbs: Preprocess Whole Genome Bisulfite Sequencing (WGBS) data
* bismark_rrbs: Preprocess Reduced Representation Bisulfite Sequencing (RRBS) data
* bismark_hsbs: Preprocess Hybrid Selection Bisulfite Sequencing (HSBS) data

[Example bismark processing report](https://storage.googleapis.com/terra-featured-workspaces/methylation-preprocessing/example_output/test_HES_sample_1_R1.fastq.gz_bismark_report.html)

**Aggregation:**

* aggregate_bismark_output: Aggregates outputs from preprocessing pipelines and produces an aggregated data structure for further downstream analysis

[Example output from scmeth R package](https://storage.googleapis.com/terra-featured-workspaces/methylation-preprocessing/example_output/qcReport.html)

Based on the reference genome different method configurations could be selected. (So far we have hg38, hg19 and mm10 reference genome)

**Cost Analysis:**

| Sample size |     Per-sample preprocessing |     Aggregation and QC | Total |
| :---    | :--- | :--- | :--- |
|     | (Hours / $)                  | (Hours / $)            | (Hours / $)        |
| ..................................... | ............................................................................. | .......................................................... | ......................... |
| 10              | 0.98 ($0.93)                 | 0.97 ($0.28)           | 1.95 ($1.21)       |
| 100             | 1.47 ($8.99)                 | 6.00 ($0.86)           | 7.47 ($9.85)       |
| 1000            | 4.48 ($52.48)                | 58.01 ($13.74)         | 62.49 ($66.22)     |


Estimates were obtained when the workflows were run on the default n1-highmem-4 compute nodes (26 GB RAM with 4 CPUs) in FireCloud. Note that these times and costs should decrease as workflows are improved and compute resources become cheaper.  The samples consisted of 1000 single-cell RRBS samples with a median of 872,223 reads. ",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","aryee-lab/dna-methylation"
59,"broad-firecloud-cptac","PANOPLY_Production_Pipelines_v1_1","READER","https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_Production_Pipelines_v1_1",TRUE,FALSE,NA,NA,NA,"# PANOPLY: A cloud-based platform for automated and reproducible proteogenomic data analysis
#### Version 1.1

PANOPLY is a platform for applying state-of-the-art statistical and machine learning algorithms to transform multi-omic data from cancer samples into biologically meaningful and interpretable results. PANOPLY leverages [Terra](http://app.terra.bio)—a cloud-native platform for extreme-scale data analysis, sharing, and collaboration—to host proteogenomic workflows, and is designed to be flexible, automated, reproducible, scalable, and secure. A wide array of algorithms applicable to all cancer types have been implemented, and available in PANOPLY for analysis of cancer proteogenomic data.


![*Figure 1.* Overview of PANOPLY architecture and the various tasks that constitute the complete workflow. Tasks can also be run independently, or combined into custom workflows, including new tasks added by users. Inputs to PANOPLY consists of (i) externally characterized genomics and proteomics data (in gct format); (ii) sample phenotype and annotations (in csv format); and (iii) parameters settings (in yaml format). Panoply modules are grouped into Data Preparation Modules (green box), Data Analysis Modules (blue box) and Report Modules (red box). Data preparation modules perform quality checks on input data followed by optional normalization and filtering for proteomics data. Analysis ready data tables are then used as inputs to the data analysis modules. Results from the data analysis modules are summarized in interactive reports generated by appropriate report modules.](https://raw.githubusercontent.com/broadinstitute/PANOPLY/dev/panoply-overview.png)


PANOPLY v1.1 consists of the following components:

* A Terra production workspace on [PANOPLY_Production_Pipelines_v1_1](https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_Production_Pipelines_v1_1) with a preconfigured unified workflow to automatically run all analysis tasks on proteomics (global proteome, phosphoproteome, acetylome, ubiquitylome), transcriptome and copy number data; and an [additional workspace](https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_Production_Modules_v1_1) that includes separate methods for each analysis component. 
* An interactive Jupyter notebook (included in the Terra workspaces) that provides step-by-step instructions for uploading data, identifying data types, specifying parameters, and setting up the PANOPLY workspace for analyzing a new dataset.
* A GitHub [repository](https://github.com/broadinstitute/PANOPLY) that contains code for all PANOPLY tasks and workflows, including R code for implementing analysis algorithms, task module creation, and release management. A GitHub [wiki](https://github.com/broadinstitute/PANOPLY/wiki) includes documentation and description of algorithms. 
* A [tutorial](https://github.com/broadinstitute/PANOPLY/wiki/PANOPLY-Tutorial) illustrating the application of PANOPLY to a published breast cancer dataset and demonstrating the practical relevance of PANOPLY by regenerating many of the results described in (Mertins et al) (1) with minimal effort. The [PANOPLY_Tutorial](https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_Tutorial) workspace contains data and results from running the tuorial.
* Terra workspaces showing case studies of applying PANOPLY to the analysis of [CPTAC BRCA](https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_CPTAC_BRCA) (7) and [CPTAC LUAD](https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_CPTAC_LUAD) (8) datasets. The workspaces include all data, parameter settings and results.


PANOPLY provides a comprehensive collection of proteogenomic data analysis methods including sample QC (sample quality evaluation using profile plots and tumor purity scores (1), identify sample swaps, etc.), association analysis, RNA and copy number correlation (to proteome), connectivity map analysis (1 ,2), outlier analysis using BlackSheep (3), PTM-SEA (4), GSEA (5) and single-sample GSEA (6), consensus clustering, and multi-omic clustering using non-negative matrix factorization (NMF). Most analysis modules include a report generation task that outputs a HTML interactive report summarizing results from the respective analysis tasks. Mass spectrometry-based proteomics data amenable to analysis by PANOPLY includes isobaric label-based LC-MS/MS approaches like iTRAQ, TMT and TMTPro profiling the proteome and multiple PTM-omes including phospho-, acetyl-and ubiquitylomes.

Users can also [customize or create new pipelines and add their own tasks](https://github.com/broadinstitute/PANOPLY/wiki/Customizing-PANOPLY) and integrate them into the PANOPLY.


## Quick Start

* For a quick introduction and tour of PANOPLY, follow the [**tutorial**](https://github.com/broadinstitute/PANOPLY/wiki/PANOPLY-Tutorial). 
* Detailed **documentation** can be found [here](https://github.com/broadinstitute/PANOPLY/wiki).

### Contact

Email proteogenomics@broadinstitute.org with questions, comments or feedback.


### Citation

Mani, D. R. et al. PANOPLY: a cloud-based platform for automated and reproducible proteogenomic data analysis. *Nature Methods* 1–3 (2021) doi:10.1038/s41592-021-01176-6.
  

### References

1. Mertins, P. et al. Proteogenomics connects somatic mutations to signalling in breast cancer. *Nature* 534, 55–62 (2016).
2. Subramanian, A. et al. A Next Generation Connectivity Map: L1000 Platform and the First 1,000,000 Profiles. *Cell* 171, 1437–1452.e17 (2017).
3. Blumenberg, L. et al. BlackSheep: A Bioconductor and Bioconda package for differential extreme value analysis. *J of Proteome Research* 20(7), 3767-3773 (2021).
4.	Krug, K. et al. A Curated Resource for Phosphosite-specific Signature Analysis. *Mol. Cell. Proteomics* 18, 576–593 (2019).
5.	Subramanian, A. et al. Gene set enrichment analysis: a knowledge-based approach for interpreting genome-wide expression profiles. *Proc. Natl. Acad. Sci.* 102, 15545–15550 (2005).
6.	Barbie, D. A., Tamayo, P., Boehm, J. S., Kim, S. Y. & Moody, S. E. Systematic RNA interference reveals that oncogenic KRAS-driven cancers require TBK1. *Nature* (2009).
7. Gillette, M. A. et al. Proteogenomic Characterization Reveals Therapeutic Vulnerabilities in Lung Adenocarcinoma. *Cell* 182, 200–225.e35 (2020).
8. Krug, K., Jaehnig, E. J., Satpathy, S., Blumenberg, L., et al. Proteogenomic Landscape of Breast Cancer Tumorigenesis and Targeted Therapy. *Cell* 183, 1–21 (2020).
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","broad-firecloud-cptac/PANOPLY_Production_Pipelines_v1_1"
60,"broad-firecloud-tcga","TCGA_READ_hg38_OpenAccess_GDCDR-12-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_READ_hg38_OpenAccess_GDCDR-12-0_DATA",TRUE,TRUE,"Rectum adenocarcinoma","Tumor/Normal","USA","TCGA Rectum adenocarcinoma Open-Access Data Workspace

*hg38 TCGA and TARGET workspaces reference files by their GDC UUIDs.  In order to run analyses on the referenced data files, you will need to run workflows that retrieve the files from the GDC and copy them to your workspace bucket.  See   [this forum post](https://gatkforums.broadinstitute.org/firecloud/discussion/10382/populating-hg38-tcga-and-target-workspaces-with-data-files#latest) for instructions on the running of these workflows.*","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients' clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","172","Colorectal","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh38/hg38","TCGA_READ_hg38_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_READ_hg38_OpenAccess_GDCDR-12-0_DATA"
61,"broad-firecloud-cptac","PANOPLY_Production_Modules_v1_2","READER","https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_Production_Modules_v1_2",TRUE,FALSE,NA,NA,NA,"# PANOPLY: A cloud-based platform for automated and reproducible proteogenomic data analysis
#### Version 1.2

PANOPLY is a platform for applying state-of-the-art statistical and machine learning algorithms to transform multi-omic data from cancer samples into biologically meaningful and interpretable results. PANOPLY leverages [Terra](http://app.terra.bio)—a cloud-native platform for extreme-scale data analysis, sharing, and collaboration—to host proteogenomic workflows, and is designed to be flexible, automated, reproducible, scalable, and secure. A wide array of algorithms applicable to all cancer types have been implemented, and available in PANOPLY for analysis of cancer proteogenomic data.


![*Figure 1.* Overview of PANOPLY architecture and the various tasks that constitute the complete workflow. Tasks can also be run independently, or combined into custom workflows, including new tasks added by users. Inputs to PANOPLY consists of (i) externally characterized genomics and proteomics data (in gct format); (ii) sample phenotype and annotations (in csv format); and (iii) parameters settings (in yaml format). Panoply modules are grouped into Data Preparation Modules (green box), Data Analysis Modules (blue box) and Report Modules (red box). Data preparation modules perform quality checks on input data followed by optional normalization and filtering for proteomics data. Analysis ready data tables are then used as inputs to the data analysis modules. Results from the data analysis modules are summarized in interactive reports generated by appropriate report modules.](https://raw.githubusercontent.com/broadinstitute/PANOPLY/dev/panoply-overview.png)


PANOPLY v1.2 consists of the following components:

* A Terra production workspace on [PANOPLY_Production_Pipelines_v1_1](https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_Production_Pipelines_v1_1) with a preconfigured unified workflow to automatically run all analysis tasks on proteomics (global proteome, phosphoproteome, acetylome, ubiquitylome), transcriptome and copy number data; and an [additional workspace](https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_Production_Modules_v1_1) that includes separate methods for each analysis component. 
* An interactive Jupyter notebook (included in the Terra workspaces) that provides step-by-step instructions for uploading data, identifying data types, specifying parameters, and setting up the PANOPLY workspace for analyzing a new dataset.
* A GitHub [repository](https://github.com/broadinstitute/PANOPLY) that contains code for all PANOPLY tasks and workflows, including R code for implementing analysis algorithms, task module creation, and release management. A GitHub [wiki](https://github.com/broadinstitute/PANOPLY/wiki) includes documentation and description of algorithms. 
* A [tutorial](https://github.com/broadinstitute/PANOPLY/wiki/PANOPLY-Tutorial) illustrating the application of PANOPLY to a published breast cancer dataset and demonstrating the practical relevance of PANOPLY by regenerating many of the results described in (Mertins et al) (1) with minimal effort. The [PANOPLY_Tutorial](https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_Tutorial) workspace contains data and results from running the tuorial.
* Terra workspaces showing case studies of applying PANOPLY to the analysis of [CPTAC BRCA](https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_CPTAC_BRCA) (7) and [CPTAC LUAD](https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_CPTAC_LUAD) (8) datasets. The workspaces include all data, parameter settings and results.


PANOPLY provides a comprehensive collection of proteogenomic data analysis methods including sample QC (sample quality evaluation using profile plots and tumor purity scores (1), identify sample swaps, etc.), association analysis, RNA and copy number correlation (to proteome), connectivity map analysis (1 ,2), outlier analysis using BlackSheep (3), PTM-SEA (4), GSEA (5) and single-sample GSEA (6), consensus clustering, and multi-omic clustering using non-negative matrix factorization (NMF). Most analysis modules include a report generation task that outputs a HTML interactive report summarizing results from the respective analysis tasks. Mass spectrometry-based proteomics data amenable to analysis by PANOPLY includes isobaric label-based LC-MS/MS approaches like iTRAQ, TMT and TMTPro profiling the proteome and multiple PTM-omes including phospho-, acetyl-and ubiquitylomes.

Users can also [customize or create new pipelines and add their own tasks](https://github.com/broadinstitute/PANOPLY/wiki/Customizing-PANOPLY) and integrate them into the PANOPLY.


## Quick Start

* For a quick introduction and tour of PANOPLY, follow the [**tutorial**](https://github.com/broadinstitute/PANOPLY/wiki/PANOPLY-Tutorial). 
* Detailed **documentation** can be found [here](https://github.com/broadinstitute/PANOPLY/wiki).

### Contact

Email proteogenomics@broadinstitute.org with questions, comments or feedback.


### Citation

Mani, D. R. et al. PANOPLY: a cloud-based platform for automated and reproducible proteogenomic data analysis. *Nature Methods* 1–3 (2021) doi:10.1038/s41592-021-01176-6.
  

### References

1. Mertins, P. et al. Proteogenomics connects somatic mutations to signalling in breast cancer. *Nature* 534, 55–62 (2016).
2. Subramanian, A. et al. A Next Generation Connectivity Map: L1000 Platform and the First 1,000,000 Profiles. *Cell* 171, 1437–1452.e17 (2017).
3. Blumenberg, L. et al. BlackSheep: A Bioconductor and Bioconda package for differential extreme value analysis. *J of Proteome Research* 20(7), 3767-3773 (2021).
4.	Krug, K. et al. A Curated Resource for Phosphosite-specific Signature Analysis. *Mol. Cell. Proteomics* 18, 576–593 (2019).
5.	Subramanian, A. et al. Gene set enrichment analysis: a knowledge-based approach for interpreting genome-wide expression profiles. *Proc. Natl. Acad. Sci.* 102, 15545–15550 (2005).
6.	Barbie, D. A., Tamayo, P., Boehm, J. S., Kim, S. Y. & Moody, S. E. Systematic RNA interference reveals that oncogenic KRAS-driven cancers require TBK1. *Nature* (2009).
7. Gillette, M. A. et al. Proteogenomic Characterization Reveals Therapeutic Vulnerabilities in Lung Adenocarcinoma. *Cell* 182, 200–225.e35 (2020).
8. Krug, K., Jaehnig, E. J., Satpathy, S., Blumenberg, L., et al. Proteogenomic Landscape of Breast Cancer Tumorigenesis and Targeted Therapy. *Cell* 183, 1–21 (2020).
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","broad-firecloud-cptac/PANOPLY_Production_Modules_v1_2"
62,"fc-product-demo","CRDC-Dynamic-Queries-for-NIH-Genomic-Data-Commons-Projects","READER","https://app.terra.bio/#workspaces/fc-product-demo/CRDC-Dynamic-Queries-for-NIH-Genomic-Data-Commons-Projects",TRUE,FALSE,NA,NA,NA,"This workspace shows you how to take a query result from the [NCI Genomic Data Commons](https://portal.gdc.cancer.gov/) (GDC) data portal and use it as the input to a workflow (or Notebook) in FireCloud.

**To get started with this tutorial, make sure you ""clone"" this workspace so you have your own copy to work with.**  
Note: Hold down the control button – or the command key on a Mac computer – to open any links below in a new tab.

## Overview of the NCI Cancer Research Data Commons projects
The National Cancer Institute launched the [Cancer Research Data Commons ](https://datacommons.cancer.gov/) to provide researchers the means to accelerate discovery through the connection and harmonization of datasets with analytical tools in cloud native environments. From this, repositories of data known as data commons, were born.  The number of data commons and the type of data being collected continue to grow, with new data commons launching almost yearly. Among the data commons, and their associated datasets, are:

* [Genomics Data Commons](https://datacommons.cancer.gov/repository/genomic-data-commons) (GDC): Supports hosting, standardization, and analysis of genomic, clinical, and biospecimen data. The GDC harmonizes raw sequencing data, identifies and applies bioinformatics methods for generating mutation calls, structural variants and other high-level data. 
* [Proteomics Data Commons](https://datacommons.cancer.gov/repository/proteomic-data-commons) (PDC): Advances the understanding of the role that proteins play in the cancer lifecycle. In-depth analysis of proteomic data allows the study of both how and why cancer develops and informs ways of tailoring treatment for patients.
* [Imaging Data Commons](https://datacommons.cancer.gov/repository/imaging-data-commons) (IDC): Connects researchers with publicly available cancer imaging data, often linked with other types of cancer data. IDC provides the tools to search and visualize cancer imaging data, define cohorts and use those cohorts to better understand the disease. 

Other data commons within the CRDC, like the Integrated Canine Data Commons launched in 2020,  will have similar workspaces when there is sufficient data and infrastructure to support dynamically querying and pointing the data to a workspace. The workspace in particular will focus on accessing data in GDC.

## GDC data access

**Account Linking**  
To use datasets from the Genomics Data Commons [GDC](https://portal.gdc.cancer.gov/analysis_page?app=CohortBuilder&tab=available_data) in your FireCloud workspace you need to first link your GDC account with FireCloud.  Go to your [Profile page](https://app.terra.bio/#profile) and look for ""NCI CRDC Framework Services"" under ""IDENTITY & EXTERNAL SERVERS"".  Click the link to login using your [eRA Commons ID](https://public.era.nih.gov/commonsplus/public/login.era) (which you should have already if you work for an academic organization and have applied for a grant).  
Linking your accounts is required even if you don't have access to controlled access data.  If you want to access controlled access data then you will need to apply to studies via dbGaP, see [instructions on the GDC website](https://gdc.cancer.gov/access-data/obtaining-access-controlled-data).


**Confirm GDC data access**  
The [DRS standards ](https://www.ga4gh.org/news/drs-api-enabling-cloud-based-data-access-and-retrieval/) will be utilized to access GDC data by both notebooks and workflows in this workspace.  Once your GDC and FireCloud accounts have been linked, we can use a simple [md5sum workflow](https://www.dockstore.org/containers/quay.io/briandoconnor/dockstore-tool-md5sum:1.0.4?tab=info) to test access by running an open access sample through the workflow. The workflow outputs a simple md5 sum of the chosen file if data access is setup properly and it fails if not. 

## Query and point a FireCloud workspace to GDC data 

These steps will guide you through querying data from the GDC portal, downloading the query manifest, and translating the manifest to workspace data tables that can be used to run a workflow.  In this example, we will use CPTAC data.

#### Step 1 - Download Manifest from GDC Portal
**1A**.  Navigate to the  [GDC](https://portal.gdc.cancer.gov/analysis_page?app=CohortBuilder&tab=available_data)

**1B**.  Search for Access = open (Files tab) and Program = CPTAC.  **NOTE:** you can really choose anything here.  If you want to follow along with this tutorial use open access CPTAC data.  But if you want to work with different data feel free to search for whatever is appropriate given your research interest.

**1C**.  Click the download ""Manifest"" button.  A TSV that looks like the following will appear:

```
id    filename    md5    size    state
1d50ef40-b726-48f7-b81d-ae0e4dab714b    d9124538-347c-483e-aee9-83c462e87976.FPKM-UQ.txt.gz    04d6c247dada7cf0dad93a839f6b7437    438083    released
4b1c6ee1-b46a-4b9d-bb74-d303719c729f    dc383158-c7c8-4fa5-a0fc-f9b2f9d619e3.FPKM-UQ.txt.gz    b9dbd7f7b417cdc8b978f708508dd7cb    442855    released
f4f165ef-15d1-4cf4-909b-0c9b80b295c4    5a5e2ef7-89c2-455d-8d11-84b86fed0b7b.FPKM-UQ.txt.gz    0307ce18caefee845629b53bb37181d0    445557    released
7b30dc0f-017d-42de-8ec7-d9748add2c9c    077d2d30-e631-4f36-9832-04a7f1f451b6.FPKM-UQ.txt.gz    8e4fc7054339d05062c06d797bdd376d    446449    released
```

#### Step 2 - Transform Manifest to FireCloud Table Format  

FireCloud doesn't recognize this manifest format-- it will be transformed in this step into a format that is usable in FireCloud. 

Along the way we'll create DRS URIs that FireCloud can use with your eRA Commons ID to access (both open and controlled access data).  This avoids you having to manually download files from GDC and reuploading them to FireCloud.  You can just point to the DRS URI as a file input for a workflow, and FireCloud will take care of accessing the file from GDC.

In this step, we are essentially making the above manifest file from step 1 look like the following:

```
entity:drs_id	drs_uri	filename
1d50ef40-b726-48f7-b81d-ae0e4dab714b	drs://dg.4DFC:1d50ef40-b726-48f7-b81d-ae0e4dab714b	d9124538-347c-483e-aee9-83c462e87976.FPKM-UQ.txt.gz
...
```

Notice, 
- We're adding ""entity:drs_id"" at the top of the file in the header and adding a new ""drs_uri"" column.  You make this URI using:

  `  drs://dg.4DFC:<ID from first column of manifest>`
		
- The `md5` , `size`, `state` columns were removed here for simplicity, but they can be left in the transformed table. 
		

Instead of doing this manually, use the notebook in this workspace to do this for you:

**2A**. Navigate to the Data tab of your workspace.

**2B**. Click on the `Files` tab on the left hand column

**2C**. Click on the `+` icon on the bottom right and upload the manifest file you downloaded from GDC to your workspace.

**2D**. Navigate to the Notebook tab of your workspace and run the  ""Upload GDC Manifest to Workspace Data table"" notebook. 

**2E**. Follow the along with the instructions in the notebook, **making sure to run each cell in order, from top to bottom**. 


#### Step 3 - Run a Test Workflow

Now that you have a data table ""drs"" created and loaded with a few GDC DRS URIs (and other info) you can run a workflow to test that everything is working properly.  Normally, you can just go to [Dockstore.org](https://dockstore.org) and pick the ""md5sum"" WDL workflow.  This step has already been done for you as well as binding the input of ""this.drs_uri"" in the ""drs"" table to the input file of the md5sum workflow.

**NOTE:** if you are just following this tutorial to test your data access and to learn how to take a search result from GDC and work with it in FireCloud then you ***do not*** need to run the md5sum workflow on all CPTAC data!!  Before you launch the workflow choose ""SELECT DATA"", then choose ""Choose specific rows to process"", and select the first row.  That will just run md5sum on a single file and should be sufficient to test your data access.

If you run this workflow on your drs table and it finishes successfully, congratulations, you have accessed the GDC data you found in the GDC portal through the DRS standard in Terra.  This may seem like a hassle to setup but, once you do, you're saving countless hours (and $$$) by not temporarily copying data from GDC to Terra.  This approach also opens up all datasets on the GDC data portal, not just open access CPTAC data that was used in this tutorial.  You can use this approach for any open access data as well as any datasets where you have dbGaP approved access.  At the time of writing this tutorial, GDC has ~1.6PB of data available!  For details, see the table below:

| Header  | Value |
| ----------- | -------- |
| Dataset Size      | 1.57 PB |
| Programs          | 16 |
| Projects             | 67 | 
| Primary Sites    | 68 | 
| Cases      | 84,392       |
| Files   | 596,758        |

**Table 1:** GDC statistics as of March 2021.


#### Step 4 - Run Your Own Analysis

If the workflow from step 3 runs successfully on one of the data files from the drs table then you should be ready to use this technique with whatever data you're interested in from the GDC portal and with your own analysis workflows.  You can choose to use this workspace and just add additional data by searching in the GDC portal, downloading the manifest, and running the notebook again to load it.  Alternatively, if you want a clean copy to start your analysis work in, you can delete this workspace you used for the tutorial and clone the original workspace again, this time uploading a manifest from GDC that corresponds to the search you are interested in.


## Notebooks
More information about the notebook:

**Upload GDC Manifest to Workspace Data table:**  
What does it do?  
Given a GDC manifest file convert file to Terra readable TSV that can be uploaded to the Data table.

| Runteime | Value |
|--|--| 
| Environments | Default (GATK 4.1.4.1 Python3.7.8) |
| CPU Minimum | 2 |
| Disksize Minimum | 10 GB |
| Memory Minmum | 15 GB |


## Workflows 
More information about the workflow:

**dockstore-wdl-workflow-md5sum:**  
What does it do?  
This is an extremely simple workflow used to show how to call a workflow via Dockstore.  

What does it require as input?  
inputFile- File which will have its md5sum retrieved  

What does it return as output?  
Value - md5sum value


| file Name | Time | Cost $|
|--|--|--|
| htseq_counts.txt.gz | 4m | <0.01 |



### License
**Copyright Broad Institute, 2021 | BSD-3**
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at https://github.com/openwdl/wdl/blob/master/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","fc-product-demo/CRDC-Dynamic-Queries-for-NIH-Genomic-Data-Commons-Projects"
63,"help-gatk","GATKTutorials-Pipelining-June2019","READER","https://app.terra.bio/#workspaces/help-gatk/GATKTutorials-Pipelining-June2019",TRUE,FALSE,NA,NA,NA,"# What's in this workspace?
Welcome to Day 4 of the Genome Analysis Toolkit (GATK) workshop at Newcastle University in Newcastle upon Tyne, U.K.!! 

Earlier today, you learned about WDL and Cromwell, and using this empty workspace, we will practice starting a workspace from scratch. Your instructor will guide you through this workspace, but you should find the documentation on this page sufficient to run through them again on your own or to share with a friend later (and we encourage you to do so!).

### Data
You will need to download the data bundle located [here](broad.io/GATK1906) if you haven't already. This bundle contains several folders but the one you will need for this workspace is labelled **Terra**. It provides the WDL script, workspace and sample metadata, and an inputs file to run.

### Tools
There are no tools in this workspace yet. But we will be putting a tool in that runs GATK's HaplotypeCaller, using the instructions below.

**Add a new Tool**
1. In a new tab, navigate to Code & Tools. On the right hand side of the page, click on the `Broad Methods Repository` link. This currently will take you to our legacy application to upload a new Method, which is another name for Tool. In the near future, you will be able to upload your new tool directly in Terra. 
2. Click the blue `Create New Method` button in the upper right corner.
3. Fill out the window that pops up as follows:
>**Namespace:** your own name or abbreviation of your name. 

The namespace is essentially the publisher of the method. You can publish it as yourself, as we are doing here, or sometimes you may want to publish it under your lab or institution's name. 
>**Name:** `HelloGATK`

This refers to the name of your tool. You can call it whatever you want, but we've chosen to match it to the WDL script we will be using. Lastly:
>**WDL:** Click the blue `load from file...` link, and load the `hello_gatk_terra.wdl` script from your data bundle, under the Terra folder

4. Click `Upload`
5. Click the blue `Export to Workspace` button in the upper right corner, then in the dialog that pops up, select `Use Blank Configuration`
6. Under `Destination Workspace`, select your clone of this workspace, then click `Export to Workspace`
7. The page will ask you `Go to edit page now?` Click Yes, and you will be redirected back to your workspace!

**Add Data**

For our tutorial purposes, we've already uploaded your sample files to a public google bucket so that you do not need to incur storage costs. If you did want to upload your own samples to run this workflow on later, you can do so by navigating to the Data tab in your workspace, and clicking on the `Files` section in the lefthand menu. From there, you can upload whatever files you need.

**Run your new Tool!**
1. Navigate to the `Tools` tab in your workspace
2. Click on the HelloGATK tool you recently added.
3. Drag the `hello_gatk_fc.inputs.json` file from your Terra folder in the data bundle to upload it and populate the inputs section. You'll notice there are a number of `gs://` links, which represent files stored in google buckets. 
4. Click the green `Save` button to save the inputs you just uploaded
5. At the top of the page, select the radio button next to `Process single workflow from files`
6. Click the green `Run Analysis` button

**Optional: Parameterize your inputs**

You don't want to go in and change these `gs://` links for each individual sample you want to run on, especially if you're running on a large number of samples. You can run your workflow on many samples with a single click by setting up your Data tables.
1. Go to the `Data` tab in your workspace
2. Upload `participant.tsv` and `sample.tsv` from the Terra folder in your data bundle by clicking the `+` icon next to Tables in the lefthand menu
3. Click on the Workspace Data section in the lefthand menu, and upload `workspace_attributes.tsv` from the Terra folder. 
4. Navigate back to the `Tools` tab. 
5. Select the radio button for `Process multiple workflows from` and choose `Sample` from the dropdown. 
6. In the inputs table at the bottom, we now need to replace the `gs://` links with references to the data table. For the `inputBAM` and `bamIndex`, try typing `this` and see what Terra auto-fills for you as options. The keyword `this` refers to whatever data table you selected in the dropdown at the top. Since we picked `Sample`, it fills in options that exist in your `Sample` table. Click `Save` when you are done.
7. Click the green `Run Analysis` button

### Software versions
GATK4.1.1.0

## Appendix

### GATK @ Newcastle 2019 in the Terra Support Center

Get yourself oriented with the entire workshop in the [Terra Support Center](https://broadinstitute.zendesk.com/hc/en-us/community/topics). (more on Terra Support below).

### GATK documentation and support

Detailed documentation, tutorials, and more are available at the [GATK web site](https://software.broadinstitute.org/gatk/). If you are still running into issues after consulting the User Guide, have a question, or just want to say hello, reach out to the GATK team via the [GATK Support Forum](https://gatkforums.broadinstitute.org/gatk)!

### Terra documentation and support

Documents and helpful guides to support your journey through Terra are available in the Terra Support Center. Navigate there by following the “Learn About Terra” link in the left-side menu. At the time of this workshop, we are still actively writing and publishing documents but we’ve got a good set of docs in the Quick Start Guide to get you going.


",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/GATKTutorials-Pipelining-June2019"
64,"broad-firecloud-tcga","TCGA_KICH_hg38_OpenAccess_GDCDR-12-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_KICH_hg38_OpenAccess_GDCDR-12-0_DATA",TRUE,TRUE,"Kidney Chromophobe","Tumor/Normal","USA","TCGA Kidney Chromophobe Open-Access Data Workspace

*hg38 TCGA and TARGET workspaces reference files by their GDC UUIDs.  In order to run analyses on the referenced data files, you will need to run workflows that retrieve the files from the GDC and copy them to your workspace bucket.  See   [this forum post](https://gatkforums.broadinstitute.org/firecloud/discussion/10382/populating-hg38-tcga-and-target-workspaces-with-data-files#latest) for instructions on the running of these workflows.*","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients' clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","113","Kidney","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh38/hg38","TCGA_KICH_hg38_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_KICH_hg38_OpenAccess_GDCDR-12-0_DATA"
65,"lincs-phosphodia","Avant-garde_TripleProteome","READER","https://app.terra.bio/#workspaces/lincs-phosphodia/Avant-garde_TripleProteome",TRUE,FALSE,NA,NA,NA,"Triple proteome dataset",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","lincs-phosphodia/Avant-garde_TripleProteome"
66,"uk-biobank-sek","genetic-association-of-albuminuria-with-cardiometabolic-disease-and-blood-pressure","READER","https://app.terra.bio/#workspaces/uk-biobank-sek/genetic-association-of-albuminuria-with-cardiometabolic-disease-and-blood-pressure",TRUE,TRUE,NA,NA,NA,"# Genetic Association of Albuminuria with Cardiometabolic Disease and Blood Pressure

This workspace reproduces some analysis results from:

> Haas, ME et al. (2018) **Genetic Association of Albuminuria with
> Cardiometabolic Disease and Blood Pressure**. AJHG volume
103, issue 4, p461-473.
> [*doi:10.1016/j.ajhg.2018.08.004*](https://doi.org/10.1016/j.ajhg.2018.08.004)

At a high level:

* Researchers[[*1*]](http://www.nealelab.is/blog/2017/9/11/details-and-considerations-of-the-uk-biobank-gwas) have taken most columns from the raw UK Biobank phenotypes and run GWAS.
* In this paper, Haas et. al. have combined several phenotypes into one that is clinically robust.
* They then performed GWAS against that derived phenotype, generating very good results.

----------------------------

## How long will it take to run? How much will it cost?

This workspace takes as input controlled-access data from UK Biobank. If you have access to the underlying data, you can clone this workspace and reproduce the results yourself, if you like. Otherwise, you can **read these notebooks**, but you will not be able to run them successfully.

**Time:**  The notebooks that take the most time to run are the two that perform analyses on the UK Biobank genotypes.

* **4_perform_quality_control_on_genotypes.ipynb** - 47 minutes
* **5_perform_albuminuria_gwas.ipynb** - 51 minutes

**Cost:**  The cluster size recommended for use with the two computationally intensive notebooks listed above costs $47 per hour to run. There is no need to run a cluster that large for any of the other notebooks, so their costs are very small in comparison.

----------------------------

## Analysis Artifacts

* **1_create_derived_phenotypes_for_albuminuria_gwas.ipynb** - Create the derived phenotypes for use in the GWAS by combining urinary albumin and creatinine measures.
* **2_prepare_haplotype_reference_consortium_snp_filter.ipynb** - Create a filter holding only Haplotype Reference Consortium reference panel SNPs occuring on the autosomes.
* **3_prepare_imputation_quality_snp_annotation.ipynb** - Prepare imputation quality SNP annotation.
* **4_perform_quality_control_on_genotypes.ipynb** - Compute several quality control measures on the genotypes such as allele frequency and Hardy-Weinberg equilibrium.
* **5_perform_albuminuria_gwas.ipynb** - Perform GWAS for cardiovascular disease phenotypes, i.e. urinary albumin (continuous trait).
* **6_explore_albuminuria_gwas_results.ipynb** - Explore GWAS results with static and interactive plots.
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","uk-biobank-sek/genetic-association-of-albuminuria-with-cardiometabolic-disease-and-blood-pressure"
67,"degenome","degenome","READER","https://app.terra.bio/#workspaces/degenome/degenome",TRUE,TRUE,NA,NA,NA,"[DEGenome](https://github.com/eweitz/degenome) transforms differential expression data into inputs for [exploratory genome analysis with Ideogram.js](https://eweitz.github.io/ideogram/differential-expression?annots-url=https://www.googleapis.com/storage/v1/b/degenome/o/GLDS-4_array_differential_expression_ideogram_annots.json).  

Try the [Notebook tutorial](https://app.terra.bio/#workspaces/degenome/degenome/notebooks/launch/degenome-tutorial.ipynb), where you can step through using DEGenome to analyze expression for mice flown in space!",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","degenome/degenome"
68,"ctat-firecloud","infercnv","READER","https://app.terra.bio/#workspaces/ctat-firecloud/infercnv",TRUE,FALSE,NA,NA,NA,"## InferCNV

A fully reproducible example workflow for inferring copy number from single-cell RNA sequencing data

Complete documentation for InferCNV is available on the [here](https://github.com/broadinstitute/inferCNV/wiki).

### Workflow Description

InferCNV is used to explore tumor single cell RNA-Seq data to identify evidence for somatic large-scale chromosomal copy number alterations, such as gains or deletions of entire chromosomes or large segments of chromosomes. This is done by exploring expression intensity of genes across positions of tumor genome in comparison to a set of reference 'normal' cells. A heatmap is generated illustrating the relative expression intensities across each chromosome, and it often becomes readily apparent as to which regions of the tumor genome are over-abundant or less-abundant as compared to that of normal cells.

InferCNV provides access to several residual expression filters to explore minimizing noise and further revealing the signal supporting CNA. Additionally, inferCNV includes methods to predict CNA regions and define cell clusters according to patterns of heterogeneity.

InferCNV is one component of the TrinityCTAT toolkit focused on leveraging the use of RNA-Seq to better understand cancer transcriptomes. To find out more about Trinity CTAT please visit [TrinityCTAT](https://github.com/NCIP/Trinity_CTAT/wiki).


### Input

- a raw counts matrix of single-cell RNA-Seq expression
- an annotations file which indicates which cells are tumor vs. normal.
- a gene/chromosome positions file

These required inputs are described in more detail [here](https://github.com/broadinstitute/inferCNV/wiki/File-Definitions).


### Output

- infercnv.preliminary.png : the preliminary inferCNV view (prior to denoising or HMM prediction)
- infercnv.png : the final heatmap generated by inferCNV with denoising methods applied.
- infercnv.references.txt : the 'normal' cell matrix data values.
- infercnv.observations.txt : the tumor cell matrix data values
- infercnv.observation_groupings.txt : group memberships for the tumor cells as clustered.
- infercnv.observations_dendrogram.txt : the newick formatted dendrogram for the tumor cells that matches the heatmap

### Example data

The workflow in this workspace is preconfgured with a downsampled expression matrix from oligodendroglioma containing hallmark chr 1p and 19q deletions


### Time and cost estimates
Below is an example of the time and cost for running the workflow with the example data.

| Cost    | Time |       
| ------------- |:-------------:| 
| < $0.01 | 30 minutes


Note: Cost and time will vary with the size of your dataset and use of preemptible instances.

### Example Output

![Example](https://github.com/broadinstitute/inferCNV/wiki/images/infercnv.png)
This figure shows scRNA-Seq expression of oligodendroglioma with hallmark chr 1p and 19q deletions.

### Contact Information
Questions can be directed to the Trinity CTAT email list: trinity_ctat_users@googlegroups.com

### License
Copyright Broad Institute, 2021 | BSD-3
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at https://raw.githubusercontent.com/NCIP/ctat-mutations/master/LICENSE.txt).",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","ctat-firecloud/infercnv"
69,"featured-workspaces-hca","HCA_Optimus_Pipeline","READER","https://app.terra.bio/#workspaces/featured-workspaces-hca/HCA_Optimus_Pipeline",TRUE,FALSE,NA,NA,NA,"# Optimus Pipeline for Analysis of 3’ Single-cell and Single-nucleus Transcriptomic Data


The Optimus pipeline, developed in collaboration with the [Human Cell Atlas](https://www.humancellatlas.org/) Data Coordination Platform (HCA DCP) and the [BRAIN Initiative Cell Census Network](https://biccn.org/) (BICCN), processes 3 prime single-cell or single-nucleus transcriptome data from the [10x Genomics v2 or v3](https://www.10xgenomics.com/solutions/single-cell/) assay. This workspace currently describes `v7.4.0` of the Optimus pipeline and provides fully reproducible examples of the workflow. 

Scroll down for details on the workflow, including input and output descriptions and requirements, estimated run times and costs, and information on downsampled example data.       

**Note that cost and time estimates will vary** with the use of [Preemptibles](https://cloud.google.com/preemptible-vms/).  
You can also use [Google's cost calculator](https://cloud.google.com/products/calculator/) to estimate cost to run. 

**For helpful hints on controlling Cloud costs**, see [Overview: Controlling Cloud costs on Terra](https://support.terra.bio/hc/en-us/articles/360029748111).  

---

##  Optimus 

### What does it do?   

This Optimus workflow has a quality control, alignment, and transcriptome quantification module. It partitions input FASTQ files (when input files are larger than 30 GB), corrects Cell Barcodes (CBs) and Unique Molecular Identifiers (UMIs), aligns reads to the genome, generates a count matrix in a UMI-aware manner, detects empty droplets (single-cell mode only), merges partitioned files, calculates summary metrics for genes and cells, returns read outputs in BAM format, and returns raw counts in H5AD file format. 

Special care is taken to avoid the removal of reads that are not aligned or that do not contain recognizable barcodes. This design provides flexibility to the downstream user and allows for alternative filtering or leveraging the data for novel methodological development.

For more details about the pipeline, see the [Optimus Overview](https://broadinstitute.github.io/warp/docs/Pipelines/Optimus_Pipeline/README) in the [WARP documentation](https://broadinstitute.github.io/warp/).  

---
### How to run the workflow

This workspace is preloaded with  **downsampled example datasets and workflow configurations** (see table below) designed to demo Optimus on samples that are:
- 10x v2 or v3 
- human or mouse 
- single-cell or single-nucleus  

The workspace workflows are configured to leverage the `sample` and `sample_set`  data tables in the workspace Data tab. 

To run the workflow: 

1. Identify the example data (sample_set) you want to test and its corresponding workflow (see table). 
2. Navigate to the Workflows tab and choose the appropriate workflow configuration for the selected data. 
3. Make sure the root entity type is `sample_set` (selected by default) on the workflow configuration page.
4. Choose the dataset of interest.
5. Select the checkbox next to `Use reference disks`.
6. Run the analysis


| Sample_set name | Description | Source | Workflow configuration to choose |
| --- | --- | --- | --- | 
| pbmc4k_human | Human 10x v2 chromosome 21 dataset;  may be run as single-cell or single-nucleus | 10x Genomics [PBMC4k human dataset](https://support.10xgenomics.com/single-cell-gene-expression/datasets/2.0.1/pbmc4k) | [Optimus_Human_v2](https://app.terra.bio/#workspaces/featured-workspaces-hca/HCA_Optimus_Pipeline/workflows/featured-workspaces-hca/Optimus_Human_v2) (single-cell) or [Optimus_Human_v2_snRNA ](https://app.terra.bio/#workspaces/featured-workspaces-hca/HCA_Optimus_Pipeline/workflows/featured-workspaces-hca/Optimus_Human_v2_snRNA)(single-nucleus) |
| neurons2k_mouse | Mouse 10x v2 chromosome 19 single-cell dataset | 10x Genomics [neuron 2k dataset](https://support.10xgenomics.com/single-cell-gene-expression/datasets/2.1.0/neurons_2000) | [Optimus_Mouse_v2](https://app.terra.bio/#workspaces/featured-workspaces-hca/HCA_Optimus_Pipeline/workflows/featured-workspaces-hca/Optimus_Mouse_v2) |
| pbmc_human_v3 | Human 10x v3 chromosome 21 single-cell dataset | 10x Genomics [PMBC_10K_V3 human dataset](https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/pbmc_10k_v30) | [Optimus_Human_v3](https://app.terra.bio/#workspaces/featured-workspaces-hca/HCA_Optimus_Pipeline/workflows/featured-workspaces-hca/Optimus_Human_v3) |


**Optimus is a single sample pipeline, but can take in multiple sets of FASTQs for a sample that has been split over lanes of sequencing.** 

See the workspace data tab for examples of how to configure tables with multiple sequencing lanes per sample. If you are running Optimus on your own data, it is important that you first set up a `sample` data table and then a `sample_set` table. The pipeline requires a set table even if you only have one lane of sequencing. 


#### Running single-nucleus

If you are running the pipeline on your own data, be aware that the pipeline runs in single-cell (sc_rna) mode by default. To use the sn_rna mode for single-nucleus analyses, change the `counting_mode` input attribute to `sn_rna`.  An example of this is shown in the [Human_v2_snRNA workflow configuration](https://app.terra.bio/#workspaces/featured-workspaces-hca/HCA_Optimus_Pipeline/workflows/featured-workspaces-hca/Optimus_Human_v2_snRNA).

---
### What does Optimus require as input?

The Optimus workflow requires the following input:

| Name       |  Description      |
| ------------ | ------------------- |
| cloud_provider | String describing the cloud provider that should be used to run the workflow; value should be ""gcp"" or ""azure"". |
| r1_fastq | Array of FASTQs containing forward reads, contains the unique molecular identifier (UMI) and cell barcode sequences. |
| r2_fastq | Array of FASTQs containing reverse reads, contains the alignable genomic information from the mRNA transcript. |
| input_id | Unique identifier describing the biological sample or replicate that corresponds with the original FASTQ files; can be a human-readable name or UUID; must be entered as a String. |
| tar_star_reference | TAR file containing a species-specific reference genome and GTF; it is generated using the [BuildIndices workflow](https://github.com/broadinstitute/warp/blob/master/pipelines/skylab/build_indices/BuildIndices.wdl).  |
| annotations_gtf |  GTF containing gene annotations used for gene tagging (must match GTF in STAR reference). |
| tenx_chemistry_version | Integer that specifies if data was generated with 10x v2 or v3 chemistry. Optimus validates this chemistry by examining the UMIs and CBs in the first read 1 FASTQ file. If the chemistry does not match, the pipeline will fail. You can remove the check by setting ""ignore_r1_read_length = true"" in the input JSON. |

#### Reference data description and location  
The required reference genomes (human and mouse) and additional resources for the tools in this workspace are included in the `Workspace` data table. The references are preconfigured in the workspace example workflow configurations.

##### Reference genomes: 
The reference genome for human is hg38 (GRCh38), the [GENCODE v43](https://www.gencodegenes.org/human/release_43.html) primary assembly gene annotation list. The reference for mouse is GRCm39, the [GENCODE M32](https://www.gencodegenes.org/mouse/release_M32.html). 

##### Enabling reference disks
The suggested workflow configuration (see ""Running the workflow"" above) uses [reference disks](https://support.terra.bio/hc/en-us/articles/360056384631-Reference-Disks-in-Terra). That means when Terra kicks off the workflow on a virtual computer, it attaches a portable disk (kind of like a flash-drive) that is preloaded with the reference files needed for the workflow. This saves time and cost running the workflow. 

##### Whitelists:
There are two 10x cell barcode whitelists: `whitelist_v2` which is compatible with the 10x Genomics v2 chemistry and `whitelist_v3` which is compatible with 10x Genomics v3 chemistry. 

### Optional parameters 

The Optimus workflow offers optional inputs such as the following (for a more complete list, see the [Optimus overview](https://broadinstitute.github.io/warp/docs/Pipelines/Optimus_Pipeline/README#introduction-to-the-optimus-workflow)):

| Name       |  Description      |
| ------------ | ------------------- |
| expected_cells | Optional integer input for the expected number of cells, which is used calculate library-level metrics. The default is set to 3,000 |
|  gex_nhash_id | Optional input parameter, a string identifier for a library aliquot that is echoed in the h5ad cell by gene matrix (in the data.uns) and the library metrics CSV output; default is set to nul. |
| soloMultiMappers | Optional string describing whether or not the Optimus (GEX) pipeline should run STARsolo with the `--soloMultiMappers` flag; default is ""Uniform"". |
| counting_mode | String describing whether data is single-cell or single-nucleus. Single-cell mode counts reads aligned to the gene transcript, whereas single-nucleus counts whole transcript to account for nuclear pre-mRNA; default is set to ""sc_rna"". | 
| i1_fastq | Array of FASTQs containing index read. | 
| output_bam_basename | String used as a basename for output BAM file; the default is set to the string used for the `input_id` parameter. |
| input_name | String that can be used to further identify the original biological sample. | 
| input_id_metadata_field | String describing, when applicable, the metadata field containing the input_id. |
| input_name_metadata_field | String describing, when applicable, the metadata field containing the input_name. |
| mt_genes | File containing mitochondrial gene names for a specific species; used for calculating gene metrics. |
| whitelist | List of known CBs; the workflow automatically selects the [10x Genomics](https://www.10xgenomics.com/) whitelist that corresponds to the v2 or v3 chemistry based on the input `tenx_chemistry_version`. A custom whitelist can also be provided if the input data was generated with a chemistry different from 10x Genomics v2 or v3. To use a custom whitelist, set the input `ignore_r1_read_length` to ""true"". | 
| emptydrops_lower | UMI threshold for emptyDrops detection; default is 100. |
| ignore_r1_read_length | Boolean that overrides a check on the 10x chemistry; default is ""false""; if ""true"", the workflow will not ensure that the 10x_chemistry_version input matches the chemistry in the read 1 FASTQ. |
| use_strand_info | Optional string to describe if sample is stranded; default is set to ""false"". To use, set to ""true"". |
| count_exons | Optional Boolean indicating whether exon counts should be calculated in sn_rna mode; if ""true"", an additional layer for the Loom file will be output in sn_rna mode and the workflow will return an error in sc_rna mode; default is ""false"". |

---
### What does Optimus return as output?

In this workspace, the metadata for all outputs are written to the workspace data table. These outputs are described in the following table:  

| Name       |  Description      |
| ------------ | ------------------- |
| library_metrics | Optional CSV file containing all library-level metrics calculated with STARsolo for gene expression data. See the [Library-level metrics](https://broadinstitute.github.io/warp/docs/Pipelines/Optimus_Pipeline/Library-metrics/) for how metrics are calculated. |
| multimappers_PropUnique_matrix | Optional output produced when `soloMultiMappers` is ""PropUnique""; see STARsolo [documentation](https://github.com/alexdobin/STAR/blob/master/docs/STARsolo.md#multi-gene-reads) for more information. |
| multimappers_Rescue_matrix | Optional output produced when `soloMultiMappers` is ""Rescue""; see STARsolo [documentation](https://github.com/alexdobin/STAR/blob/master/docs/STARsolo.md#multi-gene-reads) for more information. | 
| multimappers_Uniform_matrix |Optional output produced when `soloMultiMappers` is ""Uniform""; see STARsolo  [documentation](https://github.com/alexdobin/STAR/blob/master/docs/STARsolo.md#multi-gene-reads) for more information. |
| multimappers_EM_matrix | Optional output produced when `soloMultiMappers` is ""EM""; see STARsolo [documentation](https://github.com/alexdobin/STAR/blob/master/docs/STARsolo.md#multi-gene-reads) for more information. |
| aligner_metrics | Per barcode metrics (CellReads.stats) produced by the STARsolo aligner. |
| pipeline_version_out | Version of the processing pipeline run on this data. |
| bam | Merged and sorted BAM file. |
| matrix | Sparse count matrix in numpy format. |
| matrix_row_index | Sparse count matrix row names in numpy format. |
| matrix_col_index | Sparse count matrix column names in numpy format. |
| cell_metrics  | Cell metrics table in text format. |
| gene_metrics | Gene metrics table in text format. | 
| genomic_reference_version | Genomic reference version. |
| cell_calls | Cell metadata from emptyDrops; only output in sc_rna mode. |
| h5ad_output_file | h5ad file with count data (exonic or whole transcript depending on the counting_mode) and metadata. |

The final h5ad matrix contains UMI-corrected counts, as well as the cell and gene metrics detailed in the [Optimus Count Matrix Overview](https://broadinstitute.github.io/warp/docs/Pipelines/Optimus_Pipeline/Loom_schema).  Although the count matrices are UMI-corrected, they are unfiltered in order to provide all possible data to the end researcher. 

Optimus matrices are compatible with multiple downstream community analysis tools. For a tutorial on using an example Optimus matrix with [Seurat](https://satijalab.org/seurat/index.html), [Scanpy](https://scanpy.readthedocs.io/en/stable/), [Cumulus](https://cumulus.readthedocs.io/en/latest/index.html), or [Pegasus](https://pegasus.readthedocs.io/en/stable/#), see the public [Intro-to-HCA-data-on-Terra workspace](https://app.terra.bio/#workspaces/featured-workspaces-hca/Intro-to-HCA-data-on-Terra) (login required) and its accompanying [step-by-step guide](https://support.terra.bio/hc/en-us/articles/360060041772). 

**Zarr array deprecation notice**: As of June 2020 (Optimus v.3.0.0), the previously used Zarr array has been deprecated. The Loom file is now the new default output.

---
### Estimated time and cost to run on sample data 

The following estimates are based on three sets of data, human and mouse, each containing different numbers of samples. All details of each set are listed to give insight into time and cost.
 
| Sample Set Name | Set Size | Sample Set R1.fastq Size | Sample Set R2.fastq Size | Time | Cost $ |
| :---:  | :---: | :---: | :---: | :---: | :---: |
| neurons2k_mouse | 6 entities | 88.26 MB | 277.58 MB | 1:21:00 | 0.16 | 
| pbmc4k_human | 2 entities | 26.84 MB | 59.58 MB | 1:43:00 | 0.20 |
| pbmc_human_v3 | 2 entities | 106.95 MB | 220.04 MB | 1:51:00 | 0.21 |

 
**For helpful hints on controlling Cloud costs, see [Overview: Controlling Cloud costs on Terra](https://support.terra.bio/hc/en-us/articles/360029748111).**   

---
### Optimus validation reports

Optimus has been validated for analyzing both [human](https://github.com/broadinstitute/warp/blob/master/pipelines/skylab/optimus/benchmarking/v1_Apr2019/optimus_report.rst) and [mouse](https://docs.google.com/document/d/1_3oO0ZQSrwEoe6D3GgKdSmAQ9qkzH_7wrE7x6_deL10/edit) single-cell datasets, as well as [single-nucleus datasets](https://docs.google.com/document/d/1rv2M7vfpOzIOsMnMfNyKB4HV18lQ9dnOGHK2tPikiH0/edit). More details about the human validation can be found in the [in the original report](https://docs.google.com/document/d/158ba_xQM9AYyu8VcLWsIvSoEYps6PQhgddTr9H0BFmY/edit).

---
### Versions

You can access previous versions of this pipeline by cloning the workspace and choosing a version in the version dropdown. 

All versions listed here are available by cloning this workspace and selecting the version on the Optimus method. For a complete version list, please read the [Optimus changelog in GitHub](https://github.com/broadinstitute/warp/blob/master/pipelines/skylab/optimus/Optimus.changelog.md)

| Terra Compatible Version Name | Optimus Release Version | Date | Release Note | 
| :---:  | :---: | :---: | :--- |
| Optimus_v7.4.0  | v7.4.0 |07/2024 | Updated the docker for the MergeStarOutput task to include STARsolo v2.7.11a. Updated SnapATAC2 docker to SnapATAC2 v2.6.3. The ubuntu_16_0_4 docker image version was pinned instead of using the latest tag.|
| Optimus_v6.3.1 | v6.3.1 | 01/2024 | Updated Optimus pipeline to include STARsolo v2.7.11a. Added sF tag to STARsolo aligner and TagSort metrics parameters. Modified H5adUtils task to include new metrics in the final Optimus h5ad. Removed DropSeq metrics task. Updated the Metrics task so that Cell Metrics and Gene Metrics now calculate intronic, intronic_as, exonic, exonic_as, and intergenic metrics from unique reads only using the NH:i:1 tag in the BAM. Added the latest warp-tools docker to the Metrics task; this allows use of REFSEQ references. |
| Optimus_v5.8.4 | v5.8.4 | 08/2023 | Added a new task to the worklow that reads the tar_star_reference file to obtain the genomic reference source, build version, and annotation version and outputs the information as txt file. Updated tool docker images. Removed empty columns from output matrices. Modified the stranded input parameter to be called star_strand_mode. Added Dropseq cell metrics. Added h5ad as a format option for the cell by gene matrix output. The h5ad has the same layers and global attributes (unstructured data in h5ad) as the previous Loom output. Updated STARsolo argument for counting mode to GeneFull_Ex50pAS. |
| Optimus_v5.7.0 | v5.7.0 | 03/2023 | Added optional input `mt_genes` for mitochondrial gene names, added the `input_id` to the names of output metrics files, and changed the `chemistry` input to an integer `tenx_chemistry_version` that accepts either 2 or 3 and selects the appropriate whitelist. |
| Optimus_v5.5.0 | v5.5.0 | 05/2022 | Fixed a bug in the output loom matrix where gene names were inappropriately assigned to counts. Any data previously processed with Optimus version 5.0.0 and above should be re-analyzed. |
| Optimus_v5.4.3 | v5.4.3 | 04/2022 | Optimus pipeline now partitions input files by CB when files are larger than 30 GB. Added an optional input `count_exons` default value of `false`. EmptyDrops no longer runs in sn_rna mode. |
| Optimus_v5.1.1 | v5.1.1 | 10/2021 | Added the option ""--soloBarcodeReadLength 0"" to STARsoloFastq task to ignore Barcode + UBI read of incorrect length. |
| Optimus_v5.0.0 | v5.0.0 |  9/2021 |  Optimus pipeline now uses STAR solo for alignment and count matrix production. Read the full [release note](https://github.com/broadinstitute/warp/blob/master/pipelines/skylab/optimus/Optimus.changelog.md).  This version is optimized for samples under 100 GB; for larger samples, memory upgrades will be necessary. |
| Optimus_v4.2.7 | v4.2.7 |  8/2021 |  Updated the workspace workflow to v4.2.7 which includes STAR to v2.7.9a. |
| Optimus_v4.2.3 | v4.2.3 |  3/2021 | Updated the emptydrops tool wrappper script so that instead of failing in cases with small number of cells, it now creates empty drop result files with NAs. |


---


### Contact information
* For workspace questions and feedback, email the Broad pipelines team at warp-pipelines-help@broadinstitute.org or create a post on the [Featured Workspaces community forum](https://support.terra.bio/hc/en-us/community/topics/360001603491) (login required). Tag @Alexander Baumann in the **Details** section of your post.

* You can also Contact the Terra team for questions from the Terra main menu. When submitting a request, it can be helpful to include:
    * Your Project ID
    * Your workspace name
    * Your Bucket ID, Submission ID, and Workflow ID
    * Any relevant log information  

---


### License
**Copyright WARP,  2024 | BSD-3**

All rights reserved. Full license text at https://github.com/broadinstitute/warp/blob/master/LICENSE. Note that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.

---",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","featured-workspaces-hca/HCA_Optimus_Pipeline"
71,"help-terra","FISS Tutorial","READER","https://app.terra.bio/#workspaces/help-terra/FISS%20Tutorial",TRUE,TRUE,NA,NA,NA,"Learn how to use the FIreCloud Service Selector command utility to automate and scale your analysis on Terra! This tutorial workspace includes two notebooks that walk through 1) Creating a workspace and populating it with input and Workspace Data tables and 2) configuring and running a workflow on inputs stored in a table. 

## What is FISS?
FireCloud Service Selector (FISS) is a utility command module that allows API (Application Programming Interface) calls from a notebook or the Cloud Environment terminal to the workspace. Scripting with FISS is much like you would run on a local machine, but with Terra's built-in security and cloud integration.   

To learn more about scripting on top of the Terra and Google Cloud environments using FISS to access Terra's APIs, see <a href=""https://support.terra.bio/hc/en-us/articles/360042259232-Managing-data-and-automating-workflows-with-the-FISS-API"" target=""_blank"">this article</a>.      

For the full list of definitions and arguments of FISS commands, see the  <a href=""https://github.com/broadinstitute/fiss/blob/master/firecloud/api.py#L163"" target=""_blank"">FISS GitHub</a>.
<br>   

## Workspace overview 

### First: Create your own template (clone) of this WORKSPACE to work from       

To create your own customizable template, click on the round circle with three dots in the upper right corner of this page:   
    ![Clone_ScreenShot](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/Clone_workspace_Screen%20Shot.png)

| ![tip icon](https://storage.googleapis.com/terra-featured-workspaces/QuickStart/video-icon-final_scaled-2.png) |<a href=""https://youtu.be/mYUNQyAJ6WI"" target=""_blank"">**Click for a video walkthrough of making your own workspace copy**</a>  ![white space](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/white-space.jpg)  |     
|-------|-------|    
<br>   


### Tutorial 1: Create a workspace and add data in a table
In your copy of the workspace, open and run the notebook  `FISS-tutorial-1_Create-a-workspace-and-add-a-data-table` . This notebook creates a workspace and uploads data in a table (i.e. TSV file)  programmatically using the FIreCloud Service Selector (FISS) command utility.

#### Requirements 
- **You need to be the Billing Project owner to see and add data to the workspace!**     
- Use the default Cloud Environment

### Tutorial 2: Upload genomic data and generate a data table   
In your copy of the workspace, open and run the notebook  `FISS-tutorial-2-Upload-genomic-data-and-generate-a-sample-table`. This notebook uploads data programmatically from your local machine to your Terra workspace using gsutil cp and generate a data table that contains your samples' CRAM and CRAI files.

#### Requirements 
- Use the default Cloud Environment
- You need to have pairs of CRAM and CRAI data files (stored locally)  
<br>

## Additional FISS resources
Several additional notebooks have been included in this workspace for reference. Developed for specific use-cases, they are not stand-alone tutorials, but the tutorial notebooks should give enough background for you to customize them for your own needs.     

Below are some more advanced resources for next steps.     

* <a href=""https://github.com/broadinstitute/fiss"" target=""_blank"">FISS on GitHub</a>        
* <a href=""https://api.firecloud.org/"" target=""_blank"">FireCloud/Terra APIs (Swagger)</a>    
<br>


## Contact information  
This material is provided by the Terra Team. Please post any questions or concerns to our <a href=""https://broadinstitute.zendesk.com/hc/en-us/community/topics/360000500432-General-Discussion"" target=""_blank"">Terra forum site</a>.
<br>


## License  
**Copyright Broad Institute, 2020 | BSD-3**  
All code provided in this workspace is released under the WDL open source code license (BSD-3). Full license text at <a href=""https://github.com/openwdl/wdl/blob/master/LICENSE"" target=""_blank"">https://github.com/openwdl/wdl/blob/master/LICENSE</a>. Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-terra/FISS Tutorial"
72,"lincs-phosphodia","Avant-garde_Production_v1_0","READER","https://app.terra.bio/#workspaces/lincs-phosphodia/Avant-garde_Production_v1_0",TRUE,FALSE,NA,NA,NA,"Production workspace for cloud-based Avant-garde workflow.",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","lincs-phosphodia/Avant-garde_Production_v1_0"
73,"warp-pipelines","Slide-seq","READER","https://app.terra.bio/#workspaces/warp-pipelines/Slide-seq",TRUE,FALSE,NA,NA,NA,"# Slide-seq Pipeline for Analysis of Spatial Transcriptomic Data

The Slide-seq pipeline is an open-source, cloud-optimized pipeline developed in collaboration with the [BRAIN Initiative Cell Census Network](https://biccn.org/) (BICCN) and the BRAIN Initiative Cell Atlas Network (BICAN). It supports the processing of spatial transcriptomic data generated with the [Slide-seq](https://www.science.org/doi/10.1126/science.aaw1219) (commercialized as [Curio Seeker](https://curiobioscience.com/product/)) assay. This workspace currently describes `v3.1.2` of the Slide-seq pipeline and provides a fully reproducible example of the workflow. 

Scroll down for details on the workflow, including input and output descriptions and requirements, estimated run times and costs, and information on downsampled example data.       

**Note that cost and time estimates will vary** with the use of [Preemptibles](https://cloud.google.com/compute/docs/instances/preemptible).  
You can also use [Google's cost calculator](https://cloud.google.com/products/calculator/) to estimate the cost to run. 

**For helpful hints on controlling Cloud costs**, see [Overview: Controlling Cloud costs on Terra](https://support.terra.bio/hc/en-us/articles/360029748111).  

#### Where did the pipeline come from?

The Broad Pipelines Development team worked closely with members of the Evan Macosko Laboratory to adapt an on-premise pipeline into the cloud-based Slide-seq pipeline described in this workspace. Thank you to James Webber, Jonah Langlieb, Nina Sachdev, and the rest of the Evan Macosko Lab for their commitment to [FAIR Principles](https://www.nature.com/articles/sdata201618) and for helping create the Slide-seq pipeline.

---

##  Slide-seq 

### What does it do?   

This Slide-seq workflow has a quality control, alignment, and transcriptome quantification module. It corrects bead barcodes, aligns reads to the genome, generates a count matrix, calculates summary metrics for genes, barcodes, and UMIs, returns read outputs in BAM format, and returns counts in NumPy and h5ad file formats. 

For more details about the pipeline, see the [Slide-seq Overview](https://broadinstitute.github.io/warp/docs/Pipelines/SlideSeq_Pipeline/README) in the [WARP documentation](https://broadinstitute.github.io/warp/).  

---

### How to run the workflow

This workspace is preloaded with a **[downsampled example mouse dataset](https://github.com/broadinstitute/warp/blob/develop/pipelines/skylab/slideseq/test_inputs/test_data_overview.md#enroll-beta) and workflow configuration** (see table below) designed to demo Slide-seq quickly and inexpensively.

The workspace workflow is configured to leverage the `sample` and `sample_set`  data tables in the workspace Data tab. 

To run the workflow: 

1. Navigate to the Workflows tab and choose the Slide-seq workflow configuration. 
1. Make sure the root entity type is `sample_set` (selected by default) on the workflow configuration page.
1. Choose the `Puck210817_11` sample set.
1. Select the checkbox next to `Use reference disks`.
1. Run the analysis.


---
### What does Slide-seq require as input?

The Slide-seq workflow requires the following input:

| Input Name | Input Description | Input Type |
| --- | --- | --- |
|  r1_fastq | Array of Read 1 FASTQ files split across multiple sequencing lanes; forward reads containing bead barcodes and Unique Molecular Identifiers (UMIs); order of files must match the `r2_fastq` array. | Array[File] |
| r2_fastq | Array of Read 2 FASTQ files split across multiple sequencing lanes; reverse reads containing the cDNA fragment generated from captured mRNA; order of files must match the `r1_fastq` array. | Array[File] |
| input_id | Unique identifier describing the biological sample or replicate that corresponds with the FASTQ files; inserted into read group header and used to name output files. | String |
| read_structure | Description of the UMI (M) and Barcode (C) positions in the Read 1 FASTQ; used to trim spacer sequences (X) for use by STARsolo; ex. ""8C18X6C9M1X"". | String |
| tar_star_reference | TAR file containing a species-specific reference genome and GTF; generated using the [BuildIndices workflow](https://github.com/broadinstitute/warp/tree/master/pipelines/skylab/build_indices/BuildIndices.wdl). | File | 
| annotations_gtf | GTF containing gene annotations used for gene tagging (must match GTF in STAR reference). | File | 
| bead_locations | TSV file containing bead barcodes and XY coordinates on a single line for each bead; determined by sequencing prior to mRNA transfer and library preparation. | File |
| output_bam_basename | String used for the output BAM file basename. | String |

**Slide-seq is a single sample pipeline, but can take in multiple sets of FASTQs for a puck that has been split over lanes of sequencing.** 

See the workspace data tab for an example of how to configure tables with multiple sequencing lanes per sample. If you are running Slide-seq on your own data, it is important that you first set up a `sample` data table and then a `sample_set` table. The pipeline requires a set table even if you only have one lane of sequencing. 

#### Reference data description and location  
The mouse reference genome, GRCm38 ([GENCODE M23](https://www.gencodegenes.org/mouse/release_M23.html)),  is included in the `Workspace` data table along with additional resources for the tools in this workspace. The references are preconfigured in the workspace example workflow configurations.

##### Enabling reference disks
The suggested workflow configuration (see ""Running the workflow"" above) uses [reference disks](https://support.terra.bio/hc/en-us/articles/360056384631-Reference-Disks-in-Terra). That means when Terra kicks off the workflow on a virtual computer, it attaches a portable disk (kind of like a flash-drive) that is preloaded with the reference files needed for the workflow. This saves time and cost running the workflow. 

### Optional parameters 

The Slide-seq workflow offers the following optional inputs. For a complete list, see the [Slide-seq Overview](https://broadinstitute.github.io/warp/docs/Pipelines/SlideSeq_Pipeline/README#inputs).

| Input Name | Input Description | Input Type |
| --- | --- | --- |
| i1_fastq | Optional array of i1 (index) FASTQ files; index reads used for demultiplexing of multiple samples on one flow cell. | Array[File] | 
| count_exons | Optional boolean indicating if the workflow should calculate exon counts; default is set to “true” and produces an h5ad file containing both whole-gene counts and exon counts in an additional layer; when set to “false”, an h5ad file containing only whole-gene counts is produced. | Boolean |

---
### What does Slide-seq return as output?

In this workspace, the metadata for all outputs are written to the workspace data table. These outputs are described in the following table:  

| Output Name | Output Description | Output Type |
| --- | --- | --- |
| bam | Aligned BAM file | BAM |
| cell_metrics | Cell metrics based on bead barcodes | Compressed CSV |
| fastq_barcode_distribution | Metric file containing the distribution of reads per bead barcode that were calculated prior to alignment. | TXT |
| fastq_reads_per_cell | Metric file containing the number of reads per barcode that were calculated prior to alignment. | TXT |
| fastq_reads_per_umi | Metric file containing the number of reads per UMI that were calculated prior to alignment. | TXT |
| fastq_umi_distribution | Metric file containing the distribution of reads per UMI that were calculated prior to alignment. | TXT |
| gene_metrics  | Gene metrics | Compressed CSV |
| genomic_reference_version | File containing genomic reference source, build, and annotation versions. | TXT |
| h5ad_output_file | h5ad file containing count data and metadata. | h5ad | 
| matrix | Converted sparse matrix file from the MergeStarOutputs task. | NPZ |
| matrix_col_index  | Index of genes in count matrix. | NPY |
| matrix_row_index | Index of beads in count matrix. | NPY |
| pipeline_version_out | Version of the processing pipeline run on this data. | String |
| umi_metrics | UMI metrics | Compressed CSV |

The h5ad matrix is the default output. This matrix contains the unnormalized (unfiltered) count matrices, as well as the gene and bead barcode metrics detailed in the [Slide-seq Count Matrix Overview](https://broadinstitute.github.io/warp/docs/Pipelines/SlideSeq_Pipeline/count-matrix-overview).

---

### Estimated time and cost to run on sample data 

The following estimates are based on the **downsampled example mouse dataset** with reference disks enabled.
 
| Sample Set Name | Set Size | Sample Set R1.fastq Size | Sample Set R2.fastq Size | Time | Cost $ |
| :---:  | :---: | :---: | :---: | :---: | :---: |
| Puck210817_11 | 4 entities | 13 MB | 17 MB | 0:45:00 | 0.09 | 

The following estimates are based on **full-size datasets**.
| Sample Name | Sample Set R1.fastq Size | Sample Set R2.fastq Size | Time | Cost $ |
| :---: | :---: | :---: | :---: | :---: |
| Puck_210727_12 | 24.23 GB | 35.37 GB | 17:29:00 | 3.09 | 
| Puck_210727_17 | 27.25 GB | 40.07 GB | 11:45:00 | 3.02 | 
| Puck_210817_07 | 22.61 GB | 33.65 GB | 10:46:00 | 2.56 | 
| Puck_210817_11 | 36.63 GB | 57.32 GB | 12:51:00 | 3.41 | 
| Puck_210817_12 | 23.32 GB | 34.52 GB | 11:07:00 | 2.64 | 
| Puck_210817_13 | 28.21 GB | 42.52 GB | 12:30:00 | 3.19 | 
| Puck_211013_03 | 59.38 GB | 88.99 GB | 25:50:00 | 6.16 | 
 
**For helpful hints on controlling Cloud costs, see [Overview: Controlling Cloud costs on Terra](https://support.terra.bio/hc/en-us/articles/360029748111).**   

---
### Versioning and testing

You can access previous versions of this pipeline by cloning the workspace and choosing a version in the version dropdown. 

All versions listed here are available by cloning this workspace and selecting the version of the Slide-seq method. All Slide-seq pipeline releases are documented in the [changelog](https://github.com/broadinstitute/warp/blob/master/pipelines/skylab/slideseq/SlideSeq.changelog.md) and tested using [plumbing and scientific test data](https://github.com/broadinstitute/warp/blob/develop/pipelines/skylab/slideseq/test_inputs/test_data_overview.md). To learn more about WARP pipeline testing, see [Testing Pipelines](https://broadinstitute.github.io/warp/docs/About_WARP/TestingPipelines).

| Terra Compatible Version Name | Slide-seq Release Version | Date | Release Note | 
| :---:  | :---: | :---: | :--- |
| SlideSeq_v3.1.2 | v3.1.2 | 03/2024 | Updated the SlideSeq WDL output to utilize the h5ad format in place of Loom; updated StarAlign metrics to include shard IDs. |
| SlideSeq_v2.1.2 | v2.1.2 | 02/2024 | Updated STARsolo to v2.7.11a and added SF tag for  metric calculation; updated the Metrics task so that Cell Metrics and Gene Metrics now calculate intronic, intronic_as, exonic, exonic_as, and intergenic metrics from unique reads only using the NH:i:1 tag in the BAM; added support for REFSEQ references. |
| SlideSeq_v1.0.10 | v1.0.10 | 08/2023 | Updated task docker images; removed empty columns from output Loom file. |
| SlideSeq_v1.0.1 | v1.0.1 | 03/2023 | First release of the pipeline. |

---

### Contact information

* For workspace questions and feedback, email the Broad pipelines team at warp-pipelines-help@broadinstitute.org or create a post on the [Featured Workspaces community forum](https://support.terra.bio/hc/en-us/community/topics/360001603491) (login required). Tag @Alexander Baumann in the **Details** section of your post.

* You can also Contact the Terra team for questions from the Terra main menu. When submitting a request, it can be helpful to include:
    * Your Project ID
    * Your workspace name
    * Your Bucket ID, Submission ID, and Workflow ID
    * Any relevant log information

---

### License
**Copyright WARP,  2024 | BSD-3**

All rights reserved. Full license text at https://github.com/broadinstitute/warp/blob/master/LICENSE. Note that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.

---",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","warp-pipelines/Slide-seq"
74,"broad-firecloud-ccle","CCLE_v2","READER","https://app.terra.bio/#workspaces/broad-firecloud-ccle/CCLE_v2",TRUE,FALSE,"cancer","Cell lines characterization data",NA,"# CCLE V2.1 = CCLE v2 Official Dataset + gap filling sequencing 
This workspace contains the omics data for the cell lines from [the CCLE2 paper](doi:10.1038/s41586-019-1186-3) which have been sequenced at the Broad institute and have derivative data released publically.
(for access to the Sanger lines, please visit https://cellmodelpassports.sanger.ac.uk/)_

The following raw sequencing data is available for download:
- RNAseq (1025) [HG38]
- WES (478) [HG38 + HG19]
- WGS (329) [HG38 + HG19]
- RRBS (unfiltered) (927) [HG19]
- HC (976) [HG19]
- Raindance (782) [HG19]


Next-generation characterization of the Cancer Cell Line Encyclopedia
Ghandi, M., Huang F. et al.
Nature doi:10.1038/s41586-019-1186-3 / May 8, 2019


> Maintained by Javad Noorbaksh & Jérémie Kalfon


## Using the data
To view the paths to the data you would need to **clone** the workspace using **your own billing account** and use it as a base for your own workspace (from which you can analyze the data).

Otherwise:
1. you can download any file from Google Cloud storage using the gs:// link and **gsutil** from your terminal (links can be accessed by downloading the data as csv). Since the bam and bai files are stored in requester-pays buckets, you need to provide a billing project to access the files using commands such as **gsutil -u [name of your billing project] cp gs://cclebams/rnasq_hg38/*.bam .**
2. you can access the **CCLE_v2** workspace programatically using python's firecloud-dalmatian

### Note: You might not be able to view file details when you click on file links in the data table in this workspace. This is expected, and does not suggest that the file paths are corrupted, since loading metadata from requester-pays buckets requires a billing project and this workspace isn't attached to any. However, you should be able to view them once you've cloned this workspace under your billing project.

## Remarks

1. The germline VCF files have been computed using **Haplotype Caller** and **CNN variant Filter** on HG38 preprocessed bam files, using GATK 4.2
2. The germline VCF are early stage and have only been partially validated.
3. The germline VCFs do not solely contain germline mutations. As we work with cell lines that are (at least, expected to be) 100% pure tumor tissue,  ""cancer-somatic"" and germlines will often appear with the same allele frequency. Thus we can consider this dataset to **contain both somatic and germline mutations**. 
4. Most of the bam files have been reprocessed and **might be slightly different from the original CCLE2 bam** files. We do not expect it to change any results obtained from the original bam files.
5. reprocessed bam files were reprocessed using the **PreProcessingForVariantDiscovery_GATK4** workflow version 8 with gatk:4.beta.3 version

More information in our **Broadinstitute/DepMap_Omics** github repo.



## Previous attributes (deprecated)
Updated the workspace on June 2021. The following were the previous attributes:","William Sellers","Mahmoud Ghandi","The Cancer Cell Line Encyclopedia (CCLE) project is a collaboration between the Broad Institute, and the Novartis Institutes for Biomedical Research and its Genomics Institute of the Novartis Research Foundation to conduct a detailed genetic and pharmacologic characterization of a large panel of human cancer models, to develop integrated computational analyses that link distinct pharmacologic vulnerabilities to genomic patterns and to translate cell line integrative genomics into cancer patient stratification. The CCLE provides public access to genomic data, analysis and visualization for about 1000 cell lines. The CCLE phase 2 datasets in this workspace includes: RNAseq (1019), WES (326), WGS (329), RRBS (unfiltered) (928), HC (976), Raindance (782), TERT (190) cell lines.","1065",NA,"CCLE",NA,"General Research Use","GRCh37/hg19","Broad-Novartis Cancer Cell Line Encyclopedia (CCLE) v2 Official Dataset","RNAseq;WES;WGS;RRBS;HC;Raindance;targeted sequencing (TERT promoter)","broad-firecloud-ccle/CCLE_v2"
75,"help-gatk","GATKTutorials-Somatic-August2019","READER","https://app.terra.bio/#workspaces/help-gatk/GATKTutorials-Somatic-August2019",TRUE,FALSE,NA,NA,NA,"# What's in this workspace?
*This workspace was originally developed for the Genome Analysis Toolkit (GATK) workshop in São Paulo, Brazil in August 2019. Feel free to run it on your own, even if you were unable to attend that workshop!*

Welcome to Day 3 of the Genome Analysis Toolkit (GATK) workshop! Today we will focus on Somatic Variant Discovery.

Earlier today you received introductions to GATK tools and Best Practices pipelines. In this workspace we will be going over two forms of Somatic Analysis: one comparing tumor and normal samples using Mutect2 workflow for variant differences, and another using the Copy Number Alterations (CNA) workflow for copy number variations. 

Your instructor will guide you through this workspace, but you should find the documentation within the notebooks sufficient to run through them again on your own or to share with a friend later (and we encourage you to do so!).

### Data
Data associated with this workspace is located in the [gs://gatk-tutorials/workshop_1908/3-somatic](https://console.cloud.google.com/storage/browser/gatk-tutorials/workshop_1908/3-somatic/?project=broad-dsde-outreach&organizationId=548622027621) google bucket. It contains both input and resource files for the Mutect2 and CNA workflows. Along with the inputs are precomputed outputs generated by each step in the tutorial. This can be used as input for any step in the workflow, in case your generated outputs are not correct or you are unable to complete a step in the workflow. 

### Tools
There are no tools in this workspace. The tutorials are notebook-based, allowing you to run each step manually and view the intermediate outputs of the workflow. This is a great way to understand each step in the workflow and give you the chance to manipulate the parameters to see what happens with the output.

If you are interested in a WDL based workflow of these analyses, check out the [Showcase](https://app.terra.bio/#library/showcase) area in Terra, which features many of our popular GATK workflows in workspaces ready to run your data.


### Notebooks
 **1-somatic-mutect2-tutorial :**
In this hands-on tutorial, we will call somatic short mutations, both single nucleotide and indels, using the GATK4 Mutect2 and FilterMutectCalls. If you need a primer on what somatic calling is about, see the following [GATK forum Article](https://software.broadinstitute.org/gatk/documentation/article?id=11127).


| Runtime Environments | Value |
| --- | --- |
| CPU Minimum| 4|
| Disksize Minimum | 100 GB |
| Memory Minimum | 15 GB |
| Startup Script | gs://gatk-tutorials/scripts/install_gatk_4110_with_igv.sh |

**2-somatic-cna-tutorial :**
This hands-on tutorial outlines steps to detect alterations with high sensitivity in total and allelic copy ratios using GATK4's ModelSegments CNV workflow. The workflow is suitable for detecting somatic copy ratio alterations, more familiarly copy number alterations (CNAs), or copy number variants (CNVs) for whole genomes and targeted exomes.

| Runtime Environments | Value |
| --- | --- |
| CPU Minimum| 2|
| Disksize Minimum | 100 GB |
| Memory Minimum | 13 GB |
| Startup Script | gs://gatk-tutorials/scripts/install_gatk_4110_with_igv.sh |

 
### Software versions
GATK4.1.1.0

### Time and cost 
This workspace focuses on the use of notebooks, thus the time and cost of completing the tutorial depends on the runtime environment of your notebook and the length of time you spend actively working in the notebook. With the given runtime environments above, users can expect the cost to be much less than a dollar an hour for each notebook. The specific cost will be displayed on the top right corner of your screen, next to the notebook machine options. 

## Appendix

### GATK @ Brazil 2019 in the Terra Support Center

Following the conclusion of the workshop, the workshop materials will be made available in the Terra Support Center. For now, you can find these materials in a google drive folder at [broad.io/GATK1908](https://broad.io/GATK1908)


### GATK documentation and support

Detailed documentation, tutorials, and more are available at the [GATK web site](https://software.broadinstitute.org/gatk/). If you are still running into issues after consulting the User Guide, have a question, or just want to say hello, reach out to the GATK team via the [GATK Support Forum](https://gatkforums.broadinstitute.org/gatk)!

### Terra documentation and support

Documents and helpful guides to support your journey through Terra are available in the Terra Support Center. Navigate there by following the “Learn About Terra” link in the left-side menu. At the time of this workshop, we are still actively writing and publishing documents but we’ve got a good set of docs in the Quick Start Guide to get you going.


",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/GATKTutorials-Somatic-August2019"
76,"anvil-outreach","demos-combine-data-workspaces","READER","https://app.terra.bio/#workspaces/anvil-outreach/demos-combine-data-workspaces",TRUE,FALSE,NA,NA,NA,"Start to learn how to Combine Data Workspaces.
## Combine Data Workspaces
![](https://raw.githubusercontent.com/fhdsl/AnVIL_Demos/main/resources/images/combine-data-workspaces-thumbnail.png)

Check out the ""AnVIL in two minutes"" slide overview [here](https://docs.google.com/presentation/d/19A1h1t_hy14sb1W80LYmACRTT6Hl-azKuKxEBWxjtRc). You can also check out the video version [here](https://drive.google.com/file/d/1vq9l8jvTd8mIEUWdpzmSOQI7kn9vo_4g/view?usp=sharing).

Original Workspace: https://anvil.terra.bio/#workspaces/anvil-outreach/demos-combine-data-workspaces

## AnVIL Dataset Catalog

Check out the AnVIL Dataset Catalog here: https://anvilproject.org/data 

## References

- [AnVIL Support Forum](https://help.anvilproject.org)
- [AnVIL in 2 minutes playlist](https://www.youtube.com/playlist?list=PL6aYJ_0zJ4uCABkMngSYjPo_3c-nUUmio)
- [AnVIL_Book_Getting_Started](https://jhudatascience.org/AnVIL_Book_Getting_Started)
- [AnVIL Collection](https://hutchdatascience.org/AnVIL_Collection) (more activities)",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","anvil-outreach/demos-combine-data-workspaces"
77,"anvil-datastorage","AnVIL_1000G_PRIMED-data-model","READER","https://app.terra.bio/#workspaces/anvil-datastorage/AnVIL_1000G_PRIMED-data-model",TRUE,TRUE,NA,"Whole Genome",NA,"## Study overview

Study Abbreviation: 1000G

Study Name: 1000 Genomes Project

Study Description: [The 1000 Genomes Project Web Site](https://www.internationalgenome.org/)

## Data Use Limitations

No restrictions

## Data included in workspace

This workspace contains two different callsets of 1000 genomes data: 

1. Phase 3 low-coverage sequencing of 2504 samples
2. High-coverage sequencing of 3202 samples

Each callset has data in the following formats:

1. VCF files for all populations combined, per chromosome (23 files, chroms 1-22 and X)
2. PLINK2 files for all populations combined, with all chromosomes (3 files, pgen/psam/pvar)
3. PLINK2 files per population and chromosome, subset to HapMap3 or MEGA+HapMap3 SNPs (chroms 1-22, ALL x SNP sets hm3, mega+hm3 x pgen/psam/pvar = 138 files) * (26 populations + 5 superpopulations). These files can be used as input for [simulation workflows](https://dockstore.org/organizations/PRIMED/collections/simulation).
4. File with list of samples related to other samples in the dataset at a threshold >= 2nd degree, as estimated by KING. Exclude samples from this list to establish an unrelated dataset.

### Sequencing Files

Links to each sequencing data file can be found in the `sequencing_file` table. The following identifiers may be useful for finding the desired data:

Phase 3 low-coverage sequencing of 2504 samples

Populations | Variant Set | `sample_set_id` | `sequencing_datset_id` | File Types
--- | --- | --- | --- | ---
All samples from all populations combined | All | phase3_2504_samples | f2bb0d1a | VCF (per chrom), PLINK2 (all chrom)
Population or superpopulation unrelated subsets | Subset to HapMap3 | \<POP\>_phase3_unrelated | \<POP\>_hg19_hm3 | PLINK2 (per chrom, all chrom)
Population or superpopulation unrelated subsets | Subset to MEGA + HapMap3 | \<POP\>_phase3_unrelated | \<POP\>_hg19_mega_hm3 | PLINK2 (per chrom, all chrom)


High-coverage sequencing of 3202 samples

Populations | Variant Set | `sample_set_id` | `sequencing_datset_id` | File Types
--- | --- | --- | --- | ---
All samples from all populations combined | All | all_3202_samples | d41f784b | VCF (per chrom), PLINK2 (all chrom)
Population or superpopulation unrelated subsets | Subset to HapMap3 | \<POP\>_highcov_unrelated | \<POP\>_hg38_hm3 | PLINK2 (per chrom, all chrom)
Population or superpopulation unrelated subsets | Subset to MEGA + HapMap3 | \<POP\>_highcov_unrelated | \<POP\>_hg38_mega_hm3 | PLINK2 (per chrom, all chrom)




### Details on data source

Low-coverage Phase 3 VCF files are hosted in a [public Google bucket](https://console.cloud.google.com/storage/browser/terra-featured-workspaces/GWAS/1kg-genotypes/vcf).

High-coverage VCF files are from the  [1000 Genomes ftp site](https://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/1000G_2504_high_coverage/working/20220422_3202_phased_SNV_INDEL_SV/).

1000 Genomes data in PLINK2 format was obtained from the [PLINK2 website](https://www.cog-genomics.org/plink/2.0/resources#phase3_1kg).
These files were processed to partition the dataset by populations and subset SNPs.

## Data model

This workspace was prepared by the [PRIMED consortium](https://primedconsortium.org/) and conforms to the [PRIMED data model](https://github.com/UW-GAC/primed_data_models/). ",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","anvil-datastorage/AnVIL_1000G_PRIMED-data-model"
78,"broad-firecloud-tcga","TCGA_PRAD_OpenAccess_V1-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_PRAD_OpenAccess_V1-0_DATA",TRUE,TRUE,"Prostate adenocarcinoma","Tumor/Normal","USA","TCGA Prostate adenocarcinoma Open-Access Data Workspace","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients’ clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","500","Prostate","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh37/hg19","TCGA_PRAD_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_PRAD_OpenAccess_V1-0_DATA"
79,"help-gatk","Variant-Functional-Annotation-With-Funcotator","READER","https://app.terra.bio/#workspaces/help-gatk/Variant-Functional-Annotation-With-Funcotator",TRUE,FALSE,NA,NA,NA,"### GATK Best Practices for Funcotator 
**Funcotator**(FUNCtional annOTATOR) analyzes variants for their function and writes the analysis to a specified output file.

As a functional annotation tool, Funcotator adds information to given variants beyond their position and alleles.  For example, Funcotator will annotate each variant with information about the gene in which it occurs (gene name, transcript, exon position, etc.), what the predicted protein change is, the classification of this variant with respect to function (e.g. nonsense, missense, etc.) and more.  These annotations are powered by a set of data sources that contain detailed information for the organism in which the variants occur.  

This workspace uses the default set of data sources for the *human somatic* use case (another default data source set exists for the *human germline* use case).  The annotations created by these somatic use case data sources may not include specific entries that you would like.  In this case, you can use the germline data sources (*gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.6.20190124g.tar.gz*), or create your own data sources that contain the backing data to create annotations that you want to see.

Funcotator is highly configurable and this workspace is just one small example of how it can be run.  Changing the data sources, the reference version, and the transcript selection list can dramatically change the output annotations that will be created.  It is up to the user to decide which annotations are appropriate for their data sets and to configure Funcotator accordingly.  Changing data sources will have the greatest impact on which annotations are created.

For more information on Funcotator, including how to run/configure it, see the following:
* [Funcotator Information and Tutorial](https://gatk.broadinstitute.org/hc/en-us/articles/360035889931)
* [Funcotator Tool Documentation](https://gatk.broadinstitute.org/hc/en-us/articles/360042912011)
* [Funcotator Source Code (GATK)](https://github.com/broadinstitute/gatk)

***Please direct all questions to the Terra  forum***

> TAGS: Functional Annotation, Variant, Annotation, Gene, Transcript, Nonsense, Missense, Protein Prediction

## Data 
The workflow in this workspace has enabled the ""Process single workflow from files"" option and passes a single VCF file and its index through the workflow. This VCF file is a downsampled multisample VCF composed 50 samples of the 1000 Genome Project. The workflow uses an interval list thus only a subset of the VCF is being processed. 

## Workflows

### 1-Funcotator-Workflow

**What does it do?**  
Run Funcotator on a set of variants.

**What does it require as input?**       

| Input Type | Input Name | Description |
|------------|-------------|-------------|
| String | gatk_docker | GATK Docker image in which to run. |
| File | ref_fasta | Reference FASTA file. |
| File | ref_fasta_index | Reference FASTA file index. |
| File | ref_fasta_dict | Reference FASTA file sequence dictionary. |
| File | variant_vcf_to_funcotate | Variant Context File (VCF) containing the variants to annotate. |
| File | variant_vcf_to_funcotate_index | Index file corresponding to the input Variant Context File (VCF) containing the variants to annotate. |
| String | reference_version | Version of the reference being used.  Either `hg19` or `hg38`. |
| String | output_file_name | Path to desired output file. |
| String | output_format | Output file format (either VCF or MAF). |
| Boolean | compress | Whether to compress the resulting output file. |
| Boolean | use_gnomad | If true, will enable the gnomAD data sources in the data source tar.gz, if they exist. |

**What does it return as output?**  

| Output Name | Output Type | Description |
|------------|-------------|-------------|
| funcotated_file_out | File | Output VCF or MAF file containing variants and their annotations|
| funcotated_file_out_idx | File | Index file for funcotated_file_out |

**Reference/Resource data description and location**  
The reference genome for this workspace is hg38, aka GRCh38, (though Funcotator can also be run with hg19).

Required and optional references and resources for the methods are included in the [Data tab](https://app.terra.bio/#workspaces/help-gatk/Variant-Functional-Annotation-With-Funcotator/data) under the Reference table.    
 
**Example time and cost run on sample data**    


| Sample Name | Sample Size | Time | Cost $ |
| :---:  | :---: | :---: | :---: |
| 1KGP_downsample (using chr1 interval list) | 226 MB | 0:26:00 | $0.01 |
 

For helpful hints on controlling Cloud costs, see this article ([click here to view](https://support.terra.bio/hc/en-us/articles/360029748111)).       

---

### Contact information
For questions about this workspace please visit the Featured Workspaces Topic topic on the [Terra Community Forum](https://broadinstitute.zendesk.com/hc/en-us/community/topics). Use the search box to see if other users have asked the same question previously. If not, post and tag **@Jonn Smith** so that we get notified.

This material is provided by the GATK Team. Please post any questions or concerns regarding the GATK tool to the [GATK forum](https://gatk.broadinstitute.org/hc/en-us/community/topics)

### Workspace Citation 
Details on citing Terra workspaces can be found here: [How to cite Terra](https://support.terra.bio/hc/en-us/articles/360035343652)

Data Sciences Platform, Broad Institute (*Year, Month Day that this workspace was last modified*) help-gatk/Variant-Functional-Annotation-With-Funcotator [workspace] Retrieved *Month Day, Year that workspace was retrieved*,  https://app.terra.bio/#workspaces/help-gatk/Variant-Functional-Annotation-With-Funcotator


### License
Copyright (c) 2009-2019, Broad Institute, Inc. All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are met:

* Redistributions of source code must retain the above copyright notice, this
  list of conditions and the following disclaimer.

* Redistributions in binary form must reproduce the above copyright notice,
  this list of conditions and the following disclaimer in the documentation
  and/or other materials provided with the distribution.

* Neither the name Broad Institute, Inc. nor the names of its
  contributors may be used to endorse or promote products derived from
  this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""
AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 
 ### Workspace Change Log
| Date | Change | Author |
| --- | --- | --- |
|  2019-12-17 | Initial Setup | Jonn Smith |
|  2020-04-29 | Updated workflow link to point to the GATK Git Repository. | Beri Shifaw |
|  2020-06-08 | Minor update to Dashboard links. | Beri Shifaw |
|  2021-05-10 | Updated workflow to gatk 4.2.0.0, updated the data tar source | Beri Shifaw |
|  2021-09-15 | Addition of Workspace Citation section | Beri Shifaw |",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/Variant-Functional-Annotation-With-Funcotator"
80,"landmarkanvil2","Bioconductor-on-AnVIL-GPU","READER","https://app.terra.bio/#workspaces/landmarkanvil2/Bioconductor-on-AnVIL-GPU",TRUE,FALSE,NA,NA,NA,"# Bioconductor on AnVIL GPU

## Authors

Nitesh Turaga - nitesh@ds.dfci.harvar.edu

## Introduction

Bioconductor has been recieving R packages that use an interface to run python libraries. Some of these packages use machine learning and deep learning libraries that are commonly used such as  `tensorflow`, and `keras`.

- 'TensorFlow' is an end-to-end open source platform for machine learning.

- 'Keras' is an open-source software library that provides a Python interface for artificial neural networks

Some of the Bioconductor packages that use ""reticulate"" to interface with are listed below

- VAExprs

- DeepPINCS

These machine learning packages run faster when using GPU based cloud environments. This workspace demonstrates the how to use the AnVIL environment to run a Bioconductor package on a GPU enabled cloud environment.


## Steps 

1. Start a GPU enabled cloud environment from the Cloud environment launcher. Select **Customize** in the menu. 
 The cloud environment should be a **R / Bioconductor** based Jupyter environment. It will have an option to **Enable GPUs**.
 
2. Choose a GPU type, and the number of GPUs. The default setting will work for this workspace. 

![env](https://raw.githubusercontent.com/nturaga/BiocGPU/master/images/enable-gpu-on-anvil.jpg)

3. Go to the Notebooks tab, and run the **VAExprs_Bioconductor_on_terra_GPU** notehook with the GPU Enabled cloud environment just launched. 


## More information on GPU

[GPU Support page](https://support.terra.bio/hc/en-us/articles/4403006001947)",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","landmarkanvil2/Bioconductor-on-AnVIL-GPU"
81,"help-gatk","Variant_Calling_Spark_Multicore","READER","https://app.terra.bio/#workspaces/help-gatk/Variant_Calling_Spark_Multicore",TRUE,FALSE,NA,NA,NA,"### GATK Best Practices for Variant Calling with Spark on a Multicore Machine

This workspace highlights a pipeline for calling variants from aligned input data on a single multicore machine. The pipeline, `ReadsPipelineSpark`, includes the following GATK “Best Practices” tools:
* Mark Duplicates
* BQSR
* Haplotype Caller

Please note, this pipeline is still in **beta**. We are providing this workspace to test and receive feedback to potentially improve this pipeline.

**Single multicore versus Spark cluster** 
Running on a single multicore machine is simpler than running on a Spark cluster, since starting the cluster requires additional steps, including providing credentials from the user, that limit automation. 

The downside to running on a single multicore machine is it's not as fast as a cluster. Analyzing an exome using a 96-core machine takes around 45 minutes, compared to around 20 minutes on a 20-node cluster. This is still significantly faster than using the regular walker-based GATK tools.

-----
## Workflow: ReadsPipelineSpark

**What does it do?**  
Runs [Mark Duplicates](https://gatk.broadinstitute.org/hc/en-us/articles/4405443615771), [BQSR](https://gatk.broadinstitute.org/hc/en-us/articles/4405451231387), and [HaplotypeCaller](https://gatk.broadinstitute.org/hc/en-us/articles/4405451272731) on aligned reads.         

**What does it require as input?**       
An input BAM of aligned reads, hosted in a bucket on GCS. 

**What does it return as output?**  
 An output VCF of called variants. The output is written to the workspace GCP bucket, and metadata are added to the workspace data table.         

**Reference/Resource data description and location** 
- Small dataset: hg37 (included in the sample data table)                
- Exome dataset: hg18  (included in the sample data table)              
- Known sites (included in the sample data table)    
- Known sites index (dbSNP included in the sample data table)                 

-------
## How to use this workspace

Start by running the pipeline with the small dataset:     
1. In the Workflows tab in Terra, select the ReadsPipelineSparkMulticore tool     
2. Click `Select Data` to choose the NA12878_small sample
3. Make sure the other sample (exome) is _not_ checked 
4. Click `Run analysis` and then `Launch` to submit the workflow
5. To monitor progress, refresh the `Job History` tab          

After successfully running the pipeline with the small test dataset, you can try running on a full exome dataset. To run on your own dataset, fill in input paths in the template row of the sample data table.     

There are two workspace variables that can be overridden:
* `gatk` - the file path to the GATK binary in the container
* `gatk_docker` - the Docker image name in Docker Hub or the Google Container Registry 

-----
## Data 
This workspace includes two sets of sample exome data, of different sizes:     
* **Small** (subset of chromosomes 20 and 21 of NA12878; useful for testing that the pipeline runs correctly)     


| Sample Name | Reference | Sample Size | Time | Cost $ |
| :---:  | :---: | :---: | :---: | :---: |
| NA12878_small | hg37 | ~ 75MB | ~7 mins | 0.08 |     


* **Exome** (NA12878)    


| Sample Name | Reference | Sample Size | Time | Cost $ |
| :---:  | :---: | :---: | :---: | :---: |
| NA12878_exome | hg18 | ~ 20GB | ~47 mins | 2.59 |     

**For helpful hints on controlling Cloud costs**, see this article ([click here to view](https://support.terra.bio/hc/en-us/articles/360029748111)).      


---

### Contact information  
For questions about this workspace please visit the Featured Workspaces Topic topic on the [Terra Community Forum](https://broadinstitute.zendesk.com/hc/en-us/community/topics). Use the search box to see if other users have asked the same question previously. If not, post and tag **@Bhanu Gandham** so that we get notified.

This material is provided by the GATK Team. Please post any questions or concerns regarding the GATK tool to the [GATK forum](https://gatk.broadinstitute.org/hc/en-us/community/topics)

### Workspace Citation 
Details on citing Terra workspaces can be found here: [How to cite Terra](https://support.terra.bio/hc/en-us/articles/360035343652)

Data Sciences Platform, Broad Institute (*Year, Month Day that this workspace was last modified*) help-gatk/Variant_Calling_Spark_Multicore [workspace] Retrieved *Month Day, Year that workspace was retrieved*,  https://app.terra.bio/#workspaces/help-gatk/Variant_Calling_Spark_Multicore

### License  
**Copyright Broad Institute, 2019 | BSD-3**  
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at https://github.com/openwdl/wdl/blob/master/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.

### Workspace Change Log
| Date | Change | Author |
| --- | --- | --- |
|  2019-10-31 | Initial feature of workspace | Bhanu Gandam |
|  2020-08-25 | Updated GATK docker version from 4.1.4.0 to 4.1.8.1 | Beri Shifaw|
|  2020-09-08 | Changed GATK docker version from 4.1.8.1  to 4.1.4.0 due to failure | Beri Shifaw|
|  2021-09-15 | Addition of Workspace Citation section | Beri Shifaw |

",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/Variant_Calling_Spark_Multicore"
83,"help-gatk","Germline-SNPs-Indels-GATK4-hg38","READER","https://app.terra.bio/#workspaces/help-gatk/Germline-SNPs-Indels-GATK4-hg38",TRUE,FALSE,NA,NA,NA,"### GATK Best Practices for Germline SNPs & Indels
These three workflows cover pre-processing, and SNP and Indel variant and joint calling, configured to run back to back, so that the outputs from the preceeding step will automatically become the inputs for the next workflow. Each workflow implements data processing according to the GATK Best Practices on human whole-genome sequence data. Detailed description of the workflows is available in [Gatk's Best Practices Document](https://software.broadinstitute.org/gatk/best-practices/workflow).     

Scroll down for details on each workflow, including input and output descriptions and requirements, estimated run times and costs, and information on sample data.       

**Note that cost and time estimates will vary** with the use of [Preemptibles](https://gatkforums.broadinstitute.org/firecloud/discussion/11786/preemptibles#latest).  
You can also use [Google's cost calculator](https://cloud.google.com/products/calculator/) to estimate cost to run.  **For helpful hints on controlling Cloud costs**, see [this article](https://support.terra.bio/hc/en-us/articles/360029748111).        

The following material is provided by the GATK Team. Please post any questions or concerns to one of our forum sites : [GATK](https://gatkforums.broadinstitute.org/gatk/categories/ask-the-team/) , [Terra](https://broadinstitute.zendesk.com/hc/en-us/community/topics/360000500432-General-Discussion) , or [WDL/Cromwell](https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team).

**Note:** This workspace is superseded by the [GATK4-Germline-Preprocessing-VariantCalling-JointCalling](https://app.terra.bio/#workspaces/help-gatk/GATK4-Germline-Preprocessing-VariantCalling-JointCalling) workspace and is no longer being updated. 


---
---

###  1-Processing-For-Variant-Discovery  
**What does it do?**    
This WDL pipeline takes sequencing data in unmapped BAM (uBAM) format and outputs a clean BAM file and its index, suitable for variant discovery analysis. **Note** If your input data are in a different format, see [this format conversion workplace](https://app.terra.bio/#workspaces/help-gatk/Sequence-Format-Conversion) for conversion tools.        

**What does it require as input?**    
The 1-Processing-For-Variant-Discovery workflow accepts a file containing a list of unaligned BAMS. The input data are samples:

* Pair-end sequencing data in unmapped BAM (uBAM) format
* One or more read groups, one per uBAM file, all belonging to a single sample (SM)

Input uBAM files must comply with the following requirements:

* Filenames all have the same suffix (we use "".unmapped.bam"")
* Files must pass validation by ValidateSamFile
* Reads are provided in query-sorted order
* All reads must have an RG tag
* Reference index files must be in the same directory as source (e.g. reference.fasta.fai in the same directory as reference.fasta)    

**If your sequencing data is not in uBAM format**, check out this file conversion workspace, [https://app.terra.bio/#workspaces/help-gatk/Sequence-Format-Conversion](https://app.terra.bio/#workspaces/help-gatk/Sequence-Format-Conversion) for workflows to convert:    

1. Interleaved FASTQ to paired FASTQ
2. Paired FASTQ to unmapped BAM
3. BAM to unmapped BAM
4. CRAM to BAM files from sequencer output for use in GATK analysis tools

**What does it return as output?**  
Metadata for all outputs are written to the workspace data table, and include a clean BAM file and its index, suitable for variant discovery analyses.          

**Sample data description and location**  
Links to the expected input types are available in the workspace data model for testing. The 1-Processing-For-Variant-Discovery (first) workflow accepts a file containing a list of unaligned bams. This workspace data model contains both a full sized and downsampled version of NA12878's unaligned bam list file under the column `flowcell_unmapped_bams_list`.      

**Reference data description and location**  
The reference genome for this workspace is hg38 (aka GRCh38). Required and optional references and resources for the methods are included in the Workspace Data table (""Workspace Attributes"" in FireCloud).           
 
**Estimated time and cost to run on sample data**    

| Sample Name | Sample Size | Time | Cost $ |
| :---:  | :---: | :---: | :---: |
| NA12878_24RG_small | 3.11 GB | 1:28:00 | 0.18 |
| NA12878 | 64.89 GB | 22:35:00 | 4.98 |     

**For helpful hints on controlling Cloud costs**, see this article ([click here to view](https://support.terra.bio/hc/en-us/articles/360029748111)).       


---


## 2-Haplotypecaller-GVCF    
**What does it do?**    
The workflow rscatters the HaplotypeCaller tool over a sample (clean BAM file and index, from the previous step) using an intervals list file. In particular, it runs the HaplotypeCaller tool from GATK4 in GVCF mode on a single sample according to GATK Best Practices. The output file produced will be a single gvcf file, which can be used by the joint-discovery workflow.   

**What does it require as input?**       
The 2-Haplotypecaller-GVCF workflow accepts an analysis ready BAM file, pre-proccessed using GATK Best Practices. In particular:     
- One analysis-ready BAM file for a single sample (as identified in RG:SM)
- A file containing a set of variant calling interval lists for the scatter

**What does it return as output?**        
One GVCF file and its index, ready for joint calling        

**Sample data description and location**  
Links to the expected input types are available in the workspace data model for testing. The data model lists sample analysis-ready BAM files under the `analysis_ready_bam` column.     

**Reference data description and location**  
The reference genome for this workspace is hg38 (aka GRCh38). Required and optional references and resources for the methods are included in the Workspace Data table (""Workspace Attributes"" in FireCloud).                 

**Estimated time and cost to run on sample data**      

| Sample Name | Sample Size | Time | Cost $ |
| :---:  | :---: | :---: | :---: |
| NA12878_24RG_small | 4.66 GB | 02:28:00 | 0.21 |
| NA12878 | 19.55 GB | 14:05:00 | 2.24 |      
   
**For helpful hints on controlling Cloud costs**, see this article ([click here to view](https://support.terra.bio/hc/en-us/articles/360029748111)).     


---

## 3-Joint-Discovery    
**What does it do?**    
This WDL implements the joint calling and VQSR filtering portion of the GATK Best Practices for germline SNP and Indel discovery in human whole-genome sequencing (WGS).

**What does it require as input?**  
- One or more GVCFs, produced by HaplotypeCaller in GVCF mode
- Bare minimum: 1 WGS sample or 30 Exome samples. Gene panels are not supported
- When determining disk size in the JSON, use the guideline below
  - small_disk = (num_gvcfs / 10) + 10
  - medium_disk = (num_gvcfs * 15) + 10
  - huge_disk = num_gvcfs + 10       

**What does it return as output?**     
A VCF file and its index, filtered using variant quality score recalibration (VQSR), with genotypes for all samples present in the input VCF. All sites that are present in the input VCF are retained. Filtered sites are annotated as such in the FILTER field.      
	
**Sample data description and location**     
Links to the expected input types are available in the workspace data model for testing. The 3-Joint-Discovery workflow accepts one or more gvcfs produced by haplotypecaller.  The `gvcf` column in the data model contains a full sized gvcf of NA12878 that will be used for the Joint-Discovery workflow.     

**Reference data description and location**  
The reference genome for this workspace is hg38 (aka GRCh38). Required and optional references and resources for the methods are included in the Workspace Data table (""Workspace Attributes"" in FireCloud).           

**Estimated time and cost to run on sample data**     

| Sample Name | Sample Size | Time | Cost $ |
| :---:  | :---: | :---: | :---: |
| NA12878 | 10.75 GB | 02:48:00 | 0.90 |     

**For helpful hints on controlling Cloud costs**, see this article ([click here to view](https://support.terra.bio/hc/en-us/articles/360029748111)).      


---
---   

### Important notes on workflow limitations 
The current version of the posted ""Generic germline short variant joint genotyping"" is derived from the Broad production version of the workflow, which was adapted for large WGS callsets of up to 20K samples.     

We believe the results of this workflow run on a single WGS sample are equally accurate, but there may be some shortcomings when the workflow is modified and run on small cohorts.  Specifically, modifying the SNP ApplyRecalibration step for higher specificity may not be effective.  The user can verify if this is an issue by consulting the gathered SNP tranches file.  If the listed `truthSensitivity` in the rightmost column is not well matched to the `targetTruthSensitivity` in the leftmost column, then requesting that `targetTruthSensitivity` from ApplyVQSR will not use an accurate filtering threshold.     

This workflow **has not been tested on exomes**.  The dynamic scatter interval creating task was optimized for genomes.  The scattered SNP VariantRecalibration may fail because of too few ""bad"" variants to build the negative model. Also, apologies that the logging for SNP recalibration is overly verbose.   

The provided tool configurations is meant to be a ready to use example of the workflows. It is the user’s responsibility to correctly set the reference and resource input variables using the [GATK Tool and Tutorial Documentations](https://software.broadinstitute.org/gatk/documentation/).

------

### Software Versions  
- GATK 4.1
- BWA 0.7.15-r1140
- Picard 2.16.0-SNAPSHOT
- Samtools 1.3.1 (using htslib 1.3.1)
- Python 2.7

----

### Contact information  
This material is provided by the GATK Team. Please post any questions or concerns to one of our forum sites : [GATK](https://gatkforums.broadinstitute.org/gatk/categories/ask-the-team/) , [Terra](https://broadinstitute.zendesk.com/hc/en-us/community/topics/360000500432-General-Discussion) , or [WDL/Cromwell](https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team).

---

### License  
**Copyright Broad Institute, 2019 | BSD-3**  
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at https://github.com/openwdl/wdl/blob/master/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/Germline-SNPs-Indels-GATK4-hg38"
84,"help-gatk","GATKTutorials-Germline","READER","https://app.terra.bio/#workspaces/help-gatk/GATKTutorials-Germline",TRUE,TRUE,NA,NA,NA,"# What's in this workspace?

Welcome to Day 2 of the Genome Analysis Toolkit (GATK) workshop! Yesterday you got an overview of the various tools and pipelines, but today will focus on Germline Variant Discovery. 

*This workspace was originally developed for the Genome Analysis Toolkit (GATK) workshop in San José, Costa Rica in February 2020. Feel free to run it on your own, even if you were unable to attend that workshop!*

Here you will be running three different notebooks. In the morning we will cover variant discovery, and in the afternoon we will look at different methods for variant filtering. This workspace is read-only, so clone your own unique copy to work with it.

Your instructor will guide you through this workspace, but you should find the documentation within the notebooks sufficient to run through them again on your own or to share with a friend later (and we encourage you to do so!).

The notebook(s) in this workspace are:
* 1-germline-variant-discovery-tutorial
* 2-gatk-hard-filtering-tutorial
* 3-gatk-cnn-tutorial

**All notebooks in this workspace** will use the following Cloud Environment settings:

| Option | Value |
| --- | --- |
| Environment | Default |
| Profile | Custom |
| CPU Minimum | 4|
| Disksize Minimum | 100 GB |
| Memory Minimum | 15 GB |


## Appendix

### GATK @ Costa Rica 2020 in the GATK Support Center

Following the conclusion of the workshop, the workshop materials will be made available in the GATK Support Center [here](https://gatk.broadinstitute.org/hc/en-us/sections/360007134472). For now, you can find these materials in a google drive folder at [broad.io/GATK2002](http:/broad.io/GATK2002)

### GATK documentation and support

Detailed documentation, tutorials, and more are available at the [GATK web site](https://gatk.broadinstitute.org/hc/en-us). If you are still running into issues after consulting the User Guide, have a question, or just want to say hello, reach out to the GATK team via the [GATK Support Forum](https://gatk.broadinstitute.org/hc/en-us/community/topics)!

### Terra documentation and support

Documents and helpful guides to support your journey through Terra are available in the Terra Support Center. Navigate there by following the “How-to Guides” link under Terra Support in the left-side menu. We are still actively writing and publishing documents but we’ve got a good set of docs in the Quick Start Guide to get you going.

### Workspace Citation 
Details on citing Terra workspaces can be found here: [How to cite Terra](https://support.terra.bio/hc/en-us/articles/360035343652)

Data Sciences Platform, Broad Institute (*Year, Month Day that this workspace was last modified*) help-gatk/GATKTutorials-Germline [workspace] Retrieved *Month Day, Year that workspace was retrieved*,  https://app.terra.bio/#workspaces/help-gatk/GATKTutorials-Germline",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/GATKTutorials-Germline"
85,"broad-firecloud-tcga","TCGA_MESO_OpenAccess_V1-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_MESO_OpenAccess_V1-0_DATA",TRUE,TRUE,"Mesothelioma","Tumor/Normal","USA","TCGA Mesothelioma Open-Access Data Workspace","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients’ clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","87","Pleura","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh37/hg19","TCGA_MESO_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_MESO_OpenAccess_V1-0_DATA"
86,"broad-fungal-firecloud","broad-fungal-gatk3","READER","https://app.terra.bio/#workspaces/broad-fungal-firecloud/broad-fungal-gatk3",TRUE,FALSE,NA,NA,NA,"This workspace includes a workflow for variant calling of haploid fungal genomes with GATK v3.7.  This has been tested extensively with small (~20Mb) haploid fungal genomes. After cloning the workspace containing the fungal-variant-call-gatk3 workflow, a new variant calling run can be started by loading a list of Bam files in an accessible google storage bucket and a formatted reference genome for variant calling. 

Steps in the GATK WDL include:
 - SamToFastq
 - bwa mem
 - Samtools view
 - Picard SortSam
 - Picard MarkDuplicates
 - Picard ReorderSam
 - Picard BuildBamIndex
 - GATK HaplotypeCaller
 - GATK CombineGVCFs
 - GATK GenotypeGVCFs
 - GATK SelectVariants
 - GATK VariantFiltration
 - GATK CombineVariants

Software provided in associated Docker:
- Picard version:  1.782
- GATK version:    3.7-93-ge9d8068
- samtools version: 1.3.1
- bwa version: 0.7.12
- tabix version: 0.2.5_r1005
- bgzip version: 1.3

Docker image is available here: https://hub.docker.com/r/broadinstitute/fungi-gatk3
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","broad-fungal-firecloud/broad-fungal-gatk3"
87,"fc-product-demo","2019_ASHG_Reproducible_GWAS","READER","https://app.terra.bio/#workspaces/fc-product-demo/2019_ASHG_Reproducible_GWAS",TRUE,FALSE,NA,NA,NA,"## **There is a new version of this workspace available. Please click [here](https://app.terra.bio/#workspaces/amp-t2d-op/2019_ASHG_Reproducible_GWAS-V2) to navigate to the updated version**

This workspace reproduces the fundamental steps in a genome wide association study (GWAS), using 1,000 Genomes Project¹ (phase 3) genotypes and simulated phenotypes.  

The analysis is structured in two parts:

1. Explore phenotypes and population structure (Jupyter Notebook - Hail/Python)         
2. Generate mixed-models for genetic association tests and visualizations (WDL workflow)    

The output of the notebook (part 1) serves as the input to the workflow (part 2).

Instructions for applying the analyses presented in this workspace on your own data are provided in the penultimate section of this documentation. 


## Notes on data in this workspace  

To demonstrate an analysis that could be run on typical whole genome sequence data, this workspace provides mock phenotype data generated from publicly available 1000 Genomes phase 3 genotypes. Phenotypes have been simulated based on individual genotypes and known associated loci for multiple complex traits. The [GCTA software](https://cnsgenomics.com/software/gcta/#Overview)⁶ was used with lists of causal variants and an estimate of narrow sense heritability⁵ for each phenotype. 

**Traits and sources for causal variants**      
a. BMI: Giant-UKBB meta-analysis²  
b. Fasting glucose: MAGIC³  
c. Fasting insulin: MAGIC³  
d. Waist-to-hip ratio: GIANT-UKBB meta-analysis²  
e. Height: GIANT-UKBB meta-analysis²  
f. HDL: MVP⁴  
g. LDL: MVP⁴  
h. Total cholesterol: MVP⁴  
i. Triglycerides: MVP⁴  

**Generating the synthetic data**    
The scripts used to create the phenotype data, as well as intermediate data files and a readme file, are in a public Google bucket (`gs://terra-featured-workspaces/GWAS/data_processing/`). To browse the Google bucket, [click here](https://console.cloud.google.com/storage/browser/terra-featured-workspaces/GWAS?project=broad-dsde-outreach&organizationId=548622027621&pli=1).     

### Navigating the workspace DATA section
Sample phenotypes and related resources have been uploaded to tables in the DATA section. 

The *sample* table includes 2,504 individuals samples (each a row in the table). Columns correspond to phenotype values for each individual, and a unique sample identifier can be found in the *sample_id* column. This synthetic phenotypic data was added to the data table by uploading a tab-delimited file, and can be downloaded back to file. 

The *Workspace Data* table (under *OTHER DATA*) contains supporting resources used in the analysis in the form of genotype data in different file formats. We will use the VCF files (*vcf_files*) in **Analysis part 1**, and the GDS files (*gds_files*) to run the association workflow. Two different file formats are used because of differences in the requirements for the notebooks and workflows.
 
**Processed data** For reference, a completed sample set and analysis results can be found in the *sample_set* data tab. This represents possible outputs generated by running both the notebook and association workflow in this workspace.

For more information on working with data in the DATA table, see this article - [""Managing data with the data table""](https://support.terra.bio/hc/en-us/articles/360025758392). 

-----
## Analysis part 1: Explore phenotypes and population structure in the 1000 Genomes samples
You will explore the phenotype and genotype data in the **GWAS_initial_analysis** notebook. This notebook will also set up the data table for running a genome-wide association workflow.  Note that there are two versions of the analysis notebook: one that demonstrates a completed analysis (GWAS_initial_analysis_complete), and one with all output data cleared (GWAS_initial_analysis_cleared), which can be used to run the analysis interactively step-by-step.    

**What's in the notebook analysis?** .   
1. Importing phenotypic data from the sample table (in the DATA section) to the notebook environment
2. Exploring and processing these data to understand their underlying structure
3. Defining an outcome and a set of covariates to use when modeling genotype-phenotype associations
4. Importing, exploring, and performing quality control on genotype data
5. Understanding population structure within the 1000 Genomes samples
6. Preparing a full set of input parameters and data for a genome-wide association analysis workflow
7. Configuring the Terra data table to run a workflow

The genetic analyses in this notebook use the [Hail](https://hail.is/) software package. Hail is a framework for distributed computing with a focus on genetics. It is particularly relevant for whole genome sequence ([WGS](https://en.wikipedia.org/wiki/Whole_genome_sequencing)) analysis as it scales extremely efficiently with regard to variant callset and sample sizes.    

The code for these analyses is written in [Python](https://www.python.org/), making use of a few packages including [Pandas](https://pandas.pydata.org/), [Numpy](https://numpy.org/), [Matplotlib](https://matplotlib.org/), and [Hail](https://hail.is/). 
To facilitate downstream analysis, this notebook is set up to save output data to the workspace bucket and write the associated metadata to the data table using the [FireCloud Service Selector (FISS) package](https://github.com/broadinstitute/fiss).     

### Recommended runtime environment (i.e. cluster configuration)

| Option | Value |
| ------ | ------ |
| Environment | Hail: (Python 3.7.6, hail 0.2.30) |
| Profile | Custom |
| Master CPUs | 8 |
| Master Memory | 30 GB |
| Master Disk size | 100 |
| Startup Script | leave blank |
| Spark Cluster | Yes |
| Workers | 4 |
| Preemptible | 0 |
| Workers CPUs | 4 |
| Workers Memory | 15 GB |
| Workers Disk size | 50 GB |

You are able to adjust these VM parameters to fit your computation needs in the Terra interface.      

### Time and cost estimate  
Time to execute all the commands is ~28 minutes which currently costs ~$0.50 to complete (with the recommended cluster configuration). 

-----
-----
## Analysis part 2: Mixed-model association test workflows


### [vcfToGds](https://dockstore.org/workflows/github.com/manning-lab/vcfToGds)

This workflow converts genotype files from Variant Call Format ([VCF](https://en.wikipedia.org/wiki/Variant_Call_Format)) to Genomic Data Structure ([GDS](https://www.biostat.washington.edu/sites/default/files/modules/GDS_intro.pdf)), the input format required by the next workflow.          

**You do not need to run this workflow to apply the analysis on the data provided in this workspace**. GDS files for the 1000 Genomes data have been pre-generated and can be found under the workspace *Data* tab. We provide this tool so you can apply the analyses in this workspace to your own data. 

#### Time and cost estimates    
Note that actual time and cost may vary due to the use of preemptible instances. 

| Sample Size | # Variants | Time | Cost $ |
| -------- | -------- | -------- | ---------- |
| 2,500 samples | 22,000 | 8m | $0.47


### [genesis_GWAS](https://github.com/AnalysisCommons/genesis_wdl)

This workflow performs mixed-model association testing and generates statistics and visualizations, using output from the notebook described above as input.    
The workflow steps:
1. Generate a mixed model under the hypothesis of no variant effect using the GENESIS software
2. Generate per-variant association statistics with a phenotype, using the null model    
3. Generate quantile-quantile (QQ) and Manhattan (MH) plots (common descriptive figures for GWAS results). 

Note that output files are stored in the workspace bucket and metadata are populated back to the workspace data model.

#### Time and cost estimates    
Note that actual time and cost may vary due to the use of preemptible instances. 

| Sample Size | Time | Cost |
| -------- | -------- | ---------- |
| 2,500 samples | 26m | $0.57


----
----

## How to run on your own data  
While we have used the 1000 Genomes project genotypes and phenotypes in this workspace, both the notebook and workflow can be adapted to other genetic datasets. The steps for adapting these tools to another dataset are outline below:

**Update the data tables**
To use alternate phenotype data, you would need to upload you own [tab-delimited file to the sample data table](https://support.terra.bio/hc/en-us/articles/360025758392). Each row of this file should correspond to one sample in the genotype data, with a unique sample identifier also present in the genotype files (this is the *sample_id* column in this workspace). You can download the example sample data table to file and use that as a template.

Under the *Workspace Data* tab, you would need to update the VCF and GDS filepaths to point to the genotype files for your sample set. 

**Update the notebook**    
Accommodating other datasets may require modifying many  parts of this notebook. Inherently, the notebook is an interactive analysis where decisions are made as you go. It is **not** recommended that the notebook be applied to another dataset without careful thought. However, the notebook *should run* properly with one modification: in the section where *vcf_paths* is defined, you need to change the Google cloud storage link to point to the dataset you wish to work with. 

**Run an additional workflow**   
To generate GDS files (from the VCF files output by the notebook analysis), we have provided another workflow, **vcfToGds**. This workflow takes a list of VCF files and converts them to GDS files that can be input to the genome-wide association workflow. More details are provided further below. For step-by-step instructions on generating a list file, [see this article](https://support.terra.bio/hc/en-us/articles/360033353952).     

----
----

## Authors and contact information

This workspace is a product of the [Manning Lab](https://manning-lab.github.io/), in collaboration with the [Data Sciences Platform](https://www.broadinstitute.org/data-sciences-platform), at [The Broad Institute of MIT and Harvard](https://www.broadinstitute.org/). Contributing authors include:

* [Tim Majarian](mailto:tmajaria@broadinsitute.org) (Manning Lab)
* Alisa Manning (Manning Lab)
* [Allie Hajian](mailto:jhajian@broadinstitute.org) (DSP User Education team)



## Additional generally helpful resources

* **[Terra knowledge base](https://support.terra.bio/hc/en-us)**   
    Check here for information on setting up billing and security, how to do research on Terra, tutorials and forum.    
		
*  **[Jupyter Notebooks 101](https://app.terra.bio/#workspaces/help-gatk/Jupyter%20Notebooks%20101)**    
    This crash course will help you get started using Jupyter notebooks. It integrates hands-on practice to solidify concepts.    

* **[Hail tutorial notebooks](https://app.terra.bio/#workspaces/help-gatk/Hail-Notebook-Tutorials)**   
    Explore materials from Hail workshops given by the Hail team at the Broad Institute.
		
* **For helpful hints on controlling cloud costs**, see [this article (https://support.terra.bio/hc/en-
us/articles/360029748111)](https://support.terra.bio/hc/en-us/articles/360029748111).      
 
 

 ## License
**Copyright Broad Institute, 2019 | BSD-3**  
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at https://github.com/openwdl/wdl/blob/master/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.
 

 ## Citations
 
1. A global reference for human genetic variation, The 1000 Genomes Project Consortium, Nature 526, 68-74 (01 October 2015) doi:10.1038/nature15393  
2. Yengo L, Sidorenko J, Kemper KE, Zheng Z, Wood AR, Weedon MN, Frayling TM, Hirschhorn J, Yang J, Visscher PM, GIANT Consortium. (2018). Meta-analysis of genome-wide association studies for height and body mass index in ~700,000 individuals of European ancestry. Biorxiv  
3. Scott RA et al. Large-scale association analyses identify new loci influencing glycemic traits and provide insight into the underlying biological pathways. Nat. Genet. 2012;44;9;991-1005  
4. Klarin D, et al. Genetics of blood lipids among ~300,000 multi-ethnic participants of the Million Veteran Program. Nat. Genet. 2018;50:1514–1523. doi: 10.1038/s41588-018-0222-9.  
5. Zheng, et al. LD Hub: a centralized database and web interface to perform LD score regression that maximizes the potential of summary level GWAS data for SNP heritability and genetic correlation analysis. Bioinformatics. 2017 Jan 15;33(2):272-279.  
6. Yang et al. (2011) GCTA: a tool for Genome-wide Complex Trait Analysis. Am J Hum Genet. 88(1): 76-82.  
7. Purcell S, Neale B, Todd-Brown K, Thomas L, Ferreira MAR, Bender D, Maller J, Sklar P, de Bakker PIW, Daly MJ & Sham PC (2007) PLINK: a toolset for whole-genome association and population-based linkage analysis. American Journal of Human Genetics, 81.  
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","fc-product-demo/2019_ASHG_Reproducible_GWAS"
88,"help-gatk","Hail-Notebook-Tutorials","READER","https://app.terra.bio/#workspaces/help-gatk/Hail-Notebook-Tutorials",TRUE,TRUE,NA,NA,NA,"**Explore materials from Hail workshops given by the Hail team at the Broad Institute.**

![image](https://repository-images.githubusercontent.com/45069467/17243d00-7409-11ea-8faa-f09d532a9e98)


Hail provides an open-source framework to analyze the largest genetic data sets in existence and to meet the exploding needs of hospitals, diagnostic labs, and industry. Hail's [efficient and scalable framework](https://www.broadinstitute.org/blog/harnessing-flood-scaling-data-science-big-genomics-era) currently powers dozens of major academic studies.

Cotton Seed and Jon Bloom co-founded the Hail project in the [Neale Lab](http://www.nealelab.is/).  More information on the project can be found on their [homepage](https://hail.is). The Hail team is embedded in the [Neale Lab](http://www.nealelab.is/) at the Stanley Center for Psychiatric Research of the Broad Institute of MIT and Harvard and the Analytic and Translational Genetics Unit of Massachusetts General Hospital.

This workspace is an introductory tutorial that demonstrates the basics of using the Hail package. For high-performance applications, please see the Hail package documentation [here](https://hail.is).

This workspace was last run in October, 2020 using Hail 2.57

## Notebooks

The following Hail tutorial notebooks were obtained from the [Hail git repository](https://github.com/hail-is/hail/tree/0.2.57/hail/python/hail/docs/tutorials). 

- **01-genome-wide-association-study.ipynb**: This notebook is designed to provide a broad overview of Hail's functionality, with emphasis on the functionality to manipulate and query a genetic dataset. We walk through a genome-wide SNP association test, and demonstrate the need to control for confounding caused by population stratification.  
- **03-tables.ipynb**: An introduction to tables in Hail. Table is Hail's distributed analogue of a data frame or SQL table.  
-  **04-aggregation.ipynb** : An introduction to aggregation in Hail. Aggregation combines many values together to create a summary.  
- **05-filter-annotate.ipynb**: An introduction to filters in Hail. You can filter the rows of a table with Table.filter. This returns a table of those rows for which the expression evaluates to True.  
- **06-joins.ipynb**: This tutorial walks through some ways to join Hail tables. We'll use a simple movie dataset to illustrate.  
- **07-matrixtable.ipynb**: The two crucial features that Hail adds are scalability and the domain-specific primitives needed to work easily with biological data. Fear not! You've learned most of the basic concepts of Hail and now are ready for the bit that makes it possible to represent and compute on genetic matrices: the MatrixTable.  
- **08-plotting.ipynb**: The Hail plot module allows for easy plotting of data. This notebook contains examples of how to use the plotting functions in this module, many of which can also be found in the first tutorial.  
 
The following notebooks are not part of the Hail tutorial:
- **Optional_Intro_to_Jupyter_Notebooks**: An introduction to working with notebooks.  
- **Optional_Update_Notebooks**: Scripts to update the workspace tutorial notebooks by pulling notebooks from the Hail repository.  

Recommended environment settings to run the notebooks are below:   

| Runtime Environments | Value |
| --- | --- |
| CPU Minimum| 4 |
| Disksize Minimum | 100 GB |
| Memory Minimum | 15 GB |
| Environment | Hail: (Python 3.7.7, Spark 2.4.5, hail 0.2.57) |


## How to use this workspace

1. Clone this workspace and open the Notebooks tab
2. Click on a notebook and follow prompts to create a runtime environment
    Note: if you don't have a runtime environment running, it will take a few minutes to create and run. 

## Hail Resources

The Hail Team at Broad gives regular workshops through Broad-E.For the most complete and up-to-date information, please defer to their coursework material on the [hail.is website](https://hail.is).

* **Contact the Hail team** at hail@broadinstitute.org
* **Follow Hail on Twitter** @hailgenetics
* **Notebook Source**  https://github.com/hail-is/hail/blob/0.2.57/hail/python/hail/docs/tutorials/
* **Citing Hail** If you use Hail for published work, please cite the software:  Hail, https://github.com/hail-is/hail

---

### Change Log
| Date | Change | Author |
| --- | --- | --- |
|  2019-09-03 | Updated code cells to correct syntax for automated capture of workspace environment variables; additional grammar and language tweaks  | Allie Hajian |
|  2020-05-19 | Specified Environment docker to use for running notebooks  | Beri Shifaw |
|  2020-10-29 | Archived previous notebooks and uploaded Hail Tutorial directly from their Git repository. Updated dashboard to account for the new notebooks  | Beri Shifaw |
|  2021-09-16 | Addition of workspace citation in dashboard  | Beri Shifaw |


### Workspace Citation 
Details on citing Terra workspaces can be found here: [How to cite Terra](https://support.terra.bio/hc/en-us/articles/360035343652)

Hail Team, Broad Institute (*Year, Month Day that this workspace was last modified*) help-gatk/Hail-Notebook-Tutorials [workspace] Retrieved *Month Day, Year that workspace was retrieved*,  https://app.terra.bio/#workspaces/help-gatk/Hail-Notebook-Tutorials

### Licensing

The MIT License:     

Copyright (c) 2015-2016, Hail Authors.  All rights reserved.

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/Hail-Notebook-Tutorials"
89,"help-gatk","GATKTutorials-Somatic","READER","https://app.terra.bio/#workspaces/help-gatk/GATKTutorials-Somatic",TRUE,TRUE,NA,NA,NA,"# What's in this workspace?
Welcome to Day 3 of the Genome Analysis Toolkit (GATK) workshop! Today we will focus on Somatic Variant Discovery. 

Earlier today you received introductions to GATK tools and Best Practices pipelines. In this workspace we will be going over two forms of Somatic Analysis: one comparing tumor and normal samples using Mutect2 workflow for variant differences, and another using the Copy Number Alterations (CNA) workflow for copy number variations. 

*This workspace was originally developed for the Genome Analysis Toolkit (GATK) workshop in San José, Costa Rica in February 2020. Feel free to run it on your own, even if you were unable to attend that workshop!*

Your instructor will guide you through this workspace, but you should find the documentation within the notebooks sufficient to run through them again on your own or to share with a friend later (and we encourage you to do so!).

### Data
Data associated with this workspace is located in the [gs://gatk-tutorials/workshop_2002/3-somatic](https://console.cloud.google.com/storage/browser/gatk-tutorials/workshop_2002/3-somatic/?project=broad-dsde-outreach&organizationId=548622027621) google bucket. It contains both input and resource files for the Mutect2 and CNA workflows. Along with the inputs are precomputed outputs generated by each step in the tutorial. This can be used as input for any step in the workflow, in case your generated outputs are not correct or you are unable to complete a step in the workflow. 

### Tools
There are no tools in this workspace. The tutorials are notebook-based, allowing you to run each step manually and view the intermediate outputs of the workflow. This is a great way to understand each step in the workflow and give you the chance to manipulate the parameters to see what happens with the output.

If you are interested in a WDL based workflow of these analyses, check out the [Showcase](https://app.terra.bio/#library/showcase) area in Terra, which features many of our popular GATK workflows in workspaces ready to run your data.


### Notebooks
 **1-somatic-mutect2-tutorial :**
In this hands-on tutorial, we will call somatic short mutations, both single nucleotide and indels, using the GATK4 Mutect2 and FilterMutectCalls. If you need a primer on what somatic calling is about, see the following [GATK forum Article](https://gatk.broadinstitute.org/hc/en-us/articles/360035890491).


| Option | Value |
| --- | --- |
| Environment | Default |
| Profile | Custom |
| CPU Minimum | 4|
| Disksize Minimum | 100 GB |
| Memory Minimum | 15 GB |


**2-somatic-cna-tutorial :**
This hands-on tutorial outlines steps to detect alterations with high sensitivity in total and allelic copy ratios using GATK4's ModelSegments CNV workflow. The workflow is suitable for detecting somatic copy ratio alterations, more familiarly copy number alterations (CNAs), or copy number variants (CNVs) for whole genomes and targeted exomes.

| Option | Value |
| --- | --- |
| Environment | Default |
| Profile | Custom |
| CPU Minimum | 4|
| Disksize Minimum | 100 GB |
| Memory Minimum | 15 GB |

 
### Software versions
GATK4.1.4.1

### Time and cost 
This workspace focuses on the use of notebooks, thus the time and cost of completing the tutorial depends on the runtime environment of your notebook and the length of time you spend actively working in the notebook. With the given runtime environments above, users can expect the cost to be much less than a dollar an hour for each notebook. The specific cost will be displayed on the top right corner of your screen, next to the notebook machine options. 

## Appendix

### GATK @ Costa Rica 2020 in the Terra Support Center

Following the conclusion of the workshop, the workshop materials will be made available in the Terra Support Center. For now, you can find these materials in a google drive folder at [broad.io/GATK2002](https://broad.io/GATK2002)


### GATK documentation and support

Detailed documentation, tutorials, and more are available at the [GATK web site](https://gatk.broadinstitute.org/hc/en-us). If you are still running into issues after consulting the User Guide, have a question, or just want to say hello, reach out to the GATK team via the [GATK Support Forum](https://gatk.broadinstitute.org/hc/en-us/community/topics)!

### Terra documentation and support

Documents and helpful guides to support your journey through Terra are available in the Terra Support Center. Navigate there by following the “Learn About Terra” link in the left-side menu. At the time of this workshop, we are still actively writing and publishing documents but we’ve got a good set of docs in the Quick Start Guide to get you going.

### Workspace Citation 
Details on citing Terra workspaces can be found here: [How to cite Terra](https://support.terra.bio/hc/en-us/articles/360035343652)

Data Sciences Platform, Broad Institute (*Year, Month Day that this workspace was last modified*) help-gatk/GATKTutorials-Somatic [workspace] Retrieved *Month Day, Year that workspace was retrieved*,  https://app.terra.bio/#workspaces/help-gatk/GATKTutorials-Somatic",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/GATKTutorials-Somatic"
91,"broad-firecloud-tcga","TCGA_SARC_OpenAccess_V1-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_SARC_OpenAccess_V1-0_DATA",TRUE,TRUE,"Sarcoma","Tumor/Normal","USA","TCGA Sarcoma Open-Access Data Workspace","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients’ clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","261","Soft tissue","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh37/hg19","TCGA_SARC_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_SARC_OpenAccess_V1-0_DATA"
92,"help-gatk","Somatic_variant_discovery_hg38_v0","READER","https://app.terra.bio/#workspaces/help-gatk/Somatic_variant_discovery_hg38_v0",TRUE,TRUE,NA,NA,NA,"### GATK Best Practices for somatic variant discovery (hg38 reference)
The purpose of this workspace is to provide a fully reproducible example of the GATK Best Practices workflows for somatic variant discovery. 

#### IMPORTANT CAVEAT: This workspace is in BETA status and is provided as-is. The resource files were lifted over from b37 versions but have not yet been fully evaluated.

#### Workspace attributes
All required and optional resources for the preconfigured methods included. The reference genome is hg38 (aka GRCh38).

#### Data 
Full exomes of the HCC1143 cell line tumor and normal samples, pre-processed according to current GATK Best Practices. See BAM headers for additional details. 

#### Method configs
This workspace contains the following preset method configurations:

- **Somatic_SNV_Indel_Discovery_MC**: Somatic SNV and Indel discovery with Mutect2. Usage instructions: launch this on any qualifying Pair or Pair Set (each pair must include a tumor BAM + its matched normal and their BAM indices, aligned against the correct reference genome). By default the workflow will run the following tasks: GATK4 Mutect2 variant calling, estimation of cross-sample contamination, collection of pre-adapter artifact metrics, filtering on LOD scores and contamination and filtering on orientation bias. Annotation using Oncotator is disabled in this workspace due to lack of Oncotator support for the hg38 reference build. Note that the Panel Of Normals (PON) provided is a generic PON built from 1000Genomes project samples made available for illustrative purposes. This may not be the most appropriate PON for your data; providing a better PON is likely to produce superior results.

#### Notes
Compared to the b37 version, this hg38 workspace lacks the exome intervals and Oncotator resources.",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/Somatic_variant_discovery_hg38_v0"
93,"warp-pipelines","ReblockGVCF","READER","https://app.terra.bio/#workspaces/warp-pipelines/ReblockGVCF",TRUE,FALSE,NA,NA,NA,"# ReblockGVCF Pipeline for Reblocking GVCF Files

ReblockGVCF is an open-source, cloud-optimized pipeline that supports reblocking of GVCF files to reduce their size. For information on reblocking, see the WARP blog post, [WARP Whole Genome and Exome Pipelines Produce Reblocked GVCFs](https://broadinstitute.github.io/warp/blog/tags/reblock/). This workspace currently describes `v2.1.12` of the ReblockGVCF pipeline and provides a fully reproducible example of the workflow. 

Scroll down for details on the workflow, including input and output descriptions and requirements, estimated run times and costs, and information on downsampled example data.       

**Note that cost and time estimates will vary** with the use of [Preemptibles](https://cloud.google.com/compute/docs/instances/preemptible).  
You can also use [Google's cost calculator](https://cloud.google.com/products/calculator/) to estimate the cost to run. 

**For helpful hints on controlling Cloud costs**, see [Overview: Controlling Cloud costs on Terra](https://support.terra.bio/hc/en-us/articles/360029748111).  

----

## ReblockGVCF

### What does it do?

The ReblockGVCF workflow takes in the GVCF files from the WholeGenomeGermlineSingleSample_v3.1.11 and ExomeGermlineSingleSample_v3.1.10 pipeline versions and **earlier** and reblocks them to a 4 blocking scheme (gnomADv4 schema) using the arguments `-do-qual-approx --floor-blocks -GQB 20 -GQB 30 -GQB 40`. This is our standard recommended practice and is a required step before running joint calling. The pipeline can also take in input gvcfs that are already reblocked. 

The workflow calls two tasks, [Reblock](https://github.com/broadinstitute/warp/blob/master/tasks/broad/GermlineVariantDiscovery.wdl) and [ValidateVCF](https://github.com/broadinstitute/warp/blob/develop/tasks/broad/Qc.wdl), and outputs a reblocked GVCF file and index.

The workflow is available in the WARP repository (see the code [here](https://github.com/broadinstitute/warp/blob/master/pipelines/broad/dna_seq/germline/joint_genotyping/reblocking/ReblockGVCF.wdl).

---

### How to run the workflow

This workspace is preloaded with an **[example human dataset](https://github.com/broadinstitute/warp/blob/master/pipelines/broad/dna_seq/germline/joint_genotyping/reblocking/test_inputs/Plumbing/G96830.NA12878.index.json) and workflow configuration** designed to demo the ReblockGVCF workflow quickly and inexpensively.

The workspace workflow is configured to leverage the `example` data table in the workspace Data tab. 

To run the workflow: 

1. Navigate to the **Workflows** tab and choose the **ReblockGVCF** workflow configuration. 
1. Make sure the root entity type is `example` (selected by default) on the workflow configuration page.
1. Choose either the `NA12878_exome` or `NA12878_wgs` sample.
1. Run the analysis.

#### Using the ReblockGVCF workflow as a precursor to another workspace
If you're reblocking samples for use in another workspace, you can copy the entire data table from this workspace over to the new workspace. To copy data tables, see the Terra support article [""How to copy data tables""](https://support.terra.bio/hc/en-us/articles/360036954991).

To export the pre-configured workflow into another workspace, see **Option 3: Share or clone a workflow** in the [Create, edit, and share a new workflow Terra support article](https://support.terra.bio/hc/en-us/articles/360031366091).

---

### What does the ReblockGVCF workflow require as input?

The ReblockGVCF workflow requires the following input:

| Input Name | Description | Type |
| --- | --- | --- |
| gvcf | Google Cloud storage path to input GVCF file to be reblocked. | FIle |
| gvcf_index | Google Cloud storage path to input GVCF index file. | FIle |
| ref_dict | Google Cloud storage path to reference sequence dictionary file. | File |
| ref_fasta | Google Cloud storage path to reference FASTA file. | File |
| ref_fasta_index | Google Cloud storage path to reference FASTA index file. | File |

#### Reference data description and location  
Human reference genomes and additional resources for the tools in this workspace are included in the **Workspace** data table. The references are preconfigured in the workspace example workflow configuration.

The reference genome for human is hg38 (GRCh38), the [GENCODE v43](https://www.gencodegenes.org/human/release_43.html) primary assembly gene annotation list.

---

### Optional parameters 

The ReblockGVCF workflow offers optional inputs such as the following:

| Name       |  Format and Description      |
| ------------ | ------------------- |
| calling_interval_list | Optional file containing the interval list to be used in ValidateVcfs. | 
| machine_mem_mb | Optional task-level integer specifying the memory (in MiB) that is required for the ValidateVcfs virtual machine; default is ""7000""; if ValidateVcfs task fails with an out of memory error, add 3200 MiB to this input for a total of 10200 MiB. |

---

### What does the ReblockGVCF workflow return as output?

In this workspace, the metadata for all outputs are written to the workspace data table. These outputs are described in the following table:  

| Output Name | Format and Description |
| --- | --- |
| output_vcf | Reblocked GVCF file. |
| output_vcf_index | Reblocked GVCF index file. |

---

### Estimated time and cost to run on sample data 

The following estimates are based on the **example human dataset**. All details of each set are listed to give insight into time and cost.

| Example Name | File Sizes | Time (HH:MM) | Cost ($) |
| --- | --- | --- | --- |
| NA12878_exome | 3.8 MB | 00:16 | N/A |
| NA12878_wgs | 3.0 MB | 00:16 | N/A |

**For helpful hints on controlling Cloud costs, see the article, [Overview: Controlling Cloud costs on Terra](https://support.terra.bio/hc/en-us/articles/360029748111).**   

---

### Versioning and testing

All versions of the pipeline listed here are available by cloning the workspace and selecting a version of the ReblockGVCF method in the version dropdown. All **official** ReblockGVCF pipeline releases are documented in the [changelog](https://github.com/broadinstitute/warp/blob/master/pipelines/broad/dna_seq/germline/joint_genotyping/reblocking/ReblockGVCF.changelog.md) and tested using [plumbing and scientific test data](https://github.com/broadinstitute/warp/tree/master/pipelines/broad/dna_seq/germline/joint_genotyping/reblocking/test_inputs). To learn more about WARP pipeline testing, see [Testing Pipelines](https://broadinstitute.github.io/warp/docs/About_WARP/TestingPipelines).

| Terra Compatible Version Name | ReblockGVCF Release Version | Date | Release Note | 
| :---:  | :---: | :---: | :--- |
| ReblockGVCF_v2.1.12 | v2.1.12 | 06/2024 | Updated to GATK version 4.5.0.0. Header documentation change for RAW_GT_COUNT annotatio; ValidateVcfs requires less memory when run without interval list. |
| ReblockGVCF_v2.1.10 | v2.1.10 | 12/2023 | Added options to Reblock task to remove annotations and move filters to genotype level and updated GATK to v4.5.0.0. |
| ReblockGVCF_v2.1.9 | v2.1.9 | 12/2023 | Added an optional memory parameter to ValidateVcfs. | 
| ReblockGVCF_v2.1.8 | v2.1.8 | 11/2023 |  Updated pipeline to be able to take in GVCFs that are not in the same location as their index file and updated ValidateVcfs task to be able to take in a VCF as `calling_interval_list` that is in a separate location from its index. | 
| ReblockGVCF_v2.1.5 | v2.1.5 | 03/2023 |  Task WDLs used by the ReblockGVCF pipeline were updated with changes that don't affect ReblockGVCF wdl. |
| ReblockGVCF_v2.1.4 | v2.1.4 | 11/2022 | Updated GATK to v4.3.0.0. |
| ReblockGVCF_v2.1.3 | v2.1.3 | 06/2022 | Task WDLs used by the ReblockGVCF pipeline were updated with changes that don't affect ReblockGVCF wdl. |
| ReblockGVCF_v2.1.2 | v2.1.2 | 06/2022 | Task WDLs used by the ReblockGVCF pipeline were updated with changes that don't affect ReblockGVCF wdl. |
| ReblockGVCF_v2.1.1 | v2.1.1 | 04/2022 | Updated to Picard version 2.26.10 and GATK version 4.2.6.1 to address log4j vulnerabilities. |
| ReblockGVCF_v2.0.5 | v2.0.5 | 03/2022 | Task WDLs used by the ReblockGVCF pipeline were updated with changes that don't affect ReblockGVCF wdl. |
| ReblockGVCF_v2.0.4 | v2.0.4 | 02/2022 | Increased disk for Reblock task. |
| ReblockGVCF_v2.0.3 | v2.0.3 | 01/2022 | Task WDLs used by the ReblockGVCF pipeline were updated with changes that don't affect ReblockGVCF wdl. |
| ReblockGVCF_v2.0.2 | v2.0.2 | 11/2021 | Added a validation task to check output reblocked GVCFs for reference block overlaps. |
| ReblockGVCF_v2.0.0 | v2.0.0 |  | First release of the workspace. |

---

### Contact information

* For workspace questions and feedback, email the Broad pipelines team at warp-pipelines-help@broadinstitute.org or create a post on the [Featured Workspaces community forum](https://support.terra.bio/hc/en-us/community/topics/360001603491) (login required). 

* You can also Contact the Terra team for questions from the Terra main menu. When submitting a request, it can be helpful to include:
    * Your Project ID
    * Your workspace name
    * Your Bucket ID, Submission ID, and Workflow ID
    * Any relevant log information

---

### License
**Copyright WARP,  2024 | BSD-3**

All rights reserved. Full license text at https://github.com/broadinstitute/warp/blob/master/LICENSE. Note that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.

---",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","warp-pipelines/ReblockGVCF"
94,"help-terra","gsutil-tutorial","READER","https://app.terra.bio/#workspaces/help-terra/gsutil-tutorial",TRUE,FALSE,NA,NA,NA,"Learn how to install and use gsutil to manage buckets and objects in Terra. gsutil is part of the gcloud shell scripts and is fully open sourced on github and under active development. Understanding gsutil is useful for navigating around Terra because it is an important Python command line tool that lets you manage buckets and objects on Google Cloud Storage. 




## Overview

[gsutil](https://cloud.google.com/storage/docs/gsutil) is a useful python tool for navigating and managing Google Cloud Storage, which is where data related to workspaces is stored. gsutil allows users to interact with the Google Cloud from the terminal on their local machine or in a workspace. gsutil can be used to help with a wide range of tasks in Google Cloud including the following:

Creating and deleting buckets.
Uploading, downloading, and deleting objects.
Listing buckets and objects.
Moving, copying, and renaming objects.
## Learning Objecties 
Use gsutil commands to:
1. List files in a directory
2. Copy files into a google bucket
3. Download from a dataset
4. Set metadata

## Estimated Time and Cost
This workspace should take under 1 hour to complete with minimal storage costs

## gsutil Workspace Summary
Workspace flow
The Workspace has three parts 

![Workspace flow](https://storage.googleapis.com/terra-featured-workspaces/QuickStart/gsutil-workspace-flow.png)

## Requirements
For this workspace you will be using gsutil commands. You will need to install gsutil to your local machine using the instructions below. 
In order to install gsutil your system will need to meet the following requirements:
Linux/Unix, Mac OS, or Windows (XP or later).
Versions 5.0 and up require Python 3

### Installing gsutil to a local machine 
You will need to install gsutil to your local machine. This can be done through the official installation and update method as part of Google Cloud CLI.   Full instructions to install gsutil on your operating system can be found on Google Cloud's [install gsutil](https://cloud.google.com/storage/docs/gsutil_install) page. We also have [documentation](https://support.terra.bio/hc/en-us/articles/10206456029083-How-to-install-gsutil-on-a-local-machine) detailing how to install gsutil on a local machine. 

Make sure the Google account you use is the same account associated with Terra, otherwise you will not have authorization to use some of the commands. 
## Step-by-step instructions
1. Setting up Cloud Environment
2. Select the Environment Configuration icon (cloud with lightning bolt) to the right of the workspace.
3. Under the Jupyter option, select Settings.
4. Under the default environment option select the Create. It will take 3-5 min to create the custom computer.
5. Go to the workspace Analyses tab and open the ""gsutil-tutorial.ipynb"" notebook in Open mode.
6. Follow the instructions listed in the notebook.


## Additional resources

### How can I learn more about Terra?

To learn more about Terra and its functionality, see [Terra Support](https://support.terra.bio/hc/en-us):       
* [Getting started on Terra](https://support.terra.bio/hc/en-us/categories/360005881492)      
* [Doing research on Terra](https://support.terra.bio/hc/en-us/categories/360001399872)  

### More resources on gsutil
To learn more about commands you can run using gsutil
* [gsutil tutorial](https://support.terra.bio/hc/en-us/articles/6453490899099-gsutil-tutorial)
* [How to install gsutil to local machine](https://support.terra.bio/hc/en-us/articles/10206456029083-How-to-install-gsutil-on-a-local-machine)


## Contact information  
This material is provided by the Terra Team. Please post any questions or concerns to our forum site: [Terra](https://support.terra.bio/hc/en-us/community/topics/360001603491-Featured-Workspaces) 
<br>
<br>

## Workspace Citation 
Details on citing Terra workspaces can be found here: [How to cite Terra](https://support.terra.bio/hc/en-us/articles/360035343652)

Data Sciences Platform, Broad Institute (*Year, Month Day that this workspace was last modified*) fc-product-demo/Terra-Notebooks-Quickstart [workspace] Retrieved *Month Day, Year that workspace was retrieved*,  https://app.terra.bio/#workspaces/fc-product-demo/Terra-Notebooks-Quickstart
<br>
<br>

## License  
**Copyright Broad Institute, 2020 | BSD-3**  
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at https://github.com/openwdl/wdl/blob/master/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.







",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-terra/gsutil-tutorial"
95,"help-gatk","contamination-identification-with-pathseq","READER","https://app.terra.bio/#workspaces/help-gatk/contamination-identification-with-pathseq",TRUE,TRUE,NA,NA,NA,"## Contamination Identification with PathSeq
### Overview

This workspace provides a fully reproducible example of GATKs PathSeq tool. PathSeqPipelineSpark is a computational pathogen discovery pipeline in the Genome Analysis Toolkit (GATK) for detecting microbial organisms in short-read deep sequencing samples taken from a host organism, such as humans. We've written documentation on [How to Run the PathSeq pipeline](https://software.broadinstitute.org/gatk/documentation/article?id=10913) located on the GATK site.

- **PathSeq Blog:** see [Cross-Species Contamination Identification with PathSeq](https://gatkforums.broadinstitute.org/gatk/discussion/23205/cross-species-contamination-identification-with-pathseq#latest)
- **GATK PathSeq published paper :** [GATK PathSeq: a customizable computational tool for the discovery and identification of microbial sequences in libraries from eukaryotic hosts](https://academic.oup.com/bioinformatics/advance-article/doi/10.1093/bioinformatics/bty501/5048938)
- **Workflows:** see [Method Configurations](https://portal.firecloud.org/#workspaces/help-gatk/contamination-identification-with-pathseq/method-configs)
- **Notebook:** [downloadable from Google Cloud Storage](https://storage.googleapis.com/gatk-best-practices/pathseq/notebook/top_contamination.ipynb) and [HTML of the notebook completed](https://storage.googleapis.com/gatk-best-practices/pathseq/notebook/top_contamination_completed.htm)
- **Precomputed results:** [view in Google Cloud Storage](https://console.cloud.google.com/storage/browser/gatk-best-practices/pathseq/)

In addition to providing an example workspace and method configuration for running PathSeq, this workspace also reenacts an actual case described in the [Cross-Species Contamination Identification with PathSeq](https://gatkforums.broadinstitute.org/gatk/discussion/23205/cross-species-contamination-identification-with-pathseq#latest) blog post where one of our lead scientist collaborated with a researcher to identify some mysterious occurrences in a sequencing project. In brief, the researcher was seeing lower-than-expected read alignments during data processing. Low read alignment can be caused by a number of reasons, one being contamination. Our scientist used the PathSeq workflow in FireCloud to determine if there were any contaminants in the sequence data, and if so, display the top contaminants in chart and visualize the correlation between the original percent of alignment vs reads percent of reads aligned to the contaminant genus using Python in a Jupyter Notebook. Reproducing this case in a public workspace would be ideal, but the data analyzed is not available to be shared publicly. Thus, we have created a similar scenario to reproduce the same process in a FireCloud workspace. 

### Experimental design
In this workspace scenario, our setting is a sequencing laboratory which uses buccal smearing (cheek swabs) from the participants to obtain genetic material for sequencing. The downside of sampling through this approach is the uncertainty of purity. The mouth can host several genetic materials that doesn't originate from the participant, for example food is a source of sequencing contamination -- meat in particular causes very problematic abnormal results during variant calling. Contaminating sequences from dietary meat can be homologous enough to align with the human reference, which leads to false positives during variant calling. In this workspace Pathseq is used to identify possible contaminants in a simulated BAM file. 

The suspected cheek swab BAMs files are processed by the PathSeq workflow, along with the relevant reference and resource files, and the tool will determine which reads aligns best to the given host vs meat references. The output of the workflow will be an annotated BAM for detailed investigations, and metrics that provide a higher level of information regarding alignment stats. 

Human sequence data contaminated with dietary meat isn't readily available for analysis, therefore we use the NA12878 human sample and a few animal sequences to create simulated contaminated BAM files. Creating the simulated contaminated bam files involves manually adding animal sequences to our human sequence using Picard DownsampleSam and MergeSam. Next, the contaminated sample is run through the PathSeq pipeline to obtain the contamination matrices. Lastly, Python scripting in a Jupyter Notebook is used to sort the top contaminants in each contaminated BAM file. 

Here are the steps to the case study:  
1. Convert any CRAM to BAM format (required by PathSeq tool)  
2. Contaminate NA12878 BAM with one animal BAM  
3. Submit contaminated BAM to PathSeq  
4. Determine the top contaminating genus from the PathSeq matrices in a Jupyter Notebook  

![alt text](https://storage.googleapis.com/gatk-best-practices/pathseq/images/Workspace_diagram.png#center)

### Detailed contents of this workspace

#### Data
The data model contains links to publicly available CRAM and BAM files originating from different meats. These files were produced by the Broad Institute and made available for testing the PathSeq pipeline. The test dataset and precomputed results are available in the PathSeq Google Cloud Storage bucket [gs://gatk-best-practices/pathseq/](https://console.cloud.google.com/storage/browser/gatk-best-practices/pathseq/)

#### Workspace attributes 
The required and optional references and resources for the methods are included in the workspace attributes. The reference genome for this workspace is hg38 (aka GRCh38). 
Please note the references are specific to this case study, users are required to make their own k mer file, host ref image , and microbe ref image taxonomy file using the instructions in the [How to Run the Pathseq pipeline](https://software.broadinstitute.org/gatk/documentation/article?id=10913) tutorial before using the PathSeq workflow. The pathseq-build-microbe-reference method listed in the method configuration tab can be used to create the microbe references. 

#### Methods
We provide preset method configurations for the following workflows:

| Name | Entity Type| Synopsis |
|---|---|---|
| cram-to-bam | sample | Converts CRAM(s) to BAM(s) generating a BAM and validation report |
| contaminant-bam | pair | Downsamples and merges two BAMs to simulate contamination |
| pathseq-pipeline-WGS-meats* | sample | Runs GATK PathSeq pipeline for detecting pathogens |

*The `min-base-quality` and `quality-threshold` were lowered due to the low base quality of the meat samples, the default parameters should be used for users with good quality reads. 

##### Note on software versions

| Tool Name | Version  |
|---|---|
| GATK | 4.0.11.0 |

### Questions
The following material is made available to you by the GATK Team. Please post any questions or concerns to one of our forum sites : [GATK](https://gatkforums.broadinstitute.org/gatk/categories/ask-the-team/) , [FireCloud](https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team) , [WDL/Cromwell](https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team).

### License
#### Copyright Broad Institute, 2018 | BSD-3
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at https://github.com/openwdl/wdl/blob/master/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/contamination-identification-with-pathseq"
96,"broad-firecloud-cptac","PANOPLY_Tutorial","READER","https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_Tutorial",TRUE,FALSE,NA,NA,NA,"# PANOPLY Tutorial

PANOPLY is a platform for applying state-of-the-art statistical and machine learning algorithms to transform multi-omic data from cancer samples into biologically meaningful and interpretable results. PANOPLY leverages [Terra](http://app.terra.bio)—a cloud-native platform for extreme-scale data analysis, sharing, and collaboration—to host proteogenomic workflows, and is designed to be flexible, automated, reproducible, scalable, and secure. A wide array of algorithms applicable to all cancer types have been implemented, and we highlight the application of PANOPLY to the analysis of cancer proteogenomic data.

This PANOPLY tutorial provides a tour of how to use the PANOPLY proteogenomic data analysis pipeline, using the breast cancer dataset published in Mertins, et. al.<sup>1</sup> The input dataset (`tutorial-brca-input.zip` file) can be found in the [tutorial](https://github.com/broadinstitute/PANOPLY/tree/dev/tutorial) subdirectory, along with a HTML version of this tutorial.

## 1. Requirements
1. To run PANOPLY, you need a Terra account. To create an account follow the steps [here](https://support.terra.bio/hc/en-us/articles/360034677651-Account-setup-and-exploring-Terra). 
2. Once you have an account, you need to create a [cloud billing account and project](https://support.terra.bio/hc/en-us/articles/360026182251-How-to-set-up-billing-in-Terra). Here, you will find step-by-step instructions to set up billing, including a free credits program for new users to try out Terra with $300 in Google Cloud credits.

## 2. Clone PANOPLY production workspace
Clone the Terra production workspace at [PANOPLY_Production_Pipelines_v1_0](https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_Production_Pipelines_v1_0).

1. Navigate to [workspace](https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_Production_Pipelines_v1_0). Click the circle with 3 dots at the top right and clone the workspace. When naming the new workspace, avoid special characters or spaces, and use *only* letters, numbers and underscores.

![*Figure 1.* Cloning the production workspace](https://raw.githubusercontent.com/broadinstitute/PANOPLY/dev/tutorial/images/clone-workspace.png)

![*Figure 2.* Naming the cloned workspace](https://raw.githubusercontent.com/broadinstitute/PANOPLY/dev/tutorial/images/name-clone-workspace.png)

## 3. Run PANOPLY startup notebook
Select the `NOTEBOOK` tab and click on `PANOPLY-startup-notebook`.

![*Figure 3.* The `NOTEBOOK` tab with the `PANOPLY-startup-notebook`](https://raw.githubusercontent.com/broadinstitute/PANOPLY/dev/tutorial/images/notebook-tab.png)

The opened notebook is shown below:

![*Figure 4.* Open PANOPLY startup notebook](https://raw.githubusercontent.com/broadinstitute/PANOPLY/dev/tutorial/images/notebook.png)

Once the notebook is open, click on the `EDIT` button on top to start the Cloud Environment. The `Cloud Environment` popup will look like this:

![*Figure 5.* Cloud Environment Popup](https://raw.githubusercontent.com/broadinstitute/PANOPLY/dev/tutorial/images/cloud-env.png)

Click on the `CUSTOMIZE` button, and choose `Custom Environment` in the `Application Configuration` pull-down menu. Enter `broadcptac/panda:latest` in the `Container image` text box:

![*Figure 6.* Cloud Environment with PANDA docker filled in](https://raw.githubusercontent.com/broadinstitute/PANOPLY/dev/tutorial/images/cloud-panda-docker.png)

Then click `NEXT`. Click `CREATE` when the Unverified Docker image` page shows up. Once the Cloud Environment is running and the notebook is in EDIT mode (this may take a few minutes), read and follow the instructions in the notebook. The notebook 

* Run the initialization code section by choosing `Cell -> Run Cells` or hitting `Shift-ENTER`.:

```
source( ""/panda/build-config.r"" )
panda_initialize()
```

* The data has already been prepared and is available as a zip file [here](https://github.com/broadinstitute/PANOPLY/blob/dev/tutorial/tutorial-brca-input.zip). Download this file to your local computer and then upload it to the google bucket using instructions in `Upload ZIP file to workspace bucket`.

* Run 
```
panda_datatypes()
```
to list numerical mapping of allowable input data types. Then run 
```
panda_input()
```
provide the to uploaded input `zip` file as input and map files with datatypes as shown below:

```
$$ Enter uploaded zip file name (test.zip): tutorial-brca-input.zip
.. brca-retrospective-v5.0-cna-data.gct: 6
.. brca-retrospective-v5.0-phosphoproteome-ratio-norm-NArm.gct: 2
.. brca-retrospective-v5.0-proteome-ratio-norm-NArm.gct: 1
.. brca-retrospective-v5.0-rnaseq-data.gct: 5
.. brca-retrospective-v5.0-groups.csv: 8
.. brca-retrospective-v5.0-sample-annotation.csv: 7
.. master-parameters_tutorial.yaml: 9
.. msigdb_v7.0_h.all.v7.0.symbols.gmt: 11
Does proteomics data need normalization? (y/n): n

============================
.. INFO. Sample annotation file successfully validated.
============================
.. INFO. Validating sample IDs in other files ..
.. INFO. cna successfully validated.
.. INFO. phosphoproteome successfully validated.
.. INFO. proteome successfully validated.
.. INFO. rna successfully validated.
============================
.. DONE.
============================
```

* Provide `groups` for annotation, association and enrichment analysis. A `groups` file is already included in the input to choose specific annotations for this purpose. Retain the groups file:

```
max.categories <- 10
panda_groups()
```

```
Groups file already present. Keep it? (y/n): y
.. Selected groups:
 1: PAM50
 2: ER.Status
 3: PR.Status
 4: HER2.Status
 5: TP53.mutation
 6: PIK3CA.mutation
 7: GATA3.mutation
============================
.. DONE.
============================
```

* Run 
```
panda_sample_subsets()
```
and do no add any additional sample subsets:
```
$$ Add additional sample subsets? (y/n): n
============================
.. Sample sets to be added to Terra Workspace: 
.. all
============================
.. DONE.
============================
```
By default a sample set with all the input samples will be created.

* Finalize options:
```
panda_finalize()
```
```
============================
.. DONE.
============================
```
and run PANDA:
```
run_panda()
```
This will take a while (a few hours) to read in all the input data types, populate the data model, and upload all the data to the google bucket associated with the workspace. After successful completion of `run_panda()`, `participants`, `samples` and `sample_set` tables in the `DATA` tab will be populated, 

![*Figure 7.* Data Tab in the workspace](https://raw.githubusercontent.com/broadinstitute/PANOPLY/dev/tutorial/images/data-tab.png)

and several directories with data files will be created on the google bucket (accessed via the `Files` button on the left side in the `DATA` tab).

![*Figure 8.* Files in google bucket associated with workspace](https://raw.githubusercontent.com/broadinstitute/PANOPLY/dev/tutorial/images/google-bucket.png)


## 4. Run Workflow
Once the startup notebook has completed running, and the data tables and files have been populated, navigate to the `WORKFLOWS` tab, locate the `panoply_main_proteome` workflow and click on it. A screen like the image below will appear:

![*Figure 9.* The `panoply_main` workflow screen for setting inputs and running the workflow](https://raw.githubusercontent.com/broadinstitute/PANOPLY/dev/tutorial/images/panoply-main.png)

Fill out the `job_identifier` with an appropriate name. All other required (and some optional) inputs will be already correctly configured to use data tables created by running the startup notebook. Further, in the top section, ensure that:

- The `Run workflow(s) with inputs defined by data table` radio button is selected
- Under `Step 1`, the root entity type is set to `sample_set`

Click `SELECT DATA` under `Step 2` and select the `all` sample_set:

![*Figure 10.* Choosing data to run the `panoply_main` workflow on](https://raw.githubusercontent.com/broadinstitute/PANOPLY/dev/tutorial/images/select-data.png)

Click `OK` on the `Select Data` screen to return to the `panoply_main` workflow. Click `SAVE` to save your choices, at which point the `RUN ANALYSIS` button will be enabled. Note that all the required and optional input for the workflow are automatically filled in, and linked to appropriate data files in the Google bucket via columns in the data table. 

Start the workflow by clicking the `RUN ANALYSIS` button, and confirm launch by clicking the `LAUNCH` button on the popup:

![*Figure 11.* The `Confirm Launch` popup](https://raw.githubusercontent.com/broadinstitute/PANOPLY/dev/tutorial/images/confirm-launch.png)

The progress of the launched job can be monitored using the `JOB HISTORY` tab:

![*Figure 12.* The `JOB HISTORY` tab](https://raw.githubusercontent.com/broadinstitute/PANOPLY/dev/tutorial/images/job-history.png)

## 5. Inspect and download results
On successful completion of the submitted job, the `JOB HISTORY` tab will show a `Succeeded` status for the job. 

![*Figure 13.* The `JOB HISTORY` tab after successful job completion.](https://raw.githubusercontent.com/broadinstitute/PANOPLY/dev/tutorial/images/job-history-success.png)

Click on `View` (circled in red in the Figure) to get to the `Job Manager` page. Here, select the `OUTPUTS` tab to see all output files:

![*Figure 14.* The `Job Manager` page.](https://raw.githubusercontent.com/broadinstitute/PANOPLY/dev/tutorial/images/job-manager.png)


## Results and output reports

Running PANOPLY workflows generate several interactive `*_report`s that are HTML files with a summary of results from the appropriate analysis modules. In addition, all the reports with single-sample GSEA output is contained in the `summary_and_ssgsea` section. The complete output including all output tables, figures and results can be found in the file pointed to by `panoply_full`. All output files reside on the Google bucket associated with the execution workspace, and can be accessed by clicking on the links in the `OUTPUTS` page. Many files can be viewed directly by following the links, and all files can be downloaded either through the browser, or using the `gsutil cp` command in a terminal window. Detailed descriptions of the reports and results can be found at [Navigating Results](https://github.com/broadinstitute/PANOPLY/wiki/Navigating-Results).


## panoply_unified_workflow

In a manner similar to running `panoply_main_proteome` (which runs the full proteogenomic analysis workflow for the global *proteome* data), the `panoply_unified_workflow` can also be run. This workflow automatically runs proteogenomic analysis for the proteome and phosphoproteome data, in addition to immune analysis (using `panoply_immune_analysis`), outlier analysis (using `panoply_blacksheep`) and NMF multi-omic clustering (using `panoply_mo_nmf`). The `panoply_unified_workflow` can also run CMAP analysis (using `panoply_cmap_analysis`) but this modules is disabled, since running it can be expensive (about $150). The CMAP module can be enabled by setting `run_cmap` to `""true""`. Again, all results and reports can be found under the `OUTPUTS` tab, with descriptions of reports at [Navigating Results](https://github.com/broadinstitute/PANOPLY/wiki/Navigating-Results).


### References

1. Mertins, P. et al. Proteogenomics connects somatic mutations to signalling in breast cancer. Nature 534, 55–62 (2016).",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","broad-firecloud-cptac/PANOPLY_Tutorial"
97,"terra-biom-mass","HPFS","READER","https://app.terra.bio/#workspaces/terra-biom-mass/HPFS",TRUE,FALSE,NA,NA,NA,"# Health Professionals Follow up Study (HPFS)
This is a workspace for the BIOM-Mass project containing the full set of multi'omic microbiome sequencing data from the [Health Professionals Follow-Up Study (HPFS)](https://sites.sph.harvard.edu/hpfs/). 

This repository contains all of the 2,598 samples sequenced (both DNA and RNA) as part of the HPFS. These sequences can also be found in NCBI SRA Project ID PRJNA354235. 

References:
1. Mehta RS, Abu-Ali GS, Drew DA, et al. Stability of the human faecal microbiome in a cohort of adult men. Nat Microbiol. 2018;3(3):347–355.[ doi:10.1038/s41564-017-0096-0](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6016839/)
2. Abu-Ali GS, Mehta RS, Lloyd-Price J, et al. Metatranscriptome of human faecal microbial communities in a cohort of adult men. Nat Microbiol. 2018;3(3):356–366. [doi:10.1038/s41564-017-0084-4](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6557121/)

**All data is hosted and access controlled by the [BIOM-Mass Portal](http://biom-mass.org/repository?searchTableTab=cases).  The BIOM-Mass Data Portal is provided by the <a href=""https://hcmph.sph.harvard.edu/"">Harvard Chan Microbiome in Public Health Center (HCMPH)</a> to manage and share microbiome profiles, sample, and population information from microbiome epidemiology studies carried out through the HCMPH <a href=""https://hcmph.sph.harvard.edu/resources/"">BIOM-Mass platform</a>.  Please <a href=""https://hcmph.sph.harvard.edu/contact/"">contact us</a> to request portal access, for data depositions, and with any other inquires.**

The BIOM-Mass (Biobank for Microbiome Research in Massachusetts) project is funded by the [Massachusetts Life Sciences Center (MLSC)](http://www.masslifesciences.com/) as a collaboration with the BWH/Harvard Cohorts Biorespository.

![MLSC](http://biom-mass.org/static/media/mlsc_logo.93b1eb31.png)


",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","terra-biom-mass/HPFS"
98,"help-gatk","Exome-Analysis-Pipeline","READER","https://app.terra.bio/#workspaces/help-gatk/Exome-Analysis-Pipeline",TRUE,FALSE,NA,NA,NA,"## The latest version of this workspace is linked here: [Exome-Analysis-Pipeline workspace](https://app.terra.bio/#workspaces/warp-pipelines/Exome-Analysis-Pipeline). Please use the latest version as this workspace will be archived. 

### GATK Best Practices for Germline SNPs and Indels as used at the Broad Institute

A fully reproducible example workflow for exome sequence data pre-processing and germline short variant discovery. A description of the workflow is available in [Gatk's Best Practices Document](https://software.broadinstitute.org/gatk/best-practices/workflow?id=11165). The workflow requires unmapped Bam as input. 


- If your sequence files are not in unmapped BAM format please review the [Sequence-Format-Conversion](https://app.terra.bio/#workspaces/help-gatk/Sequence-Format-Conversion) workspace for file conversion workflows. 
- The GVCF ouputs from this workflow are intended to be used for joint variant calling. Visit the [GATK4-Germline-Preprocessing-VariantCalling-JointCalling](https://app.terra.bio/#workspaces/help-gatk/GATK4-Germline-Preprocessing-VariantCalling-JointCalling) workspace for joint calling workflows, it contains the Generate-Sample-Map and Joint-Genotyping workflows which creates a VCF using several exome GVCFs.
- The CRAM output from this workflow can be used to perform a variety of other analysis like somatic short variant discovery, germline short variant discovery, or germline copy number variant discovery. Visit the GATK Best Practices documentation to determine what [Best Practices Workflows](https://gatk.broadinstitute.org/hc/en-us/sections/360007226651) are vailable for CRAM/BAM files.

The materials in this workspace was developed by the Data Science Platform Group at the Broad Institute. 

## Workflows

### Exome Germline Single Sample

**What does it do?**     
This WDL pipeline implements data pre-processing and initial variant calling (GVCF generation) according to the GATK Best Practices for germline SNP and Indel discovery in human exome sequencing data. The workflow takes as input an array of unmapped BAM files (all belonging to the same sample) to perform preprocessing tasks such as mapping, marking duplicates, and base recalibration then uses Haplotypecaller to generate a GVCF or VCF. By default the workflow produces a single CRAM file and a GVCF to be used in [joint calling](https://gatk.broadinstitute.org/hc/en-us/articles/360035890431), but can be set to directly output a VCF instead of a GVCF.

- For the latest version of the workflow please visit the git repository [gatk4-exome-analysis-pipeline](https://github.com/gatk-workflows/gatk4-exome-analysis-pipeline). 

**What data does it require as input?**    
- Human exome sequencing data in unmapped BAM (uBAM) format
- One or more read groups, one per uBAM file, all belonging to a single sample (SM)
- Input uBAM files must additionally comply with the following requirements:
 - Filenames all have the same suffix (we use "".unmapped.bam"")
 - Files must pass validation by ValidateSamFile
  - Reads are provided in query-sorted order
 - All reads must have an RG tag
- GVCF output names must end in "".g.vcf.gz""
- Reference genome must be Hg38 with ALT contigs
- Unique exome calling, target, and bait .interval_list obtained from sequencing provider. Generally the calling, target, and bait files will not be the same.

**What does it return as output?**     
The following files are stored in the workspace Google bucket:    
- CRAM file, CRAM index, and CRAM md5 
- GVCF and its gvcf index 
- BQSR report
- Several summary metrics       

**Sample data description and location**    

The workflow in this workspace is preconfgured with the small unaligned test data below. You can use this as a template when setting up the workflow to run on your own data.      

*   SRR099969.unmapped.bam
   
**Note**: The workflow is written in WDL1.0 and imports structs to organize and use inputs. Terra isn't currently able to breakdown structs into individual workflow inputs. Thus it is not currently possible to use the workspace data table to configure the workflow. Users will need manually enter the workflow attributes for each variable. The easiest way to configure the workflow would be to download the following [input json template](https://github.com/gatk-workflows/gatk4-exome-analysis-pipeline/blob/master/ExomeGermlineSingleSample.inputs.json), edit the variables to point to the proper input and reference files as needed, and upload the json in the workflow configuration window. The option to upload the json will be on the right side of the screen as seen in this example [image](https://drive.google.com/uc?id=1yMvxD88kzA7M8GLM48tyVyVfWQ0DwdBp).

**Reference data description and location**     
The required and optional references and resources for the workflows are set in the workflow configurations. The reference genome is hg38 (aka GRCh38).

**Time and cost estimates**         
Below is an example of the time and cost for running the workflow.

| Sample Name | Sample Size | Time | Cost $ |
| -------  | -------- | -------- | ---------- |
| SRR099969.unmapped.bam | 16.8 GB | 09:03:00 | ~$0.87 |

**Note:** Cost and time will vary with the use of [Preemptibles](https://gatkforums.broadinstitute.org/firecloud/discussion/11786/preemptibles#latest).  
Users can also use [Google's BigQuery](https://software.broadinstitute.org/firecloud/documentation/article?id=11788) for task level calculation.       

For more information about controlling Cloud costs, see [this article](https://support.terra.bio/hc/en-us/articles/360029748111).


**Software Version**      
- GATK 4.1.2.0
- BWA 0.7.15-r1140
- Picard 2.16.0-SNAPSHOT
- Samtools 1.3.1 (using htslib 1.3.1)
- Python 2.7

---

### Contact information  
For questions about this workspace please visit the Featured Workspaces Topic on the [Terra Community Forum](https://broadinstitute.zendesk.com/hc/en-us/community/topics). Use the search box to see if other users have asked the same question previously. If not, post and tag **@Beri Shifaw** so that we get notified.

Please post any questions or concerns regarding the GATK tool to the [GATK forum](https://gatk.broadinstitute.org/hc/en-us/community/topics)

### License  
**Copyright Broad Institute, 2019 | BSD-3**  
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at https://github.com/openwdl/wdl/blob/master/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.

### Workspace Change Log
| Date | Change | Author |
| --- | --- | --- |
|  2020-01-28 | Initial feature of workspace | Beri Shifaw |
|  2020-02-27 | Changed workflow version to dev temporarily , due to diskspace error for SortSam task. | Beri Shifaw |
|  2020-06-05 | Updated workflow version to 1.3.0, [link to workflow changes](https://github.com/gatk-workflows/gatk4-exome-analysis-pipeline/pull/16) | Beri Shifaw |
|  2020-08-20 | Minor changes to Dashborad description, Updated workflow version to 1.4.0, [link to workflow changes](https://github.com/gatk-workflows/gatk4-exome-analysis-pipeline/releases/tag/1.4.0) | Beri Shifaw |",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/Exome-Analysis-Pipeline"
99,"brain-initiative-bcdc","Methyl-c-seq_Pipeline","READER","https://app.terra.bio/#workspaces/brain-initiative-bcdc/Methyl-c-seq_Pipeline",TRUE,FALSE,NA,NA,NA,"## Deprecation notice 10/16/2024
The CEMBA pipeline has been officially deprecated, with support ending on October 16, 2024. Users currently utilizing this pipeline are advised to transition to other options. A recommended alternative is the [Single-nucleus Methyl seq and Chromatin Capture (snm3C-seq) workflow from Dockstore](https://dockstore.org/workflows/github.com/broadinstitute/warp/snm3C-seq:snm3C_v4.0.4?tab=versions).
# Single Cell Methyl-C Seq (CEMBA) Workflow
The single cell Methyl-C seq worklfow, produced by the Broad Data Sciences Platform in collaboration with [CEMBA](https://biccn.org/teams/u19-ecker) performs methylated base calling and quality control on multiplexed human or mouse single-cell bisulfite sequencing data. This workspace describes CEMBA v1.1.1 and provides a fully reproducible example of the workflow.

Scroll down for details on the workflow, including input and output descriptions and requirements, estimated run times and costs, and information on downsampled example data.       

**Note that cost and time estimates will vary** with the use of [Preemptibles](https://cloud.google.com/preemptible-vms/).  
You can also use [Google's cost calculator](https://cloud.google.com/products/calculator/) to estimate cost to run. 

**For helpful hints on controlling Cloud costs**, see [this article](https://support.terra.bio/hc/en-us/articles/360029748111).  

---
## CEMBA

### What does it do?

Overall, the CEMBA workflow trims adapters and adaptase tails, removes inline indices into a Cell Barcode (CB) tag, and aligns reads to a provided reference. It then sorts reads, marks duplicate reads, and  filters reads by mapping quality to produce a final indexed BAM. The pipeline produces a methylation report from the output BAM. It quantifies read level non-CG methylation levels and stores methylation levels in VCF and ALLC file formats. 

Read more about the workflow in the [CEMBA Overview](https://broadinstitute.github.io/warp/docs/Pipelines/CEMBA_MethylC_Seq_Pipeline/README) or see [the code](https://github.com/broadinstitute/warp/blob/master/pipelines/cemba/cemba_methylcseq/CEMBA.wdl) in the WARP (WDL Analysis Research Pipelines) repository on GitHub.

### How to run the workflow 
This workspace is preloaded with two mouse single-cell, paired-end, methylated sequencing datasets imported from the Neuroscinece Multi Omic archive ([NeMO](https://nemoarchive.org/)). 

When data is imported from NeMO, four data tables (detailed below) are automatically created in the workspace Data page. However, you could also create your own data tables using unique table names and information.

The workspace's CEMBA workflow is set up to use the inputs listed in the `file` data table as well as the `Workspace Data` table, both of which can be found on the workspace `Data` page.

| Table Created with NeMO Import | Description |
| :-- | :-- |
| `file` | Provides a file_id for each sequencing library preparation and contains the cloud locations for the FASTQ files associated with each sequencing sample (one sample per row) as well as sample_ids |
| `file_set` | Groups the file IDs for a set of library preparations imported together from NeMO; additional sets are created when multiple files are run together through the workflow |
| `sample` | Contains metadata related to each sample and its respective project |
| `sample_set` | Contains a set of samples belonging to the same project | 


**To run the workflow:**
1. Navigate to the workspace `Workflows` page
2. Select the `CEMBA` workflow
3. On the configuration page, choose the `file` data table as the root entity in Step 1 (should already be set by default)
4. Select one or both NeMO data files in Step 2
5. Save the workflow configuration
6. Select `Run Analysis`
7. Select `Launch`

### What does CEMBA require for input?

The worklfow accepts paired-end reads in the form of two compressed FASTQ files (fastq.gz). FASTQ files may represent a single cell sample or in the case of multiplexed samples, multiple cells. 

All inputs are detailed in the [CEMBA Overview Inputs section](https://broadinstitute.github.io/warp/docs/Pipelines/CEMBA_MethylC_Seq_Pipeline/README#inputs).

#### Reference data

The  references and workflow parameters required for this workspace are included in the `Workspace Data` table. 

The reference genome used for alignment is mm10 (GRCm38).  Bowtie reference inputs are generated using a separate reference generation pipeline made by the CEMBA group, which will be published later.

#### Cutadapt parameters

The workflow trims adapter sequences using [Cutadapt v1.11](https://cutadapt.readthedocs.io/en/stable/) with default parameters and those listed in the table below. The tool assumes multiplexed samples. 

| Input Name  (Cutadapt command)     | Input Type    |  Parameter Value used in Workspace  | Description |
| --- | --- | --- | --- | 
| quality_cutoff (-q) | Int | 20 | Quality value (assumed encoded as ascii) to use as cut off for read trimming |
| min_length_paired_end_trim (-m) | Int | 62 | Reads shorter than specified length will be removed |
| min_length_single_end_trim (-m)  | Int | 30 | Reads shorter than specified length will be removed
| read1_adapter_seq (-a) | String | AGATCGGAAGAGCACACGTCTGAAC | Adaptors used for BRAIN Initiative data |
| read2_adapter_seq (-A) | String | AGATCGGAAGAGCGTCGTGTAGGGA  |  Adaptors used for BRAIN Initiative data |
| cut_length (-u) | Int | 16 | Number of bases to remove from a read | 


#### Optional parameters
Optional parameters, including those that allow you to extract and attach cell barcodes, are listed in the table below: 

| Input Name  | Input Type | Description                                                                |
| --- | --- | --- |
| monitoring_script    | File  | Script that monitors the pipeline's resource usage      |
| barcode_white_list    | File | List of cell barcodes that are being used; enables extracting cell barcodes from reads in single end mode only |
| extract_and_attach_barcodes_in_single_end_run | Boolean | If “TRUE”, workflow will extract cell barcodes and add them to tags in the BAM. Only used in single end mode | 
| min_map_quality | Int   | Optional filter threshold for minimum mapping quality. In the example runs here, we set this to 10  |


### What does CEMBA return as output?

The workflow is configured to write outputs to the `file` data table (or whichever table is selected as the root entity). These include quality control metrics, an aligned and filtered BAM and Index file, a VCF and index with methylated base calls, and an ALLC file.

All outputs are detailed in the [CEMBA Overview Outputs section](https://broadinstitute.github.io/warp/docs/Pipelines/CEMBA_MethylC_Seq_Pipeline/README#10-compute-coverage-depth).

### Estimate time and cost to run on sample data

| Sample Name | R1.fastq Size | R2.fastq Size | Time | Cost ($) |
| :---  | :--- | :--- | :--- | :--- |
| 171213_CEMBA_mm_P56-P63_3C_MOp_CEMBA171207_3C_1_CEMBA171207_3C_2_A10_AD002 | 146.92 MB | 158.07 MB |2:12:00 | 0.45 |
| 171213_CEMBA_mm_P56-P63_3C_MOp_CEMBA171207_3C_1_CEMBA171207_3C_2_A10_AD001 | 169.65 MB | 182.03 MB |2:24:00 | 0.56 |


**For helpful hints on controlling Cloud costs, see this article ([click here to view](https://support.terra.bio/hc/en-us/articles/360029748111)).**   

### Versions

You can access previous versions of this pipeline by cloning the workspace and choosing a version in the version dropdown. All workflow versions are documented in the [CEMBA changelog](https://github.com/broadinstitute/warp/blob/master/pipelines/cemba/cemba_methylcseq/CEMBA.changelog.md).

| CEMBA Release Version | Date | Release Note | 
| :---:  | :---: | :---: |
| v1.1.1 (current version) | 3/2021 | Added ALLC files as workflow output |
| v.1.0.0 | 2020 | Updated workflow name to CEMBA |

### Contributions
The following material is provided by the Broad Pipelines Team.We'd like to acknowledge and thank Benjamin Carlin, Dan Moran, Eran Mukamel, Hanqing Liu, Chongyuan Luo and Joseph Ecker for their work on this pipeline.

### Questions and feedback
* For workspace questions and feedback, email the Broad pipelines team at warp-pipelines-help@broadinstitute.org or create a post on the [Featured Workspaces community forum](https://support.terra.bio/hc/en-us/community/topics/360001603491) (login required). Tag @Alexander Baumann in the **Details** section of your post.

* You can also Contact the Terra team for questions from the Terra main menu. When submitting a request, it can be helpful to include:
    * Your Project ID
    * Your workspace name
    * Your Bucket ID, Submission ID, and Workflow ID
    * Any relevant log information

### License
**Copyright Human Cell Atlas Authors, https://humancellatlas.org, 2020 | BSD-3**

All rights reserved. Full license text at https://github.com/broadinstitute/warp/blob/master/LICENSE. Note that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","brain-initiative-bcdc/Methyl-c-seq_Pipeline"
100,"broad-firecloud-tcga","TCGA_CHOL_hg38_OpenAccess_GDCDR-12-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_CHOL_hg38_OpenAccess_GDCDR-12-0_DATA",TRUE,TRUE,"Cholangiocarcinoma","Tumor/Normal","USA","TCGA Cholangiocarcinoma Open-Access Data Workspace

*hg38 TCGA and TARGET workspaces reference files by their GDC UUIDs.  In order to run analyses on the referenced data files, you will need to run workflows that retrieve the files from the GDC and copy them to your workspace bucket.  See   [this forum post](https://gatkforums.broadinstitute.org/firecloud/discussion/10382/populating-hg38-tcga-and-target-workspaces-with-data-files#latest) for instructions on the running of these workflows.*","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients' clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","51","Bile Duct","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh38/hg38","TCGA_CHOL_hg38_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_CHOL_hg38_OpenAccess_GDCDR-12-0_DATA"
101,"broad-firecloud-tcga","TCGA_LIHC_hg38_OpenAccess_GDCDR-12-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_LIHC_hg38_OpenAccess_GDCDR-12-0_DATA",TRUE,TRUE,"Liver Hepatocellular Carcinoma","Tumor/Normal","USA","TCGA Liver hepatocellular carcinoma Open-Access Data Workspace

*hg38 TCGA and TARGET workspaces reference files by their GDC UUIDs.  In order to run analyses on the referenced data files, you will need to run workflows that retrieve the files from the GDC and copy them to your workspace bucket.  See   [this forum post](https://gatkforums.broadinstitute.org/firecloud/discussion/10382/populating-hg38-tcga-and-target-workspaces-with-data-files#latest) for instructions on the running of these workflows.*","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients' clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","377","Liver","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh38/hg38","TCGA_LIHC_hg38_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_LIHC_hg38_OpenAccess_GDCDR-12-0_DATA"
103,"help-gatk","five-dollar-genome-analysis-pipeline_Denmark_WS","READER","https://app.terra.bio/#workspaces/help-gatk/five-dollar-genome-analysis-pipeline_Denmark_WS",TRUE,FALSE,NA,NA,NA,"### GATK Best Practices for Germline SNPs and Indels as used at the Broad Institute

The purpose of this workspace is to provide a fully reproducible example of the GATK Best Practices workflows for Data Pre-processing and Germline Short Variant Discovery. A scientific description of the workflow is available in [Gatk's Best Practices Document](https://software.broadinstitute.org/gatk/best-practices/workflow?id=11145).

The ""$5 Genome Analysis Pipeline"" name refers to the cost of running the full pipeline (with all options turned to do the maximum amount of work) on a typical whole genome dataset, on the Google Cloud Platform, as explained on the Broad site in a [Gatk blog post](https://software.broadinstitute.org/gatk/blog?id=11415).

Please direct all questions to the [Firecloud forum](https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team).

#### Workspace attributes

The required and optional references and resources for the methods are included in the workspace attributes. The reference genome for this workspace is hg38 (aka GRCh38).

#### Data

Links to the expected input types are provided in the workspace data model for testing. 

* Unaligned bam list of NA12878
* Small unaligned bam list of NA12878


#### Method configs

This workspace contains the following preset method configurations:

* five-dollar-genome-analysis-pipeline : This WDL pipeline implements data pre-processing and initial variant calling (GVCFgeneration) according to the GATK Best Practices for germline SNP and Indel discovery in human whole-genome sequencing data. 


#### Time and Cost
| Sample Name | Sample Size | Time | Cost $ |
| -------  | -------- | -------- | ---------- |
| NA12878_24RG_small | 3.11 GB | 2:35:00 | 0.66 |
| NA12878 | 64.89 GB | 21:12:00 | 4.15 |

**Cost and Time will vary due to the use of preemptibles.  
**Users can also use [Google's cost calculator](https://cloud.google.com/products/calculator/).",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/five-dollar-genome-analysis-pipeline_Denmark_WS"
104,"malaria-featured-workspaces","Malaria_Plasmodium_Illumina_Amplicon","READER","https://app.terra.bio/#workspaces/malaria-featured-workspaces/Malaria_Plasmodium_Illumina_Amplicon",TRUE,FALSE,NA,NA,NA,"# Malaria Plasmodium Illumina Amplicon Workspace for Processing of Amplicon Sequencing Data

This workspace hosts workflows for processing Plasmodium amplicon sequencing data. Amplicon sequencing is a targeted DNA sequencing technique that selectively amplifies and sequences specific regions of interest within a genome. The workflow will perform detection of contamination resulting from improper sequencing protocols (in development), removal of erroneous SNPs calls caused by sequencing errors (denoising), and reporting of Drug Resistant Genotypes in the dataset. This is a standardized, panel agnostic workflow, capable of processing amplicon sequencing data produced with any amplicon panel.

## Overview of Workflows

![Overview of Amplicon Workflows](https://storage.googleapis.com/terra-featured-workspaces/Malaria/Malaria%20Featured%20Workspaces_ampseq.jpg) 

This workspace contains the following workflows:

* **01_Illumina_Amplicon_Processing**:  performs preprocessing of Illumina paired read amplicon sequencing data including sequence preparation (primer and adaptor removal), detection of inter-well contamination (in development), removal of erroneous SNPs calls caused by sequencing error (denoising). 

* **02_Plasmodium_Genotype_Population_Analysis**: takes as input amplicon sequence variants (ASVs) in pseudocigar format, performs genotyping analyses for variants of interest (for example, variants associated to resistance to antimalarials), and generates a ready-to-use report in html format. 

Note that example data and files are preconfigured in this workspace to demostrate how to use the workflow. 

More details about the example dataset and the workflow inputs and products can be found in the sections below.

## Workflows
### **01_Illumina_Amplicon_Processing**: 

#### What does it do?

This workflow performs preprocessing of Illumina paired read amplicon sequencing data, including sequence preparation (primer and adaptor removal), quality filtering, paired-end sequence merging, and denoising. Denoising is an algorithmic process for removing erroneous base calls in sequencing data. It is a necessary step before downstrean bioinformatic analyses, aimed at enhancing the fidelity of reported haplotypes. To perform denoising, 01_Illumina_Amplicon_Processing incorporates the [DADA2 algorithm](https://benjjneb.github.io/dada2/). Additional ASV filtering is perfomed based on density of variant sites (number of variant sites/amplicon length), presence of variant sites in homopolymer regions, presence of indels at the first and last position of the ASV, and the presence of bimeras (optional).

#### What does it require as input?

* Fastq files: Fastq files resulting from a sequencing experiment compressed in gzip format. File names must contain no white spaces or special characters. Furthermore, sample names must not contain underscores. Examples of correct file names are:
  * ``SRR26819135_1.subsampled.fastq.gz``
  * ``SP0101254165_S171_L001_R2_001.fastq.gz``
* Primer files: Fasta files containing the sequences of the forward and reverse primers used for amplifying the targeted amplicons. The primer names should correspond to their respective targets in the reference genome. 
* Reference sequences: A multifasta file with the reference sequence of the amplicon targets. 
* Reference bed file: A bed file with the coordinates of amplicon targets.
* Reference genomes: A multifasta file with the reference genomes of Plasmodium species in the study.
* Metadata: A csv with metadata associated to each sample in the dataset.

#### What does it return as output?

The output files are automatically stored in a google bucket and linked to the dataset used to run the pipeline.

* Amplicon_Sequence_Object: A xlsx with genotyping outcomes. 

#### Running the workflow

This workflow comes preconfigured with reference files to process _P. falciparum_ paired end reads.

1. Start by creating a sample set in the Data tab. 
2. In the Workflow interface, select ""Run workflow(s) with input defined by data table"". 
3. In Step 1, select the Data Table that points to the set.
4. In Step 2, select the checkbox for the set.
5. Populate the inputs with the variables for the analysis (if different from the pre-populated variables).
6. Click Run Analysis.

### **02_Plasmodium_Genotype_Population_Analysis**:

#### What does it do?

This workflow layouts a report summarizing Performance of Amplification, Genotype Frequencies for markers of interest, and a map of frequency of resistance to antimalarial.

#### What does it require as input?

1. Amplicon_Sequence_Object: A xlsx file produced by 01_Illumina_Amplicon_Processing with genotyping outcomes. 
2. Reference Genome: Fasta and GFF file with the Plasmodium Reference genome (PlasmoDB-59 is preloaded in the workspace).
3. Reference Alleles: A csv file with markers associated with drug resistance.
4. Selected_Checkboxes: A file indicating a subset of data to analyse. 

#### What does it return as output?

The output files are automatically stored in a google bucket and linked to the dataset used to run the pipeline.

* Genotyping Report: An html file reporting Performance of Amplification, Genotype Frequencies for markers of interest, and a map of frequency of resistance to antimalaria. 

#### Running the workflow

This workflows is designed to process outputs produced with 01_Illumina_Amplicon_Processing:

1. In the Workflow interface, select ""Run workflow(s) with input defined by data table"". 
3. In Step 1, select the Data Table that points to the set that was previously processed.
4. In Step 2, select the checkbox for the set.
5. Populate the inputs with the variables for the analysis (if different from the pre-populated variables).
6. Click Run Analysis.

## Data
#### Reference data description and location

This workspace comes preconfigured with reference data for _P. falciparum_ analyses. The reference genome is PlasmoDB-59. 

A dataset of 50 samples is preloaded to facilitate familiarization with the workflows. This dataset is composed of 50 samples collected in Mozambique, and is  subset of 558 P. falciparum-positive dried blood spot samples [PRJNA1040019](https://www.ncbi.nlm.nih.gov/bioproject/PRJNA1040019). The samples were processed and sequenced with the [MAD4HatTeR v.4](https://github.com/EPPIcenter/mad4hatter/tree/main/resources/v4) panel developed by the [EPPIcenter at UCSF School of Medicine](https://eppicenter.github.io/mad4hatter/). Metadata including geographic information and time of sampling was also collected. Additional information about the dataset can be obtained in [Brokhattingen, N., et. al (2024)](https://www.nature.com/articles/s41467-024-46535-x).

## Use case: Amplicon sequencing for antimalarial drug resistance

Among many applications, amplicon sequencing can be used to profile drug resistance in *Plasmodium falciparum*. Targeted regions of genes associated with antimalarial drug resistance, such as Pfkelch13, can be amplified, sequenced, and analyzed to screen for the presence of haplotypes that confer resistance. Ready_to_interpret report card that associate frequency of resistant haplotypes with the geographic region of origin of the sample can be produced to summarize the results of such analyses and serve as the base of public health interventions. 

## Contact Information

Have questions? Contact us at publichealthgenomics@broadinstitute.org
This email address is actively monitored and you will get a response within a business day.

### Workspace Citation

Details on citing Terra workspaces can be found here: [How to cite Terra](https://support.terra.bio/hc/en_us/articles/360035343652_How_to_cite_Terra_and_associated_resources).

Data Sciences Platform, Broad Institute (2024, Sep 13)
malaria-featured-workspaces/Malaria_Plasmodium_Illumina_Amplicon [Workspace](https://publichealth.terra.bio/#workspaces/malaria-featured-workspaces/Malaria_Plasmodium_Illumina_Amplicon).   Retrieved Month, Day, Year that workspace was retrieved, https://publichealth.terra.bio/#workspaces/malaria-featured-workspaces/Malaria_Plasmodium_Illumina_Amplicon


",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","malaria-featured-workspaces/Malaria_Plasmodium_Illumina_Amplicon"
105,"help-gatk","GATKTutorials-Germline-July2019","READER","https://app.terra.bio/#workspaces/help-gatk/GATKTutorials-Germline-July2019",TRUE,FALSE,NA,NA,NA,"# What's in this workspace?
Welcome to Day 2 of the Genome Analysis Toolkit (GATK) workshop at the University of Cambridge in Cambridge, U.K.! Yesterday you got an overview of the various tools and pipelines, but today will focus on Germline Variant Discovery. 

Here you will be running three different notebooks. In the morning we will cover variant discovery, and in the afternoon we will look at different methods for variant filtering. This workspace is read-only, so clone your own unique copy to work with it.

Your instructor will guide you through this workspace, but you should find the documentation within the notebooks sufficient to run through them again on your own or to share with a friend later (and we encourage you to do so!).

The notebook(s) in this workspace are:
* 1-germline-variant-discovery-tutorial
* 2-gatk-hard-filtering-tutorial
* 3-gatk-cnn-tutorial

All notebooks in this workspace can use the following runtime settings:

| Runtime Environments | Value |
| --- | --- |
| CPU Minimum| 4|
| Disksize Minimum | 100 GB |
| Memory Minimum | 15 GB |
| Startup Script | `gs://gatk-tutorials/scripts/install_gatk_4110_with_igv.sh` |

## Appendix

### GATK @ Cambridge 2019 in the Terra Support Center

Following the conclusion of the workshop, the workshop materials will be made available in the Terra Support Center. For now, you can find these materials in a google drive folder at [broad.io/GATK1907](http:/broad.io/GATK1907)

### GATK documentation and support

Detailed documentation, tutorials, and more are available at the [GATK web site](https://software.broadinstitute.org/gatk/). If you are still running into issues after consulting the User Guide, have a question, or just want to say hello, reach out to the GATK team via the [GATK Support Forum](https://gatkforums.broadinstitute.org/gatk)!

### Terra documentation and support

Documents and helpful guides to support your journey through Terra are available in the Terra Support Center. Navigate there by following the “How-to Guides” link under Terra Support in the left-side menu. At the time of this workshop, we are still actively writing and publishing documents but we’ve got a good set of docs in the Quick Start Guide to get you going.



",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/GATKTutorials-Germline-July2019"
106,"malaria-featured-workspaces","Malaria_Resources_Overview","READER","https://app.terra.bio/#workspaces/malaria-featured-workspaces/Malaria_Resources_Overview",TRUE,FALSE,NA,NA,NA,"# **Welcome to the Malaria Resources Overview page on Terra**

Here you will find a growing collection of validated tools (or “workflows”) to perform common data processing and analysis of common malaria genomic data types. 

The workflows are available in Terra “workspaces”—dedicated spaces where workflows and data are combined together to perform analyses in the cloud.

You can explore these workflows in the following workspaces:. 

**[Malaria_Plasmodium_Illumina_Amplicon](https://publichealth.terra.bio/#workspaces/malaria-featured-workspaces/Malaria_Plasmodium_Illumina_Amplicon)**: This workspace allows users to process Illumina-based amplicon sequencing data to filter PCR and sequencing errors, and to perform a variety of downstream analyses, including interpretation of polymorphisms associated with drug resistance. 

**[Malaria_Plasmodium_Illumina_Whole_Genome](https://publichealth.terra.bio/#workspaces/malaria-featured-workspaces/Malaria_Plasmodium_Illumina_Whole_Genome)**: This workspace allows users to processes short read whole genome sequencing data using GATK best practices and produces a dataset of high confidence SNP calls for downstream analyses, including drug resistance calls

**[MalariaGEN_Pf7_Data_Resource](https://publichealth.terra.bio/#workspaces/malaria-featured-workspaces/MalariaGEN_Pf7_Data_Resource)**. Hosted on Terra- analyze against MalariaGEN Pf7 dataset without downloading, fully hosted in the cloud.

<img src=""https://storage.googleapis.com/terra-featured-workspaces/Malaria/Malaria%20Featured%20Workspaces%20-%20Copy%20of%20Graphic%20for%20main%20landing%20page%20-%20Exec%20level%20(2).jpg"" alt=""Overview of Malaria Resources"" width=""1100""/>

More detailed documentation describing specific workflows, their validation, and use considerations can be found in each of the respective workspaces. Should you wish to use these tools outside of the Terra environment, they are also available on the malaria home page on [Dockstore,](https://https://dockstore.org/organizations/BroadInstitute/collections/Malaria) a free and open source genomics tool repository.

These workspaces have been developed and will be maintained by malaria genomics experts based at the Broad Institute and Harvard T.H. Chan School of Public Health, with valued inputs and from several collaborators and colleagues. We also welcome contributions from the malaria community! If you have a malaria genomic analysis tool you would like to implement in Terra, or a request for a new tool or feature to be created, please let us know!
 
The Malaria Resources Overview page on Terra also includes several commonly used, publicly available reference datasets, to provide contextual data for analysis. We are grateful to those who made these datasets available to support the malaria genomics community. 

### **Additional Workspaces Coming Soon:**

**Long read workspace** This workspace will enable the analysis of Oxford Nanopore long read data for *Plasmodium falciparum*.

**Barcode workspace** This workspace will enable the analysis of SNP barcode data from *Plasmodium falciparum.*

**Malaria_Anopheles_Illumina_WholeGenome** This workspace will enable users to call variants from Illumina whole genome sequencing datasets using GATK best practices.

### **What is Terra?**

Terra is a cloud-based platform that provides a secure environment where users can combine their genomic data sets (as well as publicly available contextual data, such as the reference sets described above) with best practice tools to analyze their data. 
	
### **Do I need an account?**
	
Yes, in order to use Terra you need to register using a gmail address. To create your account, see [here](https://support.terra.bio/hc/en-us/articles/360028235911-How-to-register-on-Terra-Google-SSO). 
	
### **How do I get started?**
	
To get started in Terra, you must either create your own workspace where you will upload your own data and run analysis, or clone an existing Malaria Featured Workspace to leverage the data and tools within there. To learn how to clone a workspace, see [here](https://support.terra.bio/hc/en-us/articles/360026130851-How-to-clone-your-own-workspace).

### **Do I need to pay?**
	
Use of the Terra platform itself is free, users pay only for the cloud compute resources they use to store and process their data. BMGF is currently covering these costs for the malaria community! If you would like to use Terra please contact us so we can help you get set up with an account. 
	
More information about the costs of storing and analyzing data in the cloud can be found [here](https://docs.google.com/document/d/e/2PACX-1vTPQC-bYMRxXQ4Gz9ESH_Eo-E6UXD2qOL7_3iMxJaQF3pOyW3tUyu7G9Nvk-JTf8xDkOyhftvd9L-sa/pub). 

## **Contact Us:**

Have questions? Need help creating an account, to be added to a billing account covered by BMGF contact us at publichealthgenomics@broadinstitute.org
This email address is actively monitored and you will get a response within a business day.


	
**General Terra support**
If you have general questions about Terra or run into issues, you can reach Terra support by emailing support@terra.bio.
	


",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","malaria-featured-workspaces/Malaria_Resources_Overview"
107,"anvil-datastorage","AnVIL_HPRC","READER","https://app.terra.bio/#workspaces/anvil-datastorage/AnVIL_HPRC",TRUE,TRUE,"None","TBD","","**Data Release Notes for the workspace**

Update: November 28, 2023

* On November 28, 2023, directory “temp_QC” was removed as all the data therein were unusable.  

Update: October 31, 2023

* On October 27, 2023, a top directory cleanup was performed that removed 38,233 unusable files which were a product of aborted/failed file transfers.  Please reach out to AnVIL Support for more information.  

Update: May 23, 2023

* From March 30, 2023 to May 9, 2023, the following files below were temporarily unavailable. As of May 9, 2023, these files have been restored. 

assembly_verkko_v1.2_trio.tar
m54329U_220829_095708-bc2050.5mc.hifi_reads.bam
m54329U_220107_233847-bc1016.5mc.hifi_reads.bam
m54329U_211230_014258-bc1010.5mc.hifi_reads.bam
m54329U_220117_123334-bc1012.5mc.hifi_reads.bam
m64076_220516_221911-bc2018.5mc.hifi_reads.bam
m64076_220518_191414-bc2018.5mc.hifi_reads.bam
m64076_220826_143529-bc2049.5mc.hifi_reads.bam
r54329U_20220901_220229_C01.reads.bam
r54329U_20220901_220229_D01.reads.bam
HG00621.GRCh38_no_alt.bam
HG01106.GRCh38_no_alt.bam
HG01109.CHM13Y_EBV.bam
HG01109.GRCh38_no_alt.bam
HG01978.CHM13Y_EBV.bam
HG02080.GRCh38_no_alt.bam
HG02486.GRCh38_no_alt.bam
HG02559.GRCh38_no_alt.bam
NA20129.GRCh38_no_alt.bam
NA21309.GRCh38_no_alt.bam
01_09_20_R941_GM24631_2.fast5.tar
01_09_20_R941_GM24631_3.fast5.tar
HG00733_1.fast5.tar.gz
HG00733_2.fast5.tar.gz
HG00733_3.fast5.tar.gz
m54329U_200615_084313.ccs.bam


# Human Pangenome Reference Consortium's AnVIL_HPRC Workspace

![White Logo](https://s3-us-west-2.amazonaws.com/human-pangenomics/backup/logo-proof-full.png)

This workspace holds sequencing, assembly, and pangenome data submitted to the [Human Pangenome Reference Consortium](https://humanpangenome.org/). Data is stored in this workspace to allow immediate use by researchers participating in the Human Pangenome Project. Data in this workspace is constantly being added and updated and the workspace is under active development as our production pipeline continues. Note that the HPRC Year 1 data is pending publication, currently scheduled for march of 2022. If you would like to publish using this data please contact the HPRC Coordinating Center at hprcadmin@gowustl.onmicrosoft.com.

### Data Table Organization
**Sequencing Data:** Sample Table <br/>
**Participant Information:** Participant Table <br/>
**Assemblies:** Assembly_Sample, Assembly_Annotation Tables <br/>
**Pangenomes:** Minigraph, Minigraph_CACTUS, PGGB Tables

------------------
# Sequencing Data
Production information about the HPRC Y1 samples is in the sections below. Note that samples from the HPRC_PLUS cohort had data assembled from other consortia, and the information in the sections below does not apply.

## PacBio HiFi Data From UW and WashU
PacBio HiFi sequencing was performed on the Sequel2 instrument using v2 chemistry. Samples were sequenced across 3-5 flowcells in order to generate 35-fold coverage or greater. The average coverage for the year1 samples is >40X with a median insert size of 20kb. The following file types are listed in the table:

* {flowcell_id}.ccs.bam - consensus called unaligned bam file

Consensus sequences were generated with v4.0.0 of the CCS algorithm

## Nanopore PromethION Data From UC Santa Cruz

The UCSC Nanopore PromethION HPP data were generated using the a new long read protocol (details to be published on protocols.io shortly). Briefly, this protocol uses Circulomics discs-based UHMW DNA isolation, followed by library preparation using ONT SQK-RAD004 kit and library cleanup using Circulomics discs. Typically, we used 3 RAD004-based sequencing libraries per PromethION flow cell (and 3 flow cells per individual). The observed read N50s (average) are ~80 kb, with 11x coverage in 100kb+ readsa dn 3x coverage in 200kb+ reads for each individual. The following file types are listed in the table:

* sample_Guppy_4.0.11_prom.fastq.gz

## Hi-C/Omni-C Data From UCSC
OmniC is a restriction enzyme-free HiC protocol. DNA is fragmented using an endonuclease that cuts DNA randomly, i.e., not at specific restriction sites. It uses a biotinylated linker to facilitate proximity ligation. This short linker sequence may be present within the read data. The architecture of an OmniC library molecule is:

P5adapter:GenomicRegion:Linker:GenomicRegion:P7adapter

Depending on the read length, some or all of the Linker sequence may be present in R1, R2, or both. When present, the Linker sequence is: AGGTTCGTCCATCGATCGATGGACGAACCT Note that the linker sequence is a DNA palindrome. Thus, the Linker sequence will be the same whether present in R1 or R2 (forward or reverse read).

For each HPRC sample, two OmniC libraries are generated and sequenced. Duplicates can be identified and removed within each library, but not between libraries as independent sample material goes into each. Libraries are sequenced across several Novaseq lanes, depending on lane availability, with 2x150 paired-end sequencing.

The raw fastq OmniC data can be aligned using bwa mem with flags -5SP In practice, bwa mem (local alignment) will not omit the Linker sequence from the alignment in cases when it is present in the read.

The following file types are available:

* {library_name}.R1_001.fastq.gz - forward reads
* {library_name}.R2_001.fastq.gz - reverse reads
	
## Bionano Data From Rockefeller
The following file types are available:

* {sample_name}_Saphyr_DLE1_3680591.bnx.gz - raw data view of molecule and label information and quality scores
* {sample_name}_Saphyr_DLE1_3680616.cmap - location information for label sites

For information about Bionano file specifications or utilization of Bionano files see Bionano's website.

## Strand-seq Data From EMBL
Strand-seq is a single-cell sequencing technology that resolves individual homologs within a cell by restricting sequence analysis to the DNA template strands used during DNA replication (see Sanders, et al.). Each sample has 192 fastq.gz files available -- from paired-end sequencing of 96 single cells (nuclei).

## Trio Illumina Data From NYGC
When available, we have included high coverage (30X) Illumina datasets produced by NYGC for select 1000G samples. Information about this dataset can be found at [IGSR](https://www.internationalgenome.org/data-portal/data-collection/30x-grch38).

Note that the data is stored in CRAM format (aligned to GRCHh38), but is placed in the raw_data area for each sample as the consortium uses the raw reads from the CRAMs. When used in assembly pipelines, the [extract_reads.wdl](https://github.com/human-pangenomics/hpp_production_workflows/blob/master/QC/wdl/tasks/extract_reads.wdl) is used to call samtools fastq to convert the reads to fastq format.


------------------
# Assemblies

The **assembly_sample** holds assemblies from year 1 data. Assemblies from 47 samples have been uploaded to Genbank and released. Genbank accessions can be found [here](https://github.com/human-pangenomics/HPP_Year1_Assemblies/blob/main/genbank_changes/y1_genbank_assembly_accession_ids.txt). The table has columns for maternal and paternal fastas alongside columns for each assembly aligned to both CHM13 and GRCh38. The **assembly_annotation** table holds annotation files for the assemblies found in the **assembly_sample** table.

For more information about the assembly process see the [Y1 Assembly GitHub repo](https://github.com/human-pangenomics/HPP_Year1_Assemblies)
## Assembly Metrics
**Select metrics from the [automated assembly QC]() are shown below**

### QV
![QV](https://github.com/human-pangenomics/HPP_Year1_Assemblies/blob/main/automated_qc_results/Y1_assemblies_v2_genbank_QV.png?raw=true)

### N50
![N50](https://github.com/human-pangenomics/HPP_Year1_Assemblies/blob/main/automated_qc_results/Y1_assemblies_v2_genbank_N50.png?raw=true)

### Hamming Error Rate
![Hamming](https://github.com/human-pangenomics/HPP_Year1_Assemblies/blob/main/automated_qc_results/Y1_assemblies_v2_genbank_hamming.png?raw=true)

### Switch Error Rate
![Switch](https://github.com/human-pangenomics/HPP_Year1_Assemblies/blob/main/automated_qc_results/Y1_assemblies_v2_genbank_switch.png?raw=true)

------------------

# Pangenomes
Pangenomes are available from three different strategies summarized in the table (and relevant sections) below:

| <sub>	</sub> | <sub>**Minigraph**</sub> | <sub>**Minigraph-Cactus**</sub> | <sub>**PGGB**</sub> |
| :-------- | :-------- | :------ | :------ |
| <sub> sequence comparison </sub> | <sub> reference-based, progressive </sub> | <sub> reference-based, progressive </sub> | <sub> symmetric, all-vs-all </sub> |
| <sub> resolution </sub> | <sub> SV only </sub> | <sub> base-level (via abPOA) </sub> | <sub> base-level (via abPOA) </sub> |
| <sub> scope </sub> | <sub> full assemblies </sub> | <sub> Non-centromeric </sub> | <sub> full assemblies </sub> |
| <sub> cyclic paths </sub> | <sub> no </sub> | <sub> non-reference </sub> | <sub> all </sub> |
| <sub> short read mapping </sub> | <sub> untested </sub> | <sub> yes (fast) </sub> | <sub> untested </sub> |
| <sub> long read mapping </sub> | <sub> yes (fastest) </sub> | <sub> yes </sub> | <sub> yes (slowest) </sub> |
| <sub> Assembly mapping </sub> | <sub> yes (direct) </sub> | <sub> untested </sub> | <sub> yes (via injection) </sub> |


### Assembly Inputs

The assemblies in the **assembly_sample** table were used as inputs to graph building pipelines. Of the 47 samples assembled (94 assemblies) in year 1, all but three samples were included in graph constructions (HG002, HG005 and NA19240 were excluded for evaluation purposes). GRCh38 and CHM13 were added to make the total number of haplotypes included 90.


## Graphs
### Minigraph

[Minigraph](https://github.com/lh3/minigraph) is a generalization of minimap2 (very fast) which builds the graph with iterative construction. Minigraph aligns with approximate locations and can be used to call structural variants (>50nt). Graphs were built with both GRCh38 and CHM13+Y (found [here](https://s3-us-west-2.amazonaws.com/human-pangenomics/pangenomes/freeze/freeze1/minigraph/CHM13v11Y.fa.gz)) used as reference sequences.

Graphs and associated files are available in the **minigraph** table.

### Minigraph/CACTUS

[The CACTUS pangenome pipeline](https://github.com/ComparativeGenomicsToolkit/cactus/blob/master/doc/pangenome.md) adds base-level alignments to the minigraph graphs above (so both GRCh38- and CHM13-based graphs are available). Centromeric regions are removed during  Minigraph/CACTUS graph creation.

Graphs and associated files are available in the **minigraph_cactus** table.

The graphs are available in gfa format alongside other graph and index files. Information about the associated file formats can be found:
* graph formats: [xg/gg](https://github.com/vgteam/vg/wiki/Index-Types)
* index formats: [gbwt/dist/min](https://github.com/vgteam/vg/wiki/Index-Types)
* snarls format: [snarls](https://github.com/vgteam/vg/wiki/Index-Construction)

#### Filtered Graphs
The [Giraffe](https://www.biorxiv.org/content/10.1101/2020.12.04.412486v2.abstract) short read mapper relies on the graph's snarl decomposition.  The versions of the Cactus/Minigraph graphs released here contain some spurious large deletion edges that make this decomposition less efficient, which impacts Giraffe *runtime*. Furthermore, we have found that for calling small variants with the Giraffe-[DeepVariant](https://doi.org/10.1038/nbt.4235) pipeline, *accuracy* is improved if all alleles with frequency < 10% are removed from the graph before indexing.  Two filtered versions of each of the two Minigraph/Cactus graphs are available. The graphs with `maxdel.10mb` in the name (recommended to speed up general mapping experiments) were created by removing edges that imply deletions > 10mb, and the graphs with `minaf.0.1` in the name (recommended when using with DeepVariant) were created by removing, in addition to the deletions, nodes that are covered by fewer than 9 haplotypes.

### PGGB

The Pangenome Graph Builder pipeline ([PGGB](https://github.com/pangenome/pggb)) creates and all-vs-all graph with base-level alignments and no clipping of mitochondrial or centromeric regions.

The graph files can be found in the **pggb** table








To view currently available sequencing data, navigate to the DATA tab of this workspace to view data stored in the ""sample"" data table. In-progress assembiles can also be found in the ""sample_assembly"" table. These assemblies were generated with Hifiasm v0.14.

Data can also be accessed from our [S3 Bucket](https://s3-us-west-2.amazonaws.com/human-pangenomics/index.html?prefix=working/) without egress fees. Information about the currently available data can be found on our [GitHub page](https://github.com/human-pangenomics/HPP_Year1_Data_Freeze_v1.0).

------------------

### Data Use Policy

Publication Protocol – V1.0 | April 7, 2020

The goal of the HPRC publication policy is to encourage collaboration and coordination among investigators while ensuring the timely release of research to the scientific community.  The document provides general guidelines and a description of the process for disclosure of planned publications within the consortium through submission.  This policy will be drafted and updated by the HPRC Administration and Coordination Working Group and approved by the HPRC steering committee.

**General Guidelines**
* This policy applies to all manuscripts resulting from HPRC funded activities.
* It is the responsibility of the HPRC members to take steps necessary to protect any intellectual property rights.
* Any publication must comply with the NIH Public Access policy and authors should make efforts to publish their work using open access policies when possible.
* Individual investigators funded under the HPRC may separately or collaboratively publish the results of their own work.
* Investigators within HPRC who have similar work are encouraged to work together on publications.
* A manuscript tracking sheet will be used to share information about planned manuscripts.
	* Entries in the manuscript tracking sheet are the responsibility of the corresponding author.
	* The Information requested in the manuscript tracking sheet will include the following:  Authors, Title, Status, Potential overlap with other HPRC Publications, Submission Date, Journal, Publication Date, Preprint Submission Link, PMCI Number
	* The Manuscript Tracking Sheet is available through the HPRC WIKI.

**Internal Sharing of Planned Manuscripts**
* Investigators shall notify the Steering Committee of any publication using HPRC data or analysis that may impinge on planned integrative data analysis by the consortium, so that the potential impact of the pending publication can be discussed.
* Investigators are encouraged to enter information into the tracking system as early in the process as possible to enable collaboration and minimize overlap
* Information about manuscripts shared internally within the HPRC will be treated as confidential and may not be shared. 
* HPRC members are encouraged to sharing information about publications related to the Human Reference even if not directly funded by the HPRC.

**External Sharing of Submitted Manuscripts**
 * When applicable pre-prints of submitted manuscripts must be shared via the appropriate pre-print server prior to or concurrently with the time of submission to a journal.
 * Pre-prints are considered publications and should be cited like papers. 

**Resolution of Disagreements**

If questions or disagreements arise between investigators they should work to resolve them together.  If a mutually agreeable solution cannot be attained then the investigators may petition the Steering Committee to help resolve the situation.  The decision of the Steering Committee will be final and all parties agree to abide by their decision.

------------------

### More Materials

General Reference:
* Human Pangenome Reference Consortium Homepage: [link](https://humanpangenome.org/).
* Wiki (Login Required): [link](http://wiki.humanpangenome.org/index.php/Main_Page)

Data Storage (Subject To Change):
* This Workspace: [link](https://app.terra.bio/#workspaces/anvil-datastorage/AnVIL_HPRC)
* S3 Bucket: [link](https://s3-us-west-2.amazonaws.com/human-pangenomics/index.html?prefix=submissions/).

GitHub Repositories:
* Production Workflows: [link](https://github.com/human-pangenomics/hpp_production_workflows).
* Dockstore Site: [link](https://dockstore.org/organizations/HumanPangenome).
* Y1 Data Freeze: [link](https://github.com/human-pangenomics/HPP_Year1_Data_Freeze_v1.0)
* Y1 Assemblies: [link](https://github.com/human-pangenomics/HPP_Year1_Assemblies)
* Pangenome Resources: [link](https://github.com/human-pangenomics/hpp_pangenome_resources)

------------------

Workspace contact email: juklucas@ucsc.edu

Project Name: Human Pangenome Reference 

Data permissions: Open Access
","","","","","","",NA,"NRES","","","Whole Genome","anvil-datastorage/AnVIL_HPRC"
108,"broad-firecloud-tcga","TCGA_LUAD_hg38_OpenAccess_GDCDR-12-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_LUAD_hg38_OpenAccess_GDCDR-12-0_DATA",TRUE,TRUE,"Lung Adenocarcinoma","Tumor/Normal","USA","TCGA Lung adenocarcinoma Open-Access Data Workspace

*hg38 TCGA and TARGET workspaces reference files by their GDC UUIDs.  In order to run analyses on the referenced data files, you will need to run workflows that retrieve the files from the GDC and copy them to your workspace bucket.  See   [this forum post](https://gatkforums.broadinstitute.org/firecloud/discussion/10382/populating-hg38-tcga-and-target-workspaces-with-data-files#latest) for instructions on the running of these workflows.*","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients' clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","585","Lung","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh38/hg38","TCGA_LUAD_hg38_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_LUAD_hg38_OpenAccess_GDCDR-12-0_DATA"
109,"help-gatk","Germline_variant_discovery_b37_v1","READER","https://app.terra.bio/#workspaces/help-gatk/Germline_variant_discovery_b37_v1",TRUE,TRUE,NA,NA,NA,"### GATK Best Practices for germline variant discovery (b37 reference)
The purpose of this workspace is to provide a fully reproducible example of the GATK Best Practices workflows for germline variant discovery. 

#### Workspace attributes
All required and optional resources for the preconfigured methods included. The reference genome is b37 (aka GRCh37), the Broad Institute's version of hg19. Some resource files may be named hg19 for historical reasons.

#### Data 
A set of three BAMs of NA12878 WGS data downsampled to 20% (med), 5% (small) and 20k reads, respectively. 

#### Method configs
This workspace contains the following preset method configurations:

- **HaplotypeCallerGvcf_GATK3_MC**
Runs HaplotypeCaller in GVCF mode on a single sample according to ## the GATK Best Practices (June 2016) scattered across intervals. Usage instructions: launch this on any qualifying Sample or Set of Samples (each sample must reference an analysis-ready BAM). See workflow for additional input requirements and version notes.",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/Germline_variant_discovery_b37_v1"
110,"help-gatk","Hard-Filtering-Tutorial","READER","https://app.terra.bio/#workspaces/help-gatk/Hard-Filtering-Tutorial",TRUE,TRUE,NA,NA,NA,"### Tutorial Description
This workspace is for demoing the Hard Filtering Tutorial used in the Genome Analysis Toolkit (GATK) workshop. This tutorial describes how to analyze and hard-filter raw variant calls based on site-level annotations. By hard-filtering we mean identifying thresholds on annotation values and manually filtering out variants above or below a threshold. These annotation values provide context as to how trustworthy a variant is. Our goal here is to get you on the road to understanding the meaning and role of variant annotations in variant filtering. To this end, we show you how to use GATK to plot the distribution of annotation values, using a truth set, to gauge the relative informativeness of specific variant annotations. Follow the tutorial instructions to proceed.

**Tutorial Instructions:**
1. Navigate to the Google bucket of the help-gatk/Hard-Filtering-Tutorial and download the notebook locally. 
2. In the workspace click “Clone” beneath the green “Complete” status box.
3. Edit the Workspace Name to something unique. For example, put your initials and date.
4. Select a billing project. Click “Clone Workspace” when you are done.
5. Copy this Google bucket path (you will be pasting it later): **gs://gatk-tutorials/scripts/install_gatk.sh**
6. Navigate to the Notebooks tab. Click ""Upload Notebook."" 
7. Once the notebook displays click ""Create"" in the cluster column. Give the cluster a unique name using lowercase letters and dashes instead of spaces. 
Click “Optional Settings” and set the field “Custom Script URI” to the Google bucket path you copied above.
This is a bash script located in the gatk-tutorials Google bucket that will set up your notebook so that you can use GATK.
8. Once the cluster is ""Running"" click on it.

Note: Notebooks are made of individual cell boxes that can be used to execute code or display text in markdown format. To execute the commands in the cell boxes, click on the cell and type Shift-Enter. You will not need to change any of the file paths in the notebook provided.

### Datasets
#### Truth Set 
The Genomes in a Bottle (GIAB) callset is a set of highly curated variant calls made for the NA12878 sample as part of a US National Institute for Standards and Technology (NIST) benchmarking project. This callset was made by integrating variant calls generated from different sequencing platforms (e.g. Illumina, PacBio etc) using several different variant calling tools (GATK, Freebayes etc). It is designed to be used to evaluate the accuracy and precision of different variant calling methods, on the assumption that the consensus derived from these different methods is a close approximation of the biological truth for that sample. This resource has been updated and improved over time; we will be using GIAB v3.3 in this exercise.
#### Test WGS Callset
We generated a test callset using HaplotypeCaller and GenotypeGVCFs (as prescribed in the germline Best Practices GVCF workflow) on public Illumina Platinum Genomes CEU Trio data. The trio consists of father (NA12877), mother (NA12878) and son (NA12882). The calls are subset to chromosome 20, and include all variant types (i.e. indel, SNP and mixed sites).

### Premise of the Exercise
Since we have truth data from GIAB for the mother sample, NA12878, we can identify which calls in our test callset are true positives (TP), i.e. real biological variants in her genome, and which are false positives (FP), i.e. errors due to artifacts of sequencing or failures of the variant calling algorithm. The TP calls are the ones that are present in both our test callset and the GIAB truth set, while the FP calls are the ones that are present only in our test callset but not in GIAB. We could also identify false negatives (FN), i.e. real variants that we failed to call in our test callset, by looking at the calls in the GIAB truth set that are not present in our callset, but that is outside the scope of this exercise.

Note that the makers of this resource provide a list of intervals (in BED format) that distinguishes high confidence regions (where variant calls are expected to be of high quality) from low confidence regions (where calls are of lower quality). The main difference between these regions is that the low confidence ones have messier sequencing context, which makes variant calling more difficult. This means that our assumption about how the GIAB callset approximates biological truth is more valid in the high confidence regions than in the low confidence regions. In this exercise, we use data from all regions, but we could choose to do a comparison between the subsets of regions if we wanted to evaluate how much difference that makes to the interpretation of results. 

### Plotting Annotation Profiles
One simple, visual approach to understanding how informative variant annotations can be for filtering is to plot the distribution of annotation values observed for the variants in our callset. This is especially useful when we have a truth set available, because we can differentiate the profiles of TP versus FP variants, and from that derive some expectations that can be applied to novel callsets. For example, comparing the husband callset with the GIAB mother callset will help us identify TP common variants.  
Now we will open the Jupyter Notebook where we will continue the tutorial in an interactive environment.  Inside the notebook we will execute a few GATK tools to subset the callset to the mother’s sample, annotate the true positives according to the GIAB resources, and transform the data to an R readable format. Then we use R plotting formulas to visualize the annotation profiles. Finally, we use a GATK tool to apply annotations in the “FILTER” field of our single sample vcf to identify the false positives. Note that we focus on SNPs here, but you can apply similar principles for indels.

",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/Hard-Filtering-Tutorial"
111,"clara-terra","Clara-Parabricks","READER","https://app.terra.bio/#workspaces/clara-terra/Clara-Parabricks",TRUE,FALSE,NA,NA,NA,"# NVIDIA Clara Parabricks Accelerated Genomics Workflows
[NVIDIA Clara Parabricks](https://www.nvidia.com/en-us/clara/genomics/) is a suite of accelerated genomics analysis tools that provide functionally equivalent results to their GATK counterparts. This workspace demonstrates their usage in genomics workflows and provides reference WDL implementations based on [Clara Parabricks WDL](https://github.com/clara-parabricks-workflows/parabricks-wdl).

![](https://storage.googleapis.com/clara-parabricks-public/parabricks_workflows.png)

## Available workflows:
* build_reference_indices: Build the indices used for BWA mem alignment for an input FASTA genome.
* fq2bam: Align reads with Parabricks-accelerated BWA mem
* bam2fq2bam: Extract reads in FASTQ format from a BAM file and align them to a new reference genome.
* germline_calling: Run accelerated HaplotypeCaller and DeepVariant to generate a VCF or gVCF.
* somatic_calling: Run accelerated Mutect2 to call somatic variants.
* create_pon: Create a Panel-of-Normals (PON) file for use with Mutect2.
* **NEW**: deepvariant_retraining: retrain DeepVariant on custom data using an accelerated pipeline.

## Running genomics studies with Clara Parabricks

The following steps will walk through how to run germline and somatic studies using the workflows in this repository. The first step is common to both sets of analyses:

1. Create the necessary reference indices using build_references_indices. The only input is a FASTA file; we recommend using the Homo_sapiens_assembly38.fasta  reference from the [reference bundle](https://console.cloud.google.com/storage/browser/genomics-public-data/resources/broad/hg38/v0/), for example, but you could also run using the T2T chm13 genome or one of many other human genome reference FASTA files. Note the path to the output tar file, as you will use this in subsequent analyses. You might consider adding this to your workspace attributes in Terra.
2. Run the fq2bam workflow to generate a BAM, BAI, and BQSR file for your sample(s). Note that this tool supports a ""use_best_practices"" flag that enables GATK-style best practices arguments. In addition, running BQSR requires an input set of variants (e.g., Mills_and_1000G_gold_standard.indels.hg38.vcf.gz  from the hg38 Resource Bundle).

The next steps will then depend on whether you plan to perform germline or somatic analyses:

### Germline studies
1. Run the germline_calling workflow to run DeepVariant and HaplotypeCaller. The germline_calling workflow again supports a ""use_best_practices"" argument that when set to true will set the inputs to mirror those of GATK Best Practices.


### Somatic Studies
1. Optionally, create a Panel of Normals using the create_pon workflow. The input should be a VCF file **which must [contain the contig lines in the header](Mills_and_1000G_gold_standard.indels.hg38.vcf.gz ).**
2. Run the somatic_calling workflow, which runs accelerated Mutect2. You may optionally pass a Panel of Normals (generated in the previous step).

### Realigning data to new reference genomes
The bam2fq2bam tool may be used to extract reads and realign them to updated reference genomes (e.g., to move an hg19-aligned BAM to hg38).

## Performance and Cost Optimization
### GPUs available in Terra
Terra currently supports two GPU models:
* `nvidia-tesla-t4`: Up to 4x GPUs per compute node
* `nvidia-tesla-v100`: Up to 8x GPUs per compute node

### Cost Optimization
These workflows by default use NVIDIA T4 GPUs, which are highly efficient and provide excellent performance-per-dollar; in fact, the fq2bam and germline_calling workflows  are up to 50% cheaper than the equivalent CPU versions.

We recommend running using 4 NVIDIA T4 GPUs for alignment and variant calling. We also strongly encourage using the default values for workflows, including three attempts to secure a preemptible instances before resorting to on-demand and the default CPU / RAM values. As new GPUs become available or benchmarks change, we will aim to update this section.

## Test Data
Germline data (HG001, HG002, and HG003) in this workspace is taken from the [public Genome In A Bottle trio sequencing data generated by Google](https://console.cloud.google.com/storage/browser/brain-genomics-public/research/sequencing/fastq/hiseqx/wgs_pcr_free/30x?pageState=(%22StorageObjectListTable%22:(%22f%22:%22%255B%255D%22))&prefix=&forceOnObjectsSortingFiltering=false). Tumor and matched normal data, if available, comes from the [Texas Cancer Research Board Open Access registry](https://www.nature.com/articles/sdata201610). Data in this workspace **must** be used in a manner consistent with [the TCRBOA Conditions of Data Use](http://stegg.hgsc.bcm.edu/data.html).
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","clara-terra/Clara-Parabricks"
112,"help-gatk","GATK Tutorial Notebook Example - July 2019","READER","https://app.terra.bio/#workspaces/help-gatk/GATK%20Tutorial%20Notebook%20Example%20-%20July%202019",TRUE,FALSE,NA,NA,NA,"This workspace illustrates some key features of notebooks that we use in GATK workshops to explore how the tools work, and is accompanied by [this blog post](TBD). The latest versions of the full GATK workshop tutorials are available in the [Terra Showcase](https://app.terra.bio/#library/showcase), in workspaces that are named following the convention ""GATKTutorials-Topic-MonthYear"". The showcase also features workspaces containing the Best Practices pipelines as fully configured workflows. See [this blog post](https://software.broadinstitute.org/gatk/blog?id=24139) for a walkthrough of how to use those resources effectively. 

### Instructions

#### 1. Adjust the Notebook Runtime settings
Find the grey ""Notebook Runtime"" box in the top right corner of this browser window and click on the gear icon. This will pop open a small form titled ""Runtime Environment""; in ""Profile"", select ""Custom"". Now set the parameters listed in the form to the following:

| Runtime Parameter | Value |
| --- | --- |
| CPU | 4|
| Disksize  | 50 GB |
| Memory  | 15 GB |
| Startup Script | gs://gatk-tutorials/scripts/install_gatk_4110_with_igv.sh |

Finally, click ""Apply"", then come back here and read the rest of these instructions, including the background information below. At some point the status under ""Notebook Runtime"" will change from CREATING to RUNNING. 

*Don't worry if you don't understand what just happened; we explain it further below. We have you do this first because it triggers some setup work behind the scenes that can take up to 10 minutes (usually closer to 5).*

#### 2. Find and open the notebook   
In the ""NOTEBOOKS"" tab, click on the one notebook that is listed there. This will open the notebook. If it's the first time you ever open a notebook in Terra, it may take a few minutes while Terra talks to Google Cloud to set up the necessary computing resources (for more details, read further below). In the meantime, a read-only preview of the notebook will be displayed, feel free to read through it or just go get yourself a cuppa. 

*We know it's annoying that you have to wait, and the Terra team is actively working on ways to speed up this process.* 

#### 3. Follow the instructions in the notebook
Yeah that's it, really. When you navigate away from the notebook, close your browser window or otherwise leave the notebook, the system will automatically pause your runtime to minimize running costs. You'll be able to pick up where you left off when you come back to it. 

----

## Background information about Notebooks in Terra

### Notebooks in a nutshell
A Jupyter Notebook is a special kind of document that combines static content (like text and images) with executable code. In our tutorial notebook,  we have sections of plain text that explain what's going on, GATK and related commands that you can run to actually run GATK on real data, and an integrated IGV module that allows you to view the results of the commands. A section that contains commands or code that you can run is called a ""code cell"". To run the code in a cell, you click on it and press ""shift+enter"" (your keyboard may say ""return"" instead of ""enter"") or click the ""Run"" icon in the menu bar. The output log will appear right below the code cell. For more detailed information, see [this user guide](https://support.terra.bio/hc/en-us/articles/360028237912-Getting-started-A-Jupyter-Notebooks-tutorial).

### How it works under the hood

#### The computing part
When you open a notebook in Terra, what happens behind the scenes is that the system allocates a Virtual Machine (VM) in Google Cloud, just for you. When you run commands in your Notebook, that VM is the machine that does all the work. By default Terra gives you a ""normal"" machine with commonly requested amounts of memory, local disk size and number of CPUs, with some standard software installed. If you need to run some commands that need more memory , disk or CPUs, you can change those parameters, which we call ""runtime settings"", very easily. You can even request to be given a cluster instead of a single VM. If you want the system to install additional software, you can specify a startup script that gets run when the VM is being set up -- but don't worry, you'll still be able to install additional libraries and packages (eg using pip install in a Python notebook). Collectively, all of that is called the ""Notebook Runtime"". You can change the Notebook Runtime at any point in your analysis by clicking on the gear icon in the upper right of the screen and filling out the short form accordingly, like we have you do in the first instruction earlier in this text.  

The startup script we had you add installs GATK version 4.1.1.0 as well as an IGV integration module that makes it easy to view data in IGV from within the notebook itself. The notebook itself contains instructions for using the startup script; you'll open the notebook first then follow the instructions to modify the notebook runtime while it's open. That shows you that you can do this procedure at any point, whether it's to beef up the runtime settings or to downgrade them when you're not doing any heavy lifting. 

#### The data management part
Your notebook runtime comes with local disk space; cloud subtleties notwithstanding, it's basically like a hard drive with a file system on it. You can access the file system either through a graphical file explorer or through a terminal interface that supports good ol' unix commands like `ls` and `cd`. There are a bunch of different ways to access data in a notebook; we're not going to go into the details but here's a list of the most common to give you a sense of the options. The notebook in this workspace uses options 2 and 3.

1. Upload files from your desktop through the notebook's file explorer (seriously, it can be that simple)
3. Copy data from a Google storage bucket to your notebook's local disk using `gsutil cp`
4. Run GATK4 tools directly on files in Google storage (many but not all tools support this)
5. Import tabular data from tables in the DATA tab using `fiss fapi`
6. Import tabular data from Google BigQuery, which is great when you need to store large scale data

For concrete examples, see the collection of cookbook-style notebooks in the [Terra Notebooks Playground](https://app.terra.bio/#workspaces/help-gatk/Terra%20Notebooks%20Playground) workspace. 

#### Cost of operation and runtime tuning
The cost of operating the notebook is calculated per minute, and is proportional to the runtime settings -- so a very beefy VM or cluster will cost more than a puny one. Our recommendation: keep your settings low while you're writing docs or playing around with toy examples, turn them up if you run into limitations when you're doing more serious work, then bring them back down to lower levels as soon as you can. The costs involved are mostly very small, but there's no sense in paying more than you need to! 

----

## Appendix

### GATK documentation and support

Detailed documentation, tutorials, and more are available at the [GATK web site](https://software.broadinstitute.org/gatk/). If you are still running into issues after consulting the User Guide, have a question, or just want to say hello, reach out to the GATK team via the [GATK Support Forum](https://gatkforums.broadinstitute.org/gatk)!

### Terra documentation and support

Documents and helpful guides to support your journey through Terra are available in the [Terra Support Center](https://support.terra.bio/hc/en-us). You can find the Terra support resources (including documentation guides, community forum and helpdesk form) at any time by selecting ""Terra Support"" in the left-side menu. 



",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/GATK Tutorial Notebook Example - July 2019"
113,"anvil-datastorage","ANVIL_NIA_CARD_Coriell_Cell_Lines_Open","READER","https://app.terra.bio/#workspaces/anvil-datastorage/ANVIL_NIA_CARD_Coriell_Cell_Lines_Open",TRUE,TRUE,"No phenotype data present.","Resource generation and using data for benchmarking of pipelines","USA","**Release Notes:**

Update on June 6, 2023:  
On June 6, 2023, sequence data files in directory HG00733_R10 were replaced due to an error identified in the files.  The data tables were updated to reflect the change.  

**Workspace Information:**

Here we report and share Oxford Nanopore Technologies (ONT) data from three widely used cell lines (HG002, HG02723 and HG00733) obtained from Coriell (https://www.coriell.org/). This data was generated during optimizing the ONT sequencing protocol for the long-read sequencing efforts of the NIH Intramural Center for Alzheimer’s and Related Dementias (CARD, https://card.nih.gov/). The overall aim is to generate high quality long-read data with N50s of ~30kb resulting in >30X coverage. 

**ONT sequencing protocol:**
For each of the cell-lines approximately 5x10^6 frozen cells were processed as follows:
1. We followed the Circulomics HMW DNA Extraction Cultured Cells Protocol using either the Nanobind CBB Big DNA Kit or the Nanobind Tissue Big DNA Kit.
2. Sample QC: HMW DNA was hand sheared with 1mL Luer-Lock syringes and 1.5” needles 20 times. The HMW DNA was then quantified via Qubit, Nanodrop and TapeStation.  
3. To remove fragments < 25kb we ran a SRE size selection step with the Circulomics Short Read Eliminator kit.
4. Sample QC: The HMW DNA was then quantified via Qubit.
5. The HMW DNA was sheared to a target size of ~30kb using a Megaruptor 3. Each sample was made up to 150uL and 40-50ng/uL before going into the Megaruptor 3. The HMW DNA was sheared with the Megaruptor 3 Fluid + kit, at speed 45 for 2 cycles.
6. Sample QC: The HMW DNA was then quantified with the Qubit, Nanodrop and TapeStation.  
7. Library preparation and sequencing was performed in line with Step 6 described here:  https://www.protocols.io/view/processing-human-frontal-cortex-brain-tissue-for-p-kxygxzmmov8j/v2

Sequencing output:

| SampleID      | Flowcell ID | Flowcell version | Estimated N50 (kb) | Estimated bases (gb) |
| ----------- | ----------- | ----------- | ----------- | ----------- |
| HG002 (GM24385)      | PAI80251 | R9 | 26.49 | 129.64 |
| HG00733   | PAI80410 | R9 | 31.41 | 116.9 |
| HG02723   | PAI65430 | R9 |  28.79| 128.98 |
| HG002 (GM24385) | PAK60002       | R10 | 26.66 | 145.53 |
| HG00733   | PAK58268 | R10 | 27.09  | 132.54 |
| HG02723   | PAK57726 | R10 |  29.18 | 136.23 |

**Available data structure**

Cell lines: 
* HG002
* HG00733
* HG02723

Raw data to share:
* fast5
* Basecalled reads in unaligned bam format with methylation calls
* Sequencing_reports

Results to share:
* Small variant calls (in VCF format, against grch38 and CHM13)
* Structural variant calls (in VCF format, against grch38 and CHM13)
* Assembled contigs (in fasta format)
* Methylation calls (in bed format, against grch38 and CHM13)
* Harmonized small + structural variant calls in single phased VCF (grch38 and CHM13)

**List of CARD long-read Team members**

NIH-CARD, UCSC, Baylor, NIH-NCI, NIH-NHGRI, JHU, USC, NU

**More information about this dataset can be found in this [pre-print](https://pubmed.ncbi.nlm.nih.gov/36711673/).**






","NIH Intramural Center for Alzheimer's and Related Dementias (CARD)","NHGRI","Subset of the cell lines collection used in 1kGP (https://www.internationalgenome.org/category/cell-lines/)","3","NA","Long read sequencing at Center for Alzheimer's and Related Dementias (CARD)",NA,"Unrestricted access","GRCh38, CHM13","Not applicable. Subset of the cell lines collection used in 1kGP (https://www.internationalgenome.org/category/cell-lines/)","Whole Genome","anvil-datastorage/ANVIL_NIA_CARD_Coriell_Cell_Lines_Open"
115,"biodata-catalyst","NIH Cloud Platforms - Notebooks Collection","READER","https://app.terra.bio/#workspaces/biodata-catalyst/NIH%20Cloud%20Platforms%20-%20Notebooks%20Collection",TRUE,FALSE,NA,NA,NA,"
## About the NIH Cloud Platform Interoperability (NCPI) Effort
The [NIH Cloud Platform Interoperability Effort (NCPI)](https://anvilproject.org/ncpi) is an NIH effort to establish and implement guidelines and technical standards to empower end-user analyses across participating cloud platforms and facilitate the realization of a trans-NIH, federated data ecosystem. 

## NCPI Participating Platforms
- NHGRI AnVIL
- NHLBI BioData Catalyst
- Cancer Research Data Commons
- Kids First Data Resource Center
- National Center for Biotechnology Information

## About this collection
Researchers may use one or more NCPI participating platforms for their research. Here, we aim to help researchers by showcasing demonstration code. This collection contains public notebooks that may be helpful to researchers using Terra in their NIH projects. For example, this collection includes notebooks showing how to access data via the DRS standard (drs://) and how to use Terra's data table feature to interact with an NIH data models.

### To copy a notebook to another workspace:
- Navigate to the ""Notebooks"" section of this workspace.
- Click on the menu icon (circle with three dots) in the upper right.
- Select ""copy to another workspace"".


",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","biodata-catalyst/NIH Cloud Platforms - Notebooks Collection"
116,"testmybroad","single-cell-profiling-of-endothelial-cells-during-remyelination-Goeva2024","READER","https://app.terra.bio/#workspaces/testmybroad/single-cell-profiling-of-endothelial-cells-during-remyelination-Goeva2024",TRUE,FALSE,NA,NA,NA,"This is the mouse endothelial single-nucleus dataset described in the manuscript ""HiDDEN: A machine learning label refinement method for detection of disease-relevant populations in case-control single-cell transcriptomics"" which is forthcoming in Nature Comms and described here: https://www.biorxiv.org/content/10.1101/2023.01.06.523013v1

The data is attached as an AnnData format (/endo_object.h5Seurat.h5ad) and Seurat object (""endo_object.h5Seurat)

Single-nuclei were isolated from de- and remyelinating white matter.

The metadata is as follows:

orig.ident: A unique id for each 10x well (ie. sample, in this case each biological replicate also) animal_id: A unique id for each animal (ie. biological replicate) Condition: Either PBS (vehicle control) or LPC (oligodendrocyte toxin that starts demyelination) Timepoint: Length of time after the injection of LPC or vehicle control (3, 7,12 or 18 dpi) Tube_id: Additional metadata for processing Genotype: All C57BL/6J in this study",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","testmybroad/single-cell-profiling-of-endothelial-cells-during-remyelination-Goeva2024"
118,"anvil-datastorage","AnVIL_T2T","READER","https://app.terra.bio/#workspaces/anvil-datastorage/AnVIL_T2T",TRUE,TRUE,"None","Parent-Offspring Trios","","# Telomere-to-Telomere (T2T) Consortium's AnVIL_T2T Workspace

<p align=""center""><img src=""https://schatz-lab.org/images/t2tlogo.png"" width=""50%""></p>


The [Telomere-to-Telomere (T2T) consortium](https://sites.google.com/ucsc.edu/t2tworkinggroup) is an open, community-based effort to de novo assemble the first complete reference human genome from the CHM13 hydatidiform mole. Using a combination of PacBio HiFi sequencing and Oxford Nanopore ultra long reads, the recently released CHM13v1 reference genome is nearly perfect, with an estimated sequence accuracy exceeding QV70 and only 5 rRNA arrays left unresolved. The genome includes more than 100 Mbp of novel sequence compared to GRCh38, corrects many structural errors in the GRCh38 reference genome, and unlocks the most complex regions of the genome to clinical and functional study for the first time.

### Currently Available Data
Here we use the T2T-CHM13 reference genome to investigate how it improves variant calling for individual samples, trios, and population-scale analysis. This includes 17 samples from diverse ethnicities sequenced with long reads that we analyze for SNVs, indels and structural variants using PEPPER-Margin-DeepVariant and Sniffles, along with all 3,202 short-read samples from the recently extended 1000 Genomes Project collection that we analyze using the GATK HaplotypeCaller for SNVs and indels on the NHGRI AnVIL Cloud Platform. We demonstrate that the CHM13 reference improves read mapping and variant calling across all samples in a number of major ways:
1. Adds over 80 million base pairs of sequence that can be effectively used for variant calling with long reads
2. Eliminates false positive variant calls in functional and medically relevant genes such as MAP2K3 and ZNF717 by adding sequences not present in GRCh38
3. Corrects missed variants related to false duplications in GRCh38, including within genes like CBS and KCNE1
4. Improves structural variant analysis, especially the balance of large insertion and deletion structural variant calls, by correcting collapsed tandem repeats in GRCh38
5. Reduces the rate of false de novo mutations and Mendelian violations found among trios

For more information, see the paper:

[A complete reference genome improves analysis of human genetic variation](https://doi.org/10.1101/2021.07.12.452063)<br>
Aganezov, S*, Yan, SM*, Soto, DC*, Kirsche, M*, Zarate, S*, Avdeyev, P, Taylor, DJ, Shafin, K, Shumate, A, Xiao, C, Wagner, J, McDaniel, J, Olson, ND, Sauria, MEG, Vollger, MR, Rhie, A, Meredith, M, Martin, S, Lee, J, Koren, S, Rosenfeld, J, Paten, B, Layer, R, Chin, CS, Sedlazeck, FJ, Hansen, NF, Miller, DE, Phillippy, AM, Miga, KM, McCoy, RC†, Dennis, MY†, Zook, JW†, Schatz, MC† (2021) bioRxiv doi: https://doi.org/10.1101/2021.07.12.452063
","","","","3202","","",NA,"NRES","","","Whole Genome","anvil-datastorage/AnVIL_T2T"
120,"ecaviar-production","eCAVIAR","READER","https://app.terra.bio/#workspaces/ecaviar-production/eCAVIAR",TRUE,TRUE,NA,NA,NA,"## eCAVIAR

The eCAVIAR (eQTL and GWAS Causal Variants Identification in Associated Regions) workspace provides the eCAVIAR workflow that processes input GWAS and/or eQTL data to quantify the probability of a variant to be causal in both GWAS and eQTL data without accessing individual genotype data, while also allowing for an arbitrary number of causal variants in any given locus. More information on eCAVIAR is available in [Farhad Hormozdiari](http://zarlab.cs.ucla.edu/group/), [Martijn van de Bunt](https://www.well.ox.ac.uk/people/martijn-vandebunt), [Ayellet V. Segre](https://personal.broadinstitute.org/asegre/), et al., 2016, [Colocalization of GWAS and eQTL Signals Detects Target Genes, American Journal of Human Genetics, v. 99, p. 1245 - 1260](https://www.ncbi.nlm.nih.gov/pubmed/27866706).

This workspace contains example datasets that can be leveraged for an analysis (or you can upload your own data):
* 1000 Genomes Project phase 3 (includes 2,504 samples sequenced to 30x coverage with ancestry available)
* eQTL Summary Statistics derived from GTEx version 7 
* GWAS Summary Statistics (tab delimited format)
 
Funding for the development of this workspace was provided by the V2F (Variants to Function) initiative, as part of the V2F proposal “Catalyzing a resource of data, methods, and visualizations to advance V2F activities.” The goal of V2F is to identify the functional effects of genetic variation identified in common trait association studies, including genes, cell types, and pathways that are implicated in disease processes. 

The Principal Investigators for this proposal are Jason Flannick, Noël Burtt, and Rachel Liao.

Scroll down for details on the workflow, including input and output descriptions and requirements, estimated run times and costs, and information on sample data.       


---
---

###  eCaviar 
**What does it do?**    

This WDL combines genome-wide association studies (GWASs) and quantitative trait loci (eQTL) studies in a statistical framework to quantify the probability of each variant to be causal while allowing an arbitrary number of causal variants.

**What does it require as input?** 

The eCaviar workflow requires as input the following:

| Variable   |   Format   |  Description  |
| ------------ | ------------ | ---------------- |
| phenotype_samples | .vcf.bgz | samples data in a location-sorted, block-gzipped VCF file | 
|  | .vcf.bgz.tbi | samples data tabix index file in a location-sorted, block-gzipped file |
| phenotype_variants_summary |.tsv | samples summary statistics data file containing p-values, chromosome, position, ref allele, alt alleles | 
| expression_data | .vcf.bgz | samples data in a location-sorted, block-gzipped VCF file | 
|  | .vcf.bgz.tbi | samples data tabix index file in a location-sorted, block-gzipped file |
|  | .eqtl | tissue specific gene expression statistical summary files - one per tissue |
| p_value_limit | Float | minimum phenotype p-value for which we a variant is considered to be of interest | 
| region_padding | Int | number of base pairs to include on either side of variant of interest | 

**What does it return as output?**

All outputs are written to the workspace Google bucket. 

For each cohort, i.e. combination of region, gene and tissue, eCAVIAR is run if there are at least two variants that are present in each respective data file (i.e. GWAS and EQTL, summary and sample) with the necessary data. Each run of eCAVIAR produces five files with a base name indicating the cohort:

| File  | Description |
| ----------- | ----------- |
| OUTFILE_1_set |  causal SNP in GWAS |
| OUTFILE_2_set | causal SNP in eQTL |
| OUTFILE_1_post | Causal posterior probability for each SNP in GWAS |
| OUTFILE_2_post | Causal posterior probability for each SNP in eQTL | 
| OUTFILE_col | The Colocalization posterior probability (CLPP) for each SNP |


**Sample data description and location**

Examples of the expected input types/test data are provided/linked in the eCAVIAR workflow for testing. The eCAVIAR Workflow accepts two types of data: phenotype data and expression data. 

Phenotype inputs consist of GWAS samples used in the particular study, though it is not uncommon to use other samples as an approximation, as well as the phenotype summary statistics data. In this workspace, 1000 Genomes samples where used as example inputs. It is also suggested to use samples data from individuals of similar ancestry. The phenotype summary statistics file should be provided in a .tsv file with the following columns and headers: p_value (""P""), variant_id_col (""SNP""), chromosome_col (""CHR"") , position_col (""POS""), ref_col (""A2""), alt_col (""A1""). This particular workflow only supports a single phenotype.

Expression inputs consist of GWAS samples from a study that are, in general, different than the phenotype GWAS samples as well as the expression summary statistics data. In this workspace, 1000 Genomes samples were used also as example inputs though expression samples are, in general, from a study different than those of the phenotype samples. The expression summary statistics data contains gene expression in various tissues (as opposed to the single phenotype for phenotype summary statistics) and for various genes; this means one phenotype for each tissue/gene combination. The example in this workspace points to tissues from adipose visceral omentum, pancreas, and stomach and accepts as input an array, where each element consists for a tissue name and the statistical summary data for that tissue.

The source code is available from [GitHub](https://github.com/broadinstitute/cumulonimbus/tree/master/wdl/ecaviar).


**Estimated time and cost to run on sample data** 

The following estimates are based on two sets of data, human and mouse, each containing different numbers of samples. All details of each set are listed to give insight into time and cost.
 
| Sample Set Name | Time | Cost $ |
| :---:  | :---: | :---: | 
| eCAVIAR example data | 1:21:00 | 39.85 |

**Note that cost and time estimates will vary** with the use of [Preemptibles](https://gatkforums.broadinstitute.org/firecloud/discussion/11786/preemptibles#latest).  
You can also use [Google's cost calculator](https://cloud.google.com/products/calculator/) to estimate cost to run.  **For helpful hints on controlling Cloud costs**, see [this article](https://support.terra.bio/hc/en-us/articles/360029748111).  

**For helpful hints on controlling Cloud costs**, see this article ([click here to view](https://support.terra.bio/hc/en-us/articles/360029748111)).   

**Software Versions** 

* PLINK v1.90b6.7 64-bit (2 Dec 2018)
*  tabix, gunzip
* Chowser 1.7.1
* eCAVIAR (version of August 19, 2019)

---
---

### Contact Information
This material is provided by the V2F team.  Please send questions about scientific background to flannick@broadinstitute.org and queries about the implementation of the WDL to oliverr@broadinstitute.org.

---
---

### License
**Copyright Broad Institute Data Sciences Platform, 2019 | BSD-3**

All rights reserved. Note that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.

---",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","ecaviar-production/eCAVIAR"
121,"help-gatk","rna-seq-germline-snps-indels","READER","https://app.terra.bio/#workspaces/help-gatk/rna-seq-germline-snps-indels",TRUE,TRUE,NA,NA,NA,"### GATK Best Practices for Germline SNPs and Indels

The purpose of this workspace is to provide a fully reproducible example of the GATK Best Practices workflows for Data Pre-processing and Germline Short Variant Discovery on RNA sequence data.

#### Workspace attributes

The required and optional references and resources for the methods are included in the workspace attributes. The reference genome for this workspace is hg19 (aka b37).

#### Data

Link to the example input is provided in the workspace data model for testing. 

 * rnaseq-germline-snps-indels expected entity type: `sample`
   * NA12878 BAM file

#### Method configs

This workspace contains the following preset method configurations:

* rnaseq-germline-snps-indels : Workflows for processing RNA data for germline short variant discovery with GATK (v3+v4) and related tools",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/rna-seq-germline-snps-indels"
122,"aryee-lab","bisulfite-seq-tools-grch38","READER","https://app.terra.bio/#workspaces/aryee-lab/bisulfite-seq-tools-grch38",TRUE,FALSE,NA,NA,NA,"### bisulfite-seq-tools-grch38

Methods from this workspace can be used for alignment and quality control analysis for various DNA methylation protocols including Whole Genome Bisulfite Sequencing (WGBS), Reduced Representation Bisulfite Sequencing (RRBS) and Hybrid Selection Bisulfite Sequencing (HSBS).

**Note: This workspace is pre-configured for GRCh38. There are similar workspaces available for hg19 and mm10. Please get in touch with Martin Aryee  (aryee@broadinstitute.org) if you're looking for help setting up other genomes.**

---

### Data

**Sample Data**  
All preprocessing methods require forward and reverse  fastq or fastq.gz files entered into a sample data table.  The tables below outline the format and content of the **Sample** data model tables (tab separated files with “tsv” extension).

   **Example 1:** samples.tsv for RRBS and WGBS

| entity:sample_id 	| bs_fastq1                          	| bs_fastq2                          	|
|-----------------------	|------------------------------------	|------------------------------------	|
| Sample1               	| gs://location/sample_1_R1.fastq.gz 	| gs://location/sample_1_R2.fastq.gz 	|
| Sample2               	| gs://location/sample_2_R1.fastq.gz 	| gs://location/sample_2_R2.fastq.gz 	||

   **Example 2:** Participants.tsv for HSBS requires an additional input, the target_region.

| entity:sample_id 	| bs_fastq1                          	| bs_fastq2                          	| target_region                         	|
|-----------------------	|------------------------------------	|------------------------------------	|---------------------------------------	|
| Sample1               	| gs://location/sample_1_R1.fastq.gz 	| gs://location/sample_1_R2.fastq.gz 	| gs://location/hg38_targetCoverage.bed 	|
| Sample2               	| gs://location/sample_2_R1.fastq.gz 	| gs://location/sample_2_R2.fastq.gz 	| gs://location/hg38_targetCoverage.bed 	|


   **Example 3:** sample_set_membership.tsv can be created by grouping sample_ids using this format:

| membership:sample_set_id 	| sample 	|
|-------------------------------	|-------------	|
| Human_single_cell_Expt_1      	| Sample1     	|
| Human_single_cell_Expt_1      	| Sample2     	|


This workspace includes test datasets to help you  become familiar with the workflows

| sample_set_id 	| participants 	| cell type                                             	|
|--------------------	|--------------	|-------------------------------------------------------	|
| HES                	| 3            	| Human single cell                                     	|
| HES_set2           	| 2            	| Human single cell                                     	|
| Human_wgbs         	| 1            	| Human whole genome                                    	|
| MES                	| 4            	| Mouse single cell                                     	|
| Mouse_wgbs         	| 1            	| Mouse whole genome                                    	|
| test_set_mouse_sc  	| 3            	| Small mouse single cell data set(for program testing) 	|


---

### Tools

This workspace contains the following preset method configurations, already set up for grch38, hg19 and mm10.  Other genomes can be loaded by changing the json inputs.  

* Preprocessing tools are run on a ""sample"", selecting one sample or a set of samples.  
* The aggregation tool is run as a ""sample set"" on a set of preprocessed samples aligned to the same genome.  
* Aggregation can only be done after preprocessing.      
* Both preprocessing and aggregation generate html reports (see links to examples below).  

**Preprocessing:**

* wgbs-grch38: Preprocess Whole Genome Bisulfite Sequencing (WGBS) data for GRCh38
* rrbs-grch38: Preprocess Reduced Representation Bisulfite Sequencing (RRBS) data for  GRCh38
* hsbs-grch38: Preprocess Hybrid Selection Bisulfite Sequencing (HSBS) data for  GRCh38

[Example bismark processing report](https://storage.googleapis.com/terra-featured-workspaces/methylation-preprocessing/example_output/test_HES_sample_1_R1.fastq.gz_bismark_report.html)

Similar workflows exist for mm10 and hg19.

**Aggregation:**

* aggregate_bismark_output: Aggregates outputs from preprocessing pipelines and produces an aggregated data structure for further downstream analysis

[Example output from scmeth R package](https://storage.googleapis.com/terra-featured-workspaces/methylation-preprocessing/example_output/qcReport.html)

Based on the reference genome different method configurations could be selected. So far, we have hg38, hg19 and mm10 reference genome versions of all preprocessing and aggregation tools.


---

### Contact information  

Martin Aryee
aryee@broadinstitute.org
http://aryee.mgh.harvard.edu


The paper associated with these workspace tools can be found here: https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-2750-4

---

### License  

**Copyright Broad Institute, 2019 | BSD-3**
All code provided in this workspace is released under the WDL open source code license (BSD-3) [full license text here](https://github.com/openwdl/wdl/blob/master/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.

",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","aryee-lab/bisulfite-seq-tools-grch38"
123,"help-terra","Terra-Tools","READER","https://app.terra.bio/#workspaces/help-terra/Terra-Tools",TRUE,FALSE,NA,NA,NA,"This workspace contains useful tools that may come in different flavors (Workflows, Notebooks, etc) you may need to perform some convenient Terra actions such as:

1. Remove intermediate files generated by your workflows
2. Update references in your Data Model if the location of your files changes   

Each section below contains:       
* Description of the tool
* How to implement the tool
* Notes and Considerations
 
---


## Jupyter Notebook : Remove_Workflow_Intermediates
#### **Disclaimer:** **This Notebook is to be implemented at the user’s discretion. We are not responsible for any unexpected behavior (user error or otherwise). Please ensure that you have saved the files you would like to persist to the Data Model (or a more permanent location)before running this Notebook.**

### **What is this Notebook?**
When launching large numbers of submissions, increased storage costs can be incurred due to accumulating intermediate output data. This notebook offers users a simple option to delete some of the workflow intermediates in a single Terra workspace's Google bucket.

### **What does it do?**
This Notebook utilizes the mop command from FISS, a programmatic interface to FireCloud that provides Python bindings to the API, to enable users to remove unwanted and/or unnecessary intermediate files. Outputs from a Workflow can include final products, like vcfs, as well as additional intermediate files from intermediate tasks. If intermediates are not necessary, storage in the bucket continually incurs a cost to the user. This option circumvents the need to manually delete intermediates in submissions and offers the option to keep required outputs.

### **How does it do it?**
Every submission launched (can contain single or multiple workflows) is assigned a submissionID that is used to label the “directory” in the Google bucket where output files are copied. With the individual workspace name and the Terra billing-project, a list of submissionIDs is generated and intermediate files within the submission directory are deleted.

### **Notes and Considerations**

**What gets deleted?**
Workflow output files minus logs are deleted except any outputs that are bound to the Data Model. To bind outputs to the Data Model, select Defaults from the Outputs section of the Workflow before selecting “Launch Analysis”.

**What gets left behind?**
* Files uploaded to the Google bucket that do not live inside a submission “directory” will NOT be deleted.
* Log files (stderr, stdout, .log) within a submission “directory” will NOT be deleted.
* Submission folders/“directories” will NOT be deleted - only the contents.
* Notebooks in the Google bucket will NOT be deleted.

**What should you do before using this Notebook?**
* If there are outputs that should not be deleted, they will need to be bound to the Data Model. If a file is NOT bound to the Data Model, it will be removed.
* If not bound to the Data Model, desired files should be copied to a secondary location.

### **To run this notebook**
1. Copy this Notebook into the workspace where you want to remove intermediates generated from launched submissions.
2. Open the Notebook and Create a Runtime Environment if necessary.
3. After the Notebook is open, select Cell > Run All.
4. You will be prompted to enter Yes/No before deletion begins. Enter Yes/No and press Enter.



-------

## Jupyter Notebook: Update_Data_Model_References
#### **Disclaimer:** **This Notebook is to be implemented at the user’s discretion. We are not responsible for any unexpected behavior (user error or otherwise). Please ensure that you have saved the your table .tsv files and/or data you would like to persist to a permanent location before running this Notebook.**

### **What is this Notebook?**
If a situation arises wherein a user must copy data from a current Google bucket to another, references in the Data Model pointing to the original location need to be updated. If many entities exist in a table, manual updates can be cumbersome and/or error prone. This Jupyter notebook can be used to programatically update the paths in your Data Model from existing gsutil source location to a new gsutil destination in a single Terra workspace.

### **What does it do?**
This Notebook utilizes Python code (hosted in GitHub) to call FireCloud APIs, enabling users to update paths in the Data Model from one gsutil bucket URL to a second gsutil bucket URL. This option circumvents manual updates of each entity in the Data Model should the data or reference paths change. 

### **How does it do it?**
The Python code allows the user to input the original Google bucket path (in String format) and the replacement Google bucket path (in String format). The Notebook interacts with the API to replace the original path with the new bucket paths.

### **Notes and Considerations**

**What gets updated?**
* The Notebook applies to all the entity tables in the Workspace including the Workspace References.

**What gets left behind?**
* Any entities that do not point to the original Google bucket gs:// URL will not be updated to the new gs:// URL.

**What should you do before using this Notebook?**
* If there are any important files that should be accessible, they will need to be saved to a secondary location. 

### **To run this notebook**
1. Copy this Notebook into the workspace where you want to update the Data Model references.
2. Open the Notebook and Create a Runtime Environment if necessary.
3. After the Notebook is open, select Cell > Run All.

-----

## Jupyter Notebook: Update_TCGA_Data_Model_DRS_URIs
#### **Disclaimer:** **This Notebook is to be implemented at the user’s discretion. We are not responsible for any unexpected behavior (user error or otherwise). Please ensure that you have saved the your table .tsv files and/or data you would like to persist to a permanent location before running this Notebook.**

### **What is this Notebook?**
Current TCGA data workspaces (open and controlled access) have data model tables with values that are string identifiers for the files in question. There is additional work required on the part of the user to resolve the UUID strings into formats that are ready to be used in Workflows. Running this notebook will update the tables, those that are applicable, with the new DRS 1.1 uri format that will no longer require any additional steps to access files. 

### **What does it do?**
This Notebook utilizes Python code to call FireCloud APIs, enabling users to update DRS uris in the Data Model from from string UUIDs to denote a file to a drs://dg.4DFC:UUID format.

### **How does it do it?**
The Python code allows the user to run the code in ""dry-run"" mode allowing them to first inspect potential changes. Then the user can switch the ""dry-run"" mode to `False` which will then re-run the code and update all the tables in the Data Model where relevant.

### **Notes and Considerations**

**What gets updated?**
* The Notebook applies to all the entity tables in the Workspace except tables of type, set. Set tables are not modified since they only reference the unique id of the entities in the entity table.

**What gets left behind?**
* Any entities that do not contain valid UUID formats or are in columns that are not defined to be updated.

**What should you do before using this Notebook?**
* Read the instructions in the Notebook to learn how to set the value of ""dry-run"" to `True` or `False` and to learn about what each option does.

### **To run this notebook**
1. Copy this Notebook into the workspace where you want to update the Data Model DRS uris.
2. Open the Notebook and Create a Runtime Environment if necessary.
3. After the Notebook is open, select Cell > Run All.

-----

## Jupyter Notebook: Get_Workflow_Metadata
#### **Disclaimer:** **This Notebook is to be implemented at the user’s discretion. We are not responsible for any unexpected behavior (user error or otherwise). Please ensure that you have saved the your table .tsv files and/or data you would like to persist to a permanent location before running this Notebook.**

### **What is this Notebook?**
When launching large numbers of submissions, it can be difficult to sort through the metadata of individual workflows. For example, agregating the costs of several hundred workflows requires clicking through each submission. This notebook offers users a simple option to fetch all or some of the workflow metadata, view it, and optionally export it to BigQuery.

### **What does it do?**
This notebook uses the Firecloud API to:

- List all the submissions of the current workspace.
- List all the workflows of a selected submission.
- For each workflow, parse out the cost, duration, and other metadata.


### **How does it do it?**
You may run step 1 of this notebook as-is. No modifications are necessary. Step 2 requires you to fill in the BigQuery IDs.

After running step 1 of this notebook, you will be prompted to select a `submissionId`. **Please copy and paste one from the displayed table, or type in ""all"" (without quotes) to pull all submissions**.




### **Notes and Considerations**

If your submission has several workflows, the notebook may take several minutes to run. This is because we must make one API call per workflow. For example, if your submission has 427 workflows, the notebook will make 427 sequential API calls which may take 4-6 minutes.

### **To run this notebook**
1. Copy this Notebook into the workspace that submitted the workflows.
2. Open the Notebook and Create a Runtime Environment if necessary.
3. After the Notebook is open, select Cell > Run All.


## Workspace Change Log
| Date | Change | Author |
| --- | --- | --- |
|  2020-02-18 | workspace created, dashboard updated, notebooks added | Sushma Chaluvadi |
|  2021-05-13 | drs uri notebook added, dashboard updated | Sushma Chaluvadi |
|  2022-10-28 | Get Workflow Data notebook added, dashboard updated | Willy Nojopranoto |

---

## Contact information  
For any questions, reach out to Terra-support@broadinstitute.zendesk.com or post on the [General Discussion forum](https://support.terra.bio/hc/en-us/community/topics/360000500432-General-Discussion).

---

## License  
**Copyright Broad Institute, 2019 | BSD-3**  
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at https://github.com/openwdl/wdl/blob/master/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-terra/Terra-Tools"
124,"broad-firecloud-dsde","Update_Cancer_Data_Model_Paths","READER","https://app.terra.bio/#workspaces/broad-firecloud-dsde/Update_Cancer_Data_Model_Paths",TRUE,FALSE,NA,NA,NA,"# Update Data Model paths of migrated cancer data to new locations

 **This workspace contains a Jupyter Notebook to be copied to a user's workspace requiring Data Model path updates to the new locations of migrated cancer data. Once the Notebook is copied into the workspace, execute the code within the notebook to easily update the current paths! Detailed instructions are listed below.
Additional documentation and instruction have  been added to the notebook to explain each step.**

 **Note that this Notebook was created and last updated in February of 2020 and was intended for use at that time. It has not been maintained, and the team cannot guarantee its functionality.**

#### 1. Click the *Notebooks* tab to view the *Update_Data_References_Cancer_Data* notebook. 

![click-notebooks-tab-see-notebook](https://drive.google.com/uc?id=1dAzTGXwW4OE0MF4_O2iNUQ4di_KQdkEX)



#### 2. Click the *three-dots icon* to open up menu options and select *Copy to another workspace*. 

![copy-to-another-workspace](https://drive.google.com/uc?id=1-hn5O5v5v9CRSbzBC2l8QnOGJ9K_4WH5)



#### 3. Enter the *Destination* workspace and *optionally* a new name for the notebook copy. Click *COPY*. 

![select-destination-workspace](https://drive.google.com/uc?id=1vlWidwpOZdHEWp4fr6FZ-yjkfk03Xogy)



#### 4. Click *GO TO COPIED NOTEBOOK*. 

![go-to-copied-notebook](https://drive.google.com/uc?id=1ULCcJaEqY_WVeG2ewDxleFdya_PpCBYq) 



#### 5. Click on the copied notebook to open the Preview (Read Only). 

![open-preview-mode](https://drive.google.com/uc?id=1cbogu9crOJqkewrNio3Gmbfry7Wnfjra)



#### 6. Create Notebook Runtime using the widget in the upper right-hand corner - press the *Play* button. It should change to *""Creating""*. 

![start-runtime](https://drive.google.com/uc?id=1QygCjW9kBzM_kkZpEawM8W4v0dnC3ktu)



#### 7. Once the Notebook Runtime widget shows that it is running (you will see a *Pause* icon), press *Edit* or *Playground Mode* to start the notebook. 

![edit-notebook-in-workspace](https://drive.google.com/uc?id=1jpn8vqwZo0iSjfGq3smM-ZuWgqr26N1m)



#### 8. Once notebook is open and running, select Cell (from top task bar) --> Run All. 

![run-all-cells](https://drive.google.com/uc?id=1HLeWJtKjht9ufVHuZHi1h8RlugtdD1PT)



#### 9. Click on the Google bucket link at the bottom of the notebook to access log files for information on what was updated as well as contact information for permissions issues.

![access-logs-contact](https://drive.google.com/uc?id=1RkkuZoHGvCKcmzt_8CYJJTxLA5D1I5CD)",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","broad-firecloud-dsde/Update_Cancer_Data_Model_Paths"
125,"help-gatk","GATKTutorials-BigQuery-July2019","READER","https://app.terra.bio/#workspaces/help-gatk/GATKTutorials-BigQuery-July2019",TRUE,FALSE,NA,NA,NA,"## What's in this workspace?

Welcome to Day 4 of the Genome Analysis Toolkit (GATK) workshop at the University of Cambridge in Cambridge, U.K.!  This afternoon, we will walk through a hands on data analysis exercise in real time using BiGQuery and Jupyter notebooks.    

### Data
Metadata from the publicly available 1000 Genomes [project](http://www.internationalgenome.org/data).

![image](https://storage.googleapis.com/terra-featured-workspaces/BigQuery-Tutorial/1000GenomesDataPortal.png)


### Tools
There are no tools in this workspace. The tutorials are notebook-based, allowing you to run each step manually and view the intermediate outputs of the workflow. This is a great way to understand each step in the workflow and gives you the chance to manipulate the parameters to see what happens with the output.

If you are interested in a WDL based workflow of these analyses, check out the [Showcase](https://app.terra.bio/#library/showcase) area in Terra, which features many of our popular GATK workflows in workspaces ready to run your data.


### Notebooks

This practice workspace is the foundation for Big Query workshops, and the notebooks are ordered based on dependencies.

	1. Intro to Jupyter Notebooks
	2. R_Environment_Setup
	3. R_Retrieve Data from BigQuery
	4. R_Retrieve a Cohort with BigQuery 

Your instructor will guide you through this workspace, but you should find the documentation within the notebooks sufficient to run through them again on your own or to share with a friend later (and we encourage you to do so!).

| Runtime Environments | Value |
| --- | --- |
| CPU Minimum| 4|
| Disksize Minimum | 100 GB |
| Memory Minimum | 15 GB |
 
### Software versions
R: latest

R packages: bigrquery, dplyr, skimr, ggplot2, reticulate

### Time and cost 
This workspace focuses on the use of notebooks, thus the time and cost of completing the tutorial depends on the runtime environment of your notebook and the length of time you spend actively working in the notebook. With the given runtime environments above, users can expect the cost to be much less than a dollar an hour for each notebook. The specific cost will be displayed on the top right corner of your screen, next to the notebook machine options. 

## Appendix

### GATK @ Cambridge 2019 in the Terra Support Center

Following the conclusion of the workshop, the workshop materials will be made available in the Terra Support Center. For now, you can find these materials in a google drive folder at [broad.io/GATK1907](https://broad.io/GATK1907)

### GATK documentation and support

Detailed documentation, tutorials, and more are available at the [GATK web site](https://software.broadinstitute.org/gatk/). If you are still running into issues after consulting the User Guide, have a question, or just want to say hello, reach out to the GATK team via the [GATK Support Forum](https://gatkforums.broadinstitute.org/gatk)!

### Terra documentation and support

Documents and helpful guides to support your journey through Terra are available in the Terra Support Center. Navigate there by following the “Learn About Terra” link in the left-side menu. At the time of this workshop, we are still actively writing and publishing documents but we’ve got a good set of docs in the Quick Start Guide to get you going.
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/GATKTutorials-BigQuery-July2019"
126,"help-terra","NCI_T101_Tutorial","READER","https://app.terra.bio/#workspaces/help-terra/NCI_T101_Tutorial",TRUE,FALSE,NA,NA,NA,"# NCI T101 Tutorial Workspace
This workspace is a modified version of the [CRDC Dynamic Queries for NIH Genomic Data Commons Projects workspace](https://app.terra.bio/#workspaces/fc-product-demo/CRDC-Dynamic-Queries-for-NIH-Genomic-Data-Commons-Projects) for the November 2021 FireCloud powered by Terra 101 workshop.

## What's in the workspace?
This workspace shows you how to take a query result from the [NCI Genomic Data Commons](https://portal.gdc.cancer.gov/) (GDC) data portal and use it as the input to a workflow (or Notebook) in FireCloud.

*Parts of the different workflow configurations have been removed for teaching purposes. See the [original workspace](https://app.terra.bio/#workspaces/fc-product-demo/CRDC-Dynamic-Queries-for-NIH-Genomic-Data-Commons-Projects) for a complete demo of the instructions listed below.* 

## Getting started
To get started, make a ""clone"" this workspace so you have your own copy to work with.  Then follow the instructions below.

 | Tip- opening workspace links |
 | :-- |
 | Hold down the control button – or the command key on a Mac computer – to open any links below in a new tab. |


## Overview of the NCI Cancer Research Data Commons projects
The National Cancer Institute launched the [Cancer Research Data Commons ](https://datacommons.cancer.gov/) to provide researchers the means to accelerate discovery through the connection and harmonization of datasets with analytical tools in cloud native environments. From this, repositories of data known as data commons, were born.  The number of data commons and the type of data being collected continue to grow, with new data commons launching almost yearly. Among the data commons, and their associated datasets, are:

* [Genomics Data Commons](https://datacommons.cancer.gov/repository/genomic-data-commons) (GDC): Supports hosting, standardization, and analysis of genomic, clinical, and biospecimen data. The GDC harmonizes raw sequencing data, identifies and applies bioinformatics methods for generating mutation calls, structural variants and other high-level data. 
* [Proteomics Data Commons](https://datacommons.cancer.gov/repository/proteomic-data-commons) (PDC): Advances the understanding of the role that proteins play in the cancer lifecycle. In-depth analysis of proteomic data allows the study of both how and why cancer develops and informs ways of tailoring treatment for patients.
* [Imaging Data Commons](https://datacommons.cancer.gov/repository/imaging-data-commons) (IDC): Connects researchers with publicly available cancer imaging data, often linked with other types of cancer data. IDC provides the tools to search and visualize cancer imaging data, define cohorts and use those cohorts to better understand the disease. 

Other data commons within the CRDC, like the Integrated Canine Data Commons launched in 2020,  will have similar workspaces when there is sufficient data and infrastructure to support dynamically querying and pointing the data to a workspace. The workspace in particular will focus on accessing data in GDC.

## GDC data access

**Account Linking**  
To use datasets from the Genomics Data Commons [GDC](https://portal.gdc.cancer.gov/repository) in your FireCloud workspace you need to first link your GDC account with FireCloud.  Go to your [Profile page](https://app.terra.bio/#profile) and look for ""NCI CRDC Framework Services"" under ""IDENTITY & EXTERNAL SERVERS"".  Click the link to login using your [eRA Commons ID](https://public.era.nih.gov/commons/public/login.do) (which you should have already if you work for an academic organization and have applied for a grant).  
Linking your accounts is required even if you don't have access to controlled access data.  If you want to access controlled access data then you will need to apply to studies via dbGaP, see [instructions on the GDC website](https://gdc.cancer.gov/access-data/obtaining-access-controlled-data).


**Confirm GDC data access**  
The [DRS standards ](https://www.ga4gh.org/news/drs-api-enabling-cloud-based-data-access-and-retrieval/) will be utilized to access GDC data by both notebooks and workflows in this workspace.  Once your GDC and FireCloud accounts have been linked, we can use a simple [md5sum workflow](https://www.dockstore.org/containers/quay.io/briandoconnor/dockstore-tool-md5sum:1.0.4?tab=info) to test access by running an open access sample through the workflow. The workflow outputs a simple md5 sum of the chosen file if data access is setup properly and it fails if not. 

## Query and point a FireCloud workspace to GDC data 

These steps will guide you through querying data from the GDC portal, downloading the query manifest, and translating the manifest to workspace data tables that can be used to run a workflow.  In this example, we will use CPTAC data.

#### Step 1 - Download Manifest from GDC Portal
**1A**.  Navigate to the  [GDC](https://portal.gdc.cancer.gov/repository)

**1B**.  Search for Access = open (Files tab) and Program = CPTAC.  **NOTE:** you can really choose anything here.  If you want to follow along with this tutorial use open access CPTAC data.  But if you want to work with different data feel free to search for whatever is appropriate given your research interest.

**1C**.  Click the download ""Manifest"" button.  A TSV that looks like the following will appear:

```
id    filename    md5    size    state
1d50ef40-b726-48f7-b81d-ae0e4dab714b    d9124538-347c-483e-aee9-83c462e87976.FPKM-UQ.txt.gz    04d6c247dada7cf0dad93a839f6b7437    438083    released
4b1c6ee1-b46a-4b9d-bb74-d303719c729f    dc383158-c7c8-4fa5-a0fc-f9b2f9d619e3.FPKM-UQ.txt.gz    b9dbd7f7b417cdc8b978f708508dd7cb    442855    released
f4f165ef-15d1-4cf4-909b-0c9b80b295c4    5a5e2ef7-89c2-455d-8d11-84b86fed0b7b.FPKM-UQ.txt.gz    0307ce18caefee845629b53bb37181d0    445557    released
7b30dc0f-017d-42de-8ec7-d9748add2c9c    077d2d30-e631-4f36-9832-04a7f1f451b6.FPKM-UQ.txt.gz    8e4fc7054339d05062c06d797bdd376d    446449    released
```

#### Step 2 - Transform Manifest to FireCloud Table Format  

FireCloud doesn't recognize this manifest format-- it will be transformed in this step into a format that is usable in FireCloud. 

Along the way we'll create DRS URIs that FireCloud can use with your eRA Commons ID to access (both open and controlled access data).  This avoids you having to manually download files from GDC and reuploading them to FireCloud.  You can just point to the DRS URI as a file input for a workflow, and FireCloud will take care of accessing the file from GDC.

In this step, we are essentially making the above manifest file from step 1 look like the following:

```
entity:drs_id	drs_uri	filename
1d50ef40-b726-48f7-b81d-ae0e4dab714b	drs://dg.4DFC:1d50ef40-b726-48f7-b81d-ae0e4dab714b	d9124538-347c-483e-aee9-83c462e87976.FPKM-UQ.txt.gz
...
```

Notice, 
- We're adding ""entity:drs_id"" at the top of the file in the header and adding a new ""drs_uri"" column.  You make this URI using:

  `  drs://dg.4DFC:<ID from first column of manifest>`
		
- The `md5` , `size`, `state` columns were removed here for simplicity, but they can be left in the transformed table. 
		

Instead of doing this manually, use the notebook in this workspace to do this for you:

**2A**. Navigate to the Data tab of your workspace.

**2B**. Click on the `Files` tab on the left hand column

**2C**. Click on the `+` icon on the bottom right and upload the manifest file you downloaded from GDC to your workspace.

**2D**. Navigate to the Notebook tab of your workspace and run the  ""Upload GDC Manifest to Workspace Data table"" notebook. 

**2E**. Follow the along with the instructions in the notebook, **making sure to run each cell in order, from top to bottom**. 


#### Step 3 - Run a Test Workflow

Now that you have a data table ""drs"" created and loaded with a few GDC DRS URIs (and other info) you can run a workflow to test that everything is working properly.  Normally, you can just go to [Dockstore.org](https://dockstore.org) and pick the ""md5sum"" WDL workflow.  This step has already been done for you as well as binding the input of ""this.drs_uri"" in the ""drs"" table to the input file of the md5sum workflow.

**NOTE:** if you are just following this tutorial to test your data access and to learn how to take a search result from GDC and work with it in FireCloud then you ***do not*** need to run the md5sum workflow on all CPTAC data!!  Before you launch the workflow choose ""SELECT DATA"", then choose ""Choose specific rows to process"", and select the first row.  That will just run md5sum on a single file and should be sufficient to test your data access.

If you run this workflow on your drs table and it finishes successfully, congratulations, you have accessed the GDC data you found in the GDC portal through the DRS standard in Terra.  This may seem like a hassle to setup but, once you do, you're saving countless hours (and $$$) by not temporarily copying data from GDC to Terra.  This approach also opens up all datasets on the GDC data portal, not just open access CPTAC data that was used in this tutorial.  You can use this approach for any open access data as well as any datasets where you have dbGaP approved access.  At the time of writing this tutorial, GDC has ~1.6PB of data available!  For details, see the table below:

| Header  | Value |
| ----------- | -------- |
| Dataset Size      | 1.57 PB |
| Programs          | 16 |
| Projects             | 67 | 
| Primary Sites    | 68 | 
| Cases      | 84,392       |
| Files   | 596,758        |

**Table 1:** GDC statistics as of March 2021.


#### Step 4 - Run Your Own Analysis

If the workflow from step 3 runs successfully on one of the data files from the drs table then you should be ready to use this technique with whatever data you're interested in from the GDC portal and with your own analysis workflows.  You can choose to use this workspace and just add additional data by searching in the GDC portal, downloading the manifest, and running the notebook again to load it.  Alternatively, if you want a clean copy to start your analysis work in, you can delete this workspace you used for the tutorial and clone the original workspace again, this time uploading a manifest from GDC that corresponds to the search you are interested in.


## Notebooks
More information about the notebook:

**Upload GDC Manifest to Workspace Data table:**  
What does it do?  
Given a GDC manifest file convert file to Terra readable TSV that can be uploaded to the Data table.

| Runteime | Value |
|--|--| 
| Environments | Default (GATK 4.1.4.1 Python3.7.8) |
| CPU Minimum | 2 |
| Disksize Minimum | 10 GB |
| Memory Minmum | 15 GB |


## Workflows 
More information about the workflow:

**dockstore-wdl-workflow-md5sum:**  
What does it do?  
This is an extremely simple workflow used to show how to call a workflow via Dockstore.  

What does it require as input?  
inputFile- File which will have its md5sum retrieved  

What does it return as output?  
Value - md5sum value


| file Name | Time | Cost $|
|--|--|--|
| htseq_counts.txt.gz | 4m | <0.01 |



### License
**Copyright Broad Institute, 2021 | BSD-3**
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at https://github.com/openwdl/wdl/blob/master/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-terra/NCI_T101_Tutorial"
127,"theiagen-demos","TheiaCoV_MPXV","READER","https://app.terra.bio/#workspaces/theiagen-demos/TheiaCoV_MPXV",TRUE,FALSE,NA,NA,NA,"<p align=""center"">
	<a href=""https://www.theiagen.com"">
  <img src=""https://storage.googleapis.com/theiagen-public-files/logos/2022_logos/theiagen-logo-standard_terra_background.png"" width=""400"" background=""transparent"" class=""center"" alt=""Theiagen Genomics"">
	</a>
</p>

## Terra Workspace for MPXV Analysis

#### This is a featured workspace to highlight the functionality of the TheiaCoV workflows for genomic characterization of MonkeyPox (MPXV) sequence data.

Monkeypox sequencing data generated using tiled-amplicon, [e.g. this protocol from Yale](https://www.protocols.io/view/monkeypox-virus-multiplexed-pcr-amplicon-sequencin-5qpvob1nbl4o/v2), or metagenomics approaches can be analyzed using the TheiaCoV workflows. Using TheiaCoV for MPXV requires adjusting the default parameters to enable MPXV-specific analysis. We have created JSON files that define our recommended changes for easy upload to workflow configuration. 

**Please see our [Notion page](https://theiagen.notion.site/Workspace-Reference-Materials-for-MPXV-Genomic-Characterization-a34f355c68c54c0a82e926d4de607bca) for these input JSON files, along with our most up-to-date recommendations, as resources rapidly change.**

---

### TheiaCoV versions ≥ 2.2.0 support MPXV analysis.

The following workflows are compatible with MPXV **tiled-amplicon** sequence data:

* TheiaCov_Illumina_PE
* TheiaCov_Illumina_SE
* TheiaCov_ONT

The following workflows are compatible with MPXV **metagenomic** sequence data (i.e. no multiplex-PCR amplication used. From swab to DNA extraction, to library prep for sequencing)

* TheiaCov_Illumina_PE
* TheiaCov_Illumina_SE

*Note: Support to analyze MPXV metagenomics data sequenced on ONT platforms may be added in the future.*

TheiaCov_FASTA supports MPXV if you have **genome assemblies** generated through another bioinformatics workflow or obtained from another database (GenBank/GISAID/etc.)

----

## How to use TheiaCoV

### What does the TheiaCoV workflow do?

<p align=""center"">
  <img src=""https://storage.googleapis.com/theiagen-public-files/terra/titan-files/TheiaCoV-general-2022-08-23.png"" width=""1000"" class=""center"" alt=""TheiaCoV workflow"">
</p>

**The TheiaCoV workflow series will perform:**
1. Read QC and filtering: removal of host DNA, adapters, and poor quality reads
2. Read alignment and consensus genome assembly: alignment by [BWA](https://github.com/lh3/bwa) and consensus assembly by [iVar](https://github.com/andersen-lab/ivar)
3. Species-specific analyses: 
     * SARS-CoV-2 genomic characterization (lineage and clade designation; variant and genome feature assessment), or
     * MPXV genomic characterization (lineage and clade designation; variant assessment)

In the future, the TheiaCoV workflow will be expanded to include more species-specific analyses. Any species-specific module can be indicated through the usage of the `organism` tag, explained in more detail below. 

In this workspace, we demonstrate how TheiaCoV can be used for MPXV.


### What does the TheiaCoV workflow require as input?

The TheiaCoV workflows take read data generated via tiled-amplicon or metagenomics methods  with Illumina paired or single-end sequencing approaches. Data generated by a tiled- amplicon approach and sequenced with an Oxford Nanopore platform can also be analyzed. 

**There are several input attributes that need to be adjusted from their default in order to enable MPXV genomic characterization.** We strongly recommend using an input JSON file so that these values are filled in correctly. We update these JSONs on a regular basis to ensure that the inputs reflect the most up-to-date recommendations. **Please find these JSON files on our [Notion page](https://theiagen.notion.site/Workspace-Reference-Materials-for-MPXV-Genomic-Characterization-a34f355c68c54c0a82e926d4de607bca).** You will find all of the recommended input parameters in a table. Under the ""Terra.Bio Resources for PHVG v2.2.0 for MPXV analysis"" there is a toggle item that says ""Terra.Bio Input JSONs"" where you can download the JSON file that is most appropriate for the workflow you are running.

*Note*: In these JSON files, we assume you are using the Yale tiled-amplicon approach, and have set the default reference FASTA file and primer bed file to correspond to the ones used in the Yale analysis. If you are using a different tiled-amplicon approach, please adjust these variables accordingly. 

**For MPXV tiled-amplicon sequence data** to be assembled, TheiaCoV requires the following input attributes:
* raw read data in FASTQ format (if paired-end data, use TheiaCoV_Illumina_PE; if single-end data, use TheiaCoV_Illumina_SE; if ONT data, use TheiaCoV_ONT)
* a primer bed file; this file contains the amplicon primer coordinates.
* a reference FASTA file; this file contains the relevant MPXV reference genome

**For MPXV metagenomic sequence data** to be assembled, TheiaCoV requires the following input attributes:
* raw read data in FASTQ format (if paired-end, use TheiaCoV_Illumina_PE; if single-end, use TheiaCoV_Illumina_SE)
* a reference FASTA file; this file contains the relevant MPXV reference genome.

*Note*: You may use the same JSON files as linked above but be sure to set the `trim_primers` variable to `false` to avoid primer trimming. 

**For MPXV genome assemblies,** TheiaCoV_Fasta can be used. It requires the following inputs:
* assembled MPXV genomes in FASTA format
* a reference FASTA file; this file contains the relevant MPXV reference genome

For additional input attributes that are required to enable genomic characterization, please see the Notion page linked above and find the appropriate JSON file for TheiaCoV_Fasta

*Note*: Not all output columns will be filled for TheiaCoV_Fasta as many are generated through assembly-generation measures.


### What does the TheiaCoV workflow return as output?

TheiaCoV produces a number of useful outputs. The ones you may be most interested in are as follows:
* `assembly_fasta` - the consensus assembly
* `assembly_mean_coverage` - the average coverage (depth) of your assembly
* `kraken_human` - the percentage of human data found in your sample *before* dehosting
* `kraken_human_dehosted` - the percentage of human data found in your sample *after* dehosting
* `kraken_target_org` - the percentage of target data (MPXV) found in your sample *before* dehosting
* `kraken_target_org_dehosted` - the percentage of target data (MPXV) found in your sample *after* dehosting
* `nextclade_aa_dels` and `nextclade_aa_subs` - a list of amino acid deletions/substitutions found in your sample
* `nextclade_clade` - the Nextclade clade of your sample
* `nextclade_lineage` - the Nextclade lineage of your sample
* `number_N` - the number of Ns in your assembly
* `number_Total` - the total length of your assembly
* `percent_reference_coverage` - the percent of the reference file that is covered by your assembly

Please note that some output columns may be blank. This is likely because these columns are SARS-CoV-2 specific so the analysis required to fill these columns is not performed.

There are many more columns included in the table. Please feel free to reach out if you have any questions about their content. You may also peruse our [TheiaCoV documentation](https://public-health-viral-genomics-theiagen.readthedocs.io/en/latest/theiacov_characterization_workflows.html). Please keep in mind that we are actively updating the documentation to reflect the changes we made to enable MPXV genomic characterization, and as such, these changes may not be present in the documentation at this time.

----

## About the data

The data presented in this workspace was generated by our collaborators at the Los Angeles County Public Health Laboratory. We have separated this data into two groups:
* dataset001 - this dataset has been analyzed using the TheiaCoV workflow as described above and the outputs populated to the data table accordingly
* dataset002 - this dataset includes reads that have not been analyzed that can be utilized for your testing

The data in dataset001 were last analyzed on September 1, 2022. Nextclade dataset tags are update frequently and, as such, the results presented in the data table may be outdated. Please refer to the Notion page containing the JSON input files for the most up-to-date resource recommendations. 

----

## Useful links

* [Notion page containing JSON input files and the most up-to-date resource recommendations](https://theiagen.notion.site/Workspace-Reference-Materials-for-MPXV-Genomic-Characterization-a34f355c68c54c0a82e926d4de607bca)
* [TheiaCoV documentation](https://public-health-viral-genomics-theiagen.readthedocs.io/en/latest/theiacov_characterization_workflows.html)
* [Yale tiled-amplicon sequencing protocol](https://www.protocols.io/view/monkeypox-virus-multiplexed-pcr-amplicon-sequencin-5qpvob1nbl4o/v2)
* [YouTube video explaining how to use the workflow](https://www.youtube.com/watch?v=whnFFoMY32s)



We are actively seeking feedback on the workflow!  Please contact us with any feedback on what can be improved, changed, or added, or for any support requests by emailing support@terrapublichealth.zendesk.com or support@theiagen.com.
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","theiagen-demos/TheiaCoV_MPXV"
128,"fc-product-demo","Accessing advanced GCP features","READER","https://app.terra.bio/#workspaces/fc-product-demo/Accessing%20advanced%20GCP%20features",TRUE,FALSE,NA,NA,NA,"THIS WORKSPACE HAS BEEN DEPRECATED. See [https://app.terra.bio/#workspaces/help-terra/Accessing%20advanced%20GCP%20features](https://app.terra.bio/#workspaces/help-terra/Accessing%20advanced%20GCP%20features).       

These notebooks walk through how to access additional GCP features in Terra by leveraging a proxy group and Google Cloud Platform.    


- WRITE to BigQuery     
- Interact with Cloud Storage buckets other than the workspace bucket    
- Run dsub jobs    
- Run Cloud Dataflow jobs    
- Run Cloud ML engine jobs            
 
To learn more about using proxy groups to perform Google Cloud Platform operations not currently available in the Terra UI, see [this article](https://support.terra.bio/hc/en-us/articles/360051229072).            
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","fc-product-demo/Accessing advanced GCP features"
130,"biodata-catalyst","BioData Catalyst Collection","READER","https://app.terra.bio/#workspaces/biodata-catalyst/BioData%20Catalyst%20Collection",TRUE,FALSE,NA,NA,NA,"# BioData Catalyst Collection

This workspace is a collection of notebooks that are helpful for BioData Catalyst users in Terra. The notebooks in this collection can be copied to a different workspace to aid users in their individual research needs. 

To copy a notebook to another workspace:
1. Navigate to the ""Notebooks"" section of this workspace. 
2. Click on the menu icon (circle with three dots) in the upper right.
3. Select ""copy to another workspace"".

----

## Data

These notebooks were designed to interact with data imported from the BioData Catalyst Powered by Gen3.  This workspace contains tutorial data from Gen3 that you can use to try out the notebooks. To be able to interact with the genomic data provided through DRS URIs, you will need to [link your external services](https://support.terra.bio/hc/en-us/articles/360038086332-Linking-controlled-access-authorization-external-servers-).


You can learn how to search and export data from Gen3 and interact with this data model it in the Terra data tables using these resources:

- [Understanding and using Gen3 data in Terra documentation](https://support.terra.bio/hc/en-us/signin?return_to=https%3A%2F%2Fsupport.terra.bio%2Fhc%2Fen-us%2Farticles%2F360038087312) 



## Notebooks in this collection

### 1. Utilities Notebooks
These notebooks contain functions that are helpful for interacting with Gen3 data in Terra workspaces. 

**[terra\_data\_table_util](https://app.terra.bio/#workspaces/biodata-catalyst/BioData%20Catalyst%20Collection/notebooks/launch/terra_data_table_util.ipynb)** (formerly named **terra\_data\_util**)

- This notebook combines multiple Gen3 graph-structured data tables to create a single consolidated table that is easier to use.
- The default behavior is to produce a table keyed by subject id, with one row per subject, for all subjects in a Terra Workspace. This table may include the genomic data, harmonized clinical metadata, or both, along with the associated administrative information. The content of the consolidated table produced is configurable.
- If you are new to this data model, navigate to [BioData Catalyst Gen3 data on Terra](https://terra.biodatacatalyst.nhlbi.nih.gov/#workspaces/fc-product-demo/BioDataCatalyst-Gen3-data-on-Terra-Tutorial) to learn how to use this set of utilities. 
- Suggested Cloud Environment:
	- Application Configuration: Defaut
	- Cloud compute profile: Default


**[Intro to FISS API in Python](https://app.terra.bio/#workspaces/biodata-catalyst/BioData%20Catalyst%20Collection/notebooks/launch/Intro%20to%20FISS%20API%20in%20Python.ipynb)**

- This notebook introduces users to the Firecloud API using a Python Jupyter notebook. 
- The example covers how the API communicates between the data table and notebook. The user loads an existing Terra data table into the notebook, subsets the dataframe, and saves the new dataframe as a tsv to the workspace bucket or as a new Terra data table.
- Note: a more scalable version of this process is available in the [terra\_data\_table_util](https://app.terra.bio/#workspaces/biodata-catalyst/BioData%20Catalyst%20Collection/notebooks/launch/terra_data_table_util.ipynb) notebook. 
- Suggested Notebook Runtime: 
	- Environment: Default
	- Compute Power: Default

**[Intro to FISS API in R](https://app.terra.bio/#workspaces/biodata-catalyst/BioData%20Catalyst%20Collection/notebooks/launch/Intro%20to%20FISS%20API%20in%20R.ipynb)**

- This notebook introduces users to the Firecloud API using an R Jupyter notebook. The Reticulate R package is used to call python-based FISS commands in the R language. 
- The example covers how the API communicates between the data table and notebook. The user loads an existing Terra data table into the notebook, subsets the dataframe, and saves the new dataframe as a tsv to the workspace bucket or as a new Terra data table.
- Suggested Cloud Environment:
	- Application Configuration: Defaut
	- Cloud compute profile: Default


**[Accessing GA4GH DRS URI data using TNU CLI](https://app.terra.bio/#workspaces/biodata-catalyst/BioData%20Catalyst%20Collection/notebooks/launch/Accessing%20GA4GH%20DRS%20URI%20data%20using%20TNU%20CLI.ipynb)**
	
- This Notebook provides a simple, brief introduction to accessing data identified by GA4GH Data Repository Service (DRS) URIs using the terra-notebook-utils command-line interface (CLI). This Notebook complements the Terra documentation article: Data Access with the GA4GH Data Repository Service (DRS), which is recommended reading.
- - Suggested Notebook Runtime: 
	- Environment: Default
	- Compute Power: Default

**[Accessing GA4GH DRS URI data using TNU in Python](https://app.terra.bio/#workspaces/biodata-catalyst/BioData%20Catalyst%20Collection/notebooks/launch/Accessing%20GA4GH%20DRS%20URI%20data%20using%20TNU%20in%20Python.ipynb)**
	
- This Notebook provides a simple, brief introduction to accessing data identified by GA4GH Data Repository Service (DRS) URIs using the terra-notebook-utils Python API. This Notebook complements the Terra documentation article: Data Access with the GA4GH Data Repository Service (DRS), which is recommended reading.
- Suggested Cloud Environment:
	- Application Configuration: Defaut
	- Cloud compute profile: Default

**[Accessing GA4GH DRS URI data using TNU in R](https://app.terra.bio/#workspaces/biodata-catalyst/BioData%20Catalyst%20Collection/notebooks/launch/Accessing%20GA4GH%20DRS%20URI%20data%20using%20TNU%20in%20R.ipynb)**
	
- This Notebook provides a simple, brief introduction to accessing data identified by GA4GH Data Repository Service (DRS) URIs in the R language using the terra-notebook-utils Python API. This Notebook complements the Terra documentation article: Data Access with the GA4GH Data Repository Service (DRS), which is recommended reading.
- Suggested Cloud Environment:
	- Application Configuration: Defaut
	- Cloud compute profile: Default


### 2. VCF Tools
**[head-vcf-gz](https://app.terra.bio/#workspaces/biodata-catalyst/BioData%20Catalyst%20Collection/notebooks/launch/head-vcf-gz.ipynb)**

- This notebook allows you to easily view the header information of a gz compressed VCF file in a Terra workspace bucket or Google Cloud bucket. 
- Suggested Notebook Runtime: 
	- Environment: New Default (released January 14: GATK4.1.4.1)
	- Compute Power: Default

**[Merge and subsample VCFs](https://app.terra.bio/#workspaces/biodata-catalyst/BioData%20Catalyst%20Collection/notebooks/launch/VCF%20Merge%20and%20Subsample%20Tutorial.ipynb)**

- The TopMED data indexed by BioData Catalyst has been jointly called per each Freeze. For TOPMed researchers that are not part of the TOPMed Constorium, the jointly called VCF is received as multiple VCFs that were subsetted by project and consent code. If users have access to multiple projects and consent codes, they may want to re-combine these VCFs. The tooling called in this notebook allows for multiple VCFs (that were originally jointly called) to be recombined into a single VCF.
- This notebook will produce a Terra data table that can be used as input to VCF merge workflows. There will be one workflow per chromosome, and they can be executed in parallel using the workflow tab in Terra, using the [xvcfmerge](https://dockstore.org/workflows/github.com/DataBiosphere/xvcfmerge:master?tab=info) workflow on Dockstore.
- After merging, users can use the subsample workflow to select the samples they want in the final VCF. 
- Suggested Cloud Environment:
	- Application Configuration: Defaut
	- Cloud compute profile: Default


### 3. Analysis Notebooks

**[Sample and variant QC methods in Hail](https://app.terra.bio/#workspaces/biodata-catalyst/BioData%20Catalyst%20Collection/notebooks/launch/Sample%20and%20variant%20Quality%20Control%20using%20Hail.ipynb)**
- This notebook demonstrates how, generally, a researcher can perform sample and variant QC using the Hail software.
First, VCF files are loaded to a Hail matrix table. Built-in sample and variant QC functions are then called and the results explored. Various plots showing the distribution of these matrices are created. Variant filtering is performed and the final QC'd dataset is written to a VCF file in the workspace bucket.
- Suggested Cloud Environment:
	- Application Configuration: Hail
	- Cloud compute profile: Customize a spark cluster for your needs

## Contact Us
We are a group of developers that are working to improve research in the BioData Catalyst ecosystem. We are happy to work with users to enhance and build resources. 

* [Michael Baumann](mailto:mbaumann@broadinstitute.org) (Broad Institute, Data Sciences Platform)
* [Brian Hannafious](mailto:bhannafi@ucsc.edu) (UC Santa Cruz Genomics Institute)
* [Beth Sheets](mailto:esheets@ucsc.edu) (UC Santa Cruz Genomics Institute)

## Workspace change log
| Date | Change | Author | 
| -------  | -------- | -------- |
| April 11, 2022 | archived BYOD, table transformer, IGV, workflow cost estimator to [attic](https://github.com/DataBiosphere/featured-notebooks/tree/master/attic) | Beth |
| Mar 30, 2021 | Added sample and variant QC with Hail | Beth |
| Mar 5, 2021 | Added DRS TNU Notebook in R | Michael |
| Jan 14, 2021 | Added test data | Beth |
| Nov 13, 2020 | New notebooks and notebook maintenance | Beth |
| July 2, 2020 | Added IGViewer notebook | Beth |
| June 2, 2020 | Updated dashboard with VCF and FISS tools | Beth |
| April 2, 2020 | Dashboard updates with analysis notebooks | Beth |
| March 24, 2020 | Collection created | Beth |",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","biodata-catalyst/BioData Catalyst Collection"
131,"warp-pipelines","Ultima-Genomics-Whole-Genome-Germline","READER","https://app.terra.bio/#workspaces/warp-pipelines/Ultima-Genomics-Whole-Genome-Germline",TRUE,FALSE,NA,NA,NA,"### Ultima Genomics whole genome germline pipeline for variant discovery

This workspace contains a fully reproducible example workflow for pre-processing germline whole-genome sequence data derived from the Ultima Genomics Platform. 


![](https://storage.googleapis.com/terra-featured-workspaces/UG_Whole_Genome_Germline/ug_diagram.png)

Scroll down for an overview of the workflow, instructions, example data, cost estimates, and additional resources. 

*The materials in this workspace were developed by the Data Sciences Platform at the Broad Institute.* 

## Ultima Genomics Whole Genome Germline workflow overview and instructions

### Workflow overview

The Ultima Genomics Whole Genome Germline (UG_WGS) workflow is an open-source, cloud-optimized workflow for processing whole-genome germline sequencing data generated using  Ultima Genomics sequencing, a novel technology that produces single-read, flow-based data ([Almogy et al., 2022](https://www.biorxiv.org/content/10.1101/2022.05.29.493900v1)).

The workflow requires either an aligned CRAM output from the sequencing platform **or** an unmapped BAM as input. 

Overall, the workflow aligns reads to a reference genome, marks duplicate reads, calls variants, post-processes variants in the output VCF in preparation for joint calling, and calculates quality control metrics. The workflow outputs a (re)aligned CRAM, a filtered VCF and index, an annotated GVCF with index, and quality metrics.


**What does it do?**     

- For the latest version of the UG_WGS workflow, visit the WARP repository [release page](https://github.com/broadinstitute/warp/releases). 

- Read more about the workflow modes, inputs, and tasks in the [UG_WGS Overview](https://broadinstitute.github.io/warp/docs/Pipelines/Ultima_Genomics_Whole_Genome_Germline_Pipeline/README). 

**What data does it require as input?**    
- Human whole-genome sequencing data in aligned CRAM or unmapped BAM (uBAM) format
- One read group, one per CRAM or uBAM file
- Input uBAM files must additionally comply with the following requirements:
  - Filenames all have the same suffix (we use "".unmapped.bam"")
  - Files must pass validation by ValidateSamFile
  - Reads are provided in query-sorted order
  - All reads must have an RG tag
-  Reblocked GVCF output names must end in ""rb.vcf.gz""
- Reference genome must be Hg38 with ALT contigs


**What does it return as output?**     
The following files are stored in the workspace Google bucket and links to the files are written to the `sample` data table:    
- CRAM file, CRAM index, and CRAM md5 
- Revlocked GVCF and its GVCF index 
- Filtered VCF (for those who want to evaluate the data
- Several summary metrics       

The workflow has several additional outputs that might be useful for assessing data quality or for using the data in downstream applications. All outputs are detailed in the [UG_WGS Overview](https://broadinstitute.github.io/warp/docs/Pipelines/Ultima_Genomics_Whole_Genome_Germline_Pipeline/README).


## Running the workflow    

This workspace provides **an example workflow configuration** to test the UG_WGS pipeline that uses the `sample` data table. 

#### Example data

This workspace has two example data tables  (`sample` and `Requester_pays_CRAM`) with CRAM files. The `sample` data table is the default table for this workspace. It includes an example downsampled CRAM containing genomic sequencing reads for the NA12878 sample. This example data is hosted in a public Google bucket. 

The `Requester_pays_CRAM` table contains example files from Genomes-in-a-bottle consortium. **Before trying a requester pays CRAM, read the section ""Running the Genome-in-a-bottle samples (requester pays)"".

To run the default workflow configuration with the `sample` data table, use the steps below.

#### Steps
1. Go to the Workflows tab.
2. Select the UG_WGS workflow option. 
3. In **Step 1** on the workflow setup page, select the `sample` table as the root entity.
4.  In **Step 2** on the workflow setup page, select the `downsampled_NA12878` dataset. 
5.  Select `Use reference disks`.
6.  Select `Run Analysis`.
7.  Select `Launch`.


#### Reference data description and location     

The required and optional references and resources for the workflows are set in the workflow configurations. The reference genome is hg38 (aka GRCh38). All reference files are available in a public Google bucket and the cloud paths for all reference files are listed in the `Reference data (Hg38)` table or the `Workspace Data` table.

## Running the Genome-in-a-bottle samples (requester pays)

This workspace provides CRAMs for the  [standard reference Genome-in-a-Bottle (GIAB) samples HG001-HG007](https://www.ultimagenomics.com/blog/reference-dataset-genome-in-a-bottle-giab/). These files are hosted in a public requester pays bucket, which means additional fees are incurred to move the data and run the workflow. The samples are also larger (see the cost estimates table below).

These CRAM files are listed in the `Requester_pays_CRAM` data table. The workflow is not set up to run these samples by default. To run any of the samples, you'll need to update the following on the workflow setup page:

1. In **Step 1** of the workflow setup page, choose the `Requester_pays_CRAM` data table as the root entity.
2. In **Step 2** of workflow setup, select the CRAM file you want to run.
3. In the workflow inputs, change the `base_file_name` attribute to ""this.Requester_pays_CRAM_id"".
4. In the workflow inputs, change the `input_cram_list` attribute to ""this.CRAM_cloud_path"".
5. Save the workflow setup and Run.

**Time and cost estimates**         
Below is an example of the time and cost of running the workflow.

| Workflow Configuration | Sample Name | Sample Size | Time | Cost $ |
| ---  | --- | --- | --- | --- |
| Ultima_Genomics | downsampled_NA12878 | ~3.00 GB | 4 hr 10 min | 0.93 |
| Ultima_Genomics | 004731-UGAv3-30-CTGCCAGACTGTGA | 55.62 GB  | 26 hrs | 14.82 |


**Note:** Cost and time will vary with the use of [Preemptibles](https://cloud.google.com/compute/docs/instances/preemptible).  

For more information about controlling Cloud costs, see [Overview: Controlling Cloud costs on Terra](https://support.terra.bio/hc/en-us/articles/360029748111).


## Additional resources

- Learn more about the sequencing technology in this [Terra blog](https://terra.bio/updated-gatk-pipeline-for-ultima-genomics-data). 
 
- For the latest updates on the UG_WGS pipeline, see the workflow [changelog](https://github.com/broadinstitute/warp/blob/develop/pipelines/broad/dna_seq/germline/single_sample/ugwgs/UltimaGenomicsWholeGenomeGermline.changelog.md). 

- Learn more about the UG_WGS pipeline in the [UG_WGS Overview](https://broadinstitute.github.io/warp/docs/Pipelines/Ultima_Genomics_Whole_Genome_Germline_Pipeline/README).

  
## Questions and contact information  
* For workspace questions and feedback, email the Broad pipelines team at warp-pipelines-help@broadinstitute.org or create a post on the [Featured Workspaces community forum](https://support.terra.bio/hc/en-us/community/topics/360001603491) (login required). Tag @Alexander Baumann in the **Details** section of your post.

* You can also Contact the Terra team for questions from the Terra main menu. When submitting a request, it can be helpful to include:
    * Your Project ID
    * Your workspace name
    * Your Bucket ID, Submission ID, and Workflow ID
    * Any relevant log information
 
Please post any questions or concerns regarding the GATK tool to the [GATK forum](https://gatk.broadinstitute.org/hc/en-us/community/topics).


## License  
**Copyright Broad Institute, 2023 | BSD-3**  
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at https://github.com/broadinstitute/warp/blob/develop/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.

## Workspace changelog
| Date | Change | Author |
| --- | --- | --- |
| 8/16/2023 | Updated workflow to v1.0.8. | Kaylee Mathews |
| 9/12/2022 | Updated to workflow v1.0.1. | Liz Kiernan |
| 8/29/2022 | Updated contact information. | Kaylee Mathews |
| 6/24/2022 | Added `Requester_pays_CRAM table and example CRAMs. | - |
| 5/16/2022 | First release of the workspace. | Liz Kiernan |

---",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","warp-pipelines/Ultima-Genomics-Whole-Genome-Germline"
132,"uk-biobank-sek","ml4h-toolkit-for-machine-learning-on-clinical-data","READER","https://app.terra.bio/#workspaces/uk-biobank-sek/ml4h-toolkit-for-machine-learning-on-clinical-data",TRUE,TRUE,NA,NA,NA,"# Use ml4h to review and annotate clinical data and machine learning results

In this Terra workspace we demonstrate the notebooks used by clinicians and researchers to review and annotate both clinical data model inputs, such as phenotypes, ECGs and MRIs, and model outputs such as predicted left ventricular mass.

`ml4h` is a project aimed at using machine learning to model multi-modal cardiovascular
time series and imaging data. `ml4h` began as a set of tools to make it easy to work
with the UK Biobank on Google Cloud Platform and has since expanded to include other data sources
and functionality.

Please see https://github.com/broadinstitute/ml4h/ for more details on the full project.

----------------------------
## How long will it take to run? How much will it cost?
**Time:** It takes 1 minute to run each notebook. Spend as much or as little time as you like using the interactive visualizations to explore the data.

**Cost:** Using the default notebook configuration, the Terra notebook runtime charges are $0.20/hour for Google Cloud service costs. It should cost less than a quarter to run the notebooks.

----------------------------
## Get Started

1. Clone this workspace.
1. Run notebook `ml4h_setup.ipynb` to install the ml4h Python package and data visualization Jupyter extensions on your cloud environment.
1. Run the notebooks in ""Playground Mode"" to explore model inputs and outputs!

----------------------------
## Notebooks

* **review_model_results_interactive**: Use this notebook to perform interactive quality control (QC) of a simulated ECG and MRI prediction model. NOTE: the [facets](https://pair-code.github.io/facets/) visualization does not currently display correctly on app.terra.bio, but works fine in other Jupyter environments.
* **review_one_sample_interactive**: Use this notebook to perform interactive quality control (QC) of per-patient multi-modal data for clinical machine learning models.
* **image_annotations_demo**: Use this notebook to annotate MRI images to create new input data for machine learning.
* **mnist_survival_analysis_demo**: In survival analysis, the aim is to predict when an event might occur, such as a heart attack, stroke, or the onset of heart feailure. This notebook uses the MNIST dataset to develop a toy model of survival analysis using ML4H.

### Cloud Environment

When you create your cloud environment, you can specify that it be [GPU-enabled](https://support.terra.bio/hc/en-us/articles/4403006001947).  While all of the notebooks above will run fine on CPUs, the `mnist_survival_analysis_demo.ipynb` notebook in particular, which trains several ML models, will benefit from using a GPU.

| Option | Value |
| --- | --- |
| Environment | Default 'application environment' (GATK, Python, R) |
| CPU Minimum | 4|
| Disk size Minimum | 50 GB |
| Memory Minimum | 15 GB |
| GPU | optional, but beneficial for model training|

----------------------------
## Next steps

* Try these notebooks on your own data.
* Read more about the ml4h project on  https://github.com/broadinstitute/ml4h/
* Ask questions https://github.com/broadinstitute/ml4h/issues
* Apply those resources to your own research!

---

### Contact information

You can also reach us on GitHub by [filing an issue](https://github.com/broadinstitute/ml4h/issues).

### License
Please see the BSD-3-Clause [license on GitHub](https://github.com/broadinstitute/ml4h/blob/master/LICENSE.TXT)

### Workspace Change Log
Please see the [pull request history](https://github.com/broadinstitute/ml4h/pulls?q=is%3Apr+) on GitHub.",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","uk-biobank-sek/ml4h-toolkit-for-machine-learning-on-clinical-data"
133,"broad-firecloud-tcga","TCGA_ESCA_hg38_OpenAccess_GDCDR-12-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_ESCA_hg38_OpenAccess_GDCDR-12-0_DATA",TRUE,TRUE,"Esophageal Carcinoma","Tumor/Normal","USA","TCGA Esophageal carcinoma Open-Access Data Workspace

*hg38 TCGA and TARGET workspaces reference files by their GDC UUIDs.  In order to run analyses on the referenced data files, you will need to run workflows that retrieve the files from the GDC and copy them to your workspace bucket.  See   [this forum post](https://gatkforums.broadinstitute.org/firecloud/discussion/10382/populating-hg38-tcga-and-target-workspaces-with-data-files#latest) for instructions on the running of these workflows.*","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients' clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","185","Esophagus","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh38/hg38","TCGA_ESCA_hg38_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_ESCA_hg38_OpenAccess_GDCDR-12-0_DATA"
134,"help-gatk","Somatic-CNVs-GATK4","READER","https://app.terra.bio/#workspaces/help-gatk/Somatic-CNVs-GATK4",TRUE,FALSE,NA,NA,NA,"### GATK Best Practices for Somatic Copy Number Variation   
The variant discovery portion of GATK CNV is presented in this workspace in the form of two workflows and one tutorial notebook. One workflow creates a panel of normals (PON) and a second runs the GATK CNV pipeline on a matched pair with Funcotator. The tutorial notebook walks users through the creation of a PON and CNV calling using the GATK CNV tools. Detailed descriptions of the workflows are available in the [GATK's Best Practices Document](https://gatk.broadinstitute.org/hc/en-us/sections/360007226651) and the tutorial is documented in [How-to-part-I-Sensitively-detect-copy-ratio-alterations-and-allelic-segments](https://gatk.broadinstitute.org/hc/en-us/articles/360035531092) and [How-to-part-II-Sensitively-detect-copy-ratio-alterations-and-allelic-segments](https://gatk.broadinstitute.org/hc/en-us/articles/360035890011--How-to-part-II-Sensitively-detect-copy-ratio-alterations-and-allelic-segments).   


Scroll down for detailed information on each workflow   

- Workflow description and function
- Input data description and requirements
- Sample data description    
- Optional workflow-level and task-level parameters
- Time and cost estimates for running the workflows

Additional (general) information at the end includes
- Contact information 
- License information   

The following material is provided by the GATK Team. Please check the Contact Us section for instructions on posting questions .

## Notebooks
**Somatic-CNA-Tutorial :**
This hands-on tutorial outlines steps to detect alterations with high sensitivity in total and allelic copy ratios using GATK4's ModelSegments CNV workflow. The workflow is suitable for detecting somatic copy ratio alterations, more familiarly copy number alterations (CNAs), or copy number variants (CNVs) for whole genomes and targeted exomes.

| Option | Value |
| --- | --- |
| Environment | Default |
| Profile | Custom |
| CPU Minimum | 4|
| Disksize Minimum | 100 GB |
| Memory Minimum | 15 GB |

 
### Software versions
GATK4.1.4.1

### Time and cost 
This workspace focuses on the use of notebooks, thus the time and cost of completing the tutorial depends on the runtime environment of your notebook and the length of time you spend actively working in the notebook. With the given runtime environments above, users can expect the cost to be much less than a dollar an hour for each notebook. The specific cost will be displayed on the top right corner of your screen, next to the notebook machine options. 

## Workflows  

The workflows are directly linked to the GATK repo via [Dockstore](https://dockstore.org/organizations/BroadInstitute/collections/GATKWorkflows) so its possible to select a particular workflow version based off the GATK version. If requested by the workflow inputs, be sure to provide the corresponding GATK docker image for the workflow version. For example if using workflow version 4.0.0.0 then the proper GATK docker to use would be us.gcr.io/broad-gatk/gatk:4.0.0.0. 

### 1-cnv_somatic_panel_workflow     
**What does it do?**   
Creates a GATK CNV Panel of Normals (PoN), given a list of normal samples.  

**What dates it take as input?**   
The workflow arequitres a sample set as input. It accepts two normal BAMs (SM-74NEG and SM-74NF5)  to create a PON. The two BAMs are organized in a sample set called ""panel_of_normals"" in the `sample set` table of the workspace Data tab. 

**What does it output?**     
- Read-count PoN in HD5 format
- Additional metrics       

**Important information on creating a panel of normals for sequence analysis**     
The `normal_bams` samples used by `1-cnv_somatic_panel_workflow` can be used test the wdl. However, they are **not appropriate** for creating a panel of normals for sequence analysis. 

For instructions on creating a proper PON please refer to  the documents [Panel of Normals](https://software.broadinstitute.org/gatk/documentation/article?id=11053) and [Generate a CNV panel of normals with CreateReadCountPanelOfNormals](https://gatk.broadinstitute.org/hc/en-us/articles/360035531092--How-to-part-I-Sensitively-detect-copy-ratio-alterations-and-allelic-segments#2). 

 
**Requirements/expectations**      
(Note: the reference used must be the same for PoN and case samples)      

| Parameter | Description |
| :-------  | :-------- |
|  ``CNVSomaticPanelWorkflow.gatk_docker`` | GATK Docker image (e.g., ``broadinstitute/gatk:latest``) |
|  ``CNVSomaticPanelWorkflow.intervals`` | Picard- or GATK-style interval list.  For WGS, this should typically only include the autosomal chromosomes. |
|  ``CNVSomaticPanelWorkflow.normal_bais`` | List of BAI files.  This list must correspond to `normal_bams`.  For example, `[""Sample1.bai"", ""Sample2.bai""]`. |
|  ``CNVSomaticPanelWorkflow.normal_bams`` | List of BAM files.  This list must correspond to `normal_bais`.  For example, `[""Sample1.bam"", ""Sample2.bam""]`. |
|  ``CNVSomaticPanelWorkflow.pon_entity_id`` | Name of the final PoN file. |
|  ``CNVSomaticPanelWorkflow.ref_fasta_dict`` | Path to reference dict file. |
|  ``CNVSomaticPanelWorkflow.ref_fasta_fai`` | Path to reference fasta fai file. |
|  ``CNVSomaticPanelWorkflow.ref_fasta`` | Path to reference fasta file. |


**Sample Data**  
Links to sample data for the expected input types are provided in the workspace data table for testing.      
  
**Where to find required and optional references**  
Required and optional references, and resources for the workflows are included in the workspace Data table (""Workspace Attributes"" in FireCloud). The reference genome for this workspace is b37.    

**Time and cost estimates**   

| Sample Name | Sample Total Size | Time | Cost $ |
| :-------:  | :--------: | :--------: | :----------: |
| panel_of_normals |  78.76 GB | 3:10:00 | 0.12 |     

See [this article](https://support.terra.bio/hc/en-us/articles/360029748111) for more detail on cost-controlling options.     


**To  run 2-cnv_somatic_pair_workflow cheaper and faster to check if the workflow is operational** use the pair `SM-74-v1-chr20-downsampled`. The downsample dataset uses truncated versions of the reference/resources so make sure to configure the workflow to use the correct files mentioned in the Workspace Data subsection.     

**Optional workflow-level and task-level parameters**    
These may be set by advanced users, and include:

| Parameter | Description |
| :-------  | :-------- |
|  ``CNVSomaticPanelWorkflow.do_explicit_gc_correction`` |  (optional) If true, perform explicit GC-bias correction when creating PoN and in subsequent denoising of case samples.  If false, rely on PCA-based denoising to correct for GC bias. |
|  ``CNVSomaticPanelWorkflow.PreprocessIntervals.bin_length`` | Size of bins (in bp) for coverage collection.  *This must be the same value used for all case samples.* |
|  ``CNVSomaticPanelWorkflow.PreprocessIntervals.padding`` | Amount of padding (in bp) to add to both sides of targets for WES coverage collection.  *This must be the same value used for all case samples.* |  

 Further explanation of other task-level parameters may be found by invoking the ``--help`` documentation available in the gatk.jar for each tool.     

### 2-cnv_somatic_pair_workflow 
Runs the GATK CNV pipeline on a matched pair and runs Funcotator.

**What input data does it take?**     
The workflow takes in a pair of tumor and normal BAM files (SM-74P4M and  SM-74NEG), using the pair entity type file `HCC1143`.     

**What does it output?**    
- Modeled segments for tumor and normal
- Modeled segments plot for tumor and normal
- Denoised copy ratios for tumor and normal
- Denoised copy ratios plot for tumor and normal
- Denoised copy ratios lim 4 plot for tumor and normal
- Additional metrics         

**Sample Data**  
Links to sample data for the expected input types are provided in the workspace data table for testing.      

**When using testing datasets**    
The downsample dataset used for testing the workflow uses truncated versions of the reference/resources, these files are listed in the workspace attributes under the following variable names: `ref_fasta_truncated` (index and dict), `read_count_pon_truncated,` `common_sites_chr20,` and `intervals_chr20.`

**Requirements/Expectations**     
(note: the reference and bins, if specified, must be the same for both PoN and case samples)

| Parameter | Description |
| :-------  | :-------- |
|  ``CNVSomaticPairWorkflow.common_sites`` | Picard- or GATK-style interval list of common sites to use for collecting allelic counts. | 
|  ``CNVSomaticPairWorkflow.gatk_docker`` | GATK Docker image (e.g., ``broadinstitute/gatk:latest``). |
|  ``CNVSomaticPairWorkflow.intervals`` | Picard or GATK-style interval list.  For WGS, this should typically only include the autosomal chromosomes. |
|  ``CNVSomaticPairWorkflow.normal_bam`` | Path to normal BAM file. |
|  ``CNVSomaticPairWorkflow.normal_bam_idx`` | Path to normal BAM file index. |
|  ``CNVSomaticPairWorkflow.read_count_pon`` | Path to read-count PoN created by the panel workflow.  |
|  ``CNVSomaticPairWorkflow.ref_fasta_dict`` | Path to reference dict file. |
|  ``CNVSomaticPairWorkflow.ref_fasta_fai`` | Path to reference fasta fai file. |
|  ``CNVSomaticPairWorkflow.ref_fasta`` | Path to reference fasta file. |
|  ``CNVSomaticPairWorkflow.tumor_bam`` | Path to tumor BAM file. |
|  ``CNVSomaticPairWorkflow.tumor_bam_idx`` | Path to tumor BAM file index. |


**Optional workflow-level and task-level parameters**     
Several task-level parameters may be set by advanced users as described above for `1-cnv_somatic_panel_worflow.`    

 To invoke Oncotator on the called tumor copy-ratio segments:
   
| Parameter | Description |
| :-------  | :-------- |
|  ``CNVSomaticPairWorkflow.is_run_oncotator`` | (optional) If true, run Oncotator on the called copy-ratio segments.  This will generate both a simple TSV and a gene list. |  

Further explanation of these task-level parameters may be found by invoking the ``--help`` documentation available in the gatk.jar for each tool.    


**Time and Cost Estimates**     

| Sample Name | Sample Total Size | Time | Cost $ |
| :-------:  | :--------: | :--------: | :----------: |
| HCC1143 | 73.02 GB | 4:25:00 | 0.21 |    
| SM-74-v1-chr20-downsampled | 3.36 MB | 0:30:00 | TBD | 

See [this article](https://support.terra.bio/hc/en-us/articles/360029748111) for more detail on cost-controlling options. 

### General Note:
- When changing the workflow version being used, be sure to select the corresponding GATK Docker version. For instance, when using workflow version **4.1.7.0** the coresponding GATK docker version would be  us.gcr.io/broad-gatk/gatk:**4.1.7.0**


---

### Contact information  
For questions about this workspace please visit the Featured Workspaces Topic topic on the [Terra Community Forum](https://broadinstitute.zendesk.com/hc/en-us/community/topics). Use the search box to see if other users have asked the same question previously. If not, post and tag **@Samantha** so that we get notified.

This material is provided by the GATK Team. Please post any questions or concerns regarding the GATK tool to the [GATK forum](https://gatk.broadinstitute.org/hc/en-us/community/topics)

### Workspace Citation 
Details on citing Terra workspaces can be found here: [How to cite Terra](https://support.terra.bio/hc/en-us/articles/360035343652)

Data Sciences Platform, Broad Institute (*Year, Month Day that this workspace was last modified*) help-gatk/Somatic-CNVs-GATK4 [workspace] Retrieved *Month Day, Year that workspace was retrieved*,  https://app.terra.bio/#workspaces/help-gatk/Somatic-CNVs-GATK4

### License  
**Copyright Broad Institute, 2019 | BSD-3**  
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at https://github.com/openwdl/wdl/blob/master/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.

### Workspace Change Log
| Date | Change | Author |
| --- | --- | --- |
|  2019-10-15 | Updated workflow and Docker to GATK 4.1.0.4 | Beri Shifaw |
|  2020-04-15 | Updated workflows and Docker to GATK 4.1.6.0. Workflows now point to Dockstore workflows registered from GATK repo. Updated cost and time. | Beri Shifaw |
|  2020-10-05 | Updated workflows and Docker to GATK 4.1.7.0.  Formating changes and link updates to dashboard. CNA notebook tutorial added. | Beri Shifaw |
|  2021-04-21 | Updated workflows and Docker to GATK 4.2.0.0.  | Beri Shifaw |
|  2021-09-15 | Addition of Workspace Citation section | Beri Shifaw |",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/Somatic-CNVs-GATK4"
136,"amp-pd-community-workspaces","AMP PD - Simulated Proteomics Data Set","READER","https://app.terra.bio/#workspaces/amp-pd-community-workspaces/AMP%20PD%20-%20Simulated%20Proteomics%20Data%20Set",TRUE,FALSE,NA,NA,NA,"# AMP PD - Simulated Proteomics Data Set

### This workspace has been designed specifically for the AMP PD Terra Workspaces Webinar Series and incorporates notebooks from previously released proteomics workspaces. 

The notebooks in this workspace work on **simulated data**. The data was generated by taking the real AMP PD data and adjusting it to mimic the format while still preserving potential biological insights. This data is **not real** and is not to be used for research. 

This workspace contains three notebooks with a GCS and BQ version for each. 

- R - Proteomics - Analysis - (BQ) - NPX Boxplots.ipynb
- R - Proteomics - Analysis - (BQ) -  PCA.ipynb
- R - Proteomics - (BQ) - APOE Proteomics Case Study.ipynb

This workspace also includes a start here notebook to get you familiar with the data sets.

- R - Proteomics - Start Here.ipynb


The intent of these notebooks is to be an introduction to Terra. They instruct new users to clone workspaces, run notebooks, work with data from GCS, and edit and merge dataframes.

### Next steps
Are you new to the Terra environment? From the hamburger menu (the 3 horizontal lines above), select Terra Support and then How-to Guides. You'll find many articles to help you get started.

The notebooks in this workspace have been provided in a state where they contain the output from having been Run already. This is done such that you can explore the expected content without first running the notebooks yourself. When you are ready to run the notebooks yourself, you can start your own work by making a `Clone` of this workspace to a new workspace from the ""snowman"" menu in the upper-right. Note that you'll need to have set up a Terra Billing Project for this.


### Below this line is material from the featured public [AMP PD In Terra workspace](https://app.terra.bio/#workspaces/amp-pd-public/AMP-PD-In-Terra)
<p>
		<img src=""https://storage.googleapis.com/amp-pd-public-content/featured-workspaces/amp-pd-in-terra/about-icon-image.png"" width=250px align=""left""></img>
AMP PD (Accelerating Medicine Partnership in Parkinson's Disease) was launched in 2018 as a public/private partnership intended to build a database with deep molecular characterization and longitudinal clinical profiling of PD patient data and biosamples with the goal of identifying and validating diagnostic, prognostic and/or disease progression biomarkers for PD.

A critical component of this partnership is broad sharing of the AMP PD data and analyses with the biomedical community to advance research in PD. AMP PD utilizes well characterized cohorts with existing biosamples and clinical data that were collected under comparable protocols and using common data elements.

A list of participating organizations and contributing studies can be found below.
</p>

![white space](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/white-space.jpg)
![white space](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/white-space.jpg)
## Data

### Data Overview
The current release of AMP PD is release 3. See below for release history.

AMP PD Release 3 contains
- 10,807 participants
- 10,418 whole genome sequence (WGS) samples
- 8,461 transcriptomics (RNASeq) samples
- 413 proteomics participants with multiple timepoints

AMP Harmonized Data includes:
- Clinical Data (Demographic, Medical History, MDS-UDPRS, MoCA, UPSIT)
- Transcriptomic Data (such as RNASeq from whole blood)
- Genomic Data (Whole Genome Sequencing from whole blood)
- Proteomic Data (Targeted proteomics from CSF and plasma)

### Browse AMP PD Data

There are 3 different data browsers available, depending on your level of approved access.
| Access   | Browser |
| ----------- | ----------- |
| Public | [Summary Data Dashboard](https://amp-pd.org/data/summary-data-dashboard)|
| Tier 1 | [Clinical Data Explorer](https://clinical-data-explorer.amp-pd.org/?filter=&extraFacets=)|
| Tier 2 | [Clinical and Omics Data Explorer](https://data-explorer.amp-pd.org/?filter=&extraFacets=)|

### Release History
- Release 1 (October 2019):  [Workspaces](https://app.terra.bio/#workspaces?projectsFilter=amp-pd-release-v1), [Release Notes](https://amp-pd.org/news/release-notes-october-2019)
- Release 2 (December 2020):  [Workspaces](https://app.terra.bio/#workspaces?projectsFilter=amp-pd-release-v2), [Release Notes](https://amp-pd.org/news/release-notes-december-2020)
- Release 2.5 (May 2021):  [Workspaces](https://app.terra.bio/#workspaces?projectsFilter=amp-pd-release-v2-5), [Release Notes](https://amp-pd.org/release-notes-may-2021)
   - Update (May 2022) [Release Notes](https://amp-pd.org/news/release-notes-may-2022)
- Release 3.0 (November 2022) [Workspaces](https://app.terra.bio/#workspaces?projectsFilter=amp-pd-release-v3), [Release Notes](https://amp-pd.org/news/amp-v3-release-notes-december-2022)

![white space](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/white-space.jpg)
## Registering for Access
AMP PD is a controlled access dataset.

There are two tiers of access to AMP PD data:
- Tier 1 provides access to clinical data.
- Tier 2 provides access to omics data.

To request access, you'll need to register on the [AMP PD website](https://amp-pd.org/register-for-amp-pd):
- Tier 1 access requires submission of the AMP PD Data Use Agreement with an individual's signature.
- Tier 2 access requires submission of the AMP PD Data Use Agreement with an individual and institutional signature.

Complete instructions for registering can be found on the [Registration](https://amp-pd.org/register-for-amp-pd) page and getting set up on the [How To](https://amp-pd.org/how-to) page.

![Diagram illustrating AMP PD access request process](https://storage.googleapis.com/amp-pd-public-content/featured-workspaces/amp-pd-in-terra/AMP-PD-easy-form-workflow.png)

Please be sure to read the [Safe Computing Guidelines](https://amp-pd.org/safe-computing-guidelines) for specific guidance on working with AMP PD data.

![white space](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/white-space.jpg)
## AMP PD Resources in Terra
When you are granted access to AMP PD, you will immediately gain access to a set
of workspaces designed to accelerate your understanding and exploration of the
data. You'll also gain access to workflows and notebooks to help you get
started with your analysis.

### Tier 1 - Clinical Access
| Workspace   | Description |
| ----------- | ----------- |
| [Getting Started Tier 1 - Clinical Access](https://app.terra.bio/#workspaces/amp-pd-release-v3/Getting%20Started%20Tier%201%20-%20Clinical%20Access)| Get started with AMP PD clinical data. Load clinical data into a Python 3 or R Jupyter notebook. Save a cohort using Data Explorer.|

### Tier 2 - Clinical and Omics Access
| Workspace   | Description |
| ----------- | ----------- |
| [Getting Started Tier 2 - Clinical and Omics Access](https://app.terra.bio/#workspaces/amp-pd-release-v3/Getting%20Started%20Tier%202%20-%20Clinical%20and%20Omics%20Access)| Get started with AMP PD clinical, RNASeq, and WGS data. Tour the layout of files in Cloud Storage and tables in BigQuery. Load data into a Python 3 or R notebook for analysis and visualization.|
| [Proteomics Getting Started](https://app.terra.bio/#workspaces/amp-pd-release-v3/Getting%20Started%20Tier%202%20-%20Proteomics) | Get started using Terra notebooks to analyze AMP PD proteomics data. |
| [Proteomics QC and Analysis](https://app.terra.bio/#workspaces/amp-pd-release-v3/AMP%20PD%20-%20Proteomics%20QC%20and%20Analysis) | Access QC and analysis notebooks for AMP PD.
| [RNASeq Release Workflows - v3](https://app.terra.bio/#workspaces/amp-pd-release-v3/AMP%20PD%20-%20RNASeq%20Release%20Workflows)| Learn how to run workflows on AMP PD RNASeq data in Terra.|
| [RNASeq QC and PCA - v3](https://app.terra.bio/#workspaces/amp-pd-release-v3/AMP%20PD%20-%20RNASeq%20QC%20and%20PCA)   | Understand AMP PD RNASeq data, data quality, and analysis tools available. |

![white space](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/white-space.jpg)
## Community-Provided Workspaces
| Workspace   | Description |
| ----------- | ----------- |
| [AMP-PD RNA-Seq Explorer](https://www.google.com/url?q=https://app.terra.bio/%23workspaces/amp-pd-community-workspaces/AMP-PD%2520RNA-Seq%2520Explorer&sa=D&source=docs&ust=1657566743903647&usg=AOvVaw3Ba_j1CtbMRZqgTrZpQ7pR)| Visualization tool for individual and summary level data within the Terra-based AMP-PD Knowledge Platform developed by the Craig Laboratory at USC.|

![white space](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/white-space.jpg)
## Getting Help
If you have questions about AMP PD data access or governance, contact the Access Control Team at ACT@amp-pd.org

If you have questions about AMP PD workspaces in Terra or AMP PD data, contact admin@amp-pd.org.

If you have questions about Terra, contact [Terra Support](https://terra.bio/resources/help/#)

![white space](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/white-space.jpg)
## AMP PD Consortium
### Government
- [Food and Drug Administration (FDA)](http://www.fda.gov/)
- [National Institute of Neurological Disorders and Stroke (NINDS)](https://www.ninds.nih.gov/)
- [National Institute on Aging (NIA)](https://www.nia.nih.gov/)
### Industry
- [Bristol-Myers Squibb](https://www.bms.com/)
- [GSK](https://www.gsk.com/)
- [Pfizer](https://www.pfizer.com/)
- [Sanofi](https://www.sanofi.us/)
- [Verily](https://verily.com/)
### Nonprofit
- [Aligning Science Across Parkinson's (ASAP) Initiative](https://parkinsonsroadmap.org/#)
- [The Michael J. Fox Foundation (MJFF)](https://www.michaeljfox.org/)
### Data
- [MJFF and NINDS BioFIND Study](https://amp-pd.org/unified-cohorts/biofind)
- [NIA International Lewy Body Dementia Genetics Consortium Genome Sequencing in Lewy body dementia case-control cohort (LBD)](https://amp-pd.org/unified-cohorts/lbd)
- [MJFF LRRK2 Cohort Consortium (LCC)](https://amp-pd.org/unified-cohorts/llc)
- [Brigham and Women's Hospital/MGH Harvard Biomarkers Study](https://amp-pd.org/unified-cohorts/hbs)
- [NINDS Parkinson's Disease Biomarkers Program](https://amp-pd.org/unified-cohorts/pdbp)
- [MJFF Parkinson’s Progression Markers Initiative](https://amp-pd.org/unified-cohorts/ppmi)
- [NINDS Study of Isradipine as a Disease Modifying Agent in Subjects With Early Parkinson Disease, Phase 3 (STEADY-PD3).](https://amp-pd.org/unified-cohorts/steady-pd-3)
- [The MJFF and NINDS Study of Urate Elevation in Parkinson’s Disease, Phase 3 (SURE-PD3)](https://amp-pd.org/unified-cohorts/sure-pd3)
- [Global Parkinson's Genetics Program (GP2)](https://amp-pd.org/federated-cohorts/gp2)
### Managing
- [Foundation for the NIH (FNIH)](https://fnih.org/)",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","amp-pd-community-workspaces/AMP PD - Simulated Proteomics Data Set"
137,"fc-product-demo","RShiny WGS Association Visualization","READER","https://app.terra.bio/#workspaces/fc-product-demo/RShiny%20WGS%20Association%20Visualization",TRUE,FALSE,NA,NA,NA,"## Introduction

This workspace illustrates launching the **WGSAssociationVisualization** RShiny app in RStudio. This application is designed to produce regional plots of a given search range from a bgzipped WGS summary statistics file stored in your Google bucket. The regional plots thus produced can be downloaded as PNG files.

* Author: Shruti Parpattedar (parpattedar.s@husky.neu.edu)
* Maintainer: Manning Lab
* Version: 0.1

For more details about the app, see https://github.com/manning-lab/WGSAssociationVisualization

## Instructions

### Create an LD file (optional)
For plotting linkage disequalibrium information, this app was designed to take the outputs of the [LDGds WDL](https://dockstore.org/workflows/github.com/AnalysisCommons/LDGds/LDGds:master?tab=info), but can take any plain text file in the correct format. LD data should be stored in a plain, delimited text file as either a matrix or a row vector. Either format must have variant identifiers as row and column names. See below for an example of LD data in the correct format for the app.

| | rs12345 | rs54321 | rs98765 |
---|-------|----------|-------------
| rs12345 |	1	| 0.0001 |	0.1001 |
| rs54321 | 0.0001 | 1 | 0.82 |
| rs98765 | 0.1001 | 0.82 | 1 |

To plot your own linkage disequalibrium values, check out the LDGds WDL in this workspace. This workflow takes your genotype file and calculates the LD within a specified region or for a specific variant.


### Launch the application

1. Launch an RStudio cloud environment from this workspace with at least 2 CPUs and 7.5 GB of RAM.
2. Within RStudio, select **File** -> **New Project** -> **Version Control** -> **Git** 
3. Enter the following information:
   * Repository URL: **https://github.com/manning-lab/WGSAssociationVisualization**
   * Project directory name: **WGSAssociationVisualization**
   * Create project as subdirectory of: **~**
4. Click **Create Project**
5. Install dependencies. Note this takes about 10-15 mins, but only needs to be done once (unless you delete your persistent disk).
    >  
    >  BiocManager::install(c(""biomaRt"", ""GenomicRanges"", ""Gviz"", ""EnsDb.Hsapiens.v75"", ""EnsDb.Hsapiens.v86""))
    >  
    >  devtools::install_github(""manning-lab/WGSregionalPlot"")
    >  
    >  install.packages(c('shinyBS', 'shinyjs', 'rjson'))
    >  
6. In the Files view, navigate to **manninglab-wgs-visualization/appfiles**
7. Click in **app.R**
8. Click **Run App** (you might need to allow pop-ups)
9. After a few seconds, you should be greeted with the application in a new window.

### Use the application

1. Enter the link to the Google bucket containing your data.
   * The bucket link can be found in the Workspace Dashboard page, or by inspecting the **$WORKSPACE_BUCKET** environment variable in RStudio.
   * This workspace has some example data from 1kg, but feel free to try this on your own GWAS summary statistics. See [here](https://github.com/manning-lab/WGSAssociationVisualization#data-required-to-run-this-application) for more details about data required to run this application.
2. Click the **Submit bucket** link button. This will auto populate the drop down list below with the .gz files in your bucket which have corresponding tabix indexed files.
3. Select the desired file from the drop down list.
4. Enter the range you want to search in (for example **20:60900000-61100000**).
5. Enter the columns numbers for the Variant ID (a unique variant identifier), chromosome of the variant, position of the variant and p-value of the variant. For example, the input for the demo file would be **1**, **2**, **3** and **9** respectively.
6. Select the appropriate genome build from the given options: **hg19** and **hg38**.
7. (Optional) The Google bucket link for the LD file gets auto populated but you can edit this link if your file exists in another location and click the **Submit bucket** link button.
   * This will populate the drop down list below with all the .txt, .tsv and .csv files in that location.
8. (Optional) Enter the LD reference variant (for example **20-61000005-A-G**).
9. Click the **View plot** button to view the plot.
10. Clicking the Download button will download the plot as a .png file (for example **Regional_plot_10:112948590-113048589.png**).

Here is a screenshot of the running application:

![](https://storage.googleapis.com/rtitle_test/manninglab-wgs-visualization-launch.png)
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","fc-product-demo/RShiny WGS Association Visualization"
138,"broad-firecloud-tcga","TCGA_BLCA_OpenAccess_V1-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_BLCA_OpenAccess_V1-0_DATA",TRUE,TRUE,"Bladder Urothelial Carcinoma","Tumor/Normal","USA","TCGA Bladder Urothelial Carcinoma Open-Access Data Workspace","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients’ clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","412","Bladder ","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh37/hg19","TCGA_BLCA_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_BLCA_OpenAccess_V1-0_DATA"
139,"fc-product-demo","Terra Training - Cumulus","READER","https://app.terra.bio/#workspaces/fc-product-demo/Terra%20Training%20-%20Cumulus",TRUE,FALSE,NA,NA,NA,"## Single-Cell RNA-Seq Analysis using Cumulus



This workspace is a showcase of [Cumulus](https://cumulus.readthedocs.io), a cloud-based single-cell/single-nucleus data analysis framework. It uses a large-scale single-cell dataset, and demonstrates Cumulus on both workflow and interactive analysis.

Users can simply clone this workspace, and reproduce this showcase on their own by following the instructions below.
 

------------------

## Contents   
(Scroll down to find the following sections)     

* Data Table
* Workflow
* Interactive Analysis
* Data Visualizer
* More Materials & Contact Info
* Licensing

------------

### Data  Table

The bone marrow dataset in the [Census of Immune Cells](https://data.humancellatlas.org/explore/projects/cc95ff89-2e68-4a08-a234-480eca21ce79) study consists of 63 10x Genomics channels (v2 chemistry) collected from 8 donors.

We picked 8 channels (one channel per donor) from the bone marrow dataset as our test dataset.

Information of  FASTQ files of the 8 channels are listed in `sample` table of `DATA` tab of this workspace. This table is critical for running workflows.

If you create your own workspace by cloning from this one, you'll need to update the Google bucket URLs in `sample` table to refer to your own workspace. Below are the steps:

1. Look for Google bucket ID of your workspace. You can get it from the bottom of the right panel of this page, which starts with ""fc-"" and follows by a sequence of heximal numbers.
2. Open `sample` table in `DATA` tab, then replace the GS URLs in `Cellranger_output_directory` and `Analysis_output_directory` by yours. For example, if your workspace's GS ID is `gs://xxx`, and replace `Cellranger_output_directory` from `gs://yyy/cellranger_output` to `gs://xxx/cellranger_output`. Similarly for `Analysis_output_directory`.

--------------

### Workflow

There are 2 steps for the analysis:

* Generate RNA gene-count matrices. (cumulus/cellranger_workflow)
* Process the gene-count matrices for single-cell RNA-seq analysis, including data aggregation, quality-control, dimension reduction, clustering analysis, visualization, differential expression analysis, cell-type annotation, etc. (cumulus/cumulus)

Each step uses one Cumulus workflow.

In this section, I'll first provide a quick guide on running workflows, then discuss advanced settings on each step.

#### Quick Guide

1. Open `cellranger_workflow` configuration page from `WORKFLOWS` tab. Click `RUN ANALYSIS` botton. Then the workflow will use the information from `sample` table to run the job.
2. Open `cumulus` configuration page from `WORKFLOWS` tab. Click `RUN ANALYSIS` botton. Then the workflow will use the information from `sample` table to run the job.

When finished, you can find `cellranger_workflow` output at `cellranger_output` folder on your workspace, and `cumulus` output at `analysis_output` folder.

Notice that by using the preset configurations to run these workflows, you are actually using input files from the original public workspace.

A runtime summary is the following:

|Step|CPU|Memory|Time|Cost|
|---|---|---|:---:|---|
|cellranger_workflow|32 * 8|120 GB * 8|1h14min|$2.42|
|cumulus|32|200 GB|22min|$0.16|

In the output directory, there is a subfolder with the sample name ""MantonBM_subset"". All the output files are stored there. Below is a brief description of these files:
* `MantonBM_subset.aggr.zarr.zip`: The aggregated gene-count matrix of all 8 channels. Notice that Cumulus workflow default setting has already filtered out cell barcodes with less than 100 genes expressed in data aggregation.
* `MantonBM_subset.zarr.zip`: Count matrix containing clustering result.
* `MantonBM_subset.GRCh38-rna.h5ad`: Analysis result in `h5ad` format. This example only generates one h5ad file, as there is only one modal data with key `GRCh38-rna`.
* `MantonBM_subset.GRCh38-rna.filt.*.pdf`: Quality-control plots.
* `MantonBM_subset.GRCh38-rna.filt.xlsx`: Quality-control stats spreadsheet.
* `MantonBM_subset.GRCh38-rna.fitsne.pdf`: The FIt-SNE embedding plot.
* `MantonBM_subset.GRCh38-rna.umap.pdf`: The UMAP embedding plot.
* `MantonBM_subset.GRCh38-rna.*.composition.pdf`: The composition plot regarding cluster labels and channels.
* `MantonBM_subset.GRCh38-rna.de.xlsx`: The differential expression analysis result in spreadsheet.
* `MantonBM_subset.GRCh38-rna.anno.txt`: The putative cluster-specific cell type annotation result.
* `MantonBM_subset.log`: Analysis log file.

#### Advanced on Step 1

This step uses Cellranger to generate a gene-count matrix for each of the 8 channels from FASTQ files.

Since those FASTQ files are stored in the original public workspace, you don't have to fetch them to your own workspace. In this way, it's fine to use the sample sheet located at `Cellranger_input_csv` of your `sample` table, which refers to the one at the original public workspace.

The Cellranger workflow by default uses [Cellranger v4.0](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/release-notes), which is released on 2020/07/07 and achieves 2-4 times faster in the Count step comparing with its previous version.

The Cellranger sample sheet has the following structure:

|Sample|Reference|Flowcell|
|---|---|---|
|MantonBM1_HiSeq_1|GRCh38-2020-A|`Flowcell`|
|MantonBM2_HiSeq_1|GRCh38-2020-A|`Flowcell`|
|MantonBM3_HiSeq_1|GRCh38-2020-A|`Flowcell`|
|MantonBM4_HiSeq_1|GRCh38-2020-A|`Flowcell`|
|MantonBM5_HiSeq_1|GRCh38-2020-A|`Flowcell`|
|MantonBM6_HiSeq_1|GRCh38-2020-A|`Flowcell`|
|MantonBM7_HiSeq_1|GRCh38-2020-A|`Flowcell`|
|MantonBM8_HiSeq_1|GRCh38-2020-A|`Flowcell`|

where
* `Flowcell` is short for the value of `Flowcell` in `sample` table. The Flowcell column specifies the Google bucket URL to top-level folders containing all the samples' FASTQ files.
* The genome reference `GRCh38-2020-A` is chosen from a list of [prebuilt references](https://cumulus.readthedocs.io/en/latest/cellranger.html#sample-sheet). This is the human genome reference built by Cellranger v4.0.
* As this example starts from FASTQ files, sample names cannot be chosen arbitrarily, but should be consistent with subfolders within `Flowcell`.

Moreover, since we start from FASTQ files, we only need to run `cellranger count`, so setting `run_mkfastq` to `false` in cellranger_workflow configuration page is necessary.

See [here](https://cumulus.readthedocs.io/en/latest/cellranger.html#run-cellranger-count-only) for details on this step.

#### Advanced on Step 2

This step uses the output of Step 1 for downstream analysis. So we first need to wait for Step 1 to finish.

After that, we need to create a sample sheet for cumulus workflow. Let `gs://xxx/cellranger_output` be the output directory of Step 1. Then your sample sheet should be the following (notice that CSV file uses comma to seprate values):

|Sample|Location|
|---|---|
|MantonBM1_HiSeq_1|gs://xxx/cellranger_output/MantonBM1_HiSeq_1/raw_feature_bc_matrix.h5|
|MantonBM2_HiSeq_1|gs://xxx/cellranger_output/MantonBM2_HiSeq_1/raw_feature_bc_matrix.h5|
|MantonBM3_HiSeq_1|gs://xxx/cellranger_output/MantonBM3_HiSeq_1/raw_feature_bc_matrix.h5|
|MantonBM4_HiSeq_1|gs://xxx/cellranger_output/MantonBM4_HiSeq_1/raw_feature_bc_matrix.h5|
|MantonBM5_HiSeq_1|gs://xxx/cellranger_output/MantonBM5_HiSeq_1/raw_feature_bc_matrix.h5|
|MantonBM6_HiSeq_1|gs://xxx/cellranger_output/MantonBM6_HiSeq_1/raw_feature_bc_matrix.h5|
|MantonBM7_HiSeq_1|gs://xxx/cellranger_output/MantonBM7_HiSeq_1/raw_feature_bc_matrix.h5|
|MantonBM8_HiSeq_1|gs://xxx/cellranger_output/MantonBM8_HiSeq_1/raw_feature_bc_matrix.h5|

The sample name this time can be changed for your convenience, as long as they are distinct to each other. These will be the labels of `Channel` cell attribute in the analysis.

Once done with creating the sample sheet, upload it to Google bucket of your workspace. Here are two ways:
1. Click ""Open in browser"" link at bottom-right of this page, then click ""Upload Files"" button in the new page.
2. Use [gsutil](https://cloud.google.com/sdk) to upload from command-line on your local machine:

```
gsutil cp count_matrix.csv gs://xxx/
```

where `count_matrix.csv` should be replaced by your sample sheet filename on your local machine, and `gs://xxx` be placed by GS URL to the target location.

Now go to `sample` table in `DATA` tab, change `Analysis_input_csv` to location of your sample sheet file. 

In Cumulus configuration page in `WORKFLOWS` tab, several input fields are preset:

* `input_file`: link to input file. In this example, we use a CSV format sample sheet. In other cases, Cumulus also accepts a single count matrix file, or multiple files via data table. See [details](https://cumulus.readthedocs.io/en/latest/cumulus.html#prepare-input-data).
* `output_directory`: Google bucket URL of the top-level output directory. If there are multiple samples analyzed seperately, each of them will have a dedicated subfolder within this output directory.
* `output_name`: Name of subfolder and filename prefix of analysis result files of of this sample.

These 3 input fields above are required. All the others are optional:

* **Aggregate_matrix**: Use default settings. So aggregate gene-count matrices of 8 channels into one, while filtering out cells with fewer than 100 genes expressed.
* **Cluster**:
	* Preprocessing: with default settings, keep cells with 500 < number of genes <= 6000 expressed, generate quality-control plots and stats, select 2000 highly variable features, calculate 50 PCs in PCA, and construct nearest neighbor graph of 100 neighbors. Fields of non-default settings are below:
		* `percent_mito`: Set to `10`, meaning to keep cells with mitochondrial ratio less than 10% of total counts.
		* `correct_batch_effect`: Set to `true`. By default, use [Harmony](https://www.nature.com/articles/s41592-019-0619-0) algorithm for batch correction.
	* Clustering:
		* `run_louvain`: Set to `false`. Louvain is the default clustering algorithm, but we won't use it in this example.
		* `run_leiden`: Set to `true`. We use Leiden for clustering.
	* Visualization: By default, UMAP embedding of cells is calculated.
		* `run_fitsne`: Set to `true`. We also want to see FIt-SNE embedding of cells.
* **Differential Expression (DE) analysis**: Run the default t test on clusters, calculate AUROC, and annotate cluster-specific cell types based on DE result.
	* `cluster_labels`: Set to `""leiden_labels""`. Use Leiden clustering result for DE analysis. Its default is `""louvain_labels""`, the Louvain clustering result which we don't have here.
	* `annotate_cluster`: Set to `true`. For cell type annotation, by default, use t test result and preset `""human_immune""` cell markers, report putative cell types with scores higher than `0.5`.
* **Plotting**:
	* `plot_composition`: Set to `""leiden_labels:Channel""`. We want to see the composition of each cluster regarding samples.
	* `plot_fitsne`: Set to `""leiden_labels,Channel""`. We want to see FIt-SNE plots regarding clusters and samples, respectively.
	* `plot_umap`: Set to `leiden_labels,Channel`. We want to see UMAP plots regarding clusters and samples, respectively.

See [here](https://cumulus.readthedocs.io/en/latest/cumulus.html) for details on input data preparation and description of Cumulus workflow inputs.

-------------

### Interactive Analysis

Besides running cumulus workflow, users can also analyze the data interactively via Terra Notebook.

First, follow [this instruction](https://pegasus.readthedocs.io/en/latest/terra_notebook.html) to create your own Terra Runtime. This tutorial uses `cumulusprod/pegasus-terra:1.0` docker image.

When Runtime is created and started, open `Pegasus Tutorial` notebook in `NOTEBOOKS` tab. Click `EDIT` to enter the edit mode.

Now follow the notebook tutorial to get familar with Pegasus API on single-cell data analysis.

Pegasus is the analysis module of Cumulus, which can be used as an independent Python package. See [here](https://pegasus.readthedocs.io) for its documentation.

When finished, before leaving, don't forget to stop your Terra Runtime.

---------------

### Data Visualizer

Cirrocumulus is a cloud-based interactive data visualizer. Below is how to use it on Terra:

1. Start your Terra notebook runtime, then click ""Open terminal"" button on its panel.
2. In terminal, copy the data file you want to visualize (must in `h5ad` format) from Google bucket to the environment. For example:
```
gsutil -m cp gs://xxx/result.h5ad .
```
3. Launch Cirrocumulus with this data file:
```
cirro launch result.h5ad &
```
This will run Cirrocumulus on `localhost:5000`.
4. However, we can't directly view it by typing the address above in our web browser. We need to export it to a public URL using ngrok:
```
ngrok http 5000
```
And in the pop-up terminal, click the URL ending with "".ngrok.io"" to open Cirrocumulus in your web browser.

Now enjoy using Cirrocumulus to go over the dataset.

-------------

### More Materials

Cumulus is published in Nature Methods as an [article](https://www.nature.com/articles/s41592-020-0905-x). You can access to a view-only version of this paper [here](https://rdcu.be/b5R5B).

We have [tutorial videos](https://www.youtube.com/watch?v=-azWohOas7g&list=PLIv12jQJ5nmaZprdUzBie0OkKU7yEPTyk), introducing Cumulus workflows, Cirrocumulus visualizer, and Pegasus analysis package.


More tutorials:
* Pegasus: [link](https://pegasus.readthedocs.io/en/latest/tutorials.html).
* PegasusIO: Module on data I/O and manipulation for Cumulus. [link](https://pegasusio.readthedocs.io/en/latest/tutorials.html).
* Cirrocumulus: Data visualizer. [link](https://cirrocumulus.readthedocs.io/en/latest/tutorial.html).
* Cumulus featured workspace: [link](https://app.terra.bio/#workspaces/kco-tech/Cumulus). This workspace demonstrates how Cumulus works on a hashing single-nucleus dataset.

Documentation:
* Cumulus: [link](https://cumulus.readthedocs.io).
* Pegasus: [link](https://pegasus.readthedocs.io).
* PegasusIO: [link](https://pegasusio.readthedocs.io).
* Cirrocumulus: [link](https://cirrocumulus.readthedocs.io).

GitHub Repository:
* Cumulus: [link](https://github.com/klarman-cell-observatory/cumulus).
* Pegasus: [link](https://github.com/klarman-cell-observatory/pegasus).
* PegasusIO: [link](https://github.com/klarman-cell-observatory/pegasusio).

### Contact Information

This workspace is documented by Bo Li (<libo@broadinstitute.org>, <bli28@mgh.harvard.edu>) and Yiming Yang (<yyang@broadinstitute.org>, <yyang43@mgh.harvard.edu>).

For questions regarding Cumulus workflows, please [contact us](https://cumulus.readthedocs.io/en/latest/contact.html).

Readers can also direct questions to the Terra [Forum](https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team) for generic Terra questions.

--------------

### License

**Copyright Broad Institute, 2018 | BSD-3**

*Cumulus* is under BSD-3 lincense (see https://github.com/klarman-cell-observatory/cumulus/blob/master/LICENSE). All the workflows in Cumulus project are also under WDL's BSD-3 open-source code license (see https://github.com/openwdl/wdl/blob/master/LICENSE).

This version of the workspace was created for training with Pfizer on July 27th, 2020.",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","fc-product-demo/Terra Training - Cumulus"
140,"broad-firecloud-tcga","TCGA_UCS_OpenAccess_V1-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_UCS_OpenAccess_V1-0_DATA",TRUE,TRUE,"Uterine Carcinosarcoma","Tumor/Normal","USA","TCGA Uterine Carcinosarcoma Open-Access Data Workspace","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients’ clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","57","Uterus","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh37/hg19","TCGA_UCS_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_UCS_OpenAccess_V1-0_DATA"
142,"broad-firecloud-dsde-methods","2023_Cambridge_Workshop - GATK4-Germline-Preprocessing-VariantCalling-JointCalling","READER","https://app.terra.bio/#workspaces/broad-firecloud-dsde-methods/2023_Cambridge_Workshop%20-%20GATK4-Germline-Preprocessing-VariantCalling-JointCalling",TRUE,FALSE,NA,NA,NA,"### GATK Best Practices for Germline SNPs & Indels
This workspace contains tutorial notebooks and workflows that cover pre-processing, SNP and Indel variant calling. 

There are two ways to run the workflows, in the single sample case one sample is run through the first two workflows and a single VCF file for the sample is generated. In the cohort case, all four workflows are run on several samples to generate a multi-sample VCF. The workflows are properly configured in this workspace to run back to back, so that outputs from each step will automatically become the inputs for the next. 

Each workflow follows the GATK Best Practices on human whole-genome sequence data. Detailed description of the workflows is available in [Gatk's Best Practices Document](https://gatk.broadinstitute.org/hc/en-us/articles/360035535932).  

To learn more about how GATK workflows and Best Practices are used for production at the Broad Institute, you can also view the [Whole Genome Analysis Pipeline Workspace](https://app.terra.bio/#workspaces/warp-pipelines/Whole-Genome-Analysis-Pipeline). 

The workspace material is provided by the GATK team. Please post any questions or concerns to one of our forum sites : [GATK](https://gatk.broadinstitute.org/hc/en-us/community/topics) or [Terra](https://broadinstitute.zendesk.com/hc/en-us/community/topics/360000500432) 


## Notebooks
Here you will be running two different tutorial notebooks describing variant calling and different methods for variant filtering. This workspace is read-only, so clone your own unique copy to work with it.

You should find the documentation within the notebooks sufficient to run through them again on your own or to share with a friend later (and we encourage you to do so!).

The notebook(s) in this workspace are: 
* 02-germline-variant-discovery-with-HaplotypeCaller
* 03-germline-variant-exploration
* 04-gatk-variant-filtration
* 05-gatk-variant-refinement
* 08-gatk-somatic-cnv-tutorial
* 09-gatk-germline-cnv-tutorial
* 10-intro-scrnaseq-tertiary-analysis

All notebooks in this workspace can use the following runtime settings:

| Option | Value |
| --- | --- |
| Environment | Default (GATK 4.1.4.1) |
| Profile | Custom |
| CPU Minimum | 4|
| Disksize Minimum | 100 GB |
| Memory Minimum | 15 GB |

## Workflows overview 

1. **Preprocessing-For-Variant-Discovery**:  takes as input an unmapped BAM list file (text file containing paths to unmapped bam files) to perform preprocessing tasks such as mapping, marking duplicates, and base recalibration. It produces a single BAM file      

2.  **Haplotypecaller**:  takes as input a bam file and produces a file GVCF ( precursor to a VCF )    

3.  **Generate-Sample-Map**:  takes as input several GVCF files and generates a sample map file, which is text file where the first coloumn is the name of the sample and the second column contains the path to the samples GVCF file    

4.  **Joint-Genotyping**: takes as input a sample map file to perform variants calling all the provided GVCF files and filtering to produce a multi-sample VCF (minimum of 50 samples is required)   

Scroll down for details on each workflow, including input and output descriptions and requirements, estimated run times and costs, and information on sample data.   

### Workflow Naming Schema

There are two complete sets of workflows. Those with a ""1"" in front use hg38 reference data and those that begin with a ""2"" use b37. The table below lists the workflows and the corresponding reference.


|Workflow Name | Reference Data |
|---|---|
|1-1-Preprocessing-For-Variant-Discovery| hg38 |
|1-2-Haplotypecaller | hg38 |
|1-3-Generate-Sample-Map | hg38 |
|1-4-Joint-Genotyping | hg38 |
|2-1-Preprocessing-For-Variant-Discovery| b37 |
|2-2-Haplotypecaller | b37 |
|2-3-Generate-Sample-Map | b37 |
|2-4-Joint-Genotyping | b37 |


### Input and output data files overview


 **Single sample case**    
This will produce a VCF file for a single sample.

 ![drawing](https://storage.googleapis.com/terra-featured-workspaces/Germline-VariantCalling-JointCalling/images/GVCFdisabled.png)
 
 To run the workflows  in single sample mode:   

1.   Head to the workflows tab
2.   Click on 1-1-Preprocessing-For-Variant-Discovery-HG38 workflow  
		A.  Click on ""Select Data""  
		B.  Select ""Choose existing sets""  
		C.  Check the row with ""1kgp-50-wgs” as the sample_set  
		D.  Click ""OK""  
3. Click on Run analysis  
4. Once the workflow has completed head to the Workflows tab and on to the next workflow.   
5. Click on 1-2-Haplotypecaller-HG38 workflow   
	 A.  Click on ""Select Data""  
	 B.  Select ""Choose existing sets""  
	 C.  Check the row with ""1kgp-50-wgs” as the sample_set  
	 D.  Click ""OK""  
	 E. In the Input tab set the ""make_gvcf"" parameter to `False`
	 F. In the output tab set the output_vcf and output_vcf_index parmeter to `this.downsampled_hg38_vcf` and `this.downsampled_hg38_vcf_index`
6. Click on Run analysis  

**Cohort sample case**
 
 It's possible to generate a multi-sample VCF instead of a single sample VCF by setting the ""make_gvcf"" parameter  in the **Haplotypecaller** workflow to `True`. Haplotypecaller will then create a GVCF which can be used in the preceding workflows to create the multisample VCF. Running all four workflows takes an unmapped BAM (uBAM) input file and returns a multi-sample VCF. The first two workflows are run once for each sample. 

 ![drawing](https://storage.googleapis.com/terra-featured-workspaces/Germline-VariantCalling-JointCalling/images/GVCFenabled.png)


To run the workflows  in cohort mode:   

1.   Head to the workflows tab
2.   Click on 1-1-Preprocessing-For-Variant-Discovery-HG38 workflow  
		A.  Click on ""Select Data""  
		B.  Select ""Choose existing sets""  
		C.  Check the row with ""1kgp-50-wgs” as the sample_set  
		D.  Click ""OK""  
3. Click on Run analysis  
4. Once the workflow has completed head to the Workflows tab and on to the next workflow.   
5. Click on 1-2-Haplotypecaller-HG38 workflow   
	 A.  Click on ""Select Data""  
	 B.  Select ""Choose existing sets""  
	 C.  Check the row with ""1kgp-50-wgs” as the sample_set  
	 D.  Click ""OK""  
	 E. Set the ""make_gvcf"" parameter to `True`
	 F. In the output tab set the output_vcf and output_vcf_index parmeter to `this.downsampled_hg38_gvcf` and `this.downsampled_hg38_gvcf_index`
6. Click on Run analysis  
7. Once the workflow has completed head to the Workflows tab and on to the next workflow.  
8. Click on 1-3-Generate-Sample-Map-HG38 workflow  
  A.  Click on ""Select Data""  
  B.  Check the row with ""1kgp-50-wgs” as the sample_set  
  C.  Click ""OK""  
9. Click on Run analysis  
10. Once the workflow has completed head to the Workflows tab and on to the next workflow.  
11. Click on 1-4-Joint-Genotyping-HG38 workflow  
  A.  Click on ""Select Data""  
  B.  Check the row with ""1kgp-50-wgs” as the sample_set  
  C.  Click ""OK""  
12. Click on Run analysis  

If you would like to run the workflows again from scratch, delete the sample and sample_set table and re-upload both tables from this [google bucket](https://console.cloud.google.com/storage/browser/terra-featured-workspaces/Germline-VariantCalling-JointCalling/?project=broad-dsde-outreach&organizationId=548622027621). 

## Workspace Data 
There are two sets of sample tables in the DATA tab - one corresponding to template input files, and another with examples of processed data (i.e. example_sample_output).

**Template input data**    
These are the  tables you would use to run the workflow pipeline from the beginning. 

- participant - Individual per row. 
This won't be used directly in the workspace but it's best practice to include this in the DATA tab.
- sample - Sample from a participant  per row.      
This will be used by the preprocessing and haplotypecaller workflows (*-1 and *-2). 
- sample_set - A set of samples per row. 
This will be used by the Generate-sample-map and joint genotyping workflows (*-3 and *-4)

**Examples of processed data**    
Once all the workflows have been executed you will see several additional columns within the sample and sample_set tables. Examples of what you should see are in the following tables:      
- example_sample_output
- example_sample_set_output  

## Workflows

###  1-Preprocessing-For-Variant-Discovery  
**What does it do?**    
This WDL takes sequencing data in unmapped BAM (uBAM) format and outputs a clean BAM file and its index, suitable for variant discovery analysis. 

**What does it require as input?**    
The 1-Preprocessing-For-Variant-Discovery workflow accepts a file containing a list of unaligned BAMS. To learn more about how to generate a list file, see [this article](https://support.terra.bio/hc/en-us/articles/360033353952).     

The input data are samples:

* Pair-end sequencing data in unmapped BAM (uBAM) format
* One or more read groups, one per uBAM file, all belonging to a single sample (SM)

Input uBAM files must comply with the following requirements:

* Filenames all have the same suffix (we use "".unmapped.bam"")
* Files must pass validation by ValidateSamFile
* Reads are provided in query-sorted order
* All reads must have an RG tag
* Reference index files must be in the same directory as source (e.g. reference.fasta.fai in the same directory as reference.fasta)    

**If your sequencing data is not in uBAM format** (e.g. FASTQ), check out this file conversion workspace, [https://app.terra.bio/#workspaces/help-gatk/Sequence-Format-Conversion](https://app.terra.bio/#workspaces/help-gatk/Sequence-Format-Conversion) for workflows to convert:    

1. Interleaved FASTQ to paired FASTQ
2. Paired FASTQ to unmapped BAM
3. BAM to unmapped BAM
4. CRAM to BAM files from sequencer output for use in GATK analysis tools

**Sample data description and location**  
The workspace DATA tab contains downsampled 1000 Genome Project unaligned BAM list files in the `sample` table under the column `flowcell_unmapped_bams_list`.   

**What does it return as output?**  
The workflow generates a clean BAM file and its index, suitable for variant discovery analyses and stored in the workspace bucket. Metadata for all outputs are written to the `sample` table in the workspace DATA tab.           
   
**Reference data description and location**  
Required and optional references and resources for the workflows are included in the Workspace DATA tab in the Reference Data tables. The input unmapped BAM samples have yet to be aligned to a reference so they are not restricted to a particular reference. Only after the first workflow will the samples be restricted to working with a reference, because the unmapped BAM files will be mapped after that point. Again there are two sets of each workflow, one set configured with hg38 references and the other set configured with b37 references. 
 
 
**Estimated time and cost to run on sample data**    

| Sample Name | Sample Size | Time | Cost $ |
| :---:  | :---: | :---: | :---: |
| NA12878_24RG_small | 3.11 GB | 1:28:00 | 0.18 |
| NA12878 | 64.89 GB | 22:35:00 | 4.98 |  
| downsampled-1kgp-50-exomes | 32.13 GB | 02:07:00 | 7.29 |       



### 2-Haplotypecaller 
**What does it do?**    
The workflow scatters the HaplotypeCaller tool over a sample (clean BAM file and index, from the previous step) using an intervals list file. In particular, it runs the HaplotypeCaller tool from GATK4 on a single sample according to GATK Best Practices. The output file produced will be a single VCF or GVCF file depending on the mode in which it is run. If a GVCF is produced then it can be used by the joint genotyping workflow.   

**What does it require as input?**       
The workflow accepts:  In particular:     
- One analysis-ready BAM file for a single sample (as identified in RG:SM), pre-processed using GATK Best Practices. 
- A file containing a set of variant calling interval lists for the scatter

**What does it return as output?**        
One VCF or GVCF file and its index      

**Sample data description and location**  
Links to the expected input types are available in the `sample` data table (processed example) for testing. The `sample` data table lists analysis-ready BAM files under the `analysis_ready_bam` column.     

**Reference data description and location**  
Required and optional references and resources for the workflows are included in the Workspace DATA tab in the Reference Data tables. The **1-2_Haplotypecaller** workflow is configured with HG38 references and **2-2_Haplotypecaller** is configured with B37 references. 
          

**Estimated time and cost to run on sample data**      

| Sample Name | Sample Size | Time | Cost $ |
| :---:  | :---: | :---: | :---: |
| NA12878_24RG_small | 4.66 GB | 02:28:00 | 0.21 |
| NA12878  (CRAM)| 19.55 GB | 14:05:00 | 2.24 |    
| NA12878  (BAM)| 68.00 GB | 03:44:00 | 1.37 |    
| downsampled-1kgp-50-exomes | 23.00 GB | 01:12:00 | 24.25 |  
   

### 3-Generate-Sample-Map

**What does it do?**    
This WDL generates a sample_map file, which can be used for the Joint-Genotyping workflow. A sample map is a tab-delimited text file of 2 columns; 1. the name of the sample and 2. the file path (in this case the Google bucket path of the file).

**What does it require as input?**  
- An array of file names
- An array of file paths
- Name of output sample_map 

**What does it return as output?**    
- Sample map file

**Reference data description and location**  
Reference files are not used in this workflow. 


### 4-Joint-Genotyping    
**What does it do?**    
This WDL implements the joint calling and variant quality score recalibration (VQSR) filtering portion of the GATK Best Practices.

**What does it require as input?**  
- GVCFs produced by HaplotypeCaller in GVCF mode
- Bare minimum: 50 samples. Gene panels are not supported

**What does it return as output?**     
A VCF file and its index, filtered using VQSR, with genotypes for all samples present in the input VCF. All sites that are present in the input VCF are retained. Filtered sites are annotated as such in the FILTER field.      
	
**Sample data description and location**     
Links to the expected input types are available in the processed example data table for testing. The 4-Joint-Genotyping workflow accepts one or more GVCFs produced by haplotypecaller.  The `gvcf` column in the data table contains a full-sized GVCF of NA12878 that will be used for the Joint-Genotyping workflow.     

**Reference data description and location**  
Required and optional references and resources for the workflows are included in the Workspace DATA tab in the Reference Data tables. **1-4_JointGenotyping** is configured with hg38 references and **2-4_JointGenotyping** is configured with the b37 reference. 
 

**Estimated time and cost to run on sample data**     

| Sample Name | Sample Size | Time | Cost $ |
| :---:  | :---: | :---: | :---: | 
| downsampled-1kgp-50-exomes | 9.52 GB | 03:17:00 | 1.35 |  
 
### Optional Workflows
Additional workflows have been added for your convenience.   

**Optional-Paired-FASTQ-to-Unmapped-BAM**: 
This WDL converts paired FASTQ to uBAM and adds read group information.

Requirements/expectations
* Pair-end sequencing data in FASTQ format (one file per orientation)
* The following metadata descriptors per sample:
  * readgroup
  * sample_name
  * library_name
  * platform_unit
  * run_date
  * platform_name
  * sequecing_center
Outputs
  * Unmapped BAM

**Optional-Gatk-GatherVCFsCloud**:  
This tool combines together rows of variant calls from multiple VCFs, e.g. those produced by scattering calling across genomic intervals, into a single VCF. This tool enables scattering operations, e.g. in the cloud, and is preferred for such contexts over Picard MergeVcfs or Picard GatherVCfs. The input files need to have the same set of samples but completely different sets of loci. These input files must be supplied in genomic order and must not have events at overlapping positions. 

Input  
* A set (array) of VCF files, sorted by genomic position. Its also possible to provide a file containing a list of VCF files, each row being an individual VCF file path.  

Output  
* A single VCF file containing the variant call records from the multiple VCFs. 

**Optional-ReblockGVCF-gatk4_exomes_goodCompression**:  
Users working with large sample sets can invoke the GnarlyGenotyper task in the JointGenotyping.wdl workflow. However, the [ReblockGVCF](https://gatk.broadinstitute.org/hc/en-us/articles/360037593171-ReblockGVCF-BETA-) tool must be run for all GVCFs produced by HaplotypeCaller before they can be appropriately processed by GnarlyGenotyper. 

Input  
* A GVCF file 

Output  
* Reblocked GVCF file


### Important notes on workflow limitations 
**Small Cohorts**

We believe the results of this workflow run on a single WGS sample are equally accurate, but there may be some shortcomings when the workflow is modified and run on small cohorts.  Specifically, modifying the SNP ApplyRecalibration step for higher specificity may not be effective.  You can verify if this is an issue by consulting the gathered SNP tranches file.  If the listed `truthSensitivity` in the rightmost column is not well matched to the `targetTruthSensitivity` in the leftmost column, then requesting that `targetTruthSensitivity` from ApplyVQSR will not use an accurate filtering threshold.     

Additionally 
- No allele subsetting for the Joint-Genotyping workflow
  - For large cohorts, even exome callsets can have more than 1000 alleles at low complexity/STR sites
  - For sites with more than six alternate alleles (by default) called genotypes will be returned, but without the PLs since the PL arrays get enormous
  - Allele-specific filtering could be performed if AS annotations are present, but the data will still be in the VCF in one giant INFO field
- JointGenotyping output is divided into lots of shards
  - Desirable for use in [Hail](https://hail.is/), which supports parallel import
  - It's possible to use [GatherVcfs](https://app.terra.bio/#workspaces/help-gatk/GATK4-Germline-Preprocessing-VariantCalling-JointCalling/workflows/broad-firecloud-dsde/Optional-Gatk-GatherVCFsCloud) to combine shards.
- GnarlyGenotyper uses a QUAL score approximation
   - Dramatically improves performance compared with GenotypeGVCFs, but QUAL output (and thus the QD annotation) may be slightly discordant between the two tools.

**Exomes**

Currently the workflows are configured for WGS processing. 
The dynamic scatter interval creating task was optimized for genomes.  The scattered SNP VariantRecalibration may fail because of too few ""bad"" variants to build the negative model. Also, apologies that the logging for SNP recalibration is overly verbose.   

The provided tool configurations are meant to be a ready-to-use example of the workflows. It is the user’s responsibility to correctly set the reference and resource input variables using the [GATK Tool and Tutorial Documentations](https://software.broadinstitute.org/gatk/documentation/).

**GnarlyGenotyper**

Users working with large sample sets can invoke the GnarlyGenotyper task in the JointGenotyping.wdl workflow. However, the [ReblockGVCF](https://gatk.broadinstitute.org/hc/en-us/articles/360037593171-ReblockGVCF-BETA-) tool must be run for all GVCFs produced by HaplotypeCaller before they can be appropriately processed by GnarlyGenotyper. A workflow that applies the reblocking tool is provided here: [ReblockGVCF-gatk4_exomes_goodCompression](https://portal.firecloud.org/?return=terra#methods/methodsDev/ReblockGVCF-gatk4_exomes_goodCompression/4)


**Controlling cloud costs**

Note that cost and time estimates will vary with the use of [preemptibles](https://support.terra.bio/hc/en-us/articles/360029772212). Using preemptibles can save up to 80% on compute costs.  For further helpful hints on controlling cloud costs, see [this article](https://support.terra.bio/hc/en-us/articles/360029748111) and for additional ways to estimate cloud cost use [Google's cost calculator](https://cloud.google.com/products/calculator/).

### Software Versions  
- GATK 4.2.4.0
- BWA 0.7.15-r1140
- Picard 2.16.0-SNAPSHOT
- Samtools 1.3.1 (using htslib 1.3.1)
- Python 2.7

---

### Contact information  
For questions about this workspace please visit the Featured Workspaces Topic topic on the [Terra Community Forum](https://broadinstitute.zendesk.com/hc/en-us/community/topics). Use the search box to see if other users have asked the same question previously. If not, post and tag **@Samantha** so that we get notified.

This material is provided by the GATK Team. Please post any questions or concerns regarding the GATK tool to the [GATK forum](https://gatk.broadinstitute.org/hc/en-us/community/topics)


### Workspace Citation 
Details on citing Terra workspaces can be found here: [How to cite Terra](https://support.terra.bio/hc/en-us/articles/360035343652)

Data Sciences Platform, Broad Institute (*Year, Month Day that this workspace was last modified*) help-gatk/GATK4-Germline-Preprocessing-VariantCalling-JointCalling [workspace] Retrieved *Month Day, Year that workspace was retrieved*,  https://app.terra.bio/#workspaces/help-gatk/GATK4-Germline-Preprocessing-VariantCalling-JointCalling

### License  
**Copyright Broad Institute, 2019 | BSD-3**  
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at https://github.com/openwdl/wdl/blob/master/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.


### Workspace Change Log
| Date | Change | Author |
| --- | --- | --- |
|  2020-01-14 | Initial feature of workspace | Beri Shifaw |
|  2020-02-24 | Added Optional-ReblockGVCF-gatk4_exomes_goodCompression workflow | Beri Shifaw |
|  2020-04-02 | Updated Preprocessing-For-Variant-Discovery to v2.0.0, Reuploaded HC and Generate Map workflow  | Beri Shifaw |
|  2020-06-22 | Added workflow overview image to dashboard and additional description to workflows, Updated Joint Genotyping workflow, removed ""GVCF"" from Haplotypecaller name to indicate the workflow can be run to create VCF  | Beri Shifaw |
|  2020-09-28 | Updated Haplotypecaller and JointGenotype workflow to release 2.2.0  | Beri Shifaw |
|  2020-11-28 | Updated Haplotypecaller to release 2.3.0, updated JointGenotype workflow source to Warp repo, added GATK Notebook tutorials | Beri Shifaw |
|  2020-12-07 | Updated data tables to use downsample wgs  | Beri Shifaw |
|  2021-01-29 | Updated dashboard to refer to [Whole Genome Analysis Pipeline Workspace](https://app.terra.bio/#workspaces/warp-pipelines/Whole-Genome-Analysis-Pipeline) | Beri Shifaw |
|  2021-09-15 | Addition of Workspace Citation section | Beri Shifaw |
|  2023-04-11 | Updated with data and notebooks for more GATK workshop topics. | Jonn Smith |",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","broad-firecloud-dsde-methods/2023_Cambridge_Workshop - GATK4-Germline-Preprocessing-VariantCalling-JointCalling"
143,"broad-firecloud-tcga","TCGA_DiagnosticPathologySlides_V1-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_DiagnosticPathologySlides_V1-0_DATA",TRUE,TRUE,"cancer","Tumor/Normal","USA","This workspace contains references to TCGA Diagnostic Images from the GDC’s Legacy Archive.  The TCGA program generated two types of slide images: tissue slide images and diagnostic slide images.  The tissue slide images were generated from a thin slice of the snap-frozen block of tissue collected from each donor .   The snap-frozen tissue blocks were the source of DNA and RNA and the pathology review of the tissue slide images determined whether a frozen tissue block from which the slide orginated was qualified to make DNA/RNA for TCGA.  

Diagnostic slide images were generated from a thin slice of the formalin-fixed paraffin embedded (FFPE) tisse blocks that were used by hospitals for patient diagnosis.  These FFPE blocks of tissue did not provide any RNA or DNA for the TCGA project.  The tissue for producing a diagnostic slide was geographcially adjacent to the tissue for producing the corresponding tissue slide.

Ths snap freezing process bursts many of the cell nuclei; hence the tissue slide images are not amenable to processing with computational workflows for extracting nuclear morphometry features.  FFPE preservation, however, preserves nucleus integrity.  Thus, the firecloud workspace we created for the analysis of TCGA pathology imaging data contains references to the Diagnostic Slide Images, not the Tissue Slide Images  

The GDC Legacy Archive contains both Diagnostic and Tissue Slide Images (the GDC’s repository of hg38 harmonized data does not include TCGA image data).  Unfortunately, while the Tissue Slide Images are linked to their corresponding TCGA cases, the Diagnostic Slide Images are not; i.e., there is no metadata linking a diagnostic slide image to its corresponding case.  The GDC is aware of this problem and has committed to addressing the problem in a future data release.  Nevertheless, this presented a challenge for the creation of workspaces organizing TCGA Diagnostic Image files.  

Leveraging the fact that TCGA barcodes were embedded in the SVS (image) file names, we were able to link the diagnostic image files to GDC case UUIDs.  The workspace contains participant entities, identified by their GDC-defined UUIDs, and participant sets for each disease cohort.  Each participant entity contains a reference to the Diagnostic Slide Image file in the legacy archive.   Users wanting to conduct analysis on these image files will need to clone the workspace and run the GDC file downloader workflow to retrieve the desired image files.  The workspace contains a method configuration for doing this.  The workspace also contains a method configuration for running Lee Cooper’s HistXtract pipeline for extracting nuclear morphometry features from these slide images.

Once the GDC corrects the linking of the diagnostic slide images into the legacy archive’s data model, we will construct new workspaces that integrate genomic and image data.
","NCI","Chet Birger","This workspace contains references to TCGA Diagnostic Images from the GDCs Legacy Archive. The TCGA program generated two types of slide images: tissue slide images and diagnostic slide images. The tissue slide images were generated from a thin slice of the snap-frozen block of tissue collected from each donor . The snap-frozen tissue blocks were the source of DNA and RNA and the pathology review of the tissue slide images determined whether a frozen tissue block from which the slide orginated was qualified to make DNA/RNA for TCGA. Diagnostic slide images were generated from a thin slice of the formalin-fixed paraffin embedded (FFPE) tissue blocks that were used by hospitals for patient diagnosis. These FFPE blocks of tissue did not provide any RNA or DNA for the TCGA project. The tissue for producing a diagnostic slide was geographcially adjacent to the tissue for producing the corresponding tissue slide. Ths snap freezing process bursts many of the cell nuclei; hence the tissue slide images are not amenable to processing with computational workflows for extracting nuclear morphometry features. FFPE preservation, however, preserves nucleus integrity. Thus, the firecloud workspace we created for the analysis of TCGA pathology imaging data contains references to the Diagnostic Slide Images, not the Tissue Slide Images The GDC Legacy Archive contains both Diagnostic and Tissue Slide Images (the GDCs repository of hg38 harmonized data does not include TCGA image data). Unfortunately, while the Tissue Slide Images are linked to their corresponding TCGA cases, the Diagnostic Slide Images are not; i.e., there is no metadata linking a diagnostic slide image to its corresponding case. The GDC is aware of this problem and has committed to addressing the problem in a future data release. Nevertheless, this presented a challenge for the creation of workspaces organizing TCGA Diagnostic Image files. Leveraging the fact that TCGA barcodes were embedded in the SVS (image) file names, we were able to link the diagnostic image files to GDC case UUIDs. The workspace contains participant entities, identified by their GDC-defined UUIDs, and participant sets for each disease cohort. Each participant entity contains a reference to the Diagnostic Slide Image file in the legacy archive. Users wanting to conduct analysis on these image files will need to clone the workspace and run the GDC file downloader workflow to retrieve the desired image files. The workspace contains a method configuration for doing this. The workspace also contains a method configuration for running Lee Coopers HistXtract pipeline for extracting nuclear morphometry features from these slide images. Once the GDC corrects the linking of the diagnostic slide images into the legacy archives data model, we will construct new workspaces that integrate genomic and image data.","0",NA,"TCGA",NA,"GRU",NA,"TCGA Diagnostic Side Images","Diagnostic Slide Image","broad-firecloud-tcga/TCGA_DiagnosticPathologySlides_V1-0_DATA"
145,"help-terra","International_Neuroimmune_Consortium_Workshop_2022","READER","https://app.terra.bio/#workspaces/help-terra/International_Neuroimmune_Consortium_Workshop_2022",TRUE,FALSE,NA,NA,NA,"# International Neuroimmune Consortium Workshop
## September 20th, 2022

This tutorial workspace is the hands-on component of the **International Neuroimmune Consortium Intro to Terra Workshop** held on September 20, 2022.

It includes a step-by-step guide to analyzing mouse single-cell data in Terra. Using this workspace, you will:

1. Filter, normalize, and cluster a raw count matrix with the **Cumulus workflow**.
2. Integrate and explore single-cell data in the **integrative_analysis Jupyter Notebook**.
3. Learn tips and tricks for using workspace data in a Cloud Environment with the **Intro-to-Cloud-Environment notebook** and the **Intro to FISS API in R** notebook.


# Instructions

## Set up the tutorial workspace 

Create your own editable copy (clone) of this workspace:

1. Click the round circle with three dots in the upper right corner of the workspace.
2.  Choose **Clone**.


![](https://storage.googleapis.com/terra-featured-workspaces/CFOS/Clone.png)


## Step 1. Filter, cluster and normalize a raw count matrix using Cumulus

### Cumulus Overview

Cumulus is a cloud-based analysis workflow for large-scale single-cell and single-nucleus RNA-seq data ([Li et al. 2020](https://doi.org/10.1038/s41592-020-0905-x)). 

See the [Cumulus Featured Workspace](https://app.terra.bio/#workspaces/kco-tech/Cumulus) for additional details on using Cumulus on Terra or read the [Cumulus documentation ](https://cumulus.readthedocs.io/en/stable/) for workflow parameters. 

### Sample data
This step uses a single raw cell-by-gene count matrix as Cumulus input, but additional matrices could be added to the set. 
The example matrix is available from the BRAIN Initiative's Cell Census Network (BICCN) and is derived from mouse motor cortex ([Yao et al., 2021](https://pubmed.ncbi.nlm.nih.gov/34616066/)). The associated files are in mtx format and links to the files are available in the workspace **library** data table. 

The files and metadata are stored in a public Google bucket which you can access at the URI gs://terra-featured-workspaces/CFOS/BICCN_data_for_Cumulus.

**This workspace data is for demo purposes only**. All donor and biosample metadata presented in the workspace data tables is synthetic and is not associated with actual sequencing reads presented in the library table.

### Running Cumulus
There are two configurations available for testing the workflow:
* The **Cumulus_Hands_on_Exercise** configuration is a hands-on exercise that walks you through setting up the configuration. 
* The **Cumulus** configuration is a preconfigured workflow that you can run the same way you run the hands-on exercise. This configuration allows you to check your work.

To do the hands-on activity and run the workflow, follow the quick-start instructions below: 

#### Quick-start instructions
1. Select the workspace **Data tab**.
2. Click the **library_set** data table.
3. Select the checkbox left of the **BICCN_data** library set.
4. Choose ""**Open with**"" above the table.
5.  Select **Workflow**.
6.  Select the **Cumulus_Hands_On_Exercise workflow**.
7.  Go to the **Inputs** section of the workflow configuration.
8.  Set up the following input variables:
| Input variable | Attribute |
| --- | --- |
| `input_file` | this.sample_map |
| `output_directory` | Paste your workspace Bucket ID where it says PASTE_BUCKET_ID_HERE (""gs://PASTE_BUCKET_ID_HERE/Cumulus/""). The final attribute should look similar to ""gs://fc-a8dd893f-84cf-4d28-952d-be4ae5805ebf/Cumulus/"". |
| `output_name` | workspace.Name_for_BICCN_data |
10. Select **Save**.
11. Choose the **Outputs** tab.
12. Set the output variable `output_de_xlsx` to `this.my_ouput`.
13. Select **Save**.
14. Select **Run**.
15. Select **Launch**.

### Outputs

The Cumulus workflow in this workspace is set up to output files to the Cumulus folder in the Files section of the Data tab. Additionally, the workflow is configured to write an excel file with the differential expression results to the library_set data table. 


## Step 2. Explore single-cell data using Jupyter Notebooks
There are three notebooks available in this workspace: 

* The **integrative_analysis** notebook combines single-cell data across multiple. data sets, allowing you to integrate different cell populations.

* The **Intro-to-Cloud-Environments** notebook provides hands-on tips and tricks for manipulating workspace and cloud data in your custom Terra Cloud Environment.

* The **Intro to FISS API in R** notebook provides hands-on tips and tricks for manipulating workspace and cloud data in your custom Terra Cloud Environment.


### Quick-start instructions
1. Select the Environment Configuration icon to the right of the workspace.
2. Under the Jupyter option, select **Environment Settings**.
3. Select **R/Bioconductor** image from the Application Configuration drop-down.
4. In the **Cloud compute profile**, select **4 CPUs** from the CPU drop-down.
5. Create the Environment.
6. Go to the workspace **Analyse**, page and open the notebook of choice in edit mode.
7. Follow the instructions listed in the notebook.

## Workspace costs and timing

| Workflow or notebook | Timing | Cost $ |
| --- | --- | --- |
| Cumulus | 14 min | 0.07 | 
| integrative_analysis | < 1 hr | < 0.96 |
| Intro-to-Cloud-Environments | ~15 min | ~0.05 |
| Intro to FISS API in R | ~ 15 min | ~0.05 |

## Additional Resources
* See the list of all workspaces and data available for the International Neuroimmune Consortium in this Collection workspace: https://app.terra.bio/#workspaces/ad-data-platform/International%20Neuroimmune%20Consortium%20Data%20Collection

Additional workspaces include:
* [iNPH data set](https://app.terra.bio/#workspaces/broad-cfos-data-platform1/iNPH-Kuopio)
* [iMGL data set](https://app.terra.bio/#workspaces/broad-cfos-data-platform1/CIRM%20-%20iMGL)
* [UK DRI data set](https://app.terra.bio/#workspaces/broad-cfos-data-platform1/Imperial%20-%20UK%20DRI)

### References
Butler, A., Hoffman, P., Smibert, P. et al. Integrating single-cell transcriptomic data across different conditions, technologies, and species. Nat Biotechnol 36, 411–420 (2018). https://doi.org/10.1038/nbt.4096

Li, B., Gould, J., Yang, Y. et al. Cumulus provides cloud-based data analysis for large-scale single-cell and single-nucleus RNA-seq. Nat Methods 17, 793–798 (2020). https://doi.org/10.1038/s41592-020-0905-x

Stuart T, Butler A, Hoffman P, Hafemeister C, Papalexi E, Mauck WM 3rd, Hao Y, Stoeckius M, Smibert P, Satija R. Comprehensive Integration of Single-Cell Data. Cell. 2019 Jun 13;177(7):1888-1902.e21. doi: 10.1016/j.cell.2019.05.031. Epub 2019 Jun 6. PMID: 31178118; PMCID: PMC6687398.

### Acknowledgements
Special thanks to Brian Herb and the NeMO team, the Cumulus team (Bo Li and Yiming Yang), Sid Cox, and Vahid Gazestani for their feedback and amazing contributions to this workspace.

### License
**Copyright Broad Institute, 2022 | BSD-3**

All rights reserved. Full license text available [here](https://github.com/broadinstitute/warp/blob/master/LICENSE). Note that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.

---









",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-terra/International_Neuroimmune_Consortium_Workshop_2022"
146,"ASHG2022","HPRC-Giraffe-Demo-ASHG2022","READER","https://app.terra.bio/#workspaces/ASHG2022/HPRC-Giraffe-Demo-ASHG2022",TRUE,FALSE,NA,NA,NA,"This is a demonstration workspace for a workshop at the American Society of Human Genetics (ASHG) 2022 meeting. The workspace demonstrates variant calling using the Human Pangenome Reference Consortium's (HPRC) year 1 Minigraph/CACTUS pangenome with the Giraffe/DeepVariant pipeline for calling germline small variants. This workspace is intended to be a demonstration of utilizing a pangenome from the HPRC in AnVIL and Terra. Slides presented during the workshop can be found [here](https://drive.google.com/file/d/1YvU9ejr46O_keTSO5jJZ1vJ42Z-lpvTR/view?usp=sharing).


Note that the WDL included in the workspace is still under active development and has not been peer-reviewed. For up-to-date best practices for both Giraffe and DeepVariant please see the links included in the relevant sections below.

## Tasks

1. Clone workspace
2. Have a look at the ""sample"" Table, the ""Workspace Data"", and the ""Files""
3. Launch a workflow on the sample in the ""sample"" Table.
4. Monitor running job
5. Notebook
   1. Launch an Analyses jupyter notebook
   2. Once the workflow job is done (3-4 above), start running code block in the notebook.
	 3. Discuss pangenome (+reads) representations. What's going on?
	 4. Blast a read to GRCh38. Why would GRCh38 lead to false-negative in this region?

Side tasks:

1. Find the GiraffeDeepVariant workflow in dockstore
2. Find data in AnVIL, e.g. 1000GP reads, HPRC assemblies/pangenomes

Detailed instructions for running this workspace can be downloaded from [here](https://docs.google.com/document/d/1yuRIsy7XRxS-pwQ2kfmiMKLPkdQ_6WrYh4d3Kp6Zyyg/edit?usp=sharing).

## Data

For this demonstration, we will work with a region of chromosome 1 containing, among others, the RHCE gene which is a a challenging medically-relevant gene. ""Challenging medically-relevant genes"" are difficult to assess with short-read sequencing but are part of a recent benchmark truthset ([CRMG v1.0](https://www.nature.com/articles/s41587-021-01158-1)). The genomic interval extracted was `chr1:25053647-25685365` ([see in the UCSC Genome Browser](https://genome.ucsc.edu/cgi-bin/hgTracks?db=hg38&position=chr1%3A25053647%2D25685365)). 
The reads and pangenome in this workspace correspond to this slice of the genome.

### HG002 Illumina Reads (30X)

The sample-based data we will use in this workspace is loaded into the sample data table. An Illumina dataset (produced by Google and made publicly available) with 30X coverage of HG002 has been sliced on the relevant region (as explained above) and imported into the workspace. The sample table has multiple columns and the columns with the prefix input_ (e.g. input_fastq_1) are used as inputs for the Giraffe/DeepVariant workflow. The workflow has been prerun in a separate table ""sample_prerun"" for convenience, and the outputs have been written to columns with names prepended with ""output_"".

### Minigraph/CACTUS Pangenome

This workspace uses one of the HPRC's year 1 pangenomes created with the [Minigraph/CACTUS pipeline](https://github.com/ComparativeGenomicsToolkit/cactus/blob/master/doc/pangenome.md). For the Giraffe/DeepVariant pipeline, it is recommended to use a filtered version of the GRCh38-based graph. Information about the HPRC's pangenome releases can be found in the HPRC's [publicly available AnVIL workspace](https://app.terra.bio/#workspaces/anvil-datastorage/AnVIL_HPRC) as well as the HPRC's [pangenome resources GitHub repo](https://github.com/human-pangenomics/hpp_pangenome_resources).

For this demonstration, we extracted the sub-graph of the chr1 region from the non-filtered GRCH38-based pangenome, in order to deal with only one pangenome for both variant calling and visualization of all the haplotypes.

### DeepVariant (DV) Model

DeepVariant uses a learned model to call variants in aligned sequencing data. A model has been imported into this workspace and can be found in the Data tab under ```Files --> dv-giraffe-model/.```. The model was trained with DV 1.3 on the GRCh38-based Cactus-Minigraph pangenome, on ~30-40x coverage read sets.

## Workflow: Variant Calling With a Pangenome & Giraffe/DeepVariant


### [GiraffeDeepVariant](https://dockstore.org/workflows/github.com/vgteam/vg_wdl/GiraffeDeepVariantLite:giraffe-dv-dt-hprcy1)

This workflow aligns reads to a pangenome graph and uses Google's DeepVariant caller to produce an output VCF. The steps are summarized below:

1. Split reads
2. Align reads to pangenome with Giraffe
3. Realign around InDels (optional)
4. Call variants with DeepVariant


#### Recommended Inputs

**This workflow has a number of inputs. For convenience the recommended workflow inputs have been pre-populated in the workflow inputs tab. Alternatively, users may navigate to the [example input json](https://drive.google.com/file/d/1zYfiUPYS8ZaWnWhaHMNnGCYfu5XRHlZj/view?usp=sharing) and [example output json](https://drive.google.com/file/d/1gaDxfT2u0U5avmpGet_KdJhD9M4ex6bX/view?usp=sharing) included as part of this workspace.**


#### Outputs

* output_vcf: VCF output from DeepVariant
* output_calling_gam: reads aligned to the pangenome with Giraffe as a GAM file.

#### Time and cost estimates    
Note that actual time and cost may vary due to the use of preemptible instances. 

| Input Coverage | Time | Cost |
| -------- | -------- | ---------- |
| 30X | < 5 minutes | $1 |

*(for the subset data included in this workspace)*

## Notebook: Visualize pangenome and reads around a called variant

The notebook can be found in the *ANALYSES* tab.

The ""Environment configuration"" to run it should be:

- *Jupyter* environment
	- Custom environment
		- *Application configuration*: *Custom environment*
		- *Container image*:  `jmonlong/terra-notebook-vg:1.0` 

### Alternative: interactive visualization with the SequenceTubeMap

The approach showed in the notebook can be useful to look at a small subgraph or automate image creation. 
For other use cases, we tend to use the interactive (and better-looking) [SequenceTubeMap](https://github.com/vgteam/sequenceTubeMap). 
Below is what the notebook example would look like:

![](http://public.gi.ucsc.edu/~jmonlong/hprc/ashg2022-hprc-workshop-tubemap-example.small.png)


## Next Steps
To run this pipeline on your own data you have to upload your data to the Google Cloud bucket for your version of the workspace. You can find the bucket information in the Google Bucket section on the right hand side of this page. In that section you can find the bucket name for gsutil commands, or you can open the bucket in your web browser. You can also (optionally) create a new data table in your workspace which points to the data you uploaded.

----
----
## Authors and contact information

This workspace is a product of the Human Pangenome Reference Consortium [HPRC](https://humanpangenome.org/) and [the AnVIL](https://anvilproject.org/). Contributors include:

* Jean Monlong: jmonlong@ucsc.edu (UCSC Computational Genomics Lab)
* Julian Lucas: juklucas@ucsc.edu (UCSC Computational Genomics Platform)

The Giraffe/DeepVariant workflow was developed by the VG team at UCSC and Google. Contributors include:

* Charles Markello (UCSC)
* Jean Monlong (UCSC)
* Adam Novak (UCSC)
* Maria Nattestad (Google)
* Pichuan Chang (Google)
* Andrew Carroll (Google)

## Additional generally helpful resources

* **[HPRC Pangenome GitHub](https://github.com/human-pangenomics/hpp_pangenome_resources)**   
    Description of HRPC's currently available (release) pangenomes.   
		 
		
* **For helpful hints on controlling cloud costs**, see [this article (https://support.terra.bio/hc/en-
us/articles/360029748111)](https://support.terra.bio/hc/en-us/articles/360029748111).      
 

 ## Citations
 
1. Sirén, Jouni, et al. ""Pangenomics enables genotyping of known structural variants in 5202 diverse genomes."" Science 374.6574 (2021): abg8871.
2. Poplin, Ryan, et al. ""A universal SNP and small-indel variant caller using deep neural networks."" Nature biotechnology 36.10 (2018): 983-987.

### Change log  
24 October 2022 - Initial release",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","ASHG2022/HPRC-Giraffe-Demo-ASHG2022"
147,"anvil-datastorage","1000G-high-coverage-2019","READER","https://app.terra.bio/#workspaces/anvil-datastorage/1000G-high-coverage-2019",TRUE,TRUE,"None","Parent-Offspring Trios",NA,"Data Release notes for the workspace

1000 Genomes 3202 phase 3, Update May 23, 2023

From March 8, 2023 to May 8, 2023, the following files below were temporarily unavailable. As of May 8, 2023, these files have been restored. 

HG00257.final.cram
HG00369.final.cram
HG00657.final.cram
HG00684.final.cram
HG01079.final.cram
HG01110.final.cram
HG01122.final.cram
HG01491.final.cram
HG01697.final.cram
HG01777.final.cram
HG01976.final.cram
HG02164.final.cram
HG02312.final.cram
HG02380.final.cram
HG02512.final.cram
HG02557.final.cram
HG02681.final.cram
HG02769.final.cram
HG03063.final.cram
HG03563.final.cram
HG03844.final.cram
HG03898.final.cram
HG03937.final.cram
HG04141.final.cram
NA18912.final.cram
NA19060.final.cram
NA19238.final.cram
NA19360.final.cram
NA19474.final.cram
NA20790.final.cram
NA21126.final.cram
HG00408.final.cram
HG00639.final.cram
HG01096.final.cram
HG01529.final.cram
HG02821.final.cram
HG03047.final.cram
HG03487.final.cram
NA18487.final.cram
NA18500.final.cram
NA20279.final.cram
HG01141.haplotypeCalls.er.raw.vcf.gz
NA10830.haplotypeCalls.er.raw.vcf.gz
NA18863.haplotypeCalls.er.raw.vcf.gz


1000 Genomes 3202 phase 3 panel samples sequenced to high coverage
==================================================================

This policy refers to 30x Illumina NovaSeq sequencing of 3202 samples from the 1000 Genomes project phase 3 sample set, along with 698 family members that complete trios in the phase 3 data . These data were generated at the New York Genome Center with funds provided by NHGRI Grant 3UM1HG008901. Please email service@nygenome.org with questions or interest in undertaking collaborative analysis of this dataset. All cell lines were obtained from the Coriell Institute for Medical Research and were consented for full public release of genomic data. Please see Coriell (https://www.coriell.org) for more information about specific cell lines. The following cell lines/DNA samples were obtained from the NIGMS Human Genetic Cell Repository at the Coriell Institute for Medical Research: [NA06984, NA06985, NA06986, NA06989, NA06994, NA07000, NA07037, NA07048, NA07051, NA07056, NA07347, NA07357, NA10847, NA10851, NA11829, NA11830, NA11831, NA11832, NA11840, NA11843, NA11881, NA11892, NA11893, NA11894, NA11918, NA11919, NA11920, NA11930. NA11931, NA11932, NA11933, NA11992, NA11994, NA11995, NA12003, NA12004, NA12005, NA12006, NA12043, NA12044, NA12045, NA12046, NA12058, NA12144, NA12154, NA12155, NA12156, NA12234, NA12249, NA12272, NA12273, NA12275, NA12282, NA12283, NA12286, NA12287, NA12340, NA12341, NA12342, NA12347, NA12348, NA12383, NA12399, NA12400, NA12413,, NA12414, NA12489, NA12546, NA12716, NA12717, NA12718, NA12748, NA12749, NA12750, NA12751, NA12760, NA12761, NA12762, NA12763, NA12775, NA12776, NA12777, NA12778, NA12812, NA12813, NA12814, NA12815, NA12827, NA12828, NA12829, NA12830, NA12842, NA12843, NA12872, NA12873, NA12874, NA12878, NA12889, NA12890, NA12376, NA10838, NA12329, NA10852, NA10840, NA12386, NA12864, NA12801, NA12344, NA10861, NA07029, NA12753, NA12832, NA12485, NA12802, NA12739, NA10856, NA10845, NA12818, NA10831, NA12766, NA10864, NA10843, NA12877, NA12335, NA12817, NA12752, NA12767, NA10855, NA12707, NA10857, NA10839, NA12740, NA10837, NA10836, NA07348, NA11993, NA12057, NA11839, NA06993, NA07014, NA06995, NA12146, NA12865, NA10859,
NA06991, NA12336, NA10860, NA12145, NA07045, NA07349, NA07031, NA07345, NA12891, NA07055, NA07435, NA10835, NA12274, NA12875, NA10842, NA12239, NA10830, NA12056, NA11917, NA12892, NA06997, NA07022, NA12264, NA11891, NA07034, NA12248, NA10865, , NA10863, NA10854, NA11882, NA07346, NA07019, NA12343, NA10846].

If using this data please acknowledge that: â€œThese data were generated at the New York Genome Center with funds provided by NHGRI Grant 3UM1HG008901-03S1.â€  Additionally, if data from any of the cell lines listed were used, then they should be cited specifically as coming from NIGMS.

For more general information about the consent for the samples in this data set, please see http://www.internationalgenome.org/about#g1k_data_reuse.

For general enquiries, please contact info@1000genomes.org.


==================================================================

1000 Genomes Processing README

This	README	contains	information	relating	to	data	associated	with	the	1000	Genomes	
resequencing	done	at	New	York	Genome	Center.
Alignment, post-processing and variant calling
Alignment	and	post-processing	are	performed	exactly	as	outlined	by	the	Center	for
Common	Disease	Genomics	project:	https://github.com/CCDG/PipelineStandardization/blob/master/PipelineStandard.md .
Programs and reference data
The	data	was	aligned	to	the	reference	genome	using	the	following	programs	and	reference	
datasets:
1. BWA-MEM bwakit-0.7.15
2. Samtools-1.3.1
3. Picard-2.4.1
4. GATK-3.5-0
5. Resource	files
– All	the	resource	files	used	in	the	analysis	can	be	obtained	here:	
https://console.cloud.google.com/storage/browser/genomics-publicdata/resources/broad/hg38/v0/ .
Reference genome: GRCh38 with alternative sequences, plus decoys and HLA
The	reference	genome	that	the	data	was	aligned	to	can	be	obtained	here:	
ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/GRCh38_reference_genome
/GRCh38_full_analysis_set_plus_decoy_hla.fa

==================================================================

Analysis work is being done by a number of groups, working toward variant calling, including identification of structural variation.

Initial analysis has been done by NYGC, including aligning the data to GRCh38, creating the CRAMs in ENA. The document [NYGC_b38_pipeline_description.pdf](http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/1000G_2504_high_cov/NYGC_b38_pipeline_description.pdf) contains a description of that analysis work and details of the alignment pipeline.

Should you have questions about this data please contact info@1000genomes.org
","AnVIL Team","AnVIL Team","High Coverage 1000G Genomes and VCFs made available through the NHGRI Anvil.","3202","n/a","1000Genomes",NA,"NRES",NA,"1000 Genomes","Whole Genome","anvil-datastorage/1000G-high-coverage-2019"
148,"broad-firecloud-dsde-methods","FunctionalEquivalence","READER","https://app.terra.bio/#workspaces/broad-firecloud-dsde-methods/FunctionalEquivalence",TRUE,FALSE,NA,NA,NA,"# Functional Equivalence Workflow

*Michael Gatzen, Geraldine Van der Auwera, Takuto Sato, Christopher Kachulis*

This workflow performs an evaluation of *functional equivalence*. Prompted by scientific need to combine results from multiple sources into larger datasets, functional equivalence is answering the question of how we can ensure that genomic data from different sources, processed with different pipelines, can be used interchangeably without risking batch effects.


For more information, please find the description of this workflow on [GitHub](https://github.com/broadinstitute/palantir-workflows/tree/main/FunctionalEquivalence).

## Data

This workflow is demonstrated with data from the [Genome in a Bottle project](https://www.nist.gov/programs-projects/genome-bottle) [1]. Specifically, the raw Illumina sequencing for NA12878 and HG002 was downloaded to a coverage of ~30x and then processed using the [WARP pipeline](https://broadinstitute.github.io/warp/), once with GATK 3.5 and once with GATK 4.2.0.0.

[1] Zook, J., Catoe, D., McDaniel, J. et al. Extensive sequencing of seven human genomes to characterize benchmark reference materials. *Sci Data 3*, 160025 (2016). [https://doi.org/10.1038/sdata.2016.25](https://doi.org/10.1038/sdata.2016.25)

## Workflows

### FunctionalEquivalence

#### What does it do?
This workflow performs the functional equivalence evaluation, including all necessary comparisons between replicates and the truth.

#### What does it require as input?
For a current list of inputs, please refer to the description of this workflow on [GitHub](https://github.com/broadinstitute/palantir-workflows/tree/main/FunctionalEquivalence).

#### What does it return as output?
For a current list of outputs, please refer to the description of this workflow on [GitHub](https://github.com/broadinstitute/palantir-workflows/tree/main/FunctionalEquivalence).

#### Reference/Resource data description and location
This workspace uses hg38 (aka GRCh38) as a reference. The required files are publicly accessible and available in this [Google bucket](https://console.cloud.google.com/storage/browser/gcp-public-data--broad-references/hg38/v0).

#### Estimated time and cost to run on sample data

| Sample set | No. Replicates | Time | Cost $ |
| --- | --- | --- | --- |
| HG002 | 3 | 90 min | 2.88 |
| HG002 DRAGEN | 3 | 90 min | 2.30 |

## Contact Information
For any questions or comments, please reach out any time to [Michael Gatzen](https://github.com/michaelgatzen) (email in GitHub profile).

### Workspace Change Log
| Date | Change | Author |
| --- | --- | --- |
2021-07-30 | Initial setup of workspace | Michael Gatzen |
2021-10-04 | Preparation for featuring workspace | Michael Gatzen |
2023-04-17 | Adding DRAGEN-GATK comparisons | James Emery |",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","broad-firecloud-dsde-methods/FunctionalEquivalence"
149,"help-gatk","Introduction-to-Anvil-CCDG-Dataset","READER","https://app.terra.bio/#workspaces/help-gatk/Introduction-to-Anvil-CCDG-Dataset",TRUE,FALSE,NA,NA,NA,"# Workspace Overview    

Use this workspace to practice accessing and analysing CCDG data with some basic analysis Tools (workflows for processing data) and interactive notebooks. For example, you can create a multi-sample VCF using the Tools 1-Haplotypecaller-GVCF and 2-Joint-Discovery. You can then use the VCF for further analysis in the sample notebook.     


## What is AnVIL?

The NHGRI Genomic Data Science [Analysis, Visualization, and Informatics Lab-space](https://www.genome.gov/Funded-Programs-Projects/Computational-Genomics-and-Data-Science-Program/Genomic-Analysis-Visualization-Informatics-Lab-space-AnVIL) (AnVIL) is a scalable and interoperable resource to help accelerate research within the global genomic scientific community. The cloud-based infrastructure democratizes genomic data access and enables seamless sharing and computing on and across large datasets generated by [the National Human Genome Research Institute](https://www.genome.gov/27563570/) (NHGRI) programs and initiatives funded by the National Institutes of Health (NIH) or by other agencies that support human genomics research. One of NHGRI’s programs is the CCDG dataset (see description below).


## What is the CCDG Dataset?   

[Centers for Common Disease Genomics](https://www.genome.gov/Funded-Programs-Projects/NHGRI-Genome-Sequencing-Program/Centers-for-Common-Disease-Genomics) (CCDG) is a collaborative, large-scale genome sequencing effort to comprehensively identify rare risk and protective variants contributing to multiple common disease phenotypes. Funded by NHGRI, this initiative will explore a range of diseases with the ultimate goals of:

* Better understanding the general genomic architecture underlying common, complex inherited diseases. Key to this understanding are sufficient numbers of large data sets representing  distinct examples of disease architectures and study designs.   
* Understanding how best to design rare variant studies for common diseases.
* Developing resources, informatics tools, and innovative approaches and technologies for a range of disease research communities and the wider biomedical research community.

To find out more about what datasets are coming into the AnVIL, visit the [AnVIL project website](http://anvilproject.org/).

## Data Access
Access to the CCDG Dataset is restricted to registered users. To register, please visit the [Anvil Project Site](https://anvilproject.org/) for more details.

Once you have access, you’ll be able to browse the CCDG Datasets in the [Broad Data Libary](https://portal.firecloud.org/?return=terra#library). In the library the CCDG dataset is composed of ~55K samples divided into cohorts amongst 53 data workspaces.

# Data

**How to Access CCDG Sample Data**  
For hands-on experience accessing CCDG data, copy metadata from the Data Library to this workspace by following the instructions below.    
- You’ll first need to make a [clone](https://broadinstitute.zendesk.com/hc/en-us/articles/360026130851-How-to-share-or-clone-a-workspace) of this workspace.
- Go to [Broad’s Data Libary](https://portal.firecloud.org/?return=terra#library).      
- Once in the Data Library, search for ""CCDG"" in the top left corner search box.    
- For this example we will use the CCDG_Broad_Kathiresan_CVD_BRAVE_WGS data workspace but you should be able to perform these steps with other CCDG workspaces. Open the  [CCDG_Broad_Kathiresan_CVD_BRAVE_WGS](https://app.terra.bio/#workspaces/anvil-datastorage/CCDG_Broad_CVD_Stroke_BRAVE_WGS) data workspace by clicking on the cohort name. If you are having trouble viewing or opening this workspace, please visit the [Anvil Project Site](https://anvilproject.org/)  for more details. Do not contact the workspace project owner.     
- Open the **Data** tab in the data workspace.
- Download all metadata tables by first clicking on the link for a table table (participant, sample, sample_set), checking off all the boxes and then clicking on ""Download Table TSV."" You will need to repeat for each table.  
- You’ll now populate this Intro to CCDG Datasets workspace with the metadata from the CCDG cohort in the Broad’s Data Library by uploading the tsvs you’ve just grabbed (click on the “+” icon in the **Data** tab).

Congratulations! Now that you have uploaded the CCDG data table to the workspace, you’ve linked the data (in the cloud) to you workspace and you will be able to access the data for analysis.    

**Workspace Data**  
Required and optional references and resources for the methods are included in the Workspace Data table (""Workspace Attributes"" in FireCloud). The reference genome for this workspace is hg38 (aka GRCh38).

# Tools

In the Tool tab you will find example workflows you can run on the cram files, VCFs, and multi-sample VCFs for all samples within each CCDG cohort workspace. You can also generate new multi-sample VCFs from specific samples in the cohort using Haplotypecaller and Joint Discovery.   

**Beware of the cost when running the two Tools!** CCDG data contains WGS cram files, which can be pricey to run on the full cohort. If you’re interested in testing or practicing using 1-Haplotypecaller-GVCF and 2-Joint-Discovery in a Terra workspace, select three samples as input instead of the whole cohort. The notebook plots will not be as interesting with a three-sample multi-VCF. However, there are plans to generate a multi-sample VCF for each CCDG data workspace containing all the samples within that cohort workspace. In the future you can use this as input for the notebook at a more reasonable cost. 

This workspace contains the following tools (""Methods"" in FireCloud):

**1-Haplotypecaller-GVCF**: This workflow runs the HaplotypeCaller Tool from GATK4 in GVCF mode on a single sample according to GATK Best Practices. The workflow scatters the HaplotypeCaller Tool over a sample using an intervals list file. The output file will be a single GVCF file, which can then be used as input for the joint-discovery workflow.

Entity Type  
- Sample

Requirements/expectations  
- One analysis-ready BAM file for a single sample (as identified in RG:SM)
- A file containing a set of variant calling interval lists for the scatter

Outputs  
- One GVCF file and its index

**2-Joint-Discovery** : This WDL implements the joint calling and VQSR filtering portion of the GATK Best Practices for germline SNP and Indel discovery in human whole-genome sequencing (WGS).

Entity Type  
- Sample Set

Requirements/expectations  
- One or more GVCFs, produced by HaplotypeCaller in GVCF mode
- Bare minimum: 1 WGS sample or 30 Exome samples. Gene panels are not supported.
- To determine the disk size in the JSON, use the guideline below:
  - small_disk = (num_gvcfs / 10) + 10
  - medium_disk = (num_gvcfs * 15) + 10
  - huge_disk = num_gvcfs + 10

Outputs  
- A VCF file and its index, filtered using variant quality score recalibration
  (VQSR), with genotypes for all samples present in the input VCF. All sites that
  are present in the input VCF are retained. Filtered sites are annotated as such
  in the FILTER field.
	
## Example Time and Cost to Run Workflow   

**1-HaplotypeCaller-GVCF**  

| Sample Name | Sample Size | Time | Cost $ |
| ---  | :---: | :---: | :---: |
| G101442_BR_10001_v3_WGS_GCP | 11.33 GB | 06:24:00 | 2.46 |
| G101442_BR_10003_v3_WGS_GCP | 12.28 GB | 05:33:00 | 3.46 |
| G101442_BR_10004_v3_WGS_GCP | 11.29 GB | 04:37:00 | 1.79 |

**2-Joint-Discovery**  

| Sample Name | Sample Size | Time | Cost $ |
| ---  | :---: | :---: | :---: |
| 3 g.vcf samples | 27.76 GB | 03:14:00 | 0.94 |

Cost and Time will vary with the use of [Preemptibles](https://gatkforums.broadinstitute.org/firecloud/discussion/11786/preemptibles#latest).  

Users can also use [Google's cost calculator](https://cloud.google.com/products/calculator/) for cost estimates.   

### Software Version  
- GATK 4.1

# Interactive Analysis in a Notebook

An example notebook demonstrate the sorts of interactive analysis that would typically build on the output of the processing workflows HaplotypeCaller and JointDiscovery. A great tool for this sort of analysis is [Hail](https://hail.is/docs/0.2/overview/index.html), “a library for analyzing structured tabular and matrix data. Hail contains a collection of primitives for operating on data in parallel, as well as a suite of functionality for processing genetic data.” The notebook in this workspace runs through some introductory commands in Hail to show how to access your data from the notebook and perform some simple QC functions.    

If you are unfamiliar with Jupyter notebooks and how to use them for interactive analysis, check out this [Jupyter Notebooks 101](https://app.terra.bio/#workspaces/help-gatk/Jupyter%20Notebooks%20101) workspace.      

 **Sample_and_Variant_QC_with_Hail**

| Runtime Environments | Value |
| --- | --- |
| CPU Minimum| 4|
| Disksize Minimum | 100 GB |
| Memory Minimum | 15 GB |
| Startup Script | - |

---

## Contact information  
This material is provided by the GATK Team. Please post any questions or concerns to one of our forum sites: [GATK](https://gatkforums.broadinstitute.org/gatk/categories/ask-the-team/), [FireCloud](https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team), [Terra](https://broadinstitute.zendesk.com/hc/en-us/community/topics/360000500432-General-Discussion), or [WDL/Cromwell](https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team).

## License  
**Copyright Broad Institute, 2019 | BSD-3**  
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at https://github.com/openwdl/wdl/blob/master/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/Introduction-to-Anvil-CCDG-Dataset"
151,"broad-firecloud-dsde","Hyperparameter Tuning with Optuna","READER","https://app.terra.bio/#workspaces/broad-firecloud-dsde/Hyperparameter%20Tuning%20with%20Optuna",TRUE,FALSE,NA,NA,NA,"# Hyperparameter Optimization with Optuna

 _This workspace is a guide on how hyperparameter optimization works and a walkthrough for how to do it in Terra using Optuna._


## Overview

Hyperparameter optimization (sometimes called tuning) is a method that can be used to partially automate the process of designing machine learning models, saving you the time of experimenting manually. You design an experiment, specify a range of values, then let the experiment run and determine the optimal model design based on a performance metric of your choice. 

This featured workspace walks you through how to automate machine learning model development by running hyperparameter optimization trials in Terra, using data from the Framingham Heart Study as an example. 

The “Optuna Walkthrough” notebook in this workspace covers the following topics: 



* How to import data from Terra’s data repo
* Designing and running experiments with the [Optuna package](https://optuna.org/)
* Examples with different models with varying amounts of hyperparameters (logistic regression, random forest, neural network)
* Visualizing and understanding the trials and results of your experiment

**The Optuna package for hyperparameter optimization**

There are multiple packages that support hyperparameter optimization; we chose to use [Optuna](https://optuna.org/) for the following reasons. 



1. It is open source and free to use. 
2. It has a number of features that make it easier to use compared to other options, such as parallelization and automatic pruning to stop trials early if they are performing poorly.
3. It is easy to integrate with almost any machine learning package/framework (including PyTorch, TensorFlow, Keras, XGBoost, LightGBM, CatBoost, etc).
4. It uses a [Bayesian approach](https://distill.pub/2020/bayesian-optimization/) to optimization, which is more efficient than grid or random search.  
 
* _Bayesian optimization is a method of optimizing your goal function by sequentially choosing values that minimize the uncertainty in that goal function's value, “learning” from its past mistakes. Grid search is a method of optimizing a goal function by exhaustively searching for the best values of the function's inputs, this works but is computationally expensive. Random search is a method of optimizing a goal function by randomly choosing values for the function's inputs, which is the most computationally expensive._


## Example Experiment and Results

Hyperparameter optimization is often used by experienced ML practitioners for final improvements, but if you don't have much ML experience, it can make things easier from the beginning. To demonstrate, we’ve made and tuned a model to predict heart disease based on the Framingham Heart Study. 

**Example Data**

The Framingham Heart Study is a long-term, ongoing cardiovascular cohort study that began in 1948 with 5,209 adult subjects, and has since enrolled three additional generations of subjects totaling over 16,000 individuals. The dataset is a treasure trove for research, with detailed information on participant demographics, lifestyle factors, and medical history. It’s also easily accessible in Terra’s Data Library.

Over the past few decades, the Framingham Heart Study data was used by researchers to create the ""Framingham Risk Score"", a prediction of heart disease risk using classical statistical techniques. 

Since then, recent papers have replicated and surpassed the performance of the Framingham Risk Score by [using ML methods](https://pubmed.ncbi.nlm.nih.gov/31091238/). Our approach is similar, but with hyperparameter optimization we achieved a competitive score with minimal manual experimentation and effort. 


## How to use this workspace

The notebook in this workspace can be copied to a different workspace for reuse in your individual research. 

To copy a notebook to another workspace:



1. Navigate to the ""Notebooks"" section of this workspace.
2. Click on the menu icon (circle with three dots) in the upper right.
3. Select ""copy to another workspace"".


### Cost and Configuration

We recommend choosing the default Python 3 environment with a GPU enabled in the Cloud Environment configuration menu (1 CPU, 3.75 GB memory, 1 NVIDIA Tesla T4). The total runtime of the example here is 20 minutes and it costs around 10¢.


## Conclusion

Hyperparameter optimization can improve any prediction where a machine learning model is used. It can boost performance of a model you’ve already made, or help you design one from the beginning. Everything from [diagnostic radiology](https://arxiv.org/abs/1710.04934) to [epidemiology](https://www.cambridge.org/core/services/aop-cambridge-core/content/view/60FFBF2D360CF70446AC5B4A972B299A/S2732494X21001923a.pdf/div-class-title-machine-learning-and-artificial-intelligence-applications-in-healthcare-epidemiology-div.pdf) to [cancer genomics](https://pdfs.semanticscholar.org/477d/5accecdab4f45be34cfaaad22b62a1dac9c7.pdf) to [single-cell transcriptomics](http://compbio.cs.umn.edu/wp-content/uploads/2019/05/SingleCellReview.pdf) can benefit. No matter your domain, we hope this tutorial will inspire you to try it out for yourself, in case hyperparameter tuning can save you time and improve the results of your next modeling project. 


### Contact Information

For questions about this workspace please visit the Featured Workspaces Topic on the[ Terra Community Forum](https://support.terra.bio/hc/en-us/community/topics).
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","broad-firecloud-dsde/Hyperparameter Tuning with Optuna"
152,"fccredits-silver-tan-7621","CCLE_v2","READER","https://app.terra.bio/#workspaces/fccredits-silver-tan-7621/CCLE_v2",TRUE,FALSE,"cancer","Cell lines characterization data",NA,"# CCLE V2.1 = CCLE v2 Official Dataset + gap filling sequencing 
This workspace contains the omics data for the cell lines from [the CCLE2 paper](doi:10.1038/s41586-019-1186-3) which have been sequenced at the Broad institute and have derivative data released publically.
(for access to the Sanger lines, please visit https://cellmodelpassports.sanger.ac.uk/)_

The following raw sequencing data is available for download:
- RNAseq (1025) [HG38]
- WES (478) [HG38]
- WGS (329) [HG38]
- RRBS (unfiltered) (927) [HG19]
- HC (976) [HG19]
- Raindance (782) [HG19]


Next-generation characterization of the Cancer Cell Line Encyclopedia
Ghandi, M., Huang F. et al.
Nature doi:10.1038/s41586-019-1186-3 / May 8, 2019


> Maintained by Simone Zhang & Alvin Qin


## Using the data
To view the paths to the data you would need to **clone** the workspace using **your own billing account** and use it as a base for your own workspace (from which you can analyze the data).

Otherwise:
1. you can download any file from Google Cloud storage using the gs:// link and **gsutil** from your terminal (links can be accessed by downloading the data as csv). Since the bam and bai files are stored in requester-pays buckets, you need to provide a billing project to access the files using commands such as **gsutil -u [name of your billing project] cp gs://cclebams/rnasq_hg38/CDS-xxxxxx.bam .**
2. you can access the **CCLE_v2** workspace programatically using python's firecloud-dalmatian

### Note: You might not be able to view file details when you click on file links in the data table in this workspace. This is expected, and does not suggest that the file paths are corrupted, since loading metadata from requester-pays buckets requires a billing project and this workspace isn't attached to any. However, you should be able to view them once you've cloned this workspace under your billing project.

## Remarks

1. Most of the bam files have been reprocessed and **might be slightly different from the original CCLE2 bam** files. We do not expect it to change any results obtained from the original bam files.
2. reprocessed bam files were reprocessed using the **PreProcessingForVariantDiscovery_GATK4** workflow version 8 with gatk:4.beta.3 version

More information in our **Broadinstitute/DepMap_Omics** github repo.



## Previous attributes (deprecated)
Updated the workspace on June 2021. The following were the previous attributes:","William Sellers","Mahmoud Ghandi","The Cancer Cell Line Encyclopedia (CCLE) project is a collaboration between the Broad Institute, and the Novartis Institutes for Biomedical Research and its Genomics Institute of the Novartis Research Foundation to conduct a detailed genetic and pharmacologic characterization of a large panel of human cancer models, to develop integrated computational analyses that link distinct pharmacologic vulnerabilities to genomic patterns and to translate cell line integrative genomics into cancer patient stratification. The CCLE provides public access to genomic data, analysis and visualization for about 1000 cell lines. The CCLE phase 2 datasets in this workspace includes: RNAseq (1019), WES (326), WGS (329), RRBS (unfiltered) (928), HC (976), Raindance (782), TERT (190) cell lines.","1065",NA,"CCLE",NA,"General Research Use","GRCh37/hg19","Broad-Novartis Cancer Cell Line Encyclopedia (CCLE) v2 Official Dataset","RNAseq;WES;WGS;RRBS;HC;Raindance;targeted sequencing (TERT promoter)","fccredits-silver-tan-7621/CCLE_v2"
153,"terra-biom-mass","PDGM","READER","https://app.terra.bio/#workspaces/terra-biom-mass/PDGM",TRUE,FALSE,NA,NA,NA,"# Parkinson's gut microbiome

Prospective cohort study to conduct a comprehensive investigation of the relation between human gut microbiome and Parkinson’s Disease, focusing on the pre-onset microbiome.

For more information, please contact:
* Natasha Palacios - palacios@hsph.harvard.edu - Primary investigator
* Alberto Ascherio - aascheri@hsph.harvard.edu - Co-investigator

**All data is hosted and access controlled by the [BIOM-Mass Portal](http://biom-mass.org/repository?searchTableTab=cases).  The BIOM-Mass Data Portal is provided by the <a href=""https://hcmph.sph.harvard.edu/"">Harvard Chan Microbiome in Public Health Center (HCMPH)</a> to manage and share microbiome profiles, sample, and population information from microbiome epidemiology studies carried out through the HCMPH <a href=""https://hcmph.sph.harvard.edu/resources/"">BIOM-Mass platform</a>.  Please <a href=""https://hcmph.sph.harvard.edu/contact/"">contact us</a> to request portal access, for data depositions, and with any other inquires.**

The BIOM-Mass (Biobank for Microbiome Research in Massachusetts) project is funded by the [Massachusetts Life Sciences Center (MLSC)](http://www.masslifesciences.com/) as a collaboration with the BWH/Harvard Cohorts Biorespository.

![MLSC](http://biom-mass.org/static/media/mlsc_logo.93b1eb31.png)",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","terra-biom-mass/PDGM"
155,"broad-firecloud-tcga","TCGA_LUAD_OpenAccess_V1-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_LUAD_OpenAccess_V1-0_DATA",TRUE,TRUE,"Lung Adenocarcinoma","Tumor/Normal","USA","TCGA Lung adenocarcinoma Open-Access Data Workspace","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients’ clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","585","Lung","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh37/hg19","TCGA_LUAD_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_LUAD_OpenAccess_V1-0_DATA"
156,"help-gatk","GATK4-RNA-Germline-VariantCalling","READER","https://app.terra.bio/#workspaces/help-gatk/GATK4-RNA-Germline-VariantCalling",TRUE,FALSE,NA,NA,NA,"### GATK Best Practices for Germline Variant Calling in RNAseq
Best Practices WDL workflow calls germline short variants (SNPs/Indels) from RNAseq data using GATK  v4.1 and related tools. Detailed description of the workflows is available in [Gatk's Best Practices Document](https://gatk.broadinstitute.org/hc/en-us/articles/360035531192).     
Scroll down for details on the workflow, including input and output descriptions and requirements, estimated run times and costs, and information on sample data.       

## Workflows

### 1-Gatk4-RNAseq-Germline-SNPs-Indels
#### What data does it require as input?

- unmapped BAM (uBAM) file
- Annotations GTF File

Input uBAM files must comply with the following requirements:
- Filenames all have the same suffix (we use "".unmapped.bam"")
- Files must pass validation by ValidateSamFile
- Reads are provided in query-sorted order
- All reads must have an RG tag
- Reference index files must be in the same directory as source (e.g. reference.fasta.fai in the same directory as reference.fasta)

If your sequencing data is not in uBAM format, check out this file conversion workspace, [help-gatk/Sequence-Format-Conversion](https://app.terra.bio/#workspaces/help-gatk/Sequence-Format-Conversion) for workflows to convert:
- Interleaved FASTQ to paired FASTQ
- Paired FASTQ to unmapped BAM
- BAM to unmapped BAM
- CRAM to BAM files from sequencer output for use in GATK analysis tools

#### What does it output? 

- A recalibrated BAM file and its index
- A VCF file and its index
- A Filtered VCF file and its index

Output data are stored in the workspace Google bucket and written to the data table by default. 
 
#### Sample data description and location

The workflow accepts unmapped BAM file from a single sample. The Data table contains an example of a single sample, an unmapped BAM of RNAseq data from sample NA12878. 

#### Reference data description and location

The required and optional references and resources for the workflows are included in the Reference Data table. The reference genome for this workspace is b37.

#### Time and cost estimates

| Sample Name | Sample Size | Time | Cost $ |
| :---:  | :---: | :---: | :---: |
| NA12878 | 3.09 GB | 9:32:00 | 0.47 |     

**For helpful hints on controlling Cloud costs**, see [this article](https://support.terra.bio/hc/en-us/articles/360029748111).       

----

### Software Versions  
- GATK 4.1
- STAR 2-pass

----

### Contact information  
For questions about this workspace please visit the Featured Workspaces Topic topic on the [Terra Community Forum](https://broadinstitute.zendesk.com/hc/en-us/community/topics). Use the search box to see if other users have asked the same question previously. If not, post and tag **@ Samantha Velasquez** so that we get notified.

This material is provided by the GATK Team. Please post any questions or concerns regarding the GATK tool to the [GATK forum](https://gatk.broadinstitute.org/hc/en-us/community/topics)

### Workspace Citation 
Details on citing Terra workspaces can be found here: [How to cite Terra](https://support.terra.bio/hc/en-us/articles/360035343652)

Data Sciences Platform, Broad Institute (*Year, Month Day that this workspace was last modified*) help-gatk/GATK4-RNA-Germline-VariantCalling [workspace] Retrieved *Month Day, Year that workspace was retrieved*,  https://app.terra.bio/#workspaces/help-gatk/GATK4-RNA-Germline-VariantCalling

### License  
**Copyright Broad Institute, 2019 | BSD-3**  
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at https://github.com/openwdl/wdl/blob/master/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.

### Workspace Change Log
| Date | Change | Author |
| --- | --- | --- |
|  2020-11-18 | Minor changes to dashboard format, removed specification for docker version in workflow config | Beri Shifaw |
|  2021-09-15 | Addition of Workspace Citation section | Beri Shifaw |",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/GATK4-RNA-Germline-VariantCalling"
157,"help-gatk","WDL-puzzles","READER","https://app.terra.bio/#workspaces/help-gatk/WDL-puzzles",TRUE,TRUE,NA,NA,NA,"## WDL puzzles: Hands-on practice for scripting and configuring Terra workflows

*Solve a WDL puzzle workflow that you can run on sample data in Terra*

This workspace tutorial provides hands-on practice with identifying and correcting  WDL workflow errors. Using the step-by-step instructions, you will:

1. Clone this workspace
2. Download and solve a WDL puzzle using a text editor
3. Upload your solved puzzle to the Broad Methods Repository
4. Export your WDL to a cloned version of this workspace
5. Set up and run your WDL using workspace data tables

![workflow_overview](https://storage.googleapis.com/terra-featured-workspaces/WDL-tutorial/workspace_workflow.png)

### How much will it cost?
Running the whole workspace will only cost you a couple of pennies!

### Work from your copy of the workspace following these step-by-step instructions 
|![doc icon](https://storage.googleapis.com/terra-featured-workspaces/QuickStart/icon_read_more%401x.png) | Read step-by-step instructions [here](https://support.terra.bio/hc/en-us/articles/360056599991) |  
| --------| ------------------|    

![white space](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/white-space.jpg)    
  

## Step 1. Clone the workspace
Before you begin, create your own editable copy (clone) of this WORKSPACE. Click the round circle with three dots in the upper right corner of this page and choose ""Clone"".


![](https://storage.googleapis.com/terra-featured-workspaces/WDL-tutorial/wdl-puzzles.png)

## Step 2. Download and solve a WDL puzzle
There are two puzzles available in the **Workspace Data** table: the ""easy-puzzle"" or the ""advanced-puzzle"". 

The easy puzzle is a modified version of the commonly-used ""Hello world!"" script. To complete the puzzle, you must identify and fill in a single missing input variable.

The advanced puzzle is a modified ValidateBam workflow  that uses the Picard tool ValidateSamFile. To complete the puzzle, you must identify and fill multiple missing input and output variables.

### Quickstart Instructions

1. Navigate to the workspace **Data Page**
2. Select the **Workspace Data** table
3. Select the link for either the **easy_puzzle.wdl** or the **advanced_puzzle.wdl**
4. Download the WDL file to your local machine
5. Open the WDL file with your favorite text editor
6. Fill in the missing variables which are demarcated with a ""...""
7. Save the solved workflow


## Step 3. Upload the solved WDL puzzle to the Broad Methods Repository
This step requires uploading to the Broad Methods Repository. You can follow the instructions below or more detailed instructions [here](https://support.terra.bio/hc/en-us/articles/360056599991). 

You can also check out this [video tutorial](https://www.youtube.com/watch?v=VtKlYqWBW6A) which walks you through multiple ways to import a workflow in Terra.

### Quickstart Instructions


1. Naviage to the [Broad Methods Repository](https://portal.firecloud.org/?return=terra#methods)
2. Select **Create New Method**
3. Fill in a namespace and workflow name (you can choose any name; only letters, numbers, underscores, dashes, and periods allowed) 
4. Upload or paste your solved WDL file
5. Select **Upload**

Note: If your puzzle contains any errors, you will receive an error message in the Upload dialogue box.

For the correct WDL puzzle answers, see the accompanying [WDL Puzzles documentation](https://support.terra.bio/hc/en-us/articles/360056599991)  or see the solved workflow script by selecting it in the workspace Workflows section.  


## Step 4. Export your WDL to your cloned workspace


### Quickstart Instructions


1. In the upper right of the Broad Methods Repository, select the **Export to Workspace**
2. Select **Use Blank Configuration**
3. Select your destination workspace (your cloned workspace)
4. Select **Export to Workspace**
5. Go to your cloned workspace
6. Verify the workflow is in the **Workflows** page


## Step 5. Set up and run your WDL using workspace data tables
This workspace contains sample data you can use to run your solved WDL puzzle. You'll need to set up both inputs and outputs. 

### Easy Puzzle
Use the **hello_world_name** data table to configure the workflow input and output. Correct examples are shown in the **easy-puzzle-solved** workflow on the workspace Workflows page or in the [documentation](https://support.terra.bio/hc/en-us/articles/360056599991).



### Advanced Puzzle
The advanced puzzle takes in an array of BAM files as input. That means you'll need to specify a set of BAMs in the workflow configuration. Use the **bam** and **bam_set** tables to set up the workflow inputs and outputs. Correct examples are shown in the **advanced_puzzle_solved** workflow on the workspace Workflows page or in the [documentation](https://support.terra.bio/hc/en-us/articles/360056599991).


## Resources
#### [OpenWDL](https://openwdl.org/)
Learn how to stay in touch and participate with the global WDL community. WDL global WDL community.

#### [WDL 1.0 Spec](https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md)
Read a detailed description of  WDL 1.0 syntax. 

#### [learn-wdl](https://github.com/openwdl/learn-wdl)
Try an open source WDL course on GitHub that includes video tutorials as well as example WDL scripts and resources. 

## License
**Copyright Broad Institute, 2020 | BSD-3**

All rights reserved. Full license text at https://github.com/broadinstitute/gatk/blob/master/LICENSE.TXT. Note that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.

---",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/WDL-puzzles"
159,"broad-dsp-spec-ops-fc","somatic_truth_data_from_cell_lineage","READER","https://app.terra.bio/#workspaces/broad-dsp-spec-ops-fc/somatic_truth_data_from_cell_lineage",TRUE,TRUE,NA,NA,NA,"This workspace provides the LinST benchmarking data described in ""A validated lineage-derived somatic truth data set enables benchmarking in cancer genome analysis"" (https://rdcu.be/cbQ34). The original bams, as well as all mixtures described in the paper are included.  Workflows to generate the mixtures and truth data are also included, all other code can be found here: https://github.com/meganshand/gatk. 

The column names under the participant section of the data tab are as follows:

participant_id: an ID made from the tumor sample name, normal sample name, and purity level separated by underscores

high_conf_region: an interval list of the high confidence region for that particular mixture

matched_normal_bam: bam to use as the matched normal (this is the sister sample to the mixed-in ""normal"")

matched_normal_bai: bam index for the matched normal

mixed_af: when the ""tumor"" and ""normal"" bams are mixed, they sometimes don't achieve a true purity that was aimed for due to lack of coverage. This value is the true estimate of the purity rate of a mixture. Even though it is labeled af, it is in fact the purity.

mixed_bam: bam to use as the ""case"" sample when running a somatic variant calling pipeline. It is a mixture of the ""tumor"" and ""normal"" sample at the given purity.

mixed_bai: bam index for the case sample

normal_bam: original bam that is used as the mixed-in ""normal"" sample

normal_bai: bam index for the normal

total_bases_in_hi_conf: the number of bases in the high confidence region for the given mixture

truth_vcf: the vcf to use as the truth when comparing output

truth_vcf_idx: vcf index of the truth

tumor_bam: original bam that is used as the mixed-in ""tumor"" sample

tumor_bai: bam index of the tumor sample

wgs_metrics: coverage metrics for the mixed case sample bam. See https://broadinstitute.github.io/picard/picard-metric-definitions.html#CollectWgsMetrics.WgsMetrics for definitions of the columns within this file.",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","broad-dsp-spec-ops-fc/somatic_truth_data_from_cell_lineage"
160,"broad-firecloud-tcga","TCGA_MESO_hg38_OpenAccess_GDCDR-12-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_MESO_hg38_OpenAccess_GDCDR-12-0_DATA",TRUE,TRUE,"Mesothelioma","Tumor/Normal","USA","TCGA Mesothelioma Open-Access Data Workspace

*hg38 TCGA and TARGET workspaces reference files by their GDC UUIDs.  In order to run analyses on the referenced data files, you will need to run workflows that retrieve the files from the GDC and copy them to your workspace bucket.  See   [this forum post](https://gatkforums.broadinstitute.org/firecloud/discussion/10382/populating-hg38-tcga-and-target-workspaces-with-data-files#latest) for instructions on the running of these workflows.*","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients' clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","87","Pleura","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh38/hg38","TCGA_MESO_hg38_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_MESO_hg38_OpenAccess_GDCDR-12-0_DATA"
161,"broad-firecloud-tcga","TARGET_AML_hg38_OpenAccess_GDCDR-12-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TARGET_AML_hg38_OpenAccess_GDCDR-12-0_DATA",TRUE,TRUE,"acute myeloid leukemia","Tumor/Normal",NA,"[TARGET Acute Myeloid Leukemia (AML) Project](https://ocg.cancer.gov/programs/target/acute-myeloid-leukemia)

*hg38 TCGA and TARGET workspaces reference files by their GDC UUIDs.  In order to run analyses on the referenced data files, you will need to run workflows that retrieve the files from the GDC and copy them to your workspace bucket.  See   [this forum post](https://gatkforums.broadinstitute.org/firecloud/discussion/10382/populating-hg38-tcga-and-target-workspaces-with-data-files#latest) for instructions on the running of these workflows.*","NCI","Chet Birger","Acute Myeloid Leukemia","98",NA,"TARGET",NA,"Open","GRCh38/hg38","TARGET Acute Myeloid Leukemia (AML) Project","Whole Exome","broad-firecloud-tcga/TARGET_AML_hg38_OpenAccess_GDCDR-12-0_DATA"
162,"gro-share-seq-computational","useful_notebooks_for_teaching_public","READER","https://app.terra.bio/#workspaces/gro-share-seq-computational/useful_notebooks_for_teaching_public",TRUE,FALSE,NA,NA,NA,"making a copy of this notebook for public sharing",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","gro-share-seq-computational/useful_notebooks_for_teaching_public"
163,"help-terra","gcloud-storage-tutorial","READER","https://app.terra.bio/#workspaces/help-terra/gcloud-storage-tutorial",TRUE,TRUE,NA,NA,NA,"Learn how to install and use `gcloud` storage command line interface to manage buckets and objects in Terra. Understanding the Python command tool gcloud storage lets you manage buckets and objects on Google Cloud Storage (in a notebook, for example), which can be useful when working in Terra.     

The gcloud storage command line interface is part of the gcloud shell scripts and is fully open sourced on GitHub and under active development. 
<br>

## Overview

The <a href=""https://cloud.google.com/sdk/gcloud/reference/storage"" target=""_blank"">gcloud storage</a> command line interface (CLI) is a useful python tool for navigating and managing data and Buckets in Google Cloud Storage, including the workspace bucket (where data generated in a workflow is stored by default). The gcloud storage commands let you interact with Google Cloud Storage from the terminal on a local machine or in a workspace and can help with a wide range of tasks in Google Cloud including:
- Creating and deleting Google Buckets
- Uploading, downloading, and deleting objects in Google Cloud Storage (GCS)    
- Listing buckets and objects in GCS       
- Moving, copying, and renaming objects in GCS         

|   ![read more icon](https://storage.googleapis.com/terra-featured-workspaces/QuickStart/icon_read_more%401x.png) | <a href=""https://support.terra.bio/hc/en-us/articles/6453490899099-gcloud-storage-tutorial"" target=""_blank"">**Click for step-by-step instructions**</a> |      
|---| ---------|       

## Learning Objectives 
Use gcloud storage commands to    
1. List files in GCS    
2. Copy files to/from a Google Bucket
3. Download from a dataset    
4. Set metadata       
<br>      

## Estimated Time and Cost
This workspace should take under 1 hour to complete with minimal storage costs.     
<br>       

## gcloud storage Workspace Summary
Workspace flow
The Workspace has three parts 

![Workspace flow](https://storage.googleapis.com/terra-featured-workspaces/QuickStart/gcloud-tutorial-flow_diagram.png)
<br>      

## Requirements
For this workspace you will be using gcloud storage CLI commands. You will need to install gcloud storage to your local machine using the instructions below.      

**In order to install gcloud, your system will need to meet the following requirements**    
Linux/Unix, Mac OS, or Windows (XP or later).
Versions 5.0 and up require Python 3

### Installing gcloud storage to a local machine 
Follow the instructions to install Google Cloud CLI on your operating system on Google Cloud's [install gcloud](https://cloud.google.com/sdk/docs/install-sdk) page.  

Make sure the Google account you use is the same account associated with Terra, otherwise you will not have authorization to use some of the commands.       
<br>      

## Step-by-step instructions
**1. Set up an interactive Cloud Environment in your Terra workspace**    
1.1. Select the ***Environment Configuration icon*** (cloud with lightning bolt) in the right sidebar.          
1.2. Under the ***Jupyter*** option, select ***Settings***.     
1.3. Under the ***default environment*** option select the ***Create button***.     
 
Note that It will take 3-5 min to create the custom VM.     
<br>
**2. Launch and run the notebook**     
2.1. Go to the workspace ***Analyses tab*** and open the ***gcloud-tutorial.ipynb*** notebook in Open mode.     
2.2. Follow the ***instructions*** in the notebook (for a notebook primer, see the [Jupyter Notebook 101 tutorial](https://app.terra.bio/#workspaces/help-terra/T101-Notebooks-Quickstart/analyses)).

<br>        

## Additional resources

### How can I learn more about Terra?

To learn more about Terra and its functionality, see <a href=""https://support.terra.bio/hc/en-us"" target=""_blank"">Terra Support</a>.       
* <a href=""https://support.terra.bio/hc/en-us/categories/360005881492"" target=""_blank"">Getting started on Terra</a>          
* <a href=""https://support.terra.bio/hc/en-us/categories/360001399872"" target=""_blank"">Doing research on Terra</a>     

### More resources on gcloud storage
To learn more about commands you can run using gcloud storage CLI, see
* <a href=""https://support.terra.bio/hc/en-us/articles/6453490899099-gcloud-storage-tutorial"" target=""_blank"">gcloud storage tutorial guide</a>    
* <a href=""https://support.terra.bio/hc/en-us/articles/10206456029083-How-to-install-gcloud-storage-on-a-local-machine"" target=""_blank"">How to install gcloud storage to local machine</a>        

<br>        

## Contact information  
This material is provided by the Terra Team. Please post any questions or concerns to our forum site: <a href=""https://support.terra.bio/hc/en-us/community/topics/360001603491-Featured-Workspaces"" target=""_blank"">Terra forum - Featured workspaces</a>.     

<br>        

## Workspace Citation 
For details on citing Terra workspaces, see <a href=""https://support.terra.bio/hc/en-us/articles/360035343652"" target=""_blank"">How to cite Terra</a>.    

Data Sciences Platform, Broad Institute (*Year, Month Day that this workspace was last modified*) fc-product-demo/Terra-Notebooks-Quickstart [workspace] Retrieved *Month Day, Year that workspace was retrieved*,  https://app.terra.bio/#workspaces/fc-product-demo/Terra-Notebooks-Quickstart

<br>        

## License  
**Copyright Broad Institute, 2024 | BSD-3**  
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at <a href=""https://github.com/openwdl/wdl/blob/master/LICENSE"" target=""_blank"">https://github.com/openwdl/wdl/blob/master/LICENSE</a>. Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.







",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-terra/gcloud-storage-tutorial"
164,"help-gatk","Terra-Tools","READER","https://app.terra.bio/#workspaces/help-gatk/Terra-Tools",TRUE,FALSE,NA,NA,NA,"# ATTENTION - DEPRECATED

The newer version of this workspace is now at: https://app.terra.bio/#workspaces/help-terra/Terra-Tools
Please use the link above to access the version of this workspace that will be updated from here on out. All Notebooks and resources in this version will no longer be changed or maintained.




This workspace contains useful tools that may come in different flavors (Workflows, Notebooks, etc) you may need to perform some convenient Terra actions such as:

1. Remove intermediate files generated by your workflows
2. Update references in your Data Model if the location of your files changes   

Each section below contains:       
* Description of the tool
* How to implement the tool
* Notes and Considerations
 
---


## Jupyter Notebook : Remove_Workflow_Intermediates
#### **Disclaimer:** **This Notebook is to be implemented at the user’s discretion. We are not responsible for any unexpected behavior (user error or otherwise). Please ensure that you have saved the files you would like to persist to the Data Model (or a more permanent location)before running this Notebook.**

### **What is this Notebook?**
When launching large numbers of submissions, increased storage costs can be incurred due to accumulating intermediate output data. This notebook offers users a simple option to delete some of the workflow intermediates in a single Terra workspace's Google bucket.

### **What does it do?**
This Notebook utilizes the mop command from FISS, a programmatic interface to FireCloud that provides Python bindings to the API, to enable users to remove unwanted and/or unnecessary intermediate files. Outputs from a Workflow can include final products, like vcfs, as well as additional intermediate files from intermediate tasks. If intermediates are not necessary, storage in the bucket continually incurs a cost to the user. This option circumvents the need to manually delete intermediates in submissions and offers the option to keep required outputs.

### **How does it do it?**
Every submission launched (can contain single or multiple workflows) is assigned a submissionID that is used to label the “directory” in the Google bucket where output files are copied. With the individual workspace name and the Terra billing-project, a list of submissionIDs is generated and intermediate files within the submission directory are deleted.

### **Notes and Considerations**

**What gets deleted?**
Workflow output files minus logs are deleted except any outputs that are bound to the Data Model. To bind outputs to the Data Model, select Defaults from the Outputs section of the Workflow before selecting “Launch Analysis”.

**What gets left behind?**
* Files uploaded to the Google bucket that do not live inside a submission “directory” will NOT be deleted.
* Log files (stderr, stdout, .log) within a submission “directory” will NOT be deleted.
* Submission folders/“directories” will NOT be deleted - only the contents.
* Notebooks in the Google bucket will NOT be deleted.

**What should you do before using this Notebook?**
* If there are outputs that should not be deleted, they will need to be bound to the Data Model. If a file is NOT bound to the Data Model, it will be removed.
* If not bound to the Data Model, desired files should be copied to a secondary location.

### **To run this notebook**
1. Copy this Notebook into the workspace where you want to remove intermediates generated from launched submissions.
2. Open the Notebook and Create a Runtime Environment if necessary.
3. After the Notebook is open, select Cell > Run All.
4. You will be prompted to enter Yes/No before deletion begins. Enter Yes/No and press Enter.



-------

## Jupyter Notebook: Update_Data_Model_References
#### **Disclaimer:** **This Notebook is to be implemented at the user’s discretion. We are not responsible for any unexpected behavior (user error or otherwise). Please ensure that you have saved the your table .tsv files and/or data you would like to persist to a permanent location before running this Notebook.**

### **What is this Notebook?**
If a situation arises wherein a user must copy data from a current Google bucket to another, references in the Data Model pointing to the original location need to be updated. If many entities exist in a table, manual updates can be cumbersome and/or error prone. This Jupyter notebook can be used to programatically update the paths in your Data Model from existing gsutil source location to a new gsutil destination in a single Terra workspace.

### **What does it do?**
This Notebook utilizes Python code (hosted in GitHub) to call FireCloud APIs, enabling users to update paths in the Data Model from one gsutil bucket URL to a second gsutil bucket URL. This option circumvents manual updates of each entity in the Data Model should the data or reference paths change. 

### **How does it do it?**
The Python code allows the user to input the original Google bucket path (in String format) and the replacement Google bucket path (in String format). The Notebook interacts with the API to replace the original path with the new bucket paths.

### **Notes and Considerations**

**What gets updated?**
* The Notebook applies to all the entity tables in the Workspace including the Workspace References.

**What gets left behind?**
* Any entities that do not point to the original Google bucket gs:// URL will not be updated to the new gs:// URL.

**What should you do before using this Notebook?**
* If there are any important files that should be accessible, they will need to be saved to a secondary location. 

### **To run this notebook**
1. Copy this Notebook into the workspace where you want to update the Data Model references.
2. Open the Notebook and Create a Runtime Environment if necessary.
3. After the Notebook is open, select Cell > Run All.

-----

## Workspace Change Log
| Date | Change | Author |
| --- | --- | --- |
|  2020-02-18 | workspace created, dashboard updated, notebooks added | Sushma Chaluvadi |


---

## Contact information  
For any questions, reach out to Terra-support@broadinstitute.zendesk.com or post on the [General Discussion forum](https://support.terra.bio/hc/en-us/community/topics/360000500432-General-Discussion).

---

## License  
**Copyright Broad Institute, 2019 | BSD-3**  
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at https://github.com/openwdl/wdl/blob/master/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/Terra-Tools"
165,"broad-firecloud-gtex","RNA_MuTect","READER","https://app.terra.bio/#workspaces/broad-firecloud-gtex/RNA_MuTect",TRUE,FALSE,NA,NA,NA,"This workspace contains the attributes and method configuration required for running RNA_MuTect.
There are three versions of the method:

1. RNA_MUTECT_paper_v1.3:
This version runs the full method, including filtering steps that use DNA and RNA Panel of Normals (PoN).

(a) To obtain access to the DNA PoN (attribute name: 'SNP_filtering_pon_file') one should get TCGA-dbGaP authorization. This is because the PoN file it uses as input was constructed from 8334 normal BAMs drawn from TCGA, and is thus restricted to users who have dbGaP authorization to access TCGA controlled access data.

(b) To obtain access to the RNA PoN (attribute name: 'pon_gtex') one should apply for a dbGAP approval to GTEx data. The file is found under /Genotype Files/phg000830.v1.GTEx_WES.panel-of-normals.c1.GRU.tar

2. RNA_MUTECT_paper_v1.3_no_RNA_PoN
This version of the method requires only the DNA PoN and skips the filtering steps that uses the RNA PoN.

3. RNA_MUTECT_paper_v1.3_no_DNA_RNA_PoN
This version of the method does not require any of the PoN files and skips both filtering steps that utilize these files.

It should be noted that in the publication both of the PoN files were used and were found to be critical for the high performance.",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","broad-firecloud-gtex/RNA_MuTect"
166,"broad-firecloud-tcga","TCGA_THYM_OpenAccess_V1-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_THYM_OpenAccess_V1-0_DATA",TRUE,TRUE,"Thymoma","Tumor/Normal","USA","TCGA Thymoma Open-Access Data Workspace","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients’ clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","124","Thymus","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh37/hg19","TCGA_THYM_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_THYM_OpenAccess_V1-0_DATA"
167,"broad-firecloud-tcga","TCGA_UCEC_hg38_OpenAccess_GDCDR-12-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_UCEC_hg38_OpenAccess_GDCDR-12-0_DATA",TRUE,TRUE,"Uterine Corpus Endometrial Carcinoma","Tumor/Normal","USA","TCGA Uterine Corpus Endometrial Carcinoma Open-Access Data Workspace

*hg38 TCGA and TARGET workspaces reference files by their GDC UUIDs.  In order to run analyses on the referenced data files, you will need to run workflows that retrieve the files from the GDC and copy them to your workspace bucket.  See   [this forum post](https://gatkforums.broadinstitute.org/firecloud/discussion/10382/populating-hg38-tcga-and-target-workspaces-with-data-files#latest) for instructions on the running of these workflows.*","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients' clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","560","Uterus","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh38/hg38","TCGA_UCEC_hg38_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_UCEC_hg38_OpenAccess_GDCDR-12-0_DATA"
168,"broad-firecloud-tcga","TCGA_HNSC_OpenAccess_V1-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_HNSC_OpenAccess_V1-0_DATA",TRUE,TRUE,"Head and Neck squamous cell carcinoma","Tumor/Normal","USA","TCGA Head and Neck squamous cell carcinoma Open-Access Data Workspace","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients’ clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","528","Head, Neck","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh37/hg19","TCGA_HNSC_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_HNSC_OpenAccess_V1-0_DATA"
169,"lincs-phosphodia","Avant-garde_TripleProteome_Pyrococcus_furiosus","READER","https://app.terra.bio/#workspaces/lincs-phosphodia/Avant-garde_TripleProteome_Pyrococcus_furiosus",TRUE,FALSE,NA,NA,NA,"Triple-proteome Pyrococcus furiosus dataset",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","lincs-phosphodia/Avant-garde_TripleProteome_Pyrococcus_furiosus"
170,"broad-firecloud-tcga","TCGA_READ_OpenAccess_V1-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_READ_OpenAccess_V1-0_DATA",TRUE,TRUE,"Rectum adenocarcinoma","Tumor/Normal","USA","TCGA Rectum adenocarcinoma Open-Access Data Workspace","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients’ clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","172","Colorectal","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh37/hg19","TCGA_READ_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_READ_OpenAccess_V1-0_DATA"
171,"broad-firecloud-tcga","TARGET_NBL_hg38_OpenAccess_GDCDR-12-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TARGET_NBL_hg38_OpenAccess_GDCDR-12-0_DATA",TRUE,TRUE,"neuroblastoma","Tumor/Normal",NA,"[TARGET Neuroblastoma (NBL) Project](https://ocg.cancer.gov/programs/target/projects/neuroblastoma)

*hg38 TCGA and TARGET workspaces reference files by their GDC UUIDs.  In order to run analyses on the referenced data files, you will need to run workflows that retrieve the files from the GDC and copy them to your workspace bucket.  See   [this forum post](https://gatkforums.broadinstitute.org/firecloud/discussion/10382/populating-hg38-tcga-and-target-workspaces-with-data-files#latest) for instructions on the running of these workflows.*","NCI","Chet Birger","Neuroblastoma","200",NA,"TARGET",NA,"Open","GRCh38/hg38","TARGET Neuroblastoma (NBL) Project","Whole Exome","broad-firecloud-tcga/TARGET_NBL_hg38_OpenAccess_GDCDR-12-0_DATA"
172,"broad-firecloud-tcga","TCGA_STAD_OpenAccess_V1-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_STAD_OpenAccess_V1-0_DATA",TRUE,TRUE,"Stomach adenocarcinoma","Tumor/Normal","USA","TCGA Stomach adenocarcinoma Open-Access Data Workspace","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients’ clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","478","Stomach","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh37/hg19","TCGA_STAD_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_STAD_OpenAccess_V1-0_DATA"
173,"broad-firecloud-cptac","PANOPLY_Production_Modules_v1_1","READER","https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_Production_Modules_v1_1",TRUE,FALSE,NA,NA,NA,"# PANOPLY: A cloud-based platform for automated and reproducible proteogenomic data analysis
#### Version 1.1

PANOPLY is a platform for applying state-of-the-art statistical and machine learning algorithms to transform multi-omic data from cancer samples into biologically meaningful and interpretable results. PANOPLY leverages [Terra](http://app.terra.bio)—a cloud-native platform for extreme-scale data analysis, sharing, and collaboration—to host proteogenomic workflows, and is designed to be flexible, automated, reproducible, scalable, and secure. A wide array of algorithms applicable to all cancer types have been implemented, and available in PANOPLY for analysis of cancer proteogenomic data.


![*Figure 1.* Overview of PANOPLY architecture and the various tasks that constitute the complete workflow. Tasks can also be run independently, or combined into custom workflows, including new tasks added by users. Inputs to PANOPLY consists of (i) externally characterized genomics and proteomics data (in gct format); (ii) sample phenotype and annotations (in csv format); and (iii) parameters settings (in yaml format). Panoply modules are grouped into Data Preparation Modules (green box), Data Analysis Modules (blue box) and Report Modules (red box). Data preparation modules perform quality checks on input data followed by optional normalization and filtering for proteomics data. Analysis ready data tables are then used as inputs to the data analysis modules. Results from the data analysis modules are summarized in interactive reports generated by appropriate report modules.](https://raw.githubusercontent.com/broadinstitute/PANOPLY/dev/panoply-overview.png)


PANOPLY v1.1 consists of the following components:

* A Terra production workspace on [PANOPLY_Production_Pipelines_v1_1](https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_Production_Pipelines_v1_1) with a preconfigured unified workflow to automatically run all analysis tasks on proteomics (global proteome, phosphoproteome, acetylome, ubiquitylome), transcriptome and copy number data; and an [additional workspace](https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_Production_Modules_v1_1) that includes separate methods for each analysis component. 
* An interactive Jupyter notebook (included in the Terra workspaces) that provides step-by-step instructions for uploading data, identifying data types, specifying parameters, and setting up the PANOPLY workspace for analyzing a new dataset.
* A GitHub [repository](https://github.com/broadinstitute/PANOPLY) that contains code for all PANOPLY tasks and workflows, including R code for implementing analysis algorithms, task module creation, and release management. A GitHub [wiki](https://github.com/broadinstitute/PANOPLY/wiki) includes documentation and description of algorithms. 
* A [tutorial](https://github.com/broadinstitute/PANOPLY/wiki/PANOPLY-Tutorial) illustrating the application of PANOPLY to a published breast cancer dataset and demonstrating the practical relevance of PANOPLY by regenerating many of the results described in (Mertins et al) (1) with minimal effort. The [PANOPLY_Tutorial](https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_Tutorial) workspace contains data and results from running the tuorial.
* Terra workspaces showing case studies of applying PANOPLY to the analysis of [CPTAC BRCA](https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_CPTAC_BRCA) (7) and [CPTAC LUAD](https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_CPTAC_LUAD) (8) datasets. The workspaces include all data, parameter settings and results.


PANOPLY provides a comprehensive collection of proteogenomic data analysis methods including sample QC (sample quality evaluation using profile plots and tumor purity scores (1), identify sample swaps, etc.), association analysis, RNA and copy number correlation (to proteome), connectivity map analysis (1 ,2), outlier analysis using BlackSheep (3), PTM-SEA (4), GSEA (5) and single-sample GSEA (6), consensus clustering, and multi-omic clustering using non-negative matrix factorization (NMF). Most analysis modules include a report generation task that outputs a HTML interactive report summarizing results from the respective analysis tasks. Mass spectrometry-based proteomics data amenable to analysis by PANOPLY includes isobaric label-based LC-MS/MS approaches like iTRAQ, TMT and TMTPro profiling the proteome and multiple PTM-omes including phospho-, acetyl-and ubiquitylomes.

Users can also [customize or create new pipelines and add their own tasks](https://github.com/broadinstitute/PANOPLY/wiki/Customizing-PANOPLY) and integrate them into the PANOPLY.


## Quick Start

* For a quick introduction and tour of PANOPLY, follow the [**tutorial**](https://github.com/broadinstitute/PANOPLY/wiki/PANOPLY-Tutorial). 
* Detailed **documentation** can be found [here](https://github.com/broadinstitute/PANOPLY/wiki).

### Contact

Email proteogenomics@broadinstitute.org with questions, comments or feedback.


### Citation

Mani, D. R. et al. PANOPLY: a cloud-based platform for automated and reproducible proteogenomic data analysis. *Nature Methods* 1–3 (2021) doi:10.1038/s41592-021-01176-6.
  

### References

1. Mertins, P. et al. Proteogenomics connects somatic mutations to signalling in breast cancer. *Nature* 534, 55–62 (2016).
2. Subramanian, A. et al. A Next Generation Connectivity Map: L1000 Platform and the First 1,000,000 Profiles. *Cell* 171, 1437–1452.e17 (2017).
3. Blumenberg, L. et al. BlackSheep: A Bioconductor and Bioconda package for differential extreme value analysis. *J of Proteome Research* 20(7), 3767-3773 (2021).
4.	Krug, K. et al. A Curated Resource for Phosphosite-specific Signature Analysis. *Mol. Cell. Proteomics* 18, 576–593 (2019).
5.	Subramanian, A. et al. Gene set enrichment analysis: a knowledge-based approach for interpreting genome-wide expression profiles. *Proc. Natl. Acad. Sci.* 102, 15545–15550 (2005).
6.	Barbie, D. A., Tamayo, P., Boehm, J. S., Kim, S. Y. & Moody, S. E. Systematic RNA interference reveals that oncogenic KRAS-driven cancers require TBK1. *Nature* (2009).
7. Gillette, M. A. et al. Proteogenomic Characterization Reveals Therapeutic Vulnerabilities in Lung Adenocarcinoma. *Cell* 182, 200–225.e35 (2020).
8. Krug, K., Jaehnig, E. J., Satpathy, S., Blumenberg, L., et al. Proteogenomic Landscape of Breast Cancer Tumorigenesis and Targeted Therapy. *Cell* 183, 1–21 (2020).
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","broad-firecloud-cptac/PANOPLY_Production_Modules_v1_1"
174,"broad-firecloud-cptac","PANOPLY_Production_Pipelines_v1_3","READER","https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_Production_Pipelines_v1_3",TRUE,FALSE,NA,NA,NA,"# PANOPLY: A cloud-based platform for automated and reproducible proteogenomic data analysis
#### Version 1.3

PANOPLY is a platform for applying state-of-the-art statistical and machine learning algorithms to transform multi-omic data from cancer samples into biologically meaningful and interpretable results. PANOPLY leverages [Terra](http://app.terra.bio)—a cloud-native platform for extreme-scale data analysis, sharing, and collaboration—to host proteogenomic workflows, and is designed to be flexible, automated, reproducible, scalable, and secure. A wide array of algorithms applicable to all cancer types have been implemented, and available in PANOPLY for analysis of cancer proteogenomic data.


| ![PANOPLY Overview ](https://raw.githubusercontent.com/broadinstitute/PANOPLY/dev/panoply-overview-v2.png) |
|:--:|
|  *Figure 1. Overview of PANOPLY architecture and the various tasks that constitute the complete workflow. Tasks can also be run independently, or combined into custom workflows, including new tasks added by users. Inputs to PANOPLY consists of (i) externally characterized genomics and proteomics data (in gct format); (ii) sample phenotype and annotations (in csv format); and (iii) parameters settings (in yaml format). Panoply modules are grouped into Data Preparation Modules (green box), Data Analysis Modules (blue box) and Report Modules (red box). Data preparation modules perform quality checks on input data followed by optional normalization and filtering for proteomics data. Analysis ready data tables are then used as inputs to the data analysis modules. Results from the data analysis modules are summarized in interactive reports generated by appropriate report modules. Modules under development are listed in grey text.* |


PANOPLY consists of the following components:

* A Terra production workspace on [PANOPLY_Production_Pipelines_v1_3](https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_Production_Pipelines_v1_3) with a preconfigured unified workflow to automatically run all analysis tasks on proteomics (global proteome, phosphoproteome, acetylome, ubiquitylome), transcriptome and copy number data; and an [additional workspace](https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_Production_Modules_v1_3) that includes separate methods for each analysis component. 
* An interactive Jupyter notebook (included in the Terra workspaces) that provides step-by-step instructions for uploading data, identifying data types, specifying parameters, and setting up the PANOPLY workspace for analyzing a new dataset.
* A GitHub [repository](https://github.com/broadinstitute/PANOPLY) that contains code for all PANOPLY tasks and workflows, including R code for implementing analysis algorithms, task module creation, and release management. A GitHub [wiki](https://github.com/broadinstitute/PANOPLY/wiki) includes documentation and description of algorithms. 
* A [tutorial](https://github.com/broadinstitute/PANOPLY/wiki/PANOPLY-Tutorial) illustrating the application of PANOPLY to a published breast cancer dataset and demonstrating the practical relevance of PANOPLY by regenerating many of the results described in (Mertins et al) (1) with minimal effort. The [PANOPLY_Tutorial](https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_Tutorial) workspace contains data and results from running the tuorial.
* Terra workspaces showing case studies of applying PANOPLY to the analysis of [CPTAC BRCA](https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_CPTAC_BRCA) (7) and [CPTAC LUAD](https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_CPTAC_LUAD) (8) datasets. The workspaces include all data, parameter settings and results.


PANOPLY provides a comprehensive collection of proteogenomic data analysis methods including sample QC (sample quality evaluation using profile plots and tumor purity scores (1), identify sample swaps, etc.), association analysis, RNA and copy number correlation (to proteome), connectivity map analysis (1 ,2), outlier analysis using BlackSheep (3), PTM-SEA (4), GSEA (5) and single-sample GSEA (6), consensus clustering, and multi-omic clustering using non-negative matrix factorization (NMF). Most analysis modules include a report generation task that outputs a HTML interactive report summarizing results from the respective analysis tasks. Mass spectrometry-based proteomics data amenable to analysis by PANOPLY includes isobaric label-based LC-MS/MS approaches like iTRAQ, TMT and TMTPro profiling the proteome and multiple PTM-omes including phospho-, acetyl-and ubiquitylomes.

Users can also [customize or create new pipelines and add their own tasks](https://github.com/broadinstitute/PANOPLY/wiki/Customizing-PANOPLY) and integrate them into the PANOPLY.


## Quick Start

* For a quick introduction and tour of PANOPLY, follow the [**tutorial**](https://github.com/broadinstitute/PANOPLY/wiki/PANOPLY-Tutorial). 
* Detailed **documentation** can be found [here](https://github.com/broadinstitute/PANOPLY/wiki).

### Contact

Email proteogenomics@broadinstitute.org with questions, comments or feedback.


### Citation

Mani, D. R. et al. PANOPLY: a cloud-based platform for automated and reproducible proteogenomic data analysis. *Nature Methods* 1–3 (2021) doi:10.1038/s41592-021-01176-6.
  

### References

1. Mertins, P. et al. Proteogenomics connects somatic mutations to signalling in breast cancer. *Nature* 534, 55–62 (2016).
2. Subramanian, A. et al. A Next Generation Connectivity Map: L1000 Platform and the First 1,000,000 Profiles. *Cell* 171, 1437–1452.e17 (2017).
3. Blumenberg, L. et al. BlackSheep: A Bioconductor and Bioconda package for differential extreme value analysis. *J of Proteome Research* 20(7), 3767-3773 (2021).
4.	Krug, K. et al. A Curated Resource for Phosphosite-specific Signature Analysis. *Mol. Cell. Proteomics* 18, 576–593 (2019).
5.	Subramanian, A. et al. Gene set enrichment analysis: a knowledge-based approach for interpreting genome-wide expression profiles. *Proc. Natl. Acad. Sci.* 102, 15545–15550 (2005).
6.	Barbie, D. A., Tamayo, P., Boehm, J. S., Kim, S. Y. & Moody, S. E. Systematic RNA interference reveals that oncogenic KRAS-driven cancers require TBK1. *Nature* (2009).
7. Gillette, M. A. et al. Proteogenomic Characterization Reveals Therapeutic Vulnerabilities in Lung Adenocarcinoma. *Cell* 182, 200–225.e35 (2020).
8. Krug, K., Jaehnig, E. J., Satpathy, S., Blumenberg, L., et al. Proteogenomic Landscape of Breast Cancer Tumorigenesis and Targeted Therapy. *Cell* 183, 1–21 (2020).
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","broad-firecloud-cptac/PANOPLY_Production_Pipelines_v1_3"
176,"warp-pipelines","Multiome","READER","https://app.terra.bio/#workspaces/warp-pipelines/Multiome",TRUE,FALSE,NA,NA,NA,"# Multiome Pipeline for Analysis of Single-cell Gene Expression and Chromatin Accessibility Data

Multiome is an open-source, cloud-optimized pipeline developed in collaboration with members of the [BRAIN Initiative](https://braininitiative.nih.gov/) (BICCN and [BICAN](https://brainblog.nih.gov/brain-blog/brain-issues-suite-funding-opportunities-advance-brain-cell-atlases-through-centers) Sequencing Working Group) and [SCORCH](https://nida.nih.gov/about-nida/organization/divisions/division-neuroscience-behavior-dnb/basic-research-hiv-substance-use-disorder/scorch-program) (see **Acknowledgements** below). It supports the processing of 10x 3' single-cell and single-nucleus gene expression (GEX) and chromatin accessibility (ATAC) data generated with the [10x Genomics Multiome assay](https://www.10xgenomics.com/products/single-cell-multiome-atac-plus-gene-expression). This workspace currently describes `v5.3.1` of the Multiome pipeline and provides a fully reproducible example of the workflow. 

Scroll down for details on the workflow, including input and output descriptions and requirements, estimated run times and costs, and information on downsampled example data.      

**Note that cost and time estimates will vary** with the use of [Preemptibles](https://cloud.google.com/compute/docs/instances/preemptible).  
You can also use [Google's cost calculator](https://cloud.google.com/products/calculator/) to estimate the cost to run. 

**For helpful hints on controlling Cloud costs**, see [Overview: Controlling Cloud costs on Terra](https://support.terra.bio/hc/en-us/articles/360029748111).  

### Acknowledgements

We are immensely grateful to the members of the BRAIN Initiative (BICAN Sequencing Working Group) and SCORCH for their invaluable and exceptional contributions to this pipeline. Our heartfelt appreciation goes to Alex Dobin, Aparna Bhaduri, Alec Wysoker, Anish Chakka, Brian Herb, Daofeng Li, Fenna Krienen, Guo-Long Zuo, Jeff Goldy, Kai Zhang, Khalid Shakir, Bo Li, Mariano Gabitto, Michael DeBerardine, Mengyi Song, Melissa Goldman, Nelson Johansen, James Nemesh, and Theresa Hodges for their unwavering dedication and remarkable efforts.

---

## Multiome

### What does it do?

The Multiome workflow is a wrapper WDL script that calls two subworkflows: the [Optimus workflow](https://broadinstitute.github.io/warp/docs/Pipelines/Optimus_Pipeline/README) for single-cell GEX data and the [ATAC workflow](https://broadinstitute.github.io/warp/docs/Pipelines/ATAC/README) for single-cell ATAC data. 

The Optimus (GEX) component corrects cell barcodes (CBs) and Unique Molecular Identifiers (UMIs), aligns reads to the genome, calculates per-barcode and per-gene quality metrics, and produces a raw cell-by-gene count matrix. 

The ATAC component corrects CBs, aligns reads to the genome, calculates per-barcode quality metrics, and produces a fragment file.

The wrapper WDL is available in the WARP repository (see the [code here](https://github.com/broadinstitute/warp/blob/master/pipelines/skylab/multiome/Multiome.wdl)).

For more details about the pipeline, see the [Multiome Overview](https://broadinstitute.github.io/warp/docs/Pipelines/Multiome_Pipeline/README/) in the [WARP documentation](https://broadinstitute.github.io/warp/).  

---

### How to run the workflow

This workspace is preloaded with a **[downsampled example human dataset](https://github.com/broadinstitute/warp/tree/develop/pipelines/skylab/multiome/test_inputs/Plumbing) and workflow configuration** (see table below) designed to demo the Multiome workflow quickly and inexpensively.

The workspace workflow is configured to leverage the `sample` and `sample_set` data tables in the workspace Data tab. 

To run the workflow: 

1. Navigate to the **Workflows** tab and choose the **Multiome** workflow configuration. 
1. Make sure the root entity type is `sample_set` (selected by default) on the workflow configuration page.
1. Choose the `10k_PBMC_downsampled` sample set.
1. Select the checkbox next to `Use reference disks`.
1. Run the analysis.

**The Multiome workflow is a single-sample pipeline, but can take in multiple sets of FASTQs for a sample that has been split over lanes of sequencing.** 

The workflow is configured to use a set table even though it only uses one lane of sequencing. If you want to use a sample with multiple lanes, each lane would be added to the sample table and a new set for all lanes created. To learn how to configure data tables with multiple sequencing lanes per sample, see [When and how to use a set table for a workflow](https://support.terra.bio/hc/en-us/articles/4417647037851). 

If you are running the Multiome workflow on your own data, it is important that you first set up a `sample` data table and then a `sample_set` table. 

#### Running the workflow in single-cell mode

If you are running the pipeline on your own data, be aware that the pipeline runs in single-nucleus (`sn_rna`) mode by default. To use the `sc_rna` mode for single-cell analyses, change the `counting_mode` input attribute to `sc_rna`.

---

### What does the Multiome workflow require as input?

The Multiome workflow requires the following input:

| Input Name | Description | Type |
| --- | --- | --- |
| vm_size | String defining the Azure virtual machine family for the workflow (default: ""Standard_M128s""). | String |
| cloud_provider | String describing the cloud provider that should be used to run the workflow; value should be ""gcp"" or ""azure"". | String |
| nhash_id | Optional identifier for the library aliquot; when specified, the workflow will echo the ID in the ATAC and gene expression output h5ads (in the adata.uns section) and in the library-level metrics CSV. | String |
| input_id | Unique identifier describing the biological sample or replicate that corresponds with the FASTQ files; can be a human-readable name or UUID. | String |
| gex_r1_fastq | Read 1 FASTQ files containing forward reads of a single GEX 10x library; contains unique molecular identifier (UMI) and cell barcode sequences. | Array[File] |
| gex_r2_fastq | Read 2 FASTQ files containing reverse reads of  a single GEX 10x library; contains the alignable genomic information from the mRNA transcript. | Array[File] |
| tar_star_reference | TAR file containing a species-specific reference genome and GTF file for Optimus (GEX) pipeline; generated using the [BuildIndices.wdl](https://github.com/broadinstitute/warp/blob/master/pipelines/skylab/build_indices/BuildIndices.wdl). | File |
| annotations_gtf | GTF file containing gene annotations used for Optimus (GEX) cell metric calculation and ATAC fragment metrics; must match the GTF file used to build the STAR aligner. | File |
| atac_r1_fastq | Read 1 paired-end FASTQ files representing a single 10x multiome ATAC library. | Array[File] |
| atac_r2_fastq | Barcodes FASTQ files representing a single 10x multiome ATAC library. | Array[File] |
| atac_r3_fastq | Read 2 paired-end FASTQ files representing a single 10x multiome ATAC library.	| Array[File] |
| tar_bwa_reference | TAR file containing the reference index files for BWA-mem alignment for the ATAC pipeline. | File |	
| chrom_sizes | File containing the genome chromosome sizes; used to calculate ATAC fragment file metrics.	| File |

#### Reference data description and location  
Human and mouse reference genomes and additional resources for the tools in this workspace are included in the **Workspace** data table. The references are preconfigured in the workspace example workflow configuration.

##### Reference genomes: 
The reference genome for human is hg38 (GRCh38), the [GENCODE v43](https://www.gencodegenes.org/human/release_43.html) primary assembly gene annotation list.

##### Enabling reference disks
The suggested workflow configuration (see **Running the workflow** above) uses [reference disks](https://support.terra.bio/hc/en-us/articles/360056384631-Reference-Disks-in-Terra). That means when Terra kicks off the workflow on a virtual computer, it attaches a portable disk (kind of like a flash-drive) that is preloaded with the reference files needed for the workflow. This saves time and cost running the workflow. 

---

### Optional parameters 

The Multiome workflow offers optional inputs such as the following (for a more complete list, see the [Multiome Overview](https://broadinstitute.github.io/warp/docs/Pipelines/Multiome_Pipeline/README#inputs):

| Input Name | Description | Type |
| --- | --- | --- |
| soloMultiMappers | Optional string describing whether or not the Optimus (GEX) pipeline should run STARsolo with the `--soloMultiMappers flag.` | String |
| counting_mode | Determines whether the Optimus (GEX) pipeline should be run in single-cell mode (`sc_rna`) or single-nucleus mode (`sn_rna`); default is ""sn_rna"". | String |
| gex_i1_fastq | Index FASTQ files representing a single GEX 10x library; multiplexed samples are not currently supported, but the file may be passed to the pipeline. | Array[File] |
| star_strand_mode | Describes whether STARsolo alignment in the Optimus (GEX) pipeline is performed on forward stranded, reverse stranded, or unstranded data; default is ""Forward"". | String |
| count_exons | Describes whether the Optimus (GEX) pipeline should calculate exon counts when in single-nucleus (`sn_rna`) mode; if ""true"" in `sc_rna` mode, the workflow will return an error; default is ""false"". | Boolean |
| adapter_seq_read1 | Adapter sequence for ATAC read 1 paired-end reads to be used during adapter trimming with Cutadapt; default is ""GTCTCGTGGGCTCGGAGATGTGTATAAGAGACAG"". | String |
| adapter_seq_read3 | Adapter sequence for ATAC read 2 paired-end reads to be used during adapter trimming with Cutadapt; default is ""TCGTCGGCAGCGTCAGATGTGTATAAGAGACAG"". | String |

---

### What does the Multiome workflow return as output?

In this workspace, the metadata for all outputs are written to the workspace data table. These outputs are described in the following table:  

| Output Name | Format and Description |
| --- | --- |
| library_metrics | Optional CSV file containing all library-level metrics calculated with STARsolo for gene expression data. |
| multimappers_EM_matrix | Optional output produced when `soloMultiMappers` is ""EM""; see STARsolo [documentation](https://github.com/alexdobin/STAR/blob/master/docs/STARsolo.md#multi-gene-reads) for more information. | 
| multimappers_Uniform_matrix | Optional output produced when `soloMultiMappers` is ""Uniform""; see STARsolo [documentation](https://github.com/alexdobin/STAR/blob/master/docs/STARsolo.md#multi-gene-reads) for more information. |
| multimappers_Rescue_matrix | Optional output produced when `soloMultiMappers` is ""Rescue""; see STARsolo [documentation](https://github.com/alexdobin/STAR/blob/master/docs/STARsolo.md#multi-gene-reads) for more information. |
| multimappers_PropUnique_matrix | Optional output produced when `soloMultiMappers` is ""PropUnique""; see STARsolo [documentation](https://github.com/alexdobin/STAR/blob/master/docs/STARsolo.md#multi-gene-reads) for more information. |
| gex_aligner_metrics | Text file containing per barcode metrics `(CellReads.stats)` produced by the GEX pipeline STARsolo aligner. |
| multiome_pipeline_version_out | String describing the version of the processing pipeline run on this data.	|
| bam_aligned_output_atac | BAM file containing aligned reads from ATAC workflow. |
| fragment_file_atac | Sorted and bgzipped TSV file containing fragment start and stop coordinates per barcode. The columns are ""Chromosome"", ""Start"", ""Stop"", ""ATAC Barcode"", ""Number of reads"", and ""GEX Barcode"". |
| fragment_file_index | tabix index file for the fragment file. |
| snap_metrics_atac | h5ad (Anndata) file containing per-barcode metrics from SnapATAC2; contains the equivalent gene expression barcode for each ATAC barcode; see the [ATAC Count Matrix Overview](https://broadinstitute.github.io/warp/docs/Pipelines/ATAC/count-matrix-overview) for a detailed list of metrics. |
| genomic_reference_version_gex | File containing the Genome build, source, and GTF annotation version. |
| bam_gex | BAM file containing aligned reads from Optimus (GEX) workflow. |
| matrix_gex | NPZ file containing raw gene by cell counts. |
| matrix_row_index_gex | NPY file containing the row indices. |
| matrix_col_index_gex | NPY file containing the column indices. |
| cell_metrics_gex | CSV file containing the per-cell (barcode) metrics. |
| gene_metrics_gex | CSV file containing the per-gene metrics. |
| cell_calls_gex | TSV file containing the EmptyDrops results when the Optimus (GEX) workflow is run in `sc_rna` mode. |
| h5ad_output_file_gex | h5ad (Anndata) file containing the raw cell-by-gene count matrix, gene metrics, cell metrics, and global attributes; contains equivalent ATAC barcode for each gene expression barcode; see the [Optimus Count Matrix Overview](https://broadinstitute.github.io/warp/docs/Pipelines/Optimus_Pipeline/Loom_schema) for a detailed list of metrics.  |

---

### Estimated time and cost to run on sample data 

The following estimates are based on the **downsampled example human dataset** with reference disks enabled. All details of each set are listed to give insight into time and cost.

| Sample Set Name | Set Size | GEX FASTQ File Sizes | ATAC FASTQ File Sizes | Time | Cost |
| --- | --- | --- | --- | --- | --- |
| 10k_PBMC_downsampled | 1 entity | 845.86 MB, 1.99 GB | 1.02 GB, 553.62 MB, 1.02 GB | 6:21:00 | $1.52 |

**For helpful hints on controlling Cloud costs, see the article, [Overview: Controlling Cloud costs on Terra](https://support.terra.bio/hc/en-us/articles/360029748111).**   

---

### Versioning and testing

All versions of the pipeline listed here are available by cloning the workspace and selecting a version of the Multiome method in the version dropdown. All **official** Multiome pipeline releases are documented in the [changelog](https://github.com/broadinstitute/warp/blob/master/pipelines/skylab/multiome/Multiome.changelog.md) and tested using [plumbing and scientific test data](https://github.com/broadinstitute/warp/tree/master/pipelines/skylab/multiome/test_inputs). To learn more about WARP pipeline testing, see [Testing Pipelines](https://broadinstitute.github.io/warp/docs/About_WARP/TestingPipelines).

**Note: This workspace showcases an unofficial pipeline release and has not been scientifically validated.**

| Terra Compatible Version Name | Multiome Release Version | Date | Release Note | 
| :---:  | :---: | :---: | :--- |
| Multiome_v5.3.1  | v5.3.1 | 08/2024 | Updated SnapATAC2 docker to SnapATAC2 v2.6.3. Added an optional input for expected cells which is used for metric calculation. Updated the Multiome.wdl to run on Azure; cloud_provider is a new, required input. |
| Multiome_v4.0.1 | v4.0.1 | 07/2024 | Added the --soloMultiMappers flag as an optional input to the StarSoloFastq task in the StarAlign.wdl; Added ""Uniform"" as the default string for STARsolo multimapping parameters; Added the gene expression library-level metrics CSV as output of the Multiome pipeline; this is produced by the Optimus subworkflow; Updated the docker for the MergeStarOutput task to include STARsolo v2.7.11a; Updated SnapATAC2 docker to SnapATAC2 v2.6.3; Updated the Multiome.wdl to run on Azure. |
| Multiome_v3.0.2 | v3.0.2 | 02/2024 | Updated the Metrics task so that Cell Metrics and Gene Metrics now calculate intronic, intronic_as, exonic, exonic_as, and intergenic metrics from unique reads only using the NH:i:1 tag in the BAM; added support for REFSEQ references; added an optional task that will run CellBender on the Optimus output h5ad file; added gene expression barcodes to the Multiome ATAC fragment file; added tabix index file as a pipeline output. |
| Multiome_v2.2.2 | v2.2.2 | 10/2023 | Updated to include STARsolo v2.7.11a; added sF tag for metric calculation; added additional metrics to outputs h5ad files; moved whitelists to public storage bucket. |
| Multiome_v1.0.1 | v1.0.1 | 07/2023 | Added STARsolo v2.7.10b metric outputs as an optional pipeline output and an output of the STARalign and MergeSTAR tasks. |
| Multiome_v1.0.0 | v1.0.0 | 06/2023 | First release of the pipeline. |

---

### Contact information

* For workspace questions and feedback, email the Broad pipelines team at warp-pipelines-help@broadinstitute.org or create a post on the [Featured Workspaces community forum](https://support.terra.bio/hc/en-us/community/topics/360001603491) (login required). 

* You can also Contact the Terra team for questions from the Terra main menu. When submitting a request, it can be helpful to include:
    * Your Project ID
    * Your workspace name
    * Your Bucket ID, Submission ID, and Workflow ID
    * Any relevant log information

---

### License
**Copyright WARP,  2024 | BSD-3**

All rights reserved. Full license text at https://github.com/broadinstitute/warp/blob/master/LICENSE. Note that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.

---",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","warp-pipelines/Multiome"
177,"anvil-workshop","HPRC-Giraffe-Demo-2023","READER","https://app.terra.bio/#workspaces/anvil-workshop/HPRC-Giraffe-Demo-2023",TRUE,FALSE,NA,NA,NA,"The workspace demonstrates variant calling using the Human Pangenome Reference Consortium's (HPRC) year 1 Minigraph/CACTUS pangenome with the Giraffe/DeepVariant pipeline for calling germline small variants. This workspace is intended to be a demonstration of utilizing a pangenome from the HPRC in AnVIL and Terra. 

Note that the WDL included in the workspace is still under active development and has not been peer-reviewed. For up-to-date best practices for both Giraffe and DeepVariant please see the links included in the relevant sections below.

## Tasks

1. Clone workspace
2. Have a look at the ""sample"" Table, the ""Workspace Data"", and the ""Files""
3. Launch a workflow on the sample in the ""sample"" Table.
4. Monitor running job
5. Notebook
   1. Launch an Analyses jupyter notebook
   2. Once the workflow job is done (steps 3-4 above), start running code in the notebook.
	 3. (Optional) Discuss pangenome (+reads) representations. What's going on?
	 4. (Optional) Blast a read to GRCh38. Why would GRCh38 lead to false-negative in this region?

Side tasks:

1. Find the GiraffeDeepVariant workflow in dockstore
2. Find data in AnVIL, e.g. 1000GP reads, HPRC assemblies/pangenomes

Detailed instructions for running this workspace can be downloaded from [here](https://docs.google.com/document/d/1BmVYP8UvQX5mRztu37kZac0BP_aOAnocL2X5caGMQvQ/edit?usp=sharing).

## Data

For this demonstration, we will work with a region of chromosome 1 containing, among others, the RHCE gene which is a a challenging medically-relevant gene. ""Challenging medically-relevant genes"" are difficult to assess with short-read sequencing but are part of a recent benchmark truthset ([CRMG v1.0](https://www.nature.com/articles/s41587-021-01158-1)). The genomic interval extracted was `chr1:25053647-25685365` ([see in the UCSC Genome Browser](https://genome.ucsc.edu/cgi-bin/hgTracks?db=hg38&position=chr1%3A25053647%2D25685365)). 
The reads and pangenome in this workspace correspond to this slice of the genome.

### HG002 Illumina Reads (30X)

The sample-based data we will use in this workspace is loaded into the sample data table. An Illumina dataset (produced by Google and made publicly available) with 30X coverage of HG002 has been sliced on the relevant region (as explained above) and imported into the workspace. The sample table has multiple columns and the columns with the prefix input_ (e.g. input_fastq_1) are used as inputs for the Giraffe/DeepVariant workflow. The workflow has been prerun in a separate table ""sample_prerun"" for convenience, and the outputs have been written to columns with names prepended with ""output_"".

### Minigraph/CACTUS Pangenome

This workspace uses one of the HPRC's year 1 pangenomes created with the [Minigraph/CACTUS pipeline](https://github.com/ComparativeGenomicsToolkit/cactus/blob/master/doc/pangenome.md). For the Giraffe/DeepVariant pipeline, it is recommended to use a filtered version of the GRCh38-based graph. Information about the HPRC's pangenome releases can be found in the HPRC's [publicly available AnVIL workspace](https://app.terra.bio/#workspaces/anvil-datastorage/AnVIL_HPRC) as well as the HPRC's [pangenome resources GitHub repo](https://github.com/human-pangenomics/hpp_pangenome_resources).

For this demonstration, we extracted the sub-graph of the chr1 region from the non-filtered GRCH38-based pangenome, in order to deal with only one pangenome for both variant calling and visualization of all the haplotypes.

### DeepVariant (DV) Model

DeepVariant uses a learned model to call variants in aligned sequencing data. A model has been imported into this workspace and can be found in the Data tab under ```Files --> dv-giraffe-model/.```. The model was trained with DV 1.3 on the GRCh38-based Cactus-Minigraph pangenome, on ~30-40x coverage read sets.

## Workflow: Variant Calling With a Pangenome & Giraffe/DeepVariant


### [GiraffeDeepVariant](https://dockstore.org/workflows/github.com/vgteam/vg_wdl/GiraffeDeepVariantLite:giraffe-dv-dt-hprcy1)

This workflow aligns reads to a pangenome graph and uses Google's DeepVariant caller to produce an output VCF. The steps are summarized below:

1. Split reads
2. Align reads to pangenome with Giraffe
3. Realign around InDels (optional)
4. Call variants with DeepVariant


#### Recommended Inputs

**This workflow has a number of inputs. For convenience the recommended workflow inputs have been pre-populated in the workflow inputs tab. Alternatively, users may navigate to the [example input json](https://drive.google.com/file/d/1zYfiUPYS8ZaWnWhaHMNnGCYfu5XRHlZj/view?usp=sharing) and [example output json](https://drive.google.com/file/d/1gaDxfT2u0U5avmpGet_KdJhD9M4ex6bX/view?usp=sharing) included as part of this workspace.**


#### Outputs

* output_vcf: VCF output from DeepVariant
* output_calling_gam: reads aligned to the pangenome with Giraffe as a GAM file.

#### Time and cost estimates    
Note that actual time and cost may vary due to the use of preemptible instances. 

| Input Coverage | Time | Cost |
| -------- | -------- | ---------- |
| 30X | < 5 minutes | $1 |

*(for the subset data included in this workspace)*

## Notebook: Visualize pangenome and reads around a called variant

The notebook can be found in the *ANALYSES* tab.

The ""Environment configuration"" to run it should be:

- *Jupyter* environment
	- Custom environment
		- *Application configuration*: *Custom environment*
		- *Container image*:  `jmonlong/terra-notebook-vg:1.1` 

### Alternative: interactive visualization with the SequenceTubeMap

The approach showed in the notebook can be useful to look at a small subgraph or automate image creation. 
For other use cases, we tend to use the interactive (and better-looking) [SequenceTubeMap](https://github.com/vgteam/sequenceTubeMap). 
Below is what the notebook example would look like:

![](http://public.gi.ucsc.edu/~jmonlong/hprc/ashg2022-hprc-workshop-tubemap-example.small.png)


## Next Steps
To run this pipeline on your own data you have to upload your data to the Google Cloud bucket for your version of the workspace. You can find the bucket information in the Google Bucket section on the right hand side of this page. In that section you can find the bucket name for gsutil commands, or you can open the bucket in your web browser. You can also (optionally) create a new data table in your workspace which points to the data you uploaded.

----
----
## Authors and contact information

This workspace is a product of the Human Pangenome Reference Consortium [HPRC](https://humanpangenome.org/) and [the AnVIL](https://anvilproject.org/). Contributors include:

* Jean Monlong: jmonlong@ucsc.edu (UCSC Computational Genomics Lab)
* Julian Lucas: juklucas@ucsc.edu (UCSC Computational Genomics Platform)

The Giraffe/DeepVariant workflow was developed by the VG team at UCSC and Google. Contributors include:

* Charles Markello (UCSC)
* Jean Monlong (UCSC)
* Adam Novak (UCSC)
* Maria Nattestad (Google)
* Pichuan Chang (Google)
* Andrew Carroll (Google)

## Additional generally helpful resources

* **[HPRC Pangenome GitHub](https://github.com/human-pangenomics/hpp_pangenome_resources)**   
    Description of HRPC's currently available (release) pangenomes.   
		 
		
* **For helpful hints on controlling cloud costs**, see [this article (https://support.terra.bio/hc/en-
us/articles/360029748111)](https://support.terra.bio/hc/en-us/articles/360029748111).      
 

 ## Citations
 
1. Sirén, Jouni, et al. ""Pangenomics enables genotyping of known structural variants in 5202 diverse genomes."" Science 374.6574 (2021): abg8871.
2. Poplin, Ryan, et al. ""A universal SNP and small-indel variant caller using deep neural networks."" Nature biotechnology 36.10 (2018): 983-987.

### Change log  
08 March 2023 - Initial release",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","anvil-workshop/HPRC-Giraffe-Demo-2023"
178,"help-terra","Accessing advanced GCP features","READER","https://app.terra.bio/#workspaces/help-terra/Accessing%20advanced%20GCP%20features",TRUE,FALSE,NA,NA,NA,"These notebooks walk through how to access additional GCP features in Terra by leveraging a proxy group and Google Cloud Platform.    


- WRITE to BigQuery     
- Interact with Cloud Storage buckets other than the workspace bucket    
- Run dsub jobs    
- Run Cloud Dataflow jobs    
- Run Cloud ML engine jobs            
 
To learn more about using proxy groups to perform Google Cloud Platform operations not currently available in the Terra UI, see [this article](https://support.terra.bio/hc/en-us/articles/360051229072).            
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-terra/Accessing advanced GCP features"
179,"help-terra","BICCN-Omics-Workshop-June-2022","READER","https://app.terra.bio/#workspaces/help-terra/BICCN-Omics-Workshop-June-2022",TRUE,TRUE,NA,NA,NA,"# BRAIN Initiative Cell Census Network (BICCN) Omics June 2022 Workshop

This tutorial workspace is a step-by-step guide to analyzing BICCN 10x Genomics single-cell data. Using this workspace, you will:

1. Import an example 10x dataset (FASTQs) from **NeMO**.
2. Align example 10x FASTQs and produce a raw count matrix with quality metrics using the **Optimus workflow**.
3. Remove background with **CellBender's `remove-background` workflow**.
4. Explore single-cell data in the **10x_analysis_in_Scanpy** Jupyter Notebook and compare CellBender results to raw data using the **Exploring_CellBender_Matrices** Jupyter Notebook. 


![](https://storage.googleapis.com/terra-featured-workspaces/BICCN_Workshop_Workspace/flow_diagram_june22.png)


### Work from your copy of the workspace following these step-by-step instructions 
|![PDF icon](https://storage.googleapis.com/terra-featured-workspaces/BICCN_Workshop_Workspace/Copy%20of%20PDF-icon_scaled.png) | Download a clickable PDF of step-by-step instructions [here](https://storage.googleapis.com/terra-featured-workspaces/BICCN_Workshop_Workspace/BICCN%20Workshop%20June%202022%20Tutorial.pdf) |  
| --------| ------------------|     


# Instructions

## Set up the tutorial workspace 

Before you begin, [create your own editable copy (clone)](https://support.terra.bio/hc/en-us/articles/360026130851) of this WORKSPACE. 

Click the round circle with three dots in the upper right corner of this page and choose ""Clone"".


![](https://storage.googleapis.com/terra-featured-workspaces/BICCN_Workshop_Workspace/clone.png)


## Step 1. Import Data from the NeMO Data Portal
### NeMO Overview
The [Neuroscience Muti-omic Data Archive (NeMO)](https://nemoarchive.org/) is a data repository for the BRAIN Initiative and related brain research initiatives.  You can access NeMO data using the NeMO [Data Portal](https://portal.nemoarchive.org/).  

### Quick-start instructions
1. Navigate to the NeMO [Data Portal](https://portal.nemoarchive.org/).
2. Use the faceted search to identify a 10x data set of interest (see the step-by-step for a suggested data set).
3. Add the selected data to the NeMO cart.
4. From the cart, select `Download` and then `Export to Terra`.
5. When prompted, choose the existing cloned version of this workspace.

##  Step 2. Process 10x data with the Optimus workflow

### Optimus Overview   

This Optimus workflow has a quality control, alignment and transcriptome quantification module. It corrects Cell Barcodes (CBs) and Unique Molecular Identifiers (UMIs), aligns reads to the genome, generates a count matrix in a UMI-aware manner, detects empty droplets (single-cell mode only), calculates summary metrics for genes and cells, returns read outputs in BAM format, and returns raw counts in Loom file format. 

For more details about the pipeline, see the [Optimus Overview](https://broadinstitute.github.io/warp/docs/Pipelines/Optimus_Pipeline/README) in the [WARP documentation](https://broadinstitute.github.io/warp/).  

### Sample data
This step uses an example mouse MOp 10x v2 data set that  that was exported from the NeMO archive.  The raw FASTQ files are listed in the  `file` data table on the Data tab. 

These FASTQ files contain two lanes of sequencing which are grouped together in the `file_set` table with the set name `Example_MOP_set`. 

The example Optimus workflow is set up to use the `file_set` table as the workflow's root entity, but parts of the set-up are missing to give you hands-on practice. 

Follow the instructions below to setup and run the workflow.

### Quick-start instructions
1.  Go to the `file_set` data table on the `Data` tab.
1.  Select the `Example_MOP_set` file-set.
1.  Open with the `1-Optimus-mouse-v2` workflow.
1.  Go to the workflow Inputs and set the r1_fastq variable to `this.files.r1_fastq`.
1.  Go to the workflow Outputs and set the output-loom-file to `this.my_loom`.
1.  Save the configuration.
1.  Select `Run Analysis` and then `Launch`.

### Outputs
The Optimus pipeline writes several outputs to the `file_set` data table, including a Loom matrix with raw counts and quality metrics which you can find in the ""my_loom"" column you created in Step 5.

## Step 3. Run CellBender's `remove-background` workflow

### CellBender Overview

The CellBender `remove-background` workflow models and removes systematic biases and background noise from raw cell-by-gene count matrices and produces improved gene expression estimates.

Overall, the module:
* Removes counts due to ambient RNA molecules and random barcode swapping from (raw) UMI-based scRNA-seq cell-by-gene count matrices. 

* Supports both the count matrices produced by the CellRanger count pipeline and Loom matrices produced with the [Optimus pipeline](https://app.terra.bio/#workspaces/featured-workspaces-hca/HCA_Optimus_Pipeline).

For more details about the pipeline, see the [CellBender documentation](https://cellbender.readthedocs.io/en/latest/).

> This workspace currently contains the development version of the CellBender workflow: cellbender/remove-background-v3-alpha/1. Check back for future updates.

### Sample data
This step uses the Loom count matrix generated with Optimus as CellBender input. By default, the workflow is set up to use the output-loom column of the `file_set` data table, which contains a ""pre-baked"" Loom file so that you don't have to run  Optimus prior to trying the CellBender workflow.  This Loom file should be identical to the one you created with Optimus in Step 2.

### Quick-start instructions
1. Go to the `file_set` data table on the `Data` tab.
1. Select the `Example_MOP_set` file-set.
1. Open with the `2-remove-background` workflow.
1. Go to the inputs section of the workflow configuration.
1. On the Inputs tab, paste your Google Bucket ID into  the `output_bucket_base_directory` attribute where it says PASTE_BUCKET_ID_HERE (""gs://PASTE_GOOGLE_BUCKET_ID_HERE""). The final attribute should look similar to ""gs://fc-a8dd893f-84cf-4d28-952d-be4ae5805ebf"".
1. Save the setup.
1. Select `Run Analysis` and then `Launch`. 


### Outputs
The `remove-background` workflow writes outputs to the `file_set` data table including an array of H5 cell-by-gene matrix files with corrected counts (one array per false positive rate - FPR).
   *  The first file path in the array is the path to the unfiltered CellBender matrix (suffix is ""out.h5"").
   *  The second file path in the array is the path to the filtered CellBender matrix (suffix is ""out_filtered.h5"").

Additional outputs are detailed in the [CellBender documentation](https://cellbender.readthedocs.io/en/latest/). 

## Step 4. Explore 10x single-cell data with Jupyter Notebooks
This workspace contains two example Jupyter Notebooks to explore the cell-by-gene outputs.

The first notebook, `10x_analysis_in_Scanpy`, is a guided tutorial for filtering, normalizing, and clustering 10x single-cell RNA-seq data that is based on Scanpy’s [Preprocessing and clustering 3k PBMCs
](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html). 

The second notebook, `Exploring_CellBender_Matrices`, is the companion notebook for the [CellBender featured workspace](https://app.terra.bio/#workspaces/help-terra/CellBender). It contains useful tools and visualizations to compare the raw Optimus cell-by-gene matrix to a matrix filtered by the CellBender `remove-background` workflow.

### Sample data

This step uses data hosted in the workspace `file_set` data table: the unfiltered CellBender output H5 file (first file from the array in the `h5_array` column) and the Optimus Loom output file (from the loom_output_file column). 

### Quick-start instructions
1. Select the Cloud Environment widget and choose the Custom option.
2. Select `Default` from the Application Configuration drop-down.
3. In the `Cloud compute profile`, select **8 CPUs** from the CPU drop-down.
4. Go to the workspace `Notebooks` page (or `Analyses` page if using the [new layout](https://support.terra.bio/hc/en-us/articles/5170361534363-Analyses-tab-FAQs)) and open either the `10x_analysis_in_Scanpy` notebook or the `Exploring_CellBender_Matrices` notebook in edit mode.
5. Follow the instructions listed in the notebook.

## References
BRAIN Initiative Cell Census Network (BICCN) et al. A multimodal cell census and atlas of the mammalian primary motor cortex. bioRxiv 2020.10.19.343129; doi: https://doi.org/10.1101/2020.10.19.343129

Stuart T, Butler A, Hoffman P, Hafemeister C, Papalexi E, Mauck WM 3rd, Hao Y, Stoeckius M, Smibert P, Satija R. Comprehensive Integration of Single-Cell Data. Cell. 2019 Jun 13;177(7):1888-1902.e21. doi: 10.1016/j.cell.2019.05.031. Epub 2019 Jun 6. PMID: 31178118; PMCID: PMC6687398.

Wolf et al. (2018), Scanpy: large-scale single-cell gene expression data analysis, [Genome Biology](https://doi.org/10.1186/s13059-017-1382-0).

## Additional resources

* [CellBender workspace](https://app.terra.bio/#workspaces/help-terra/CellBender)
* [Optimus workspace](https://app.terra.bio/#workspaces/featured-workspaces-hca/HCA_Optimus_Pipeline) 


## Acknowledgements
Special thanks to Brian Herb and the NeMO team, the Bioconductor team (Martin Morgan, Vince Carey, and Nitesh Turaga), the CellBender team (Stephen Flemming and Mehrtash Babadi), and the Broad Pipelines team (Farzaneh Khajouei and Kylee Degatano) for their feedback and amazing contributions to this workspace.

### License
**Copyright Broad Institute, 2022 | BSD-3**

All rights reserved. Full license text available [here](https://github.com/broadinstitute/warp/blob/master/LICENSE). Note that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.

---",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-terra/BICCN-Omics-Workshop-June-2022"
180,"help-gatk","Terra Notebooks Playground","READER","https://app.terra.bio/#workspaces/help-gatk/Terra%20Notebooks%20Playground",TRUE,FALSE,NA,NA,NA,"# Terra notebooks playground

A workspace for trying out Terra functionality as it evolves.

---
## What's in this workspace?

Terra includes multiple tools for interactive computational analysis, including Jupyter, a web-based application that supports code in a variety of languages (R and Python, among others), and streamlines interaction with cloud-based resources. This workspace contains a set of Jupyter Notebooks that allow users to play with this functionality.
 
The notebooks available in this workspace are listed below. If you are entirely new to Jupyter and are not sure where to start, first see the [Notebooks Quickstart Guide](https://support.terra.bio/hc/en-us/articles/360059009571-Notebooks-Quickstart-Guide).

* **Python**
    * `py3__how_to_read_data_from_bigquery.ipynb`
    * `py3__how_to_load_data_to_bigquery.ipynb`
    * `py3__how_to_read_data_from_cloud_storage.ipynb`
    * `py3__how_to_store_analysis_results_in_bigquery_and_cloud_storage.ipynb`
    * `py3__how_to_use_a_cohort.ipynb`
    * `py3__how_to_use_facets_for_interactive_visualization_of_data.ipynb`
    * `py3__how_to_use_igv_in_a_terra_notebook.ipynb`
* **R**
    * `r__how_to_read_data_from_bigquery.ipynb`
    * `r__how_to_load_data_to_bigquery.ipynb`
    * `r__how_to_save_and_load_r_objects_from_the_workspace_bucket.ipynb`
    * `r__how_to_save_images_and_tables_to_files.ipynb`
    * `r__how_to_use_a_cohort.ipynb`
    * `r__how_to_emit_markdown_from_code_cells.ipynb`
    * `r__how_to_pull_in_reusable_code_from_source_control.ipynb`
    * `r__how_to_store_revisions_and_reports.ipynb`
    * `r__analysis_example_for_exploring_power_law_distributions.ipynb`
    * `r__plotting_example_for_boston_311_data.ipynb`
* **Notebook-based tools**
    * `create_html_snapshots_of_notebooks.ipynb`
    * `hail__how_to_run_a_notebook_in_the_background.ipynb`

Details
* Only public data is used in these example notebooks. The notebooks here often use 1000 Genomes sample metadata from either of:
    * BigQuery table [`bigquery-public-data:human_genome_variants.1000_genomes_sample_info`](https://console.cloud.google.com/bigquery?ws=!1m5!1m4!4m3!1sbigquery-public-data!2shuman_genome_variants!3s1000_genomes_sample_info)
    * Cloud Storage file [`gs://genomics-public-data/1000-genomes/other/sample_info/sample_info.csv`](https://console.cloud.google.com/storage/browser/genomics-public-data/1000-genomes/other/sample_info/)
* Tip: Not all warning messages are bad! Often you will run a cell that will return warnings in the output (sometimes these warnings will even be alarmingly color-coded in red or pink). This does not mean that the cell was not successful, and these warnings can often be ignored.

Next steps:
* **Have questions, comments, feedback?** You can file a [GitHub issue](https://github.com/DataBiosphere/terra-examples/issues) or click “Contact Us” in the main menu on the left to submit feedback, or go to our [forum](https://broadinstitute.zendesk.com/hc/en-us/community/topics/360000500432-General-Discussion) by clicking “Community Discussion”, also in the left-hand menu
* **Interested in contributing new notebooks or code improvements for this workspace?** You can send a [pull request](https://github.com/DataBiosphere/terra-examples/pulls) or submit a [feature request](https://broadinstitute.zendesk.com/hc/en-us/community/topics/360000500452-Feature-Requests) by clicking the “Request a feature” button  in the main menu on the left
* **Don’t forget to come back and check for updates to this workspace!**

## Appendix

### Terra documentation and support

Please also see [Terra documentation](https://broadinstitute.zendesk.com/hc/en-us). For assistance, reach out to the support team by following the “Contact Us” link in the Terra menu.

You can also interact with the Terra team, as well as other fellow users, by clicking “Community Discussion” in the Terra menu to visit our [forum](https://broadinstitute.zendesk.com/hc/en-us/community/topics/360000500432-General-Discussion).

### R resources

For data wrangling, visualization, and general analysis:

* [R for Data Science](http://r4ds.had.co.nz/) teaches how to wrangle and visualize data
* [Cheat Sheets](https://www.rstudio.com/resources/cheatsheets/) for commonly used R packages. Of particular note:
    * [Data Transformation Cheat Sheet [dplyr]](https://raw.githubusercontent.com/rstudio/cheatsheets/main/data-transformation.pdf)
    * [Work with Strings Cheat Sheet [stringr]](https://raw.githubusercontent.com/rstudio/cheatsheets/main/strings.pdf)
    * [Dates and Times Cheat Sheet [lubridate]](https://raw.githubusercontent.com/rstudio/cheatsheets/main/lubridate.pdf)
    * [Apply Functions Cheat Sheet [purrr]](https://raw.githubusercontent.com/rstudio/cheatsheets/main/purrr.pdf)

For domain-specific analysis packages you might consider using, see 

* [CRAN Task View: Survival Analysis](https://cran.r-project.org/web/views/Survival.html)
* [CRAN Task View: Time Series Analysis](https://cran.r-project.org/web/views/TimeSeries.html)

If you are developing R packages:

* [The tidyverse style guide](https://style.tidyverse.org/)
* [Advanced R](http://adv-r.had.co.nz/)
* [R packages](http://r-pkgs.had.co.nz/)
* [RStudio, Git and GitHub](https://support.rstudio.com/hc/en-us/articles/200532077-Version-Control-with-Git-and-SVN)

### Plotting Resources

* Great [talk](https://youtu.be/IzXxTeQhdO0) on data visualization, interesting points on:
    * use of 'small multiples': same data plotted multiple ways
    * using color intentionally
    * how to decide how much time and energy to put into your plots
* [One chapter]( http://r4ds.had.co.nz/data-visualisation.html) on ggplot2
* [ggplot2 cheatsheet](https://raw.githubusercontent.com/rstudio/cheatsheets/main/data-visualization.pdf) 
* [R Graph Gallery](https://www.r-graph-gallery.com/) inspiration and help with R Graphics
* [This is the most important tip of all ;-)](https://flowingdata.com/2012/06/07/always-label-your-axes/)

---

### Contact information

In addition to Terra support, you can also reach us on GitHub by [filing an issue](https://github.com/DataBiosphere/terra-examples/issues).

### License
Please see the BSD-3-Clause [license on GitHub](https://github.com/DataBiosphere/terra-examples/blob/main/LICENSE.md)

### Workspace Change Log
Please see the [pull request history](https://github.com/DataBiosphere/terra-examples/pulls?q=is%3Apr+) on GitHub.
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/Terra Notebooks Playground"
181,"largescale-gxe-methods","GEM-showcase","READER","https://app.terra.bio/#workspaces/largescale-gxe-methods/GEM-showcase",TRUE,TRUE,NA,NA,NA,"## Genome-wide interaction analysis with GEM

This workspace demonstrates a gene-environment interaction analysis pipeline using Terra.

Specifically, we will use the software program GEM (Gene-Environment interaction analysis for Millions of samples). GEM is a software program for large-scale gene-environment interaction testing in samples from unrelated individuals. It enables genome-wide studies in up to millions of samples while allowing for multiple exposures, control for genotype-covariate interactions, and robust inference.

* GitHub (source code): https://github.com/large-scale-gxe-methods/GEM
* GitHub (workflow code): https://github.com/large-scale-gxe-methods/gem-workflow
* Dockstore (workflow repository): https://dockstore.org/workflows/github.com/large-scale-gxe-methods/gem-workflow/gem-workflow:master?tab=info
* GEM manuscript: https://doi.org/10.1093/bioinformatics/btab223

## Steps in the pipeline

* Import public genotype data (1000 Genomes Project)
	* *These files are already available in Workspace Data*
* Subset 1000 Genomes VCF files to only top variants from an existing study (Sung et al. 2018; Workflow: `subset-vcf-workflow`)
	* *These files are already available in Workspace Data* 
* Create a simulated phenotype file (Notebook: `simulate_phenotypes`)
* Convert whole-genome VCF files to PLINK2 (pgen/pvar/psam) format (Workflow: `genotype-conversion`)
	* *These files are already available in Workspace Data*
* Run genome-wide interaction analysis using GEM (Workflow: `gem-workflow`)
* Summarize and visualize the results (Notebook: `summarize_GWIS`)

## Data
This workspace uses genotype data from 1000 Genomes Project (1KG), a public resource accessible from Terra. Genotype VCF files are available here: (https://cloud.google.com/life-sciences/docs/resources/public-datasets/1000-genomes). DRS URIs (unique links to the master version of these files) have been gathered and placed in the Workspace Data area (1000G_vcf_files_autosomes and 1000G_vcf_files_X).

Phenotype data are simulated based on real summary statistics from a genome-wide interaction study (GWIS) -- see the `simulate_phenotype` notebook.

## Notebooks
**simulate_phenotypes.ipynb**: Collect the subset of relevant genotypes and simulate a phenotype with gene-environment interaction effects based on summary statistics from Sung et al. 2018.

**summarize_GWIS.ipynb**: Collect the GWIS summary statistics and create basic summary plots.

All notebooks can be run using the Terra Bioconductor base image: us.gcr.io/broad-dsp-gcr-public/terra-jupyter-bioconductor:2.0.1

## Workflows
### genotype-conversion

Converts genotype files across common file formats (in this case, VCF to PLINK2 format).

Inputs: Genotype file and string indicating the conversion of interest.

Outputs: Converted genotype file (along with any supplementary files, e.g., .pvar/.psam for PLINK2 format).

### gem-workflow

Runs a GWIS using the GEM program. Using the parameters in the `analysis` Data Table will reproduce a GEM analysis using the following command: `./GEM --pfile *example_plinkset* --pheno-file smk_bp_sim_phenos.csv --sampleid-name id --pheno-name bp_sim --covar-names age sex --exposure-names smk_sim --robust 1 --maf 0.01 --missing-value NA --threads 8 --out gem_res`

Inputs:
* Array of one or more genotype files (.bgen, .pgen, or .bed format)
* Phenotype file containing, at minimum: sample identifiers, an exposure variable, and an outcome variable
* Additional parameters: see the workflow Inputs and ""analysis"" Data Table for details.

Outputs: 
* One summary statistics file (concatenated across all input genotype files)
* Supplementary files estimating resource usage (e.g., memory usage profile)

Sample runtime and cost estimation (scales roughly linearly with sample size and # variants):

| Analysis | Sample size | # variants | Time (CPU hrs) | Cost ($) |
| :---:  | :---: | :---: | :---: | :---: |
| 1KG genome-wide interaction study | 1656 | 13.5M | 1.94 | 0.38 |

## Authors and contact information
This workspace is a product of the Manning Lab, in collaboration with the Data Sciences Platform at The Broad Institute of MIT and Harvard. Contributing authors include:

* Alisa Manning (Manning Lab; akmanning@mgh.harvard.edu)
* Ling Chen (Manning Lab; lchen30@mgh.harvard.edu)
* Kenny Westerman (Manning Lab; kewesterman@mgh.harvard.edu)
* Tim Majarian (Manning Lab) 
* Casey Marchek (Manning Lab)

## License
GEM : Gene-Environment interaction analysis for Millions of samples
Copyright (C) 2018-2021  Liang Hong, Han Chen, Duy Pham

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program.  If not, see <http://www.gnu.org/licenses/>.




",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","largescale-gxe-methods/GEM-showcase"
182,"broad-firecloud-dsde-methods","malaria-amplicon-decontamination","READER","https://app.terra.bio/#workspaces/broad-firecloud-dsde-methods/malaria-amplicon-decontamination",TRUE,FALSE,NA,NA,NA,"# Malaria Amplicon Data
This workspace aggregates several workflows to process amplicon data related to malaria profiling, mostly based on the 4CAST amplicon panel. For a full tutorial on how to input data, operate the different workflows, and download results, please visit the documentation provided in Google Drive.

![Illumina plate](https://www.neb.com/~/media/Catalog/All-Protocols/C1A38CEF02DE4C6E8F70A6D00B1EC9A7/Content/96well_deep_well_plates.jpg)

Each workflow is paired to a Jupyter Notebook. The Jupyter Notebooks assemble report-ready images and compile information for their interpretation. Such instructions are provided, inside the Jupyter Notebook, at the beginning of each code block that produces said images. In some cases, user input is necessary to operate the Notebook. Nonetheless, we have reduced to the minimum prompts that have to be inputed by the user. Furthermore, for ease of usage, code blocks that have to be manually inputed are highlighted with:

#### Input code

## 1. Amplicon decontamination with combinatorial indexing
### ci_barcode_terra
Contamination is a common problem during PCR amplifications. The Amplicon Decontamination Pipeline relies on combinatorial indexing, using i7 and i5 primers, to impute reads to specific wells in an Illumina plate. The pipeline will detect and report the source or contamination and the percentage of contaminating reads found in a given well. This workflow uses bbmerge and has been specifically designed to operate with amplicons short enough to be covered by your sequencing approach, allowing for overlap between the ends of the forward and reverse reads. 

### How to access the Amplicon Decontamination Report for the ci_barcode_terra workflow

The first part of the **Amplicon Decontamination Report** Jupyter notebook compiles the output of the Malaria Amplicon Decontamination pipeline in a ready-to-read format. The second part of the Notebook provides functionalities to produce custom plots and a table with detailed information about the source of contamination in each processed well. 

### ci_barcode_terra_dada2
This is a variation of the previous protocol that uses DADA2 to merge reads.

### How to access the Amplicon Decontamination Report for the ci_barcode_terra_dada2 workflow
Results can be accessed using the same protocol described for ci_barcode_terra.

### ci_barcode_terra_iseq
This is a variation of the previous protocol is adapted to process amplicons that are too large to be fully covered by your sequencing technology. 

### How to access the Amplicon Decontamination Report for the ci_barcode_terra_iseq workflow
Results can be accessed using the same protocol described for ci_barcode_terra.

## 2. FASTQ file cleaning protocol
### fastq_cleaning


## 3. DADA2 denoising

### dada2_terra
To analyze the genetic variation within P. falciparum, we develop a pipeline for the in-depth assessment of paired-end Illumina sequencing data from blood samples. This analysis is carried out through a tailored processing pipeline that encapsulates the [Divisive Amplicon Denoising Algorithm (dada2)](https://www.nature.com/articles/nmeth.3869), a tool developed to derive microhaplotypes. In our pipeline, these microhaplotypes are aligned to a custom database containing reference sequences from 3D7 and Dd2 for each amplicon locus. Subsequently, we refined our dataset by applying filters based on factors such as edit distance, length, and chimeric sequences, using a custom R script. The resultant sequence polymorphism are summarized by converting individual microhaplotypes into pseudo-CIGAR strings, employing a bespoke Python script.

This version of the workflow has been specifically designed to operate with amplicons short enough to be covered by your sequencing approach, allowing for overlap between the ends of the forward and reverse reads. 

### dada2_terra_iseq
This is a variation of the previous protocol is adapted to process amplicons that are too large to be fully covered by your sequencing technology. 

### How to access the DADA2 denoising results
Results of the DADA2 denoising protocol are accesible through the MHap_Analysis_terra Jupyter Notebook.

## 4. Vector Seq
The VectorSeq pipeline uses a k-mer approach to detect insecticide resistance alleles in mosquito species. Likewise, this pipeline can be used for species identification. ",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","broad-firecloud-dsde-methods/malaria-amplicon-decontamination"
183,"help-gatk","Somatic-SNVs-Indels-GATK4","READER","https://app.terra.bio/#workspaces/help-gatk/Somatic-SNVs-Indels-GATK4",TRUE,FALSE,NA,NA,NA,"### GATK Best Practices for Somatic SNVs and Indels using Mutect2

A fully reproducible example of somatic SNV and Indels variant discovery using the Mutect2 workflow. This workspace includes a WDL workflow for users ready plug in and process their data, also a notebook tutorial of the workflow that guides users step by step through each tool in the workflow. Detailed description of GATK workflows are available in [Gatk's Best Practices Document](https://gatk.broadinstitute.org/hc/en-us/articles/360035894711-About-the-GATK-Best-Practices). Two specific articles that would be helpful are:
- [(How to) Call somatic mutations using GATK4 Mutect2](https://gatk.broadinstitute.org/hc/en-us/articles/360035531132)
- [Somatic-short-variant-discovery-SNVs-Indels](https://gatk.broadinstitute.org/hc/en-us/articles/360035894731)

Additional details on Mutect2 can be found in the following BioRxiv publication : [Calling Somatic SNVs and Indels with Mutect2](https://www.biorxiv.org/content/10.1101/861054v1)

> To understand the genetics of cancer, we must accurately detect somatic mutations. Due to such factors as contaminating normal cells, subclonality, and copy number variations, somatic mutations may have low allele fractions. Such mutations are difficult to distinguish from artifacts due to sample preparation, sequencing error, and mapping error. To find somatic variants Mutect2 employs local assembly and alignment, a Bayesian somatic genotyping model, and a novel filtering scheme. As a GATK 4 tool it runs on both local and cloud-based data and has a full pipeline written in the Broad Institute’s Workflow Development Language (WDL).
> 
> Mutect2 is a somatic variant caller that uses local assembly and realignment to detect SNVs and indels. Assembly implies whole haplotypes and read pairs, rather than single bases, as the atomic units of biological variation and sequencing evidence, improving variant calling. Beyond local assembly and alignment, Mutect2 is based on several probabilistic models for genotyping and filtering that work well with and without a matched normal sample and for all sequencing depths.



Scroll down for detailed documentation for each workflow:    
- Function         
- Input and output requirements     
- Sample data description and location        
- Reference data description and location     
- Time and cost estimates for running the workflows
- Optional workflow-level and task-level parameters      

General information at the bottom include     
- Contact information 
- License information     

The following material is provided by the GATK Team. Please post any questions or concerns to one of our forum sites : [GATK](https://gatk.broadinstitute.org/hc/en-us/community/topics) or [Terra](https://broadinstitute.zendesk.com/hc/en-us/community/topics/360000500432-General-Discussion).


## Data
Data associated with the Mutect tutorial notebook is located in the [gs://gatk-tutorials/workshop_2002/3-somatic](https://console.cloud.google.com/storage/browser/gatk-tutorials/workshop_2002/3-somatic/?project=broad-dsde-outreach&organizationId=548622027621) google bucket. It contains both input and resource files for the Mutect2 notebook. Along with the inputs are precomputed outputs generated by each step in the tutorial. This can be used as input for any step in the workflow, in case your generated outputs are not correct or you are unable to complete a step in the workflow. 

The workflow input data are obtained from the [deTiN cell line validation experiment](https://portal.firecloud.org/#workspaces/broad-firecloud-testing/deTiN_release_data) dataset workspace to create the panel of normal and downsampled test data [gatk-best-practices/somatic-b37](https://console.cloud.google.com/storage/browser/gatk-best-practices/somatic-b37) bucket for testing the Mutect2 workflow. 

## Notebooks
 **1-somatic-mutect2-tutorial :**
In this hands-on tutorial, we will call somatic short mutations, both single nucleotide and indels, using the GATK4 Mutect2 and FilterMutectCalls. If you need a primer on what somatic calling is about, see the following [GATK forum Article](https://gatk.broadinstitute.org/hc/en-us/articles/360035890491).


| Option | Value |
| --- | --- |
| Environment | Default |
| Profile | Custom |
| CPU Minimum | 4|
| Disksize Minimum | 100 GB |
| Memory Minimum | 15 GB |

**Time and cost**  
This workspace focuses on the use of notebooks, thus the time and cost of completing the tutorial depends on the runtime environment of your notebook and the length of time you spend actively working in the notebook. With the given runtime environments above, users can expect the cost to be much less than a dollar an hour for each notebook. The specific cost will be displayed on the top right corner of your screen, next to the notebook machine options. 


## Workflows

The workflows are directly linked to the GATK repo via [Dockstore](https://dockstore.org/organizations/BroadInstitute/collections/GATKWorkflows) so its possible to select a particular workflow version based off the GATK version. If requested by the workflow inputs, be sure to provide the corresponding GATK docker image for the workflow version. For example if using workflow version 4.0.0.0 then the proper GATK docker to use would be us.gcr.io/broad-gatk/gatk:4.0.0.0. 

### 1-Mutect2_PON   
**What does it do?**     
Create a Mutect2 panel of normals (PoN), which is used in downstream analysis to identify chemical and instrument artifacts.       

**What is the input data?**       
Sample set of normal BAMs and BAIs    

**What does it output?**      
- PoN vcf      
- Individual vcf for each normal sample used in the PoN        
- Sample data description and location

**Sample data description and location**    
A sample set called `panel_of_normals` is provided in the workspace data model for testing. This sample set contains samples with the description ""normal sample"".     

**Reference Data**  
The reference genome for this workspace is hg19 (aka b37). The required and optional references and resources are included in the workspace data table.         

**Time and cost estimates**    
Note that time and cost estimates may vary due to the use of preemptibles.     

| Sample Set Name | Sample Size | Time | Cost $ |
| -------  | -------- | -------- | ---------- |
| panel_of_normal | 512 GB | 2:07:00 | 1.37 |       

**For more detail on cost-controlling options** see [this article](https://support.terra.bio/hc/en-us/articles/360029748111). 


### 2-Mutect2_GATK4 
**What does it do?**      
This workflow runs GATK4 Mutect 2 on a single tumor-normal pair of BAMs, or on a single tumor sample BAM, and performs additional filtering and functional annotation tasks.  

**What data does it take as input?**    
- Tumor BAM and BAI        
- Normal BAM and BAI      

**What does it output?**    
- Unfiltered vcf         
- Filtered vcf                  

**Sample data desctription and location**      
Sample files for a tumor and normal pair, HCC1143 and HCC1143_BL BAM, are provided in the workspace data model for testing. These BAM files are listed in the `sample` table with the description ""tumor/normal sample"" and paired under the `pair` table as the entity HCC1143_small.      

**Reference data description and location**  
The reference genome for this workspace is hg19 (aka b37). The required and optional references and resources for the workflows are included in the workspace data table.         

**Time and cost estimates**     
Note that time and cost estimates may vary due to the use of preemptibles.     

| Sample Name | Sample Size | Time | Cost $ |
| -------  | -------- | -------- | ---------- |
| HCC1143_small normal/tumor | 20.08 GB | 2:04:00 | 0.45 |
| HCC1143 normal/tumor | 73.02 GB | 3:36:00 | 0.86 |       
   
**For more detail on cost-controlling options** see [this article](https://support.terra.bio/hc/en-us/articles/360029748111). 


### optional-Mutect2-normal-normal  
**What does it do?**     
This optional validation workflow calls pairs of replicates as tumor-normal pairs, counts the number of variants (i.e. false positives), and reports false positive rates. 

**What does it take as input data?**    
Sample set -  one analysis-ready BAM file (and its BAI) for each replicate

**What does it output?**    
False-positive vcf files and its BAI, with summary    

**Sample data description and location**    
A sample set called `validation` containing BAM files with the description ""validation sample"" is provided in the workspace data model for testing. These BAM files are listed in the `sample` data table and grouped in the `sample_set` data table.     

**Reference data description and location**  
 The reference genome for this workspace is hg19 (aka b37). The required and optional references and resources for the workflows are included in the workspace data table.       

**Time and cost estimates**     
Note that time and cost estimates may vary due to the use of preemptibles. 

| Sample Set Name | Sample Size | Time | Cost $ |
| -------  | -------- | -------- | ---------- |
| validation | 404.98 GB | 2:33:00 | 2.65 |     

**For more detail on cost-controlling options** see [this article](https://support.terra.bio/hc/en-us/articles/360029748111) .   

---

### Contact information  
For questions about this workspace please visit the Featured Workspaces Topic topic on the [Terra Community Forum](https://broadinstitute.zendesk.com/hc/en-us/community/topics). Use the search box to see if other users have asked the same question previously. If not, post and tag **@Samantha** so that we get notified.

This material is provided by the GATK Team. Please post any questions or concerns regarding the GATK tool to the [GATK forum](https://gatk.broadinstitute.org/hc/en-us/community/topics)

### Workspace Citation 
Details on citing Terra workspaces can be found here: [How to cite Terra](https://support.terra.bio/hc/en-us/articles/360035343652)

Data Sciences Platform, Broad Institute (*Year, Month Day that this workspace was last modified*) help-gatk/Somatic-SNVs-Indels-GATK4 [workspace] Retrieved *Month Day, Year that workspace was retrieved*,  https://app.terra.bio/#workspaces/help-gatk/Somatic-SNVs-Indels-GATK4


### License  
**Copyright Broad Institute, 2019 | BSD-3**  
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at https://github.com/openwdl/wdl/blob/master/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.

### Workspace Change Log
| Date | Change | Author |
| --- | --- | --- |
|  2019-10-15 | Updated workflows and Docker to GATK 4.1.4.0, switched to funcotator | Beri Shifaw |
|  2020-1-13 | Updated workflows and Docker to GATK 4.1.4.1| Beri Shifaw |
|  2020-4-13 | Updated workflows and Docker to GATK 4.1.6.0. Workflows now point to Dockstore workflows registered from GATK repo | Beri Shifaw |
|  2020-9-23 | Added additional documentation links for workflow, added description for notebook in dashboard. Removed the CNA notebook from workspace. Updated GATK workflow and Docker version to 4.1.7.0 | Beri Shifaw |
|  2021-4-19 | Updated workflows and Docker to GATK 4.2.0.0. | Beri Shifaw |
|  2021-9-15 | Addition of Workspace Citation section | Beri Shifaw |
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/Somatic-SNVs-Indels-GATK4"
185,"broad-firecloud-tcga","TCGA_COAD_OpenAccess_V1-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_COAD_OpenAccess_V1-0_DATA",TRUE,TRUE,"Colon adenocarcinoma","Tumor/Normal","USA","TCGA Colon adenocarcinoma Open-Access Data Workspace","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients’ clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","460","Colon","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh37/hg19","TCGA_COAD_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_COAD_OpenAccess_V1-0_DATA"
186,"fbrihuman","sconline_integrative_analysis","READER","https://app.terra.bio/#workspaces/fbrihuman/sconline_integrative_analysis",TRUE,FALSE,NA,NA,NA,"We have performed single-nuclei RNA-sequencing on 52 high-quality frontal cortex biopsies (Brodmann Area 8 or 9) from individuals with suspected normal pressure hydrocephalus (NPH) who have variable degrees of local amyloid and tau histopathology. Using our optimized protocol for nuclei extraction, we sampled about 900k nuclei from the human brain identifying 82 cell types, unperturbed by death or agonal state. This empowered us to unbiasedly assess the molecular changes associated with amyloid-beta and hyperphosphorylated tau across all cell types in the neocortex. We further used this unique dataset as an anchor to computationally integrate 27 existing postmortem and mouse model single cell datasets of AD and other neurological diseases.

This Terra workspace provides access to both generated data and developed pipelines as part of this project. For a use case scenario, see [this Terra notebook](https://app.terra.bio/#workspaces/help-terra/International_Neuroimmune_Consortium_Workshop_2022/analysis/launch/integrative_analysis.ipynb).

# Data
In general, all generated data resources can be accessed from the **Files** link (left-side menu) under the **DATA** tab. 

* **annotations:** Cell type annotations of 2.4M cell profiles across 28 studies. We have also included all publicly available biological, technical, and demographic metadata on these datasets. Standardized metadata are reported in columns starting with **anno**. Dataset ids are reported in **ds_batch** column and cell barcodes are reported in **sample** column. We further report calculated cell QC metrics in columns starting with **QC**.
* **organized_data:** Arranged single-cell and single-nucleus transcriptome data from the integrated datasets (including the NPH dataset). Note that the cell type annotations within these datasets are based on previously published annotations derived when these datasets were analyzed individually. Cell type annotations from the integrative analysis can be access from the **annotations** folder.
* **NPH_DE_res:** Differentially expressed genes in samples with amyloid-beta and hyperphosphorylated tau

# Pipelines
All developed pipelines as part of the work are arranged in Jupyter notebooks under **ANALYSES** tab. We have also reported the codes used for the analyses reported in the paper in **resource_paper** folder at **Files** link under the **DATA** tab.",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","fbrihuman/sconline_integrative_analysis"
187,"lincs-phosphodia","Avant-garde_LINCS_PC3","READER","https://app.terra.bio/#workspaces/lincs-phosphodia/Avant-garde_LINCS_PC3",TRUE,FALSE,NA,NA,NA,"LINCS PC3 phospho-enriched sample dataset",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","lincs-phosphodia/Avant-garde_LINCS_PC3"
190,"help-gatk","GATKTutorials-Germline-June2019","READER","https://app.terra.bio/#workspaces/help-gatk/GATKTutorials-Germline-June2019",TRUE,FALSE,NA,NA,NA,"# What's in this workspace?
Welcome to Day 2 of the Genome Analysis Toolkit (GATK) workshop at the Newcastle University in Newcastle upon Tyne, U.K.! Yesterday you got an overview of the various tools and pipelines, but today will focus on Germline Variant Discovery. 

Here you will be running four different notebooks. In the morning we will cover variant discovery, and in the afternoon we will look at different methods for variant filtering. This workspace is read-only, so clone your own unique copy to work with it.

Your instructor will guide you through this workspace, but you should find the documentation within the notebooks sufficient to run through them again on your own or to share with a friend later (and we encourage you to do so!).

The notebook(s) in this workspace are:
* 1-germline-variant-discovery-tutorial
* 2-gatk-hard-filtering-tutorial-python
* 3-gatk-hard-filtering-tutorial-r-plotting
* 4-gatk-cnn-tutorial-python

All notebooks in this workspace can use the following runtime settings:

| Runtime Environments | Value |
| --- | --- |
| CPU Minimum| 4|
| Disksize Minimum | 100 GB |
| Memory Minimum | 15 GB |
| Startup Script | gs://gatk-tutorials/scripts/install_gatk_4110_with_condaenv.sh |

## Appendix

### GATK @ Newcastle 2019 in the Terra Support Center

Following the conclusion of the workshop, the workshop materials will be made available in the Terra Support Center. For now, you can find these materials in a google drive folder at [broad.io/GATK1906](broad.io/GATK1906)

### GATK documentation and support

Detailed documentation, tutorials, and more are available at the [GATK web site](https://software.broadinstitute.org/gatk/). If you are still running into issues after consulting the User Guide, have a question, or just want to say hello, reach out to the GATK team via the [GATK Support Forum](https://gatkforums.broadinstitute.org/gatk)!

### Terra documentation and support

Documents and helpful guides to support your journey through Terra are available in the Terra Support Center. Navigate there by following the “Learn About Terra” link in the left-side menu. At the time of this workshop, we are still actively writing and publishing documents but we’ve got a good set of docs in the Quick Start Guide to get you going.



",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/GATKTutorials-Germline-June2019"
191,"texas-cancer-research-biobank","TCRB_hg19_OA_20180102_DATA","READER","https://app.terra.bio/#workspaces/texas-cancer-research-biobank/TCRB_hg19_OA_20180102_DATA",TRUE,TRUE,NA,NA,NA,"Snapshot of open access data from the [Texas Cancer Research Biobank](http://txcrb.org/open.html) (TCRB).

The data use conditions statement for the TCRB Open Access dataset is [here](https://00e9e64bac716ceacd89d2275c11d159298d55d3f9126abc81-apidata.googleusercontent.com/download/storage/v1/b/broad-public-datasets/o/TCRB%2FREADME.txt?qk=AD5uMEvst1VrgYzj5KYOGI_aDnh-ftEFeRVDBz6owYDCMCZmaizzI68DRDfn517sz6mTMvfMh-fxzMB-vOBNyDYu1kWBLLXx19GPG2j60u05ArbWedUEb2sJ-fxl7ZzTyWHM06U0BBr3MZQD7C0sfgiKtvS43G0yhV4EEfACc7KqVEOl-ZRLK4JpDH4gKl9YTUAc8SnwKsAlPA8KAKoWnIyi65vMVIslFKSXpWg_aznYtzthEH-k9D6Gc_oK4T4VFFFndTHnokKj6Fvqni38nDoIRF43b18FOFZCDdYK0hxqhWzMAOA4lr46a44JyH1yKibPUwqZpWKrZS7d_XUGz7jc1Ct1i0-zwC5OouLV4tEapbxANa8n6f86Rufv1-GT0An_7-ogvI15SaYPSxG6fxbznrcGMk4aNRelhaEr4sia67FXWTFayIdCTt168eM_ph0ZLqoHm5UgTTFTThrQaGLP-RrXnkGkdR5q1EOcwcqsYTya84_sqpvyTON22o52AnDa7-BChsW7UuPLJt0oKKUjQ0n-ahyK2wYaIFlufAQXpL7n0tskBJuamyaBP8P0T-axW6PaIPkoOjgxeCddS8oUawId1-T5Ni8CX74txMaGHJifmjtO7gvvlHMWTf4-ofCNwwpmikU2dW-4xqVrWSBL4FJ1Gg0MIAVLI9O3Ye8z3BLcLG_vhLdlKU4DBrmCWa4srC6uk1W0N2myXMvnB1gTNJwIc4CPGmu1b526EoaZ8K1H_iEi5i1SyMBif7e0JfAtfSI1qlQW).",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","texas-cancer-research-biobank/TCRB_hg19_OA_20180102_DATA"
192,"lincs-phosphodia","Avant-garde_Tutorial","READER","https://app.terra.bio/#workspaces/lincs-phosphodia/Avant-garde_Tutorial",TRUE,FALSE,NA,NA,NA,"Tutorial workspace for cloud-based Avant-garde workflow. Please see https://github.com/broadinstitute/Avant-garde-Terra/wiki/Tutorial for step-by-step instructions associated with the analysis in this workspace.",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","lincs-phosphodia/Avant-garde_Tutorial"
193,"dsp-edu-dev","Nov_2023_Workshop - GATK4-Germline-Preprocessing-VariantCalling-JointCalling","READER","https://app.terra.bio/#workspaces/dsp-edu-dev/Nov_2023_Workshop%20-%20GATK4-Germline-Preprocessing-VariantCalling-JointCalling",TRUE,FALSE,NA,NA,NA,"### GATK Best Practices for Germline SNPs & Indels
This workspace contains tutorial notebooks and workflows that cover pre-processing, SNP and Indel variant calling. 

There are two ways to run the workflows, in the single sample case one sample is run through the first two workflows and a single VCF file for the sample is generated. In the cohort case, all four workflows are run on several samples to generate a multi-sample VCF. The workflows are properly configured in this workspace to run back to back, so that outputs from each step will automatically become the inputs for the next. 

Each workflow follows the GATK Best Practices on human whole-genome sequence data. Detailed description of the workflows is available in [Gatk's Best Practices Document](https://gatk.broadinstitute.org/hc/en-us/articles/360035535932).  

To learn more about how GATK workflows and Best Practices are used for production at the Broad Institute, you can also view the [Whole Genome Analysis Pipeline Workspace](https://app.terra.bio/#workspaces/warp-pipelines/Whole-Genome-Analysis-Pipeline). 

The workspace material is provided by the GATK team. Please post any questions or concerns to one of our forum sites : [GATK](https://gatk.broadinstitute.org/hc/en-us/community/topics) or [Terra](https://broadinstitute.zendesk.com/hc/en-us/community/topics/360000500432) 


## Notebooks
Here you will be running two different tutorial notebooks describing variant calling and different methods for variant filtering. This workspace is read-only, so clone your own unique copy to work with it.

You should find the documentation within the notebooks sufficient to run through them again on your own or to share with a friend later (and we encourage you to do so!).

Below is the list of notebooks found in this workspace. Note: the notebooks are numbered according to their session in the itinerary. The number-2 exercise for this workshop can be found in the [Tetralogy of Fallot Featured Workspace](https://app.terra.bio/#workspaces/help-gatk/Reproducibility_Case_Study_Tetralogy_of_Fallot)

The notebook(s) in this workspace are: 
* 1-germline-variant-discovery-with-HaplotypeCaller
* 3-gatk-variant-filtration
* 4-gatk-variant-refinement
* 5-somatic-mutect2-tutorial
* 6-gatk-somatic-cnv-tutorial
* 7-gatk-germline-cnv-tutorial
* 8-intro-scrnaseq-tertiary-analysis

Most notebooks in this workspace can use the following runtime settings:

| Option | Value |
| --- | --- |
| Environment | Default (GATK 4.1.4.1) |
| Profile | Custom |
| CPU Minimum | 4|
| Disksize Minimum | 100 GB |
| Memory Minimum | 15 GB |

However, the two notebooks called **gatk-somatic-cnv-tutorial** and **gatk-germline-cnv-tutorial** must be configured with a **custom docker image** that we've created for this workshop. Please read these [instructions on how to implement this configuration](https://github.com/broadinstitute/gatk-workshop-terra-jupyter-image/wiki/Using-the-gatk%E2%80%90workshop%E2%80%90terra%E2%80%90jupyter%E2%80%90image-in-the-Terra-Jupyter-environment#6-type-the-command-setup_gatk_env-in-the-terminal-and-hit-enter).

## Workflows overview 

1. **Preprocessing-For-Variant-Discovery**:  takes as input an unmapped BAM list file (text file containing paths to unmapped bam files) to perform preprocessing tasks such as mapping, marking duplicates, and base recalibration. It produces a single BAM file      

2.  **Haplotypecaller**:  takes as input a bam file and produces a file GVCF ( precursor to a VCF )    

3.  **Generate-Sample-Map**:  takes as input several GVCF files and generates a sample map file, which is text file where the first coloumn is the name of the sample and the second column contains the path to the samples GVCF file    

4.  **Joint-Genotyping**: takes as input a sample map file to perform variants calling all the provided GVCF files and filtering to produce a multi-sample VCF (minimum of 50 samples is required)   

Scroll down for details on each workflow, including input and output descriptions and requirements, estimated run times and costs, and information on sample data.   

### Workflow Naming Schema

There are two complete sets of workflows. Those with a ""1"" in front use hg38 reference data and those that begin with a ""2"" use b37. The table below lists the workflows and the corresponding reference.


|Workflow Name | Reference Data |
|---|---|
|1-1-Preprocessing-For-Variant-Discovery| hg38 |
|1-2-Haplotypecaller | hg38 |
|1-3-Generate-Sample-Map | hg38 |
|1-4-Joint-Genotyping | hg38 |
|2-1-Preprocessing-For-Variant-Discovery| b37 |
|2-2-Haplotypecaller | b37 |
|2-3-Generate-Sample-Map | b37 |
|2-4-Joint-Genotyping | b37 |


### Input and output data files overview


 **Single sample case**    
This will produce a VCF file for a single sample.

 ![drawing](https://storage.googleapis.com/terra-featured-workspaces/Germline-VariantCalling-JointCalling/images/GVCFdisabled.png)
 
 To run the workflows  in single sample mode:   

1.   Head to the workflows tab
2.   Click on 1-1-Preprocessing-For-Variant-Discovery-HG38 workflow  
		A.  Click on ""Select Data""  
		B.  Select ""Choose existing sets""  
		C.  Check the row with ""1kgp-50-wgs” as the sample_set  
		D.  Click ""OK""  
3. Click on Run analysis  
4. Once the workflow has completed head to the Workflows tab and on to the next workflow.   
5. Click on 1-2-Haplotypecaller-HG38 workflow   
	 A.  Click on ""Select Data""  
	 B.  Select ""Choose existing sets""  
	 C.  Check the row with ""1kgp-50-wgs” as the sample_set  
	 D.  Click ""OK""  
	 E. In the Input tab set the ""make_gvcf"" parameter to `False`
	 F. In the output tab set the output_vcf and output_vcf_index parmeter to `this.downsampled_hg38_vcf` and `this.downsampled_hg38_vcf_index`
6. Click on Run analysis  

**Cohort sample case**
 
 It's possible to generate a multi-sample VCF instead of a single sample VCF by setting the ""make_gvcf"" parameter  in the **Haplotypecaller** workflow to `True`. Haplotypecaller will then create a GVCF which can be used in the preceding workflows to create the multisample VCF. Running all four workflows takes an unmapped BAM (uBAM) input file and returns a multi-sample VCF. The first two workflows are run once for each sample. 

 ![drawing](https://storage.googleapis.com/terra-featured-workspaces/Germline-VariantCalling-JointCalling/images/GVCFenabled.png)


To run the workflows  in cohort mode:   

1.   Head to the workflows tab
2.   Click on 1-1-Preprocessing-For-Variant-Discovery-HG38 workflow  
		A.  Click on ""Select Data""  
		B.  Select ""Choose existing sets""  
		C.  Check the row with ""1kgp-50-wgs” as the sample_set  
		D.  Click ""OK""  
3. Click on Run analysis  
4. Once the workflow has completed head to the Workflows tab and on to the next workflow.   
5. Click on 1-2-Haplotypecaller-HG38 workflow   
	 A.  Click on ""Select Data""  
	 B.  Select ""Choose existing sets""  
	 C.  Check the row with ""1kgp-50-wgs” as the sample_set  
	 D.  Click ""OK""  
	 E. Set the ""make_gvcf"" parameter to `True`
	 F. In the output tab set the output_vcf and output_vcf_index parmeter to `this.downsampled_hg38_gvcf` and `this.downsampled_hg38_gvcf_index`
6. Click on Run analysis  
7. Once the workflow has completed head to the Workflows tab and on to the next workflow.  
8. Click on 1-3-Generate-Sample-Map-HG38 workflow  
  A.  Click on ""Select Data""  
  B.  Check the row with ""1kgp-50-wgs” as the sample_set  
  C.  Click ""OK""  
9. Click on Run analysis  
10. Once the workflow has completed head to the Workflows tab and on to the next workflow.  
11. Click on 1-4-Joint-Genotyping-HG38 workflow  
  A.  Click on ""Select Data""  
  B.  Check the row with ""1kgp-50-wgs” as the sample_set  
  C.  Click ""OK""  
12. Click on Run analysis  

If you would like to run the workflows again from scratch, delete the sample and sample_set table and re-upload both tables from this [google bucket](https://console.cloud.google.com/storage/browser/terra-featured-workspaces/Germline-VariantCalling-JointCalling/?project=broad-dsde-outreach&organizationId=548622027621). 

## Workspace Data 
There are two sets of sample tables in the DATA tab - one corresponding to template input files, and another with examples of processed data (i.e. example_sample_output).

**Template input data**    
These are the  tables you would use to run the workflow pipeline from the beginning. 

- participant - Individual per row. 
This won't be used directly in the workspace but it's best practice to include this in the DATA tab.
- sample - Sample from a participant  per row.      
This will be used by the preprocessing and haplotypecaller workflows (*-1 and *-2). 
- sample_set - A set of samples per row. 
This will be used by the Generate-sample-map and joint genotyping workflows (*-3 and *-4)

**Examples of processed data**    
Once all the workflows have been executed you will see several additional columns within the sample and sample_set tables. Examples of what you should see are in the following tables:      
- example_sample_output
- example_sample_set_output  

## Workflows

###  1-Preprocessing-For-Variant-Discovery  
**What does it do?**    
This WDL takes sequencing data in unmapped BAM (uBAM) format and outputs a clean BAM file and its index, suitable for variant discovery analysis. 

**What does it require as input?**    
The 1-Preprocessing-For-Variant-Discovery workflow accepts a file containing a list of unaligned BAMS. To learn more about how to generate a list file, see [this article](https://support.terra.bio/hc/en-us/articles/360033353952).     

The input data are samples:

* Pair-end sequencing data in unmapped BAM (uBAM) format
* One or more read groups, one per uBAM file, all belonging to a single sample (SM)

Input uBAM files must comply with the following requirements:

* Filenames all have the same suffix (we use "".unmapped.bam"")
* Files must pass validation by ValidateSamFile
* Reads are provided in query-sorted order
* All reads must have an RG tag
* Reference index files must be in the same directory as source (e.g. reference.fasta.fai in the same directory as reference.fasta)    

**If your sequencing data is not in uBAM format** (e.g. FASTQ), check out this file conversion workspace, [https://app.terra.bio/#workspaces/help-gatk/Sequence-Format-Conversion](https://app.terra.bio/#workspaces/help-gatk/Sequence-Format-Conversion) for workflows to convert:    

1. Interleaved FASTQ to paired FASTQ
2. Paired FASTQ to unmapped BAM
3. BAM to unmapped BAM
4. CRAM to BAM files from sequencer output for use in GATK analysis tools

**Sample data description and location**  
The workspace DATA tab contains downsampled 1000 Genome Project unaligned BAM list files in the `sample` table under the column `flowcell_unmapped_bams_list`.   

**What does it return as output?**  
The workflow generates a clean BAM file and its index, suitable for variant discovery analyses and stored in the workspace bucket. Metadata for all outputs are written to the `sample` table in the workspace DATA tab.           
   
**Reference data description and location**  
Required and optional references and resources for the workflows are included in the Workspace DATA tab in the Reference Data tables. The input unmapped BAM samples have yet to be aligned to a reference so they are not restricted to a particular reference. Only after the first workflow will the samples be restricted to working with a reference, because the unmapped BAM files will be mapped after that point. Again there are two sets of each workflow, one set configured with hg38 references and the other set configured with b37 references. 
 
 
**Estimated time and cost to run on sample data**    

| Sample Name | Sample Size | Time | Cost $ |
| :---:  | :---: | :---: | :---: |
| NA12878_24RG_small | 3.11 GB | 1:28:00 | 0.18 |
| NA12878 | 64.89 GB | 22:35:00 | 4.98 |  
| downsampled-1kgp-50-exomes | 32.13 GB | 02:07:00 | 7.29 |       



### 2-Haplotypecaller 
**What does it do?**    
The workflow scatters the HaplotypeCaller tool over a sample (clean BAM file and index, from the previous step) using an intervals list file. In particular, it runs the HaplotypeCaller tool from GATK4 on a single sample according to GATK Best Practices. The output file produced will be a single VCF or GVCF file depending on the mode in which it is run. If a GVCF is produced then it can be used by the joint genotyping workflow.   

**What does it require as input?**       
The workflow accepts:  In particular:     
- One analysis-ready BAM file for a single sample (as identified in RG:SM), pre-processed using GATK Best Practices. 
- A file containing a set of variant calling interval lists for the scatter

**What does it return as output?**        
One VCF or GVCF file and its index      

**Sample data description and location**  
Links to the expected input types are available in the `sample` data table (processed example) for testing. The `sample` data table lists analysis-ready BAM files under the `analysis_ready_bam` column.     

**Reference data description and location**  
Required and optional references and resources for the workflows are included in the Workspace DATA tab in the Reference Data tables. The **1-2_Haplotypecaller** workflow is configured with HG38 references and **2-2_Haplotypecaller** is configured with B37 references. 
          

**Estimated time and cost to run on sample data**      

| Sample Name | Sample Size | Time | Cost $ |
| :---:  | :---: | :---: | :---: |
| NA12878_24RG_small | 4.66 GB | 02:28:00 | 0.21 |
| NA12878  (CRAM)| 19.55 GB | 14:05:00 | 2.24 |    
| NA12878  (BAM)| 68.00 GB | 03:44:00 | 1.37 |    
| downsampled-1kgp-50-exomes | 23.00 GB | 01:12:00 | 24.25 |  
   

### 3-Generate-Sample-Map

**What does it do?**    
This WDL generates a sample_map file, which can be used for the Joint-Genotyping workflow. A sample map is a tab-delimited text file of 2 columns; 1. the name of the sample and 2. the file path (in this case the Google bucket path of the file).

**What does it require as input?**  
- An array of file names
- An array of file paths
- Name of output sample_map 

**What does it return as output?**    
- Sample map file

**Reference data description and location**  
Reference files are not used in this workflow. 


### 4-Joint-Genotyping    
**What does it do?**    
This WDL implements the joint calling and variant quality score recalibration (VQSR) filtering portion of the GATK Best Practices.

**What does it require as input?**  
- GVCFs produced by HaplotypeCaller in GVCF mode
- Bare minimum: 50 samples. Gene panels are not supported

**What does it return as output?**     
A VCF file and its index, filtered using VQSR, with genotypes for all samples present in the input VCF. All sites that are present in the input VCF are retained. Filtered sites are annotated as such in the FILTER field.      
	
**Sample data description and location**     
Links to the expected input types are available in the processed example data table for testing. The 4-Joint-Genotyping workflow accepts one or more GVCFs produced by haplotypecaller.  The `gvcf` column in the data table contains a full-sized GVCF of NA12878 that will be used for the Joint-Genotyping workflow.     

**Reference data description and location**  
Required and optional references and resources for the workflows are included in the Workspace DATA tab in the Reference Data tables. **1-4_JointGenotyping** is configured with hg38 references and **2-4_JointGenotyping** is configured with the b37 reference. 
 

**Estimated time and cost to run on sample data**     

| Sample Name | Sample Size | Time | Cost $ |
| :---:  | :---: | :---: | :---: | 
| downsampled-1kgp-50-exomes | 9.52 GB | 03:17:00 | 1.35 |  
 
### Optional Workflows
Additional workflows have been added for your convenience.   

**Optional-Paired-FASTQ-to-Unmapped-BAM**: 
This WDL converts paired FASTQ to uBAM and adds read group information.

Requirements/expectations
* Pair-end sequencing data in FASTQ format (one file per orientation)
* The following metadata descriptors per sample:
  * readgroup
  * sample_name
  * library_name
  * platform_unit
  * run_date
  * platform_name
  * sequecing_center
Outputs
  * Unmapped BAM

**Optional-Gatk-GatherVCFsCloud**:  
This tool combines together rows of variant calls from multiple VCFs, e.g. those produced by scattering calling across genomic intervals, into a single VCF. This tool enables scattering operations, e.g. in the cloud, and is preferred for such contexts over Picard MergeVcfs or Picard GatherVCfs. The input files need to have the same set of samples but completely different sets of loci. These input files must be supplied in genomic order and must not have events at overlapping positions. 

Input  
* A set (array) of VCF files, sorted by genomic position. Its also possible to provide a file containing a list of VCF files, each row being an individual VCF file path.  

Output  
* A single VCF file containing the variant call records from the multiple VCFs. 

**Optional-ReblockGVCF-gatk4_exomes_goodCompression**:  
Users working with large sample sets can invoke the GnarlyGenotyper task in the JointGenotyping.wdl workflow. However, the [ReblockGVCF](https://gatk.broadinstitute.org/hc/en-us/articles/360037593171-ReblockGVCF-BETA-) tool must be run for all GVCFs produced by HaplotypeCaller before they can be appropriately processed by GnarlyGenotyper. 

Input  
* A GVCF file 

Output  
* Reblocked GVCF file


### Important notes on workflow limitations 
**Small Cohorts**

We believe the results of this workflow run on a single WGS sample are equally accurate, but there may be some shortcomings when the workflow is modified and run on small cohorts.  Specifically, modifying the SNP ApplyRecalibration step for higher specificity may not be effective.  You can verify if this is an issue by consulting the gathered SNP tranches file.  If the listed `truthSensitivity` in the rightmost column is not well matched to the `targetTruthSensitivity` in the leftmost column, then requesting that `targetTruthSensitivity` from ApplyVQSR will not use an accurate filtering threshold.     

Additionally 
- No allele subsetting for the Joint-Genotyping workflow
  - For large cohorts, even exome callsets can have more than 1000 alleles at low complexity/STR sites
  - For sites with more than six alternate alleles (by default) called genotypes will be returned, but without the PLs since the PL arrays get enormous
  - Allele-specific filtering could be performed if AS annotations are present, but the data will still be in the VCF in one giant INFO field
- JointGenotyping output is divided into lots of shards
  - Desirable for use in [Hail](https://hail.is/), which supports parallel import
  - It's possible to use [GatherVcfs](https://app.terra.bio/#workspaces/help-gatk/GATK4-Germline-Preprocessing-VariantCalling-JointCalling/workflows/broad-firecloud-dsde/Optional-Gatk-GatherVCFsCloud) to combine shards.
- GnarlyGenotyper uses a QUAL score approximation
   - Dramatically improves performance compared with GenotypeGVCFs, but QUAL output (and thus the QD annotation) may be slightly discordant between the two tools.

**Exomes**

Currently the workflows are configured for WGS processing. 
The dynamic scatter interval creating task was optimized for genomes.  The scattered SNP VariantRecalibration may fail because of too few ""bad"" variants to build the negative model. Also, apologies that the logging for SNP recalibration is overly verbose.   

The provided tool configurations are meant to be a ready-to-use example of the workflows. It is the user’s responsibility to correctly set the reference and resource input variables using the [GATK Tool and Tutorial Documentations](https://software.broadinstitute.org/gatk/documentation/).

**GnarlyGenotyper**

Users working with large sample sets can invoke the GnarlyGenotyper task in the JointGenotyping.wdl workflow. However, the [ReblockGVCF](https://gatk.broadinstitute.org/hc/en-us/articles/360037593171-ReblockGVCF-BETA-) tool must be run for all GVCFs produced by HaplotypeCaller before they can be appropriately processed by GnarlyGenotyper. A workflow that applies the reblocking tool is provided here: [ReblockGVCF-gatk4_exomes_goodCompression](https://portal.firecloud.org/?return=terra#methods/methodsDev/ReblockGVCF-gatk4_exomes_goodCompression/4)


**Controlling cloud costs**

Note that cost and time estimates will vary with the use of [preemptibles](https://support.terra.bio/hc/en-us/articles/360029772212). Using preemptibles can save up to 80% on compute costs.  For further helpful hints on controlling cloud costs, see [this article](https://support.terra.bio/hc/en-us/articles/360029748111) and for additional ways to estimate cloud cost use [Google's cost calculator](https://cloud.google.com/products/calculator/).

### Software Versions  
- GATK 4.2.4.0
- BWA 0.7.15-r1140
- Picard 2.16.0-SNAPSHOT
- Samtools 1.3.1 (using htslib 1.3.1)
- Python 2.7

---

### Contact information  
For questions about this workspace please visit the Featured Workspaces Topic topic on the [Terra Community Forum](https://broadinstitute.zendesk.com/hc/en-us/community/topics). Use the search box to see if other users have asked the same question previously. If not, post and tag **@Samantha** so that we get notified.

This material is provided by the GATK Team. Please post any questions or concerns regarding the GATK tool to the [GATK forum](https://gatk.broadinstitute.org/hc/en-us/community/topics)


### Workspace Citation 
Details on citing Terra workspaces can be found here: [How to cite Terra](https://support.terra.bio/hc/en-us/articles/360035343652)

Data Sciences Platform, Broad Institute (*Year, Month Day that this workspace was last modified*) help-gatk/GATK4-Germline-Preprocessing-VariantCalling-JointCalling [workspace] Retrieved *Month Day, Year that workspace was retrieved*,  https://app.terra.bio/#workspaces/help-gatk/GATK4-Germline-Preprocessing-VariantCalling-JointCalling

### License  
**Copyright Broad Institute, 2019 | BSD-3**  
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at https://github.com/openwdl/wdl/blob/master/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.


### Workspace Change Log
| Date | Change | Author |
| --- | --- | --- |
|  2020-01-14 | Initial feature of workspace | Beri Shifaw |
|  2020-02-24 | Added Optional-ReblockGVCF-gatk4_exomes_goodCompression workflow | Beri Shifaw |
|  2020-04-02 | Updated Preprocessing-For-Variant-Discovery to v2.0.0, Reuploaded HC and Generate Map workflow  | Beri Shifaw |
|  2020-06-22 | Added workflow overview image to dashboard and additional description to workflows, Updated Joint Genotyping workflow, removed ""GVCF"" from Haplotypecaller name to indicate the workflow can be run to create VCF  | Beri Shifaw |
|  2020-09-28 | Updated Haplotypecaller and JointGenotype workflow to release 2.2.0  | Beri Shifaw |
|  2020-11-28 | Updated Haplotypecaller to release 2.3.0, updated JointGenotype workflow source to Warp repo, added GATK Notebook tutorials | Beri Shifaw |
|  2020-12-07 | Updated data tables to use downsample wgs  | Beri Shifaw |
|  2021-01-29 | Updated dashboard to refer to [Whole Genome Analysis Pipeline Workspace](https://app.terra.bio/#workspaces/warp-pipelines/Whole-Genome-Analysis-Pipeline) | Beri Shifaw |
|  2021-09-15 | Addition of Workspace Citation section | Beri Shifaw |
|  2023-04-11 | Updated with data and notebooks for more GATK workshop topics. | Jonn Smith |",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","dsp-edu-dev/Nov_2023_Workshop - GATK4-Germline-Preprocessing-VariantCalling-JointCalling"
194,"broad-firecloud-tcga","TCGA_PAAD_OpenAccess_V1-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_PAAD_OpenAccess_V1-0_DATA",TRUE,TRUE,"Pancreatic adenocarcinoma","Tumor/Normal","USA","TCGA Pancreatic adenocarcinoma Open-Access Data Workspace","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients’ clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","185","Pancreas","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh37/hg19","TCGA_PAAD_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_PAAD_OpenAccess_V1-0_DATA"
195,"regev-duos-glioma","PrimateRetinalCellAtlas-RegevSanes-Retina-BroadInstituteHarvardMCB-scRNA-Seq","READER","https://app.terra.bio/#workspaces/regev-duos-glioma/PrimateRetinalCellAtlas-RegevSanes-Retina-BroadInstituteHarvardMCB-scRNA-Seq",TRUE,FALSE,"NA","NA",NA,"Human retinal cells from 1 subject (approximately 4000 cells) profiled on a single channel of 10X, and subsequently sequenced on 2 lanes of Hi-Seq. Cells were dissociated from the peripheral retina and CD73 was used to deplete photoreceptors prior to single-cell profiling. 

For questions please contact the lead corresponding author Prof. Joshua R. Sanes (sanesj@mcb.harvard.edu)","Joshua Sanes, Aviv Regev, Yi-Rong Peng, Karthik Shekhar, Tave van Zyl","Aviv Regev, Joshua Sanes","Single cell RNA-Seq of 2,383 human retinal cells","1",NA,"Primate Retinal Cell Atlas",NA,"NA",NA,"PrimateRetinalCellAtlas-RegevSanes-Retina-BroadInstituteHarvardMCB-scRNA-Seq","RNA-Seq","regev-duos-glioma/PrimateRetinalCellAtlas-RegevSanes-Retina-BroadInstituteHarvardMCB-scRNA-Seq"
198,"broad-firecloud-tcga","TCGA_THCA_OpenAccess_V1-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_THCA_OpenAccess_V1-0_DATA",TRUE,TRUE,"Thyroid carcinoma","Tumor/Normal","USA","TCGA Thyroid carcinoma Open-Access Data Workspace","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients’ clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","507","Thyroid","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh37/hg19","TCGA_THCA_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_THCA_OpenAccess_V1-0_DATA"
200,"broad-firecloud-tcga","TCGA_OV_hg38_OpenAccess_GDCDR-12-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_OV_hg38_OpenAccess_GDCDR-12-0_DATA",TRUE,TRUE,"Ovarian serous cystadenocarcinoma","Tumor/Normal","USA","TCGA Ovarian serous cystadenocarcinoma Open-Access Data Workspace

*hg38 TCGA and TARGET workspaces reference files by their GDC UUIDs.  In order to run analyses on the referenced data files, you will need to run workflows that retrieve the files from the GDC and copy them to your workspace bucket.  See   [this forum post](https://gatkforums.broadinstitute.org/firecloud/discussion/10382/populating-hg38-tcga-and-target-workspaces-with-data-files#latest) for instructions on the running of these workflows.*","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients' clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","600","Ovary","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh38/hg38","TCGA_OV_hg38_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_OV_hg38_OpenAccess_GDCDR-12-0_DATA"
201,"waldronlab-terra-rstudio","mtx_workflow_biobakery_version3_template ","PROJECT_OWNER","https://app.terra.bio/#workspaces/waldronlab-terra-rstudio/mtx_workflow_biobakery_version3_template%20",TRUE,FALSE,NA,NA,NA,"# MTX workflow (bioBakery Version3)

A mtx workflow based on the bioBakery wmgx workflows version 3.

For more information about the bioBakery wmgx workflows, including a detailed diagram of tasks,  please see the user manual sections (wmgx and wmgx_vis): https://github.com/biobakery/biobakery_workflows

### Inputs

The workflow has twelve required inputs and nine optional inputs. 

#### Required inputs
The workflow requires twelve inputs for each run. Five inputs can be modified for each project where as the other six inputs would only be modified with software version changes.
* AdapterType: The type of adapter to filter (choices are: ""NexteraPE"",""TruSeq2"",""TruSeq3"")
* ProjectName : The name of the sequencing project. The final output report and zip archive will use this name (only alphanumeric characters allowed).
* InputExtension : The extension for all of the input files (example "".fastq.gz"")
* InputRead1Identifier : The identifier in the file name for those files that are read1 (example "".R1"")
* InputRead2Identifier : The identifier in the file name for those files that are read2 (example "".R2"")
* InputRead1Files : A file path (in google bucket) with a list of all of the read1 files. This file must have the full paths to all of the files and is only expected to include the read1 files (not those for read2). The names for each of the samples will be computed based on the read pair identifier and the input file extension provided. For example a file named SAMPLE1.R1.fastq.gz would have a sample name of ""SAMPLE1"", a read1 identifier of "".R1"". and an extension of "".fastq.gz"". It is expected that each sample with have two files (one file for each read of the pair). 

To generate a file to use as input for InputRead1Files, follow the Terra instructions https://support.terra.bio/hc/en-us/articles/360033353952-Creating-a-list-file-of-reads-for-input-to-a-workflow , adding to command #2 the InputRead1Identifier and the InputExtension. For example with InputRead1Identifier = "".R1"" and InputExtension = "".fastq.gz"" command #2 would now be 
`gsutil ls gs:/your_data_Google_bucket_id/ | grep "".fastq.gz"" | grep "".R1"" > ubams.list` . Also since for this workflow we are looking for fastq or fastq.gz input files you might change the name of the file list in this command from ""ubams.list"" to ""fastq_list.txt"" . 

These six required inputs would only be modified if the versions of Kneaddata and HUMAnN v3 change. These are databases that are specifically tied to the software version.
* versionSpecificChocophlan : The Chocophlan database used by HUMAnN. This is located at `databases/humann/full_chocophlan.v296_201901.tar.gz` in this workspace google bucket.
* versionSpecifichumanDB : The human reference database used by Kneaddata. This is located at `databases/kneaddata/Homo_sapiens_hg37_human_contamination_Bowtie2_v0.1.tar.gz` in this workspace google bucket.
* versionSpecifictranscriptDB : The human rrna reference database used by Kneaddata. This is located at `databases/kneaddata/Homo_sapiens_hg38_transcriptome_Bowtie2_v0.1.tar.gz` in this workspace google bucket. 
* versionSpecificrrnaDB : The SILVA rrna reference database used by Kneaddata. This is located at `databases/kneaddata/SILVA_128_LSUParc_SSUParc_ribosomal_RNA_v0.2.tar.gz` in this workspace google bucket. 
* versionSpecificUniRef90 : The uniref90 reference database used by HUMAnN. This is located at `databases/humann/uniref90_annotated_v201901.tar.gz` in this workspace google bucket.
* versionSpecificUtilityMapping : The utility mapping database used by HUMAnN. This is located at `databases/humann/full_utility_mapping_v201901.tar.gz` in this workspace google bucket.

#### Optional inputs
There are an additional nine optional inputs for each workflow run. These are not required. If not set, the default values will be used.
* bypassFunctionalProfiling (Default = false): This set to true will bypass running functional profiling and all of the downstream tasks including normalization and merging of the functional data products including gene families, ecs, and pathways.
* dataType (Default = ""mtx""): This is the sequencing data type (mtx or mgx).
* inputMetadataFile (Default = None) : This file is used with the visualization task to annotate the figures with metadata.
* MaxMemGB_FunctionalProfileTasks (Default = 32 Gb): This is the max memory to request for each HUMAnN v3.0 task. This might need to be increased for larger input files.
* MaxMemGB_QualityControlTasks (Default = 8 Gb): This is the max memory to request for each Kneaddata task. This might need to be increased for larger input files.
* MaxMemGB_TaxonomicProfileTasks (Default = 24 Gb): This is the max memory to request for each MetaPhlAn v3.0 task. This might need to be increased for larger input files.
* preemptibleAttemptsOverride (Default = 2): This setting determines how many times to rerun one of the main compute tasks (HUMAnN v3.0, Kneaddata, and MetaPhlAn v3.0) on pre-emptible instances. If set to zero a non-pre-emptible instance will be used.

There are two additional optional inputs that can be used to run with one or more custom databases.
* customQCDB1 (Default = None) : Provide a custom bowtie2 formatted datatabase to use for the quailty control step instead of the default human reference.
* customQCDB2 (Default = None) : Provide a second custom bowtie2 formatted database to be used in addition to the other custom database provided.

### Outputs

The workflow has several intermediate outputs and a final zip archive that includes a report of exploratory figures plus compiled data tables. Each task has its own folder in the google bucket output folder with a sub-folder for each time it is run. The outputs of interest, including their respective folders, are described below. `$SAMPLE_NAME` is the name of the sample included in the original raw files. For example, SAMPLE1.R1.fastq.gz would have a sample name of ""SAMPLE1"".

* call-QualityControl (with sub-folders for each sample, in each subfolder there are the following files)
  * `$SAMPLE_NAME.fastq.gz` : This is the file of reads after running through QC.
  * `$SAMPLE_NAME.log` : This is the log from Kneaddata that includes read counts.
  * `glob*/$SAMPLE_NAME_DB_contam*.fastq.gz` : These are the reads that mapped to the reference database (with name `$DB`) for this sample.
  * `glob*/$SAMPLE_NAME_[R1|R2].[html|zip]` : These are the output files from running fastqc on read1 and read2 prior to running quality control.
* call-FunctionalProfile (with sub-folders for each sample, in each subfolder there are the following files)
  * `$SAMPLE_NAME.log` : This is the log from the HUMAnN v3.0 that includes read alignment counts.
  * `glob*/$SAMPLE_NAME_bowtie2_unaligned.fa` : These are the unaligned reads from running the nucleotide search.
  * `glob*/$SAMPLE_NAME_diamond_unaligned.fa` : These are the unaligned reads from running the translated search.
* call-VisualizationReport
  * `$PROJECT_NAME_visualization.zip` : This folder contains a visualization report plus final compiled data tables.
    * `wmgx_report.pdf` : This is the exploratory report of tables and figures.
    * `data/humann_feature_counts.tsv` : This contains the feature counts (pathways, gene families, ecs) for each sample.
    * `data/humann_read_and_species_counts.tsv` : This contains the counts of reads aligning at each step plus the total number of species identified for each sample.
    * `data/kneaddata_read_count_table.tsv` : This contains the read counts (split into pairs and orphans) for each step in the quality control process for each sample.
    * `data/metaphlan_taxonomic_profiles.tsv` : This contains the merged taxonomic profiles for all samples.
    * `data/microbial_counts_table.tsv` : This table includes counts ratios for each step of the quality control process for all samples.
    * `data/pathabundance_relab.tsv` : This is a merged table of the pathway abundances for all samples normalized to relative abundance.
    * `data/qc_counts_orphans_table.tsv` : This is table with the total number of orphan reads not aligning to each of the reference tables.
    * `data/qc_counts_pairs_table.tsv` : This is table with the total number of paired reads not aligning to each of the reference tables.
    * `data/taxa_counts_table.tsv`: This table includes the total number of species and genera before and after filtering.
    * `data/top_average_pathways_names.tsv` : This table includes the top pathways by average abundance, with their full names, including average abundance and variance.

### Run a demo

A demo data set is included in this workspace. The demo set includes six paired samples (three MTX and three MGX) from IBDMDB plus a small metadata file. Using preemptive instances, this demo set will cost about $5 to run.

IBDMDB (6 sample) demo run configuration:
* AdapterType: `""NexteraPE""`
* ProjectName : `""ibdmdb_demo""` (this can be any string you would like)
* InputExtension : `"".fastq.gz""`
* InputRead1Identifier : `""_R1""`
* InputRead2Identifier : `""_R2""`
* InputRead1Files : `""gs://fc-7130738a-5cde-4238-b00a-e07eba6047f2/IBDMDB/ibdmdb_file_list.txt""`
* inputMetadataFile : `""gs://fc-7130738a-5cde-4238-b00a-e07eba6047f2/IBDMDB/ibdmdb_demo_metadata.txt""`

Required software specific databases:
* versionSpecificChocophlan : `""gs://fc-7130738a-5cde-4238-b00a-e07eba6047f2/databases/humann/full_chocophlan.v296_201901.tar.gz""`
* versionSpecifichumanDB : `""gs://fc-7130738a-5cde-4238-b00a-e07eba6047f2/databases/kneaddata/Homo_sapiens_hg37_human_contamination_Bowtie2_v0.1.tar.gz""`
* versionSpecifictranscriptDB: `""gs://fc-7130738a-5cde-4238-b00a-e07eba6047f2/databases/kneaddata/SILVA_128_LSUParc_SSUParc_ribosomal_RNA_v0.2.tar.gz""`
* versionSpecificrrnaDB : `""gs://fc-7130738a-5cde-4238-b00a-e07eba6047f2/databases/kneaddata/Homo_sapiens_hg38_transcriptome_Bowtie2_v0.1.tar.gz""` 
* versionSpecificUniRef90 : `""gs://fc-7130738a-5cde-4238-b00a-e07eba6047f2/databases/humann/uniref90_annotated_v201901.tar.gz""`
* versionSpecificUtilityMapping : `""gs://fc-7130738a-5cde-4238-b00a-e07eba6047f2/databases/humann/full_utility_mapping_v201901.tar.gz""`

Optional custom databases (to run with one or more custom databases instead of the default references used in QC)
* customQCDB1 : `""gs://fc-7130738a-5cde-4238-b00a-e07eba6047f2/databases/kneaddata/Clupus_bowtie2.tar.gz""`
* customQCDB2 : `""gs://fc-7130738a-5cde-4238-b00a-e07eba6047f2/databases/kneaddata/ClupusRNA_bowtie2.tar.gz""`

Refer to the section above for descriptions of the output files generated by running the workflow.

Example output files from running the IBDMDB data set with metadata can be found in this workspace in the folder `IBDMDB/final_outputs/ibdmdb_demo_visualizations.zip`.

",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","waldronlab-terra-rstudio/mtx_workflow_biobakery_version3_template "
202,"pathogen-genomic-surveillance","COVID-19","READER","https://app.terra.bio/#workspaces/pathogen-genomic-surveillance/COVID-19",TRUE,FALSE,NA,NA,NA,"This workspace contains COVID-19 genomic data and workflows that will enable you to perform viral genomic analysis on both public data and your own data. 

For more information on COVID-19 workspaces, data and tools in Terra, please see [here.](https://support.terra.bio/hc/en-us/articles/360041068771)

To view sequencing data generated by the Viral Genomics group at Broad, please see the Terra workspace [here.](https://app.terra.bio/#workspaces/pathogen-genomic-surveillance/COVID-19_Broad_Viral_NGS). The data in the Viral_NGS workspace provides the first high resolution view of the introductions and spread of SARS-CoV-2 in the greater Boston area based on viral genomic data. 

<br>

## **Viral Genomic Workflows**

The workflows in this workspace enable you to pull in data from NCBI's SRA or Genbank, or bring your own data. The following workflows will allow you to do a variety of analysis and data preparation steps, such as **demultiplexing, reference-based assembly**, **QC**, calling of **pango lineages** and create visualizations using **NextClade** and **NextStrain**. Depending on your starting input file, whether flowcell, SRA Accession#, .fastq, or uBAM, you can use the appropriate workflow for your data in Terra.

![Overview of the Terra Workflows in this workspace and how to use them](https://storage.googleapis.com/terra-featured-workspaces/covid-19/covid1.png)


![Overview of the Terra Workflows in this workspace and how to use them](https://storage.googleapis.com/terra-featured-workspaces/covid-19/covid2.png)

<br>
 
### Options for processing and analyzing data   

Depending on your starting point, there are several options for processing and analyzing your data in Terra.


**If you have uploaded .fastq files into a Terra workspace**, you can use the following set of workflows to go from .fastq file, through to building a NextStrain tree and generating data to submit to GenBank, SRA, and GISAID.
![Overview of the Terra Workflows in this workspace and how to use them](https://storage.googleapis.com/terra-featured-workspaces/covid-19/covid3.png)

<br>

**If you would like to use a full end-to-end workflow**, starting from your flowcell, all the way through to generation of fasta files and metadata for deposition into NCBI GenBank and SRA, as well as GISAID, the sarscov2_illumina_full is the workflow you would want to use. This workflow is a combination of several workflows you can also find as stand-alone workflows in this workspace.     
![Overview of the Terra Workflows in this workspace and how to use them](https://storage.googleapis.com/terra-featured-workspaces/covid-19/covid4.png)

<br>

**The *sarscov2_illumina_full* workflow is a combination of several workflows**, as shown below. Detailed inputs and outputs of this workflow can be found in the Workflow Inputs and Examples section.
![Overview of the Terra Workflows in this workspace and how to use them](https://storage.googleapis.com/terra-featured-workspaces/covid-19/covid5.png)

<br>

## **The Data**

The data in this workspace includes raw sequencing data (.fastq and .BAM) available from NCBI's Sequence Read Archive (SRA). These data, from around the world, have been run on the Illumina platform. All .fastq files have been processed to raw uBAM files.

Additionally, an example concatenated fasta file and corresponding metadata are provided here from NCBI's GenBank as example data for generating a NextStrain tree.

<br>

### **What is the data from NCBI SRA?**

Since this data is collected from various places around the world, from different patients and labs, data quality will vary. We have created two separate sample sets for analysis to help you decide which samples you would like to work with. 

1. ""Successful_Assembly_Group"" - these are samples that successfully ran through assembly
2. ""Failed_Assembly_Group"" - these are samples that did not successfully assemble

If this is your first time running this data through these workflows, we suggest running the ""Successful_Assembly_Group"" samples through the workflows, so that you may see what the results of good quality samples that fully assemble look like, as a means of comparison. 

You may run the ""Failed_Assembly_Group"" samples if you are interested in using all of the data, regardless of if a sample assembled or not. When these samples run through the workflow, you will see some error messages alerting you to their failed assembly. For examples of these error messages, please see the section **'Sample Quality and Sample Failures'** below.    

<br>

### **What is the data from NCBI GenBank?**

We have included in this workspace, data from NCBI Virus, including a concatenated fasta and the corresponding metadata file for the fasta. These files can be used as input in the *sarscov2_nextstrain* workflow, to generate phylogenetic trees.

<br>

## **Working with SARS-CoV-2 Sequences**

The scientific and public health community tackling SARS-CoV-2 genomics has favored simplified approaches for both data generation and data analysis, many of which are documented by the CDC. Simple align-to-reference based approaches for consensus calling (similar to those used in the study of non-diverse genomes, such as humans), provide more efficient analysis processes and ease of interpretation. Additionally, the popularity of PCR tiled amplicon-based data generation approaches (such as ARTIC) frequently necessitates specialized filtration steps to remove the primer artifacts during analysis (the iVar trimming tool from Scripps being one of the more popular tools for ARTIC+Illumina data).

We have provided our reference based viral assembly tool (assemble_refbased.wdl) that has been updated to reflect these best practices and is appropriate for use on any Illumina data generated from SARS-CoV-2. In particular, there is an optional input parameter, a BED file, to describe any PCR amplicon primers used in the process of data generation (this can be omitted if no such primers were used). 

<br>

## **Workflow Input and Output Examples**

Click on the links below to see the [Terra documentation summarizing the listed workflows](https://support.terra.bio/hc/en-us/articles/360058305512). You'll find a summary of what each workflow does, as well as a table of input and output examples for each workflow.

[**Getting Data into your workspace**](https://support.terra.bio/hc/en-us/articles/360058305512#h_01F1PPKTBF4XPAWGFA87YBB1NN)
- genbank_ingest
- fetch_sra_to_bam


[**Sequence Data processing and assembly**](https://support.terra.bio/hc/en-us/articles/360058305512#h_01F1PPNEEA7GH5BGASM9HP0VAP)
- Fastq_to_ubam
- assemble_refbased
- demux_deplete

[**Calling Lineages and Clades**](https://support.terra.bio/hc/en-us/articles/360058305512#h_01F1PPP0CHYGMNF5G0847QMQMQ)
- Sarscov2_lineages
- Sarscov2_nextclade

[**NextStrain Phylogenetic Analysis**](https://support.terra.bio/hc/en-us/articles/360058305512#h_01F1PPQ6AYHA1M5G8D1YHKSB3G)
- Sarscov2_nextstrain
- genbank_curate

[**All-in-one workflows**](https://support.terra.bio/hc/en-us/articles/360058305512#h_01F1PPQCX9CPXYAHZTSHFPVMVV)
- Sarscov2_illumina_full
- Sarscov2_sra_to_genbank
- Sarscov2_genbank


<br>

## **NextStrain Tree Building Tools**


NextStrain is a collection of open source tools that help scientists, epidemiologists and public health officials in their understanding of pathogen spread and evolution, especially in outbreak scenarios. One of these tools is [Augur](https://github.com/nextstrain/augur), which was developed in order to track pathogen evolution from sequencing data. With this tool, scientists can build trees to analyze the phylogeny and evolutionary relationships of Sars-CoV-2, including the initial emergence and sustained transmission.

To generate phylogenetic trees using the Nextstrain tools, you can use the *sarscov2_nextstrain* workflow in this workspace, along with the publicly available data from NCBI Virus. This workflow, requires as input these main files, as shown below:


Overview of the steps involved in the WDL:
![](https://storage.googleapis.com/terra-featured-workspaces/covid-19/covid6.png)


<br>

There are numerous configuration files that you can use to define colors, lat and long, etc., all of which can be found in the nextstrain data table and example provided in this workspace.

<br>

### **Using sarscov2_nextstrain.wdl to create a tree with your data**

If you would like to prepare a tree with your own data,you will need to prepare the following files:

1. fasta files
2. metadata.tsv file
3. Your build (.yaml) file

If you wish to further customize the visualization or configuration to your liking, you will need to update the following files as well:
* the augur_config file
* the colors.tsv file
* the lat_longs.tsv file

* Note: This document outlines the requirements for the metadata.tsv file: https://github.com/nextstrain/augur/blob/master/docs/faq/metadata.md#parsing-from-the-header. The metadata.tsv file has to be curated manually with any metadata that you want to use for your tree. The strain column HAS to match with the fasta headers or else it wont work.

* Note: There are several ways that you can filter and configure how you view your data. For example, the following parameters are available for filtering data (please see https://nextstrain-augur.readthedocs.io/en/stable/usage/cli/cli.html for details on each parameter)


3. Once the workflow is complete, download the auspice_input_json file to your local machine
4. Simply drop your .json file that was outputted into http://auspice-us.herokuapp.com/

* This tutorial has detailed instructions on how to set up your computer with Docker and Nextstrain (https://nextstrain.org/docs/getting-started/quickstart)
* If you would like to use GitHub to visualize, this tutorial explains the steps: https://nextstrain.org/docs/contributing/community-builds


<br>
<br>

## **Sample Quality and Sample Failures**


```PoorAssemblyError: Error: poor assembly quality, chr 1: contig length 19301, unambiguous bases 2066; bases required of reference segment length: 14951.5/29903 (50%)```

This error message occurs in the assemble_denovo_with_deplete workflow when a sample fails assembly. This error can be overcome by changing one specific variable in the workflow. This poor assembly error can be overcome if you change the ```scaffold.min_unambig float``` variable value down from 0.5 to 0.1. In most instances, this will allow the sample to pass. It should be noted, however, that this is at the discretion of the researcher and their acceptance criteria for sample quality.



<br>

## **Contact information**
For questions regarding viral genomics methods, please contact Danny Park (dpark@broadinstitute.org)",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","pathogen-genomic-surveillance/COVID-19"
203,"help-gatk","GATKTutorials-Pipelining-July2019","READER","https://app.terra.bio/#workspaces/help-gatk/GATKTutorials-Pipelining-July2019",TRUE,FALSE,NA,NA,NA,"# What's in this workspace?
Welcome to Day 4 of the Genome Analysis Toolkit (GATK) workshop at the University of Cambridge in Cambridge, U.K.!

Earlier today, you learned about WDL and Cromwell, and using this empty workspace, we will practice starting a workspace from scratch. Your instructor will guide you through this workspace, but you should find the documentation on this page sufficient to run through them again on your own or to share with a friend later (and we encourage you to do so!).

### Data
You will need to download the data bundle located [here](https://broad.io/GATK1907) if you haven't already. This bundle contains several folders but the one you will need for this workspace is labelled **Terra**. It provides the WDL script, workspace and sample metadata, and an inputs file to run.

### Tools
There are no tools in this workspace yet. But we will be putting a tool in that runs GATK's HaplotypeCaller, using the instructions below.

**Add a new Tool**
1. In a new tab, navigate to Code & Tools. On the right hand side of the page, click on the `Broad Methods Repository` link. This currently will take you to our legacy application to upload a new Method, which is another name for Tool. In the near future, you will be able to upload your new tool directly in Terra. 
2. Click the blue `Create New Method` button in the upper right corner.
3. Fill out the window that pops up as follows:
>**Namespace:** your own name or abbreviation of your name. 

The namespace is essentially the publisher of the method. You can publish it as yourself, as we are doing here, or sometimes you may want to publish it under your lab or institution's name. 
>**Name:** `HelloGATK`

This refers to the name of your tool. You can call it whatever you want, but we've chosen to match it to the WDL script we will be using. Lastly:
>**WDL:** Click the blue `load from file...` link, and load the `hello_gatk_terra.wdl` script from your data bundle, under the Terra folder

4. Click `Upload`
5. Click the blue `Export to Workspace` button in the upper right corner, then in the dialog that pops up, select `Use Blank Configuration`
6. Under `Destination Workspace`, select your clone of this workspace, then click `Export to Workspace`
7. The page will ask you `Go to edit page now?` Click Yes, and you will be redirected back to your workspace!

**Add Data**

For our tutorial purposes, we've already uploaded your sample files to a public google bucket so that you do not need to incur storage costs. If you did want to upload your own samples to run this workflow on later, you can do so by navigating to the Data tab in your workspace, and clicking on the `Files` section in the lefthand menu. From there, you can upload whatever files you need.

**Run your new Tool!**
1. Navigate to the `Tools` tab in your workspace
2. Click on the HelloGATK tool you recently added.
3. Drag the `hello_gatk_fc.inputs.json` file from your Terra folder in the data bundle to upload it and populate the inputs section. You'll notice there are a number of `gs://` links, which represent files stored in google buckets. 
4. Click the green `Save` button to save the inputs you just uploaded
5. At the top of the page, select the radio button next to `Process single workflow from files`
6. Click the green `Run Analysis` button

**Optional: Parameterize your inputs**

You don't want to go in and change these `gs://` links for each individual sample you want to run on, especially if you're running on a large number of samples. You can run your workflow on many samples with a single click by setting up your Data tables.
1. Go to the `Data` tab in your workspace
2. Upload `participant.tsv` and `sample.tsv` from the Terra folder in your data bundle by clicking the `+` icon next to Tables in the lefthand menu
3. Click on the Workspace Data section in the lefthand menu, and upload `workspace_attributes.tsv` from the Terra folder. 
4. Navigate back to the `Tools` tab. 
5. Select the radio button for `Process multiple workflows from` and choose `Sample` from the dropdown. 
6. In the inputs table at the bottom, we now need to replace the `gs://` links with references to the data table. For the `inputBAM` and `bamIndex`, try typing `this` and see what Terra auto-fills for you as options. The keyword `this` refers to whatever data table you selected in the dropdown at the top. Since we picked `Sample`, it fills in options that exist in your `Sample` table. Click `Save` when you are done.
7. Click the green `Run Analysis` button

### Software versions
GATK4.1.1.0

## Appendix

### GATK @ Cambridge 2019 in the Terra Support Center

Following the conclusion of the workshop, the workshop materials will be made available in the Terra Support Center. For now, you can find these materials in a google drive folder at [broad.io/GATK1907](https://broad.io/GATK1907)

### GATK documentation and support

Detailed documentation, tutorials, and more are available at the [GATK web site](https://software.broadinstitute.org/gatk/). If you are still running into issues after consulting the User Guide, have a question, or just want to say hello, reach out to the GATK team via the [GATK Support Forum](https://gatkforums.broadinstitute.org/gatk)!

### Terra documentation and support

Documents and helpful guides to support your journey through Terra are available in the Terra Support Center. Navigate there by following the “Learn About Terra” link in the left-side menu. At the time of this workshop, we are still actively writing and publishing documents but we’ve got a good set of docs in the Quick Start Guide to get you going.


",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/GATKTutorials-Pipelining-July2019"
204,"ctat-firecloud","Trinity","READER","https://app.terra.bio/#workspaces/ctat-firecloud/Trinity",TRUE,FALSE,NA,NA,NA,"## Trinity

A fully reproducible example workflow for RNA-Seq de-novo assembly using Trinity

Complete documentation for Trinity is available on the [Trinity Wiki](https://github.com/trinityrnaseq/trinityrnaseq/wiki).

![TrinityCompositeLogo](https://raw.githubusercontent.com/wiki/trinityrnaseq/trinityrnaseq/images/TrinityCompositeLogo.png)

### Workflow Description

 Trinity is a novel method for the efficient and robust de novo reconstruction of transcriptomes from RNA-seq data.

### Input

- Sequencing data in FASTQ format.


### Output
- Trinity.fasta - Trinity assembly
- Trinity.fasta.gene_trans_map - File that maps gene to transcripts


More information on output available [here](https://github.com/trinityrnaseq/trinityrnaseq/wiki/Output-of-Trinity-Assembly)



### Sample data description and location

The trinity_rna_seq workflow in this workspace is preconfgured with RNA-seq data consisting of 10 million reads from mouse dendritic cells. The data was used in the paper:

Grabherr MG, Haas BJ, Yassour M, Levin JZ, Thompson DA, Amit I, Adiconis X, Fan L, Raychowdhury R, Zeng Q, Chen Z, Mauceli E, Hacohen N, Gnirke A, Rhind N, di Palma F, Birren BW, Nusbaum C, Lindblad-Toh K, Friedman N, Regev A. Full-length transcriptome assembly from RNA-seq data without a reference genome. Nat Biotechnol. 2011 May 15;29(7):644-52. doi: 10.1038/nbt.1883. [PubMed PMID: 21572440](https://pubmed.ncbi.nlm.nih.gov/21572440/). 


### Time and cost estimates
Below is an example of the time and cost for running the workflow.

| Number of reads | Cost    | Time             |       
| ---------------------- |--------- | ---------------| 
| 10 million                |  $1.33 | 130 minutes |
| 50 million             |  $4.34 | 360 minutes         |


Note: Cost and time will vary with the use of preemptible instances.


### Contact Information
Questions can be posted to the [Trinityrnaseq-users Google group](https://groups.google.com/g/trinityrnaseq-users).

### License
Copyright Broad Institute, 2020 | BSD-3
All code provided in this workspace is released under the WDL open source code license (BSD-3).


",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","ctat-firecloud/Trinity"
205,"encode-tutorial","ENCODE-Tutorial-May-2020","READER","https://app.terra.bio/#workspaces/encode-tutorial/ENCODE-Tutorial-May-2020",TRUE,FALSE,NA,NA,NA,"Learn how to search, analyze, and visualize ENCyclopedia Of DNA Elements (ENCODE) data. The resources in this workspace cover binning ENCODE ChIP-seq datasets into non-overlapping 5 kB bins and determining the signal enrichment in each bin.  More information about the ENCODE project can be found here: https://www.encodeproject.org/.  

The contents in this tutorial come from the ENCODE Terra Tutorial given May, 2020 and include the following steps:

1. How to access and import selected ENCODE data from the Data Explorer
2. How to use a workflow  to calculate the Probability of Being Signal (PBS) to indicate the presence of H3K27ac histone marks
3. How to identify regions of interest by plotting  a comparison between BED files generated by the PBS Workflow in a Jupyter Notebook
4. How to zero in on regions of interest by visualizing tracks in IGV


# How to use this workspace
[Follow the step-by-step instructions in this PDF](	
https://storage.cloud.google.com/terra-featured-workspaces/encode-tutorial-2019/Encode%20Tutorial%20Handout%20May%202020.pdf). The step-by-step guide will walk you through the following:   

* Step 1 - Find and select ENCODE data and export to a cloned version of this workspace	       
* Step 2 - Verify data in the workspace data table
* Step 3 - Set up and run the Probability of Being Signal (PBS) workflow on data in the data table
* Step 4 - Monitor the status of your PBS submissions 
* Step 5 - Analyze PBS workflow output in a Jupyter Notebook to identify regions of interest   
* Step 6 - Visualize bigWig tracks in IGV	8      


## Accessing ENCODE data using the Data Explorer

ENCODE data is hosted on Terra and is available through the Terra [Data Explorer](https://broad-gdr-encode.appspot.com/), in addition to the ENCODE platform. Follow Step 1 on the [PDF tutorial guide](https://storage.cloud.google.com/terra-featured-workspaces/encode-tutorial-2019/Encode%20Tutorial%20Handout%20May%202020.pdf) to navigate to the ENCODE Data Explorer and create a custom cohort. 

The ENCODE data hosted in Terra's Data Explorer is not an exact mirror of the data that is publicly hosted on AWS. **It is a static copy of the data that was made in December 2018.**  Here is a description of what is hosted on the integrated Data Explorer as of May, 2020.

For tabular data / metadata, the bucket contains experiment information for these assay types:

* ATAC-seq
* ChIA-PET
* ChIP-seq
* DNase-seq
* Hi-C
* microRNA-seq
* polyA RNA-seq
* RRBS
* total RNA-seq
* WGBS

The following filters were applied to the references to pull metadata for linked entities:

* Human data
* Status = released
* Files that can trace back to a replicate

From that set of data, the following files were transfered from S3 to Google:

* BAMs with output type “alignments”
* bigBeds with output type “peaks”
* bigWigs with output type “fold change over control”

The integrated Data Explorer in Terra contains tabular rows for 659 donors and 159,884 “files” (the file rows contain the metadata of everything except donors.)  It's 98.23 TB in size.
For questions about data access and any information related to the ENCODE data hosted in Terra, please contact Jeff Korte: at jkorte@broadinstitute.org

## Preloaded Data
Steps 1-2 of the tutorial walk you through the process of accessing, selecting, and linking ENCODE data using the Data Explorer. If you don't want to generate your own cohorts, we include metadata for three BAM data files that are preloaded in the workspace data table that you can use for analysis (steps 3 - 6). 

#### Cohort name: heart_bam

* ChIP-seq data 
* Selected tissues (heart left ventricle and right atrium auricular region)
* Donor accession  ENCDO793LXB, ENCDO845WKR, ENCDO271OUW, ENCDO451RUA
* Target H3K27ac
* File format is BAM


## Overview of the analysis workflow 

We provide a preconfigured workflow that calculate Probability of Being Signal (PBS) values. PBS values can be used to easily identify 5 KB regions of interest and compare multiple datasets regardless of read depth and peak calling. 

**PBS-bam** - takes a BAM file as input .   

The **PBS-bam** WDL will bin the reads into 5 KB windows, fit a gamma distribution to the binned data, and calculate the probability of the bin containing a signal (PBS) for the H3K27ac modification. The WDL outputs two files: (1) a BED file named **pbs.bed** with bins and their scores and (2) a PDF file named **fit.pdf** containing a plot illustrating the empircal distribution, background distribution, and probability of being signal.  

* **Assigning the bin count value**:  After binning the data, the PBS workflow assigns a count value to each 5 KB region, based on the ratio between the number of base pairs covered by reads in each bin and the total bin size.    

*  **Calculating the probability of being signal (PBS)**: The workflow first estimates the background distribution of the data with a gamma distribution, then calculates the PBS as the fraction of bins at each count value that are signal.  This signal correlates to the right tail distribution - the signal is the measure of the difference between the bin and the expected gamma distribution curve.
 
* All alignments use the human reference genome **hg19** (also referred to as GRCh37, or b37).

* The PBS-bam workflow has been tested on data sets containing the target histone acetylation site ***H3K27ac.***  The workflow may work differently on alternative targets. 

The workflow is summarized in the graphic below.

![image](https://storage.googleapis.com/terra-featured-workspaces/encode-tutorial-2019/Binning_PBS.png)


### Software Version

| Workflow Name | Version  |
|---|---|
| PBS_bam| snapshot 21 |  

### Time and cost 

| Workflow Name                  	| 1 file (range) 	| 100 files 	| Time to Run 1 file 	| Time to Run 100 files 	|
|--------------------------------	|----------------	|-----------	|--------------------	|-----------------------	|
| PBS-bam     	| $0.03 	| < $3.00   	| 10-15 minutes          	| 15-30 minutes              	|***

The notebook is set to the default ""Medium"" configuration which costs $0.19 per hour.

* Cost and Time will vary due to the use of pre-emptibles and the size of the files.

* Users can also use [Google's cost calculator](https://cloud.google.com/products/calculator/).


------------
## Licensing

**Copyright Broad Institute, 2020 | BSD-3**
All code provided in this workspace is released under the WDL open source code license (BSD-3) [full license text here](https://github.com/openwdl/wdl/blob/master/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.

-----------
## Contact information

For questions about this tutorial and using Encode Data in general, please contact Kevin Dong (kdong@broadinstitute.org)
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","encode-tutorial/ENCODE-Tutorial-May-2020"
206,"broad-firecloud-tcga","TCGA_ESCA_OpenAccess_V1-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_ESCA_OpenAccess_V1-0_DATA",TRUE,TRUE,"Esophageal Carcinoma","Tumor/Normal","USA","TCGA Esophageal carcinoma Open-Access Data Workspace","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients’ clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","185","Esophagus","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh37/hg19","TCGA_ESCA_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_ESCA_OpenAccess_V1-0_DATA"
207,"fc-product-demo","BioDataCatalyst-Gen3-data-on-Terra-Tutorial","READER","https://app.terra.bio/#workspaces/fc-product-demo/BioDataCatalyst-Gen3-data-on-Terra-Tutorial",TRUE,FALSE,NA,NA,NA,"Welcome to Terra! We developed this hands-on tutorial to help researchers working with Gen3 data in Terra. The tutorial includes step-by-step instructions to cover the entire analysis process:     
- How to link your Gen3 and Terra accounts    
- How to find and export Gen3 data to a project workspace    
- Understanding the structure of Gen3 data in Terra and how to make exported data more familiar    
- How to run an interactive analysis in a Jupyter notebook    
- How to configure and run a bulk analysis using workflow tools    

Reading the documentation and following the step-by-step instructions will help familiarize you with Terra, so you can hit the ground running.

# Tutorial Overview

You should run through the steps below **in order**. Time estimates are conservative - many steps will take less time. **Note that you don’t have to go through all the steps in one sitting, only in the correct order.**           

![Tutorial flow diagram](https://storage.cloud.google.com/terra-featured-workspaces/BioData%20Catalyst/Gen3-data-tutorial-flow-steps_scaled.png)

### How long does the tutorial take and how much does it cost?
Working through all the tutorial steps should take just over an hour total (you can break it up and do at different times). The total computation cost of running both notebooks and the workflow will be less than $1.00.    


### Science overview
The tutorial models steps typical in a research journey using synthetic Gen3 phenotypic data and public-access 1,000 Genomes genomic data.  The data are stored in the BioData Catalyst instance on the Gen3 platform. Phenotypic data are contained in the `lab results` and `demographic` nodes in Gen3 (and corresponding tables in Terra). The data are exported to Terra in the form of fifteen individual data tables connected to each other by UUIDs. The notebook section of the tutorial demonstrates how to select a subset of Gen3 tables you need - based on the data required for your analysis - and consolidate into one table with familiar subject ID conventions. This process of converting Gen3 data to a format that is more intuitive in Terra will be the same no matter what your final analysis. Section 7 covers how to run a bulk analysis workflow on the genomic data exported from Gen3. You would use the same process to do any bulk analysis on genomic data, such as aligning reads or calling variants. 
 

### Data disclaimer
The tutorial uses a synthetic, public-access dataset derived from 1,000 Genomes data and stored in the BioData Catalyst instance in Gen3. For more information about how the synthetic data were generated, see [this workspace](https://app.terra.bio/#workspaces/amp-t2d-op/2019_ASHG_Reproducible_GWAS-V2).


# Instructions 

## Set up tutorial workspace
(one minute)     

Before you begin, you will need to create your own editable copy (clone) of this WORKSPACE  to work in, by clicking on the round circle with three dots in the upper right corner of this page and choosing ""Clone"": 
    ![Clone_ScreenShot](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/Clone_workspace_Screen%20Shot.png)
		

### Work from your copy of the workspace following these step-by-step instructions 
|![PDF icon](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/PDF-icon_scaled.png) | Download a clickable PDF of step-by-step instructions [here](https://storage.cloud.google.com/terra-featured-workspaces/BioData%20Catalyst/Worksheet%20-%20Analysing%20BioData%20Catalyst%20Gen3%20Data%20on%20Terra.pdf) |  
| --------| ------------------|    


<br>
  


## Step 1. Link your Terra profile with Gen3
(two minutes)     
If you haven't linked your Terra and Gen3 accounts yet,  you need to follow these [steps](https://support.terra.bio/hc/en-us/articles/360038086332) before you can analyze genomic data from Gen3 inside your Terra workspace.        

<br>  

## Step 2. Access synthetic dataset on Gen3 platform
(five minutes)     
This section walks through the process of accessing the public-access tutorial dataset (**tutorial-synthetic_data_set_1**) by going to the [Gen3 platform](https://gen3.biodatacatalyst.nhlbi.nih.gov).      

<br>

## Step 3. Export data to workspace    
(ten minutes - mostly exporting time)     
In this step you will practice exporting a Gen3 dataset to Terra. This process will be the same no matter what Gen3 data you are using. As part of this step, you will also learn about the structure of the exported data in a Terra workspace. 

<br>

## Gen3 data overview
To understand what you will see when you export Gen3 data to a Terra workspace, it helps to first understand how the data is organized in Gen3. A diagram of the graph structure of the synthetic data in Gen3 is below on the left. Each box is a Gen3 data node and each line represents UUIDs connecting the metadata of the two nodes.  Administrative nodes are purple, clinical data are in blue boxes, and genomic data are green boxes. When you export a project's data to a Terra workspace, each node in the graph gets imported as its own data table. Your data page will look like the figure on the right. The synthetic data in this tutorial is captured in fifteen tables, corresponding to the fifteen nodes in the graph structure.    

| Gen3 graph structure | Gen 3 tables in Terra |    
|-----|--------|      
|![Synthetic Gen3 data overview](https://storage.cloud.google.com/terra-featured-workspaces/BioData%20Catalyst/Gen3-graph-structure-synthetic-data_scaled.png)  | ![Screenshot of tables of Gen3 synthetic data imported to Terra](https://storage.cloud.google.com/terra-featured-workspaces/BioData%20Catalyst/Gen3-synthetic-data-tables_Screen%20Shot.png)  |


Each data table includes **all the metadata fields** associated with the Gen3 node, in alphabetical order. The lab_results table, for example, looks like this:   
![Screen shot of demographic table](https://storage.cloud.google.com/terra-featured-workspaces/BioData%20Catalyst/Gen3-synthetic-data-lab-results-table_Screen%20Shot.png)    

You might see from this example several features specific to the Gen3 data structure that can be challenging to work with in Terra:     
* There are fifteen separate tables from the export - not all of them include data that you will need        
 * The phenotypic data we want to use for this tutorial are in two *separate* tables (`demographic` and `lab_results`)    
* The first column in each table is not the subject ID you are used to, is not human-readable, and is not used across levels. Rather, a UUID connects each node to the node directly above it in the graph.          
 * The  familiar subject ID (called `submitter ID in Gen3 tables) is buried (in the 13th column of the lab_results table, for example)     
 *  The subject ID is only indirectly connected to the phenotypes in the `lab_results` and `demographics` tables with two different UUIDs (i.e. two separate lines connect their nodes)        
* There are sixteen columns (metadata fields) in the `lab_results` table, and the five or six we want to include in our analysis aren't easy to pick out because they are far to the right of the table 

To learn more about the structure of Gen3 data, see [this article](https://support.terra.bio/hc/en-us/articles/360038087312).  

<br>    

## Step 4. Consolidate and format exported Gen3 data
(twenty minutes)     
In this step you'll run the notebook **1-Consolidate-Gen3-data-in-Terra-tutorial**. The tutorial notebook resolves some of the challenges of Gen3 data exported to Terra. In particular, it consolidates the tables that contain phenotypic data into one Terra table and reorganizes the tables and subject IDs to be more familiar. The notebook is also a good introduction to interactive analysis in a Jupyter notebook.     

**Notebook outline**    

* Set up the notebook environment

* Discover all of the metadata available from the Gen3 export

* Define and use a set of custom functions to      
 * Merge the metadata in a consolidated Pandas dataframe
 * Reformat a bit of the Gen3 graph language to be more familiar TOPMed nomenclature
 * Push the consolidated dataframe to a workspace data table       
     
<br>  

## Step 5. Tidy up the consolidated table
(three minutes)     
The consolidated table generated in Step 4 has almost fifty columns - all the Gen3 metadata from phenotypic data nodes. To help make the table more manageable to read without scrolling left and right, you can select the columns you want to see when you expand the table. This will allow you to focus on the information you need, without having to scroll across tens of columns. Your selections will persist even when you leave the workspace.          

<br>    

## Step 6. Analyze consolidated data (""2-Analyze Gen3 data"" notebook)
(fifteen minutes)    

In this step you'll run the notebook **2-Analyze-consolidated-data-tutorial**, which pulls data from the consolidated table into the notebook environment and does a bit of plotting analysis. 

**Notebook outline**    
* Import the consolidated phenotypic data from the new data table to the notebook compute environment          
      
* This is where you would begin your project-specific analysis (in R or Python). We will explore the data by generating a few plots in the notebook.    



![white space](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/white-space.jpg)         

## Step 7. Run a workflow on genomic data 
(10 minutes)     

Another mode of analysis in the Terra platform is bulk analysis with workflows. This mode is what you would typically use to do automated analysis steps such as processing genomic data. Bulk analysis uses workflow tools, which you can add to your workspace from another workspace, or by searching Dockstore or the Broad Methods Repository. In this step you will learn how to run a bulk analysis workflow to process genomic data in the the synthetic dataset from the Gen3 platform. 



![white space](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/white-space.jpg)    
## Next steps and additional resources
Congratulations! You've accessed and analyzed your first Gen3 data in Terra. Now that you have an idea of the basics of working with Gen3 data in Terra, here are some additional BioData Catalyst and TOPMed resources to explore.     
1. **[BioData Catalyst Collection](https://app.terra.bio/#workspaces/biodata-catalyst/BioData%20Catalyst%20Collection)** 
This workspace includes the`terra_data_util` notebook that you covered in detail in this tutorial. After you complete this introduction tutorial, we suggest you copy the latest version of `terra_data_util` to your analysis workspace.

This collection holds other utility and analysis notebooks to help researchers work in the BioData Catalyst ecosystem. Check this resource often to find new code resources that can help you with your analysis. 


2.  [GWAS Tutorial using synthetic training dataset](https://app.terra.bio/#workspaces/biodata-catalyst/BioData%20Catalyst%20GWAS%201000%20Genomes%20Tutorial)

3.  [GWAS of a blood pressure trait with TOPMed data](https://app.terra.bio/#workspaces/biodata-catalyst/BioData%20Catalyst%20GWAS%20blood%20pressure%20trait)

4.  [TOPMed alignment workflow]( https://app.terra.bio/#workspaces/biodata-catalyst/TOPMed%20Aligner%20Gen3%20Data)



![white space](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/white-space.jpg)      

-------
## Authors
Workspace author: Allie Hajian  
Notebook code, edits and bug-squashing: Michael Baumann, Beth Sheets  
  
This workspace was completed under the NHLBI BioData Catalyst project.


![white space](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/white-space.jpg)    

## Workspace change log 

| Date | Change | Author | 
| -------  | -------- | -------- |
| April 27, 2020 | reorganized dashboard; added new links | Beth Sheets |
| March 22, 2020 | initial draft | Allie Hajian |
| April 21, 2020 | Tidying up dashboard | Allie Hajian |     
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","fc-product-demo/BioDataCatalyst-Gen3-data-on-Terra-Tutorial"
209,"broad-firecloud-tcga","TCGA_ACC_hg38_OpenAccess_GDCDR-12-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_ACC_hg38_OpenAccess_GDCDR-12-0_DATA",TRUE,TRUE,"Adrenocortical carcinoma","Tumor/Normal","USA","TCGA Adrenocortical carcinoma Open-Access Data Workspace

*hg38 TCGA and TARGET workspaces reference files by their GDC UUIDs.  In order to run analyses on the referenced data files, you will need to run workflows that retrieve the files from the GDC and copy them to your workspace bucket.  See   [this forum post](https://gatkforums.broadinstitute.org/firecloud/discussion/10382/populating-hg38-tcga-and-target-workspaces-with-data-files#latest) for instructions on the running of these workflows.*","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients' clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","92","Adrenal Gland","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh38/hg38","TCGA_ACC_hg38_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_ACC_hg38_OpenAccess_GDCDR-12-0_DATA"
210,"broad-firecloud-tcga","TCGA_UVM_OpenAccess_V1-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_UVM_OpenAccess_V1-0_DATA",TRUE,TRUE,"Uveal Melanoma","Tumor/Normal","USA","TCGA Uveal Melanoma Open-Access Data Workspace","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients’ clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","80","Eye","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh37/hg19","TCGA_UVM_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_UVM_OpenAccess_V1-0_DATA"
211,"help-gatk","Introduction-to-Target-Dataset","READER","https://app.terra.bio/#workspaces/help-gatk/Introduction-to-Target-Dataset",TRUE,FALSE,NA,NA,NA,"This workspace demonstrates accessing and analysing controlled-access TARGET datasets.

Practice retrieving data from the Genomic Data Commons Data Portal  and running workflows that use the TARGET dataset to create a panel-of-normals VCF (2-CNV_Somatic_Panel),  then use the that VCF to identify somatic copy number variations  (3-CNV_Somatic_Pair).

## What is Target?

The Therapeutically Applicable Research to Generate Effective Treatments (TARGET) program is an initiative organized by the Office of Cancer Genomics (OCG) and the Cancer Therapy Evaluation Program (CTEP) at the National Cancer Institute. 
The following quote is obtained from Targets project site on the [National Cancer Instute site](https://ocg.cancer.gov/programs/target).

> The Therapeutically Applicable Research to Generate Effective Treatments (TARGET) initiative employed comprehensive molecular characterization to determine the genetic changes that drive the initiation and progression of hard-to-treat childhood cancers. TARGET makes the data generated available to the research community with a goal to identify therapeutic targets and prognostic markers so that novel, more effective treatment strategies can be developed and applied.
> 
> Improved pediatric cancer treatments are needed because:
> 
> - Despite increases in overall survival rates, about 20% of pediatric cancer patients do not respond well to therapy and ultimately die from their diseases. 
> - The number of children and adolescents diagnosed with cancer is trending slightly upward.
> - Current treatments are particularly harsh on growing children. They often cause severe short- and long-term side effects, such as secondary cancers, physical and emotional health issues, developmental delays, and infertility.
> - Current treatment protocols are mostly derived from therapeutic regimens that were formulated for adult cancers. Previous genomics studies revealed that childhood cancers can be genetically distinct from their adult counterparts, suggesting the need for alternate treatment approaches.
> The TARGET initiative originated with two pilot projects characterizing the genomes and transcriptomes of “high-risk” subtypes of acute lymphoblastic leukemia (ALL) and neuroblastoma (NBL). The success of the two pilot project teams allowed TARGET to expand its efforts by incorporating higher resolution genomics approaches and the study of additional childhood cancers. To date, TARGET researchers have molecularly characterized subtypes of acute myeloid leukemia, osteosarcoma, and select kidney tumors, and additional subtypes of ALL and NBL.
> 
> TARGET was built as the collaborative effort of a large, diverse consortium of extramural and NCI investigators. Members of the Children’s Oncology Group (COG), a clinical trials group devoted exclusively to childhood and adolescent cancer research, comprise the majority of TARGET project teams. COG members were able to provide the TARGET teams access to clinical expertise and accrued tissue materials. TARGET researchers continue to work together within and across teams to generate, analyze, integrate, and interpret high quality genomics data. The goal of working with COG in this collaborative team science arena is to accelerate molecular discoveries and facilitate rapid translation of those findings into the clinic.
> 
> TARGET dataOpens in a New Tab are available to the greater research community for further investigation. For all data shared through TARGET, patient confidentiality and privacy are protected. This data sharing approach allows investigators outside the initiative to use the data, thereby increasing the chance that novel therapeutics will be developed and benefit children with cancer.


## Data Access
There are two forms of TARGET data, open access and controlled access. Open access data are unidentifiable data open to the research community at large, while controlled access data contain info that could potentially be used for patient re-identification. Therefore, controlled access data require researchers interested in the dataset be pre-approved in the form of a Data Use Certification (DUC). More information on the differences between the two levels of data can be found [here](https://ocg.cancer.gov/programs/target/using-target-data#open).

Users interested in obtaining approval for access-controlled data must 
1. [Have an eRA Commons/NIH account with dbGaP authorization](https://gdc.cancer.gov/access-data/obtaining-access-controlled-data)
2. [Request access to TCGA dataset through dbGaP account](https://dbgap.ncbi.nlm.nih.gov/aa/wga.cgi?page=login)
3. [Establish a link between your FireCloud and eRA Commons / NIH accounts](https://support.terra.bio/hc/en-us/articles/360038086332)

Please visit the read the [Accessing-TCGA-Controlled-Access-workspaces-in-Terra](https://support.terra.bio/hc/en-us/articles/360037648172) article for further details.

Once you have access, you’ll be able to browse the Target dataset in the [Genomic Data Commons Data Portal](https://portal.gdc.cancer.gov/) and be able to follow along with the steps of accessing the data in this workspace. 

## Data
In this workspace you will be executing workflows that will process aligned Target BAM files obtained from the Genomic Data Commons. Once you've obtained access to access-controlled data using the steps above you can follow along with the steps below. The steps below involve obtaining a Sample Sheet from the Genomic Data Commons Data Portal for a particular Target dataset project and uploading the Sample Sheet to the Data tab. This will allow the workflows in this workspace to download the files from GDC Data Portal to your workspace bucket and process the downloaded files using somatic copy number variation workflows.  
 
**Clone Workspace**  
Before getting started, clone this workspace using the `TARGET-dbGaP-Authorized` authorization domain. If you do have the option to select this domain then you may need [Establish a link between your Terra and eRA Commons / NIH accounts](https://support.terra.bio/hc/en-us/articles/360038086332).  **Using this authorization domain is crucial in keeping the downloaded data secure.**  

**How to Access Target Sample Data from Genomic Data Commons**  
1. Go the [GDC Data Portal](https://portal.gdc.cancer.gov/)
2. Select [Projects](https://portal.gdc.cancer.gov/projects)
3. Check ""Target"" under the ""Program"" located on the left filter column. 
4. In the search results there will be a list of projects for Target. Click on the numeric value under the ""Files"" column for the Project ""TARGET-WT"".
5. On the left filter column, select ""WXS"" and ""BAM"".
6. Click on the shopping cart to the left of the ""ACCESS"" column header in the table. This will give you two drop down options, click on  ""Add all Files to the Cart"" option.
7. Click on the cart icon at the top right of the web page.
8. Click on ""Sample Sheet"" to download the sample sheet for the chosen samples.
9. Edit the downloaded Sample Sheet in Excel so that the column headers meets Terra TSV requirements.
  -  Move ""Sample ID"" column to the beginning of the table.
  - Replace ""Sample ID"" with ""entity:sample_id""
  - Replace all spaces in the column headers with underscores
  - Replace ""File_ID"" with ""Bam_File_ID""
  - Replace ""Case_ID"" with ""participant_id""
10. Add a column called ""Bam_ID_and_File_Name"" that combines the ""Bam_File_ID"" column and the ""File_Name"" separated by ""/"". You can use the following Excel formula `=B2&""/""&C2` where B2 is the ""Bam_File_ID"" and C2 is the ""File_Name"". 
13. Create a separate TSV file with one column labeled ""entity:participant_id"". The rows under this column will be the unique values under the ""Case_ID"" column in the sample TSV created in the previous step. You could use a pre-made `participant.tsv` file [here](https://console.cloud.google.com/storage/browser/terra-featured-workspaces/Introduction-to-Target?project=broad-dsde-outreach&organizationId=548622027621)
14. Upload the participant TSV then the your sample TSV to the workspaces Data tab under tables. Be sure to check the ""Create participant, sample, and pair associations"" when uploading files.

Congratulations! Now that you have uploaded the Target Sample Sheet table to the workspace, you'll be able to use the provided workflow to download the bam files using 1-GDC_Bam_Downloader.
In order to execute the processing workflows (2-CNV_Somatic_Panel and 3-CNV_Somatic_Pair) you'll need to create a sample set table with a set that contains all the normal samples, this will be used to create a panel of normals VCF. A pair table needs to be created that pairs the tumor and normal samples for identifying somatic copy number identification. To save you time the following TSV files are available in the [gs://terra-featured-workspaces/Introduction-to-Target bucket](https://console.cloud.google.com/storage/browser/terra-featured-workspaces/Introduction-to-Target?project=broad-dsde-outreach&organizationId=548622027621) `sample_set.tsv` and `pair.tsv`. Upload them to the Data tab.


**Workspace Data**  
Required and optional references and resources for the methods are included in the Workspace Data table. The reference genome for this workspace is hg38 (aka GRCh38).

## Workflows

The Workflow tab hosts a workflow to download files from GDC and a few example workflows you can use to test your Target data. 

**Download Data from GDC**
1. First run  ""1-GDC_Bam_Downloader"" in Sample mode and use the sample set containing all the normal bam files. If you are using the `sample_set.tsv` from the workspace bucket then this should be labeled as ""panel_of_normals"". This will download a set of normal bam files, which will be used to create the panel of normals
2. Run the ""1-GDC_Bam_Downloader"" in Sample mode on the TARGET-50-CAAAAA-01A sample. This will download a tumor bam that will be used in the somatic copy number variation workflow.

**Run Somatic Copy Number Variation workflows**
Note: At this point you have downloaded all the required bam files from GDC, and are now going to run the example processing workflows.
1. Run 2-CNV_Somatic_Pane in Sample_set mode on the set to be used to create a panel of normals. If you are using the `sample_set.tsv` from the workspace bucket than this should be labeled ""panel_of_normals"". This will produce a panel of normals VCF and place it in the Workspace Data section in your Data tab. This fill will be used by 3-CNV_Somatic_Pair.
2. Run the 3-CNV_Somatic_Pair  workflow in Pair mode on TARGET-50-CAAAAA-01A and TARGET-50-CAAAAA-10A pair. If you are using the `pair.tsv` from the [featured bucket](https://console.cloud.google.com/storage/browser/terra-featured-workspaces/Introduction-to-Target?project=broad-dsde-outreach&organizationId=548622027621) than this should be labeled as TARGET-50-CAAAAA. The output produced here is copy number variation plots for the TARGET-50-CAAAAA tumor and normal pair. 

This workspace contains the following workflows:  

**1-GDC_Bam_Downloader :** Downloads Bam files and their index from GDC. None Bam files (those without index files) can be downloaded using [GDC_File_Downloader](https://portal.firecloud.org/?return=terra#methods/broadinstitute_cga/gdc_file_downloader/2) workflow.

Entity Type  
- Sample

Requirements/expectations  
- UUID and File name 
- User access token. Instructions obtaining this file can be found on the [GDC site](https://docs.gdc.cancer.gov/Data_Transfer_Tool/Users_Guide/Preparing_for_Data_Download_and_Upload/#obtaining-an-authentication-token-for-data-downloads). Once obtained, upload it to your workspace bucket and provide the gs:// path in the workflow configuration. 

Outputs  
- Bam and index

Time and Cost

| Sample set Name | Sample Size | Time | Cost $ |
| ---  | :---: | :---: | :---: |
| panel_of_normals | 840 GB | 00:48:00 | 1.33 |

**2-CNV_Somatic_Panel :** Workflow for creating a GATK CNV Panel of Normals given a list of normal samples.

Workflow for creating a GATK CNV Panel of Normals given a list of normal samples.

Entity Type
- Sample Set

Requirements/Expectations
* CNVSomaticPanelWorkflow.gatk_docker -- GATK Docker image (e.g., broadinstitute/gatk:latest).
* CNVSomaticPanelWorkflow.intervals -- Picard or GATK-style interval list. For WGS, this should typically only include the autosomal chromosomes.
* CNVSomaticPanelWorkflow.normal_bais -- List of BAI files. This list must correspond to normal_bams. For example, [""Sample1.bai"", ""Sample2.bai""].
* CNVSomaticPanelWorkflow.normal_bams -- List of BAM files. This list must correspond to normal_bais. For example, [""Sample1.bam"", ""Sample2.bam""].
* CNVSomaticPanelWorkflow.pon_entity_id -- Name of the final PoN file.
* CNVSomaticPanelWorkflow.ref_fasta_dict -- Path to reference dict file.
* CNVSomaticPanelWorkflow.ref_fasta_fai -- Path to reference fasta fai file.
* CNVSomaticPanelWorkflow.ref_fasta -- Path to reference fasta file.

Outputs
- Read count PON in HD5 format
- Addtional metrics

Time and Cost

| Sample set Name | Sample Size | Time | Cost $ |
| ---  | :---: | :---: | :---: |
| panel_of_normals | 840 GB | 00:11:00 | 0.01 |

**3-CNV_Somatic_Pair :** Workflow for running the GATK CNV pipeline on a matched pair.

Entity Type
- Pair

Requirements/Expectations
* CNVSomaticPairWorkflow.common_sites -- Picard or GATK-style interval list of common sites to use for collecting allelic counts.
* CNVSomaticPairWorkflow.gatk_docker -- GATK Docker image (e.g., broadinstitute/gatk:latest).
* CNVSomaticPairWorkflow.intervals -- Picard or GATK-style interval list. For WGS, this should typically only include the autosomal chromosomes.
* CNVSomaticPairWorkflow.normal_bam -- Path to normal BAM file.
* CNVSomaticPairWorkflow.normal_bam_idx -- Path to normal BAM file index.
* CNVSomaticPairWorkflow.read_count_pon -- Path to read-count PoN created by the panel workflow.
* CNVSomaticPairWorkflow.ref_fasta_dict -- Path to reference dict file.
* CNVSomaticPairWorkflow.ref_fasta_fai -- Path to reference fasta fai file.
* CNVSomaticPairWorkflow.ref_fasta -- Path to reference fasta file.
* CNVSomaticPairWorkflow.tumor_bam -- Path to tumor BAM file.
* CNVSomaticPairWorkflow.tumor_bam_idx -- Path to tumor BAM file index.

Outputs

* Modeled segments for tumor and normal
* Modeled segments plot for tumor and normal
* Denoised copy ratios for tumor and normal
* Denoised copy ratios plot for tumor and normal
* Denoised copy ratios lim 4 plot for tumor and normal
* Addtional metrics

Time and Cost  

| T/N Sample Name | Sample Size | Time | Cost $ |
| ---  | :---: | :---: | :---: |
| TARGET-50-CAAAAA | 21.6 GB | 00:48:00 | 0.04 |  
  	
Users can also use [Google's cost calculator](https://cloud.google.com/products/calculator/) for cost estimates.   

### Software Version  
- GATK 4.1.2.0

---

### Contact information  
For questions about this workspace please visit the Featured Workspaces Topic topic on the [Terra Community Forum](https://broadinstitute.zendesk.com/hc/en-us/community/topics). Use the search box to see if other users have asked the same question previously. If not, post and tag **@ Beri Shifaw** so that we get notified.

This material is provided by the GATK Team. Please post any questions or concerns regarding the GATK tool to the [GATK forum](https://gatk.broadinstitute.org/hc/en-us/community/topics)

### Workspace Citation 
Details on citing Terra workspaces can be found here: [How to cite Terra](https://support.terra.bio/hc/en-us/articles/360035343652)

Data Sciences Platform, Broad Institute (*Year, Month Day that this workspace was last modified*) help-gatk/Introduction-to-Target-Dataset [workspace] Retrieved *Month Day, Year that workspace was retrieved*,  https://app.terra.bio/#workspaces/help-gatk/Introduction-to-Target-Dataset

### License  
**Copyright Broad Institute, 2019 | BSD-3**  
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at https://github.com/openwdl/wdl/blob/master/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.

 ### Workspace Change Log
| Date | Change | Author |
| --- | --- | --- |
|  2019-06-27 | Addition of Dashboard, Workflows, and Data| Beri Shifaw |      
| 2020-08-03 | Updated dashboard links, updated workflow to use GATK Dockstore repo, updated data tables in featured workspace bucket | Beri Shifaw |
|  2021-09-15 | Addition of Workspace Citation section | Beri Shifaw |",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/Introduction-to-Target-Dataset"
212,"help-gatk","Featured-Workspace-Template","READER","https://app.terra.bio/#workspaces/help-gatk/Featured-Workspace-Template",TRUE,FALSE,NA,NA,NA,"
> Dashboard Documentation  
> A user should be able to easily understand and model a Featured Workspace. In general, all Featured Workspace content should emphasize the big picture - concepts and ideas that will resonate with a broad swath of users and use cases. Content should be self-explanatory - without making too many assumptions about prior knowledge - as well as concise and easy to follow.   
> TAGS - Enable searchability by choosing five to eight descriptive tags that would help the broadest subset of users to find the workspace. 


### Overview Paragraph
*A high-level description of the workspace and its purpose, the overview paragraph is an opportunity to capture the importance and relevance of the workspace and to draw potential users in to read as well as to see how they can do their own work on Terra. 
It should include an explanation of the focus (tool, data or analysis), if relevant, as well as:
Links to scientific description of the workflow
Links to “Best Practices” description
Links to any other resources (published papers or pre-prints, etc.)
“Please direct all questions to the Terra  forum”*
 
### Experimental Design (required for analysis-focused workspaces)
*One or more paragraphs describing the general motivation for the scientist/analyst’s experiment, caveats and concerns, and an ordered list of the case study steps. This section is especially important if the Featured Workspace reproduces or captures a particular experiment or publication (as with the ASHG Tetralogy of Fallot workspace).*
 

## Data 
*A real-language description of the data set (e.g. the origin of the data, important details about how it was processed, its shortcomings, whether or not it’s public) as well as where to find links to data types.*

## Notebooks (required for workspaces with notebooks)
*General specifications of all analysis notebooks, what analysis each does (in terms the scientist/analyst will understand), and their preset configurations.*    

*Software versions*   
*Explicitly state versions of all software applications (GATK4 versus GATK3, for example).  * 

*Time and cost*    
*This information is very much in-demand by users, especially users new to a cloud-based system. If it’s possible to include (even if from test runs) that would be great!*    
*Sample name*    
*Sample size*    
*Run time*   
*Cost*

E.g.
**Notebook 1-Setup-Environment**: Install apps and download data

Environment: New Default (released January 14: GATK4.1.4.1)

Computer Power: 

| Runtime  | Value |
| --- | --- |
| Environments | Default (GATK 4.1.4.1 Python3.7.8) |
| CPU Minimum| 4|
| Disksize Minimum | 100 GB |
| Memory Minimum | 15 GB |
| Startup Script | `gs://gatk-tutorials/scripts/install_gatk_4110_with_igv.sh` |
 
## Workflows (required for workspaces with workflows)
*General specifications of all tools (“methods”) and their preset configurations, as well as what each method does (in terms the scientist/analyst will understand). **Workflows that will be run back-to-back must be numbered, and configured to run without any intervention (for automated testing)** *     

*Best practices or optimization instructions should be included here.*     


### 1_Example_Workflow_Name

**What does it do?**  
*Description of workflow*     

**What does it require as input?**       
*List of inputs* .   

**What does it return as output?**  
 *List of outputs*

**Reference/Resource data description and location**  
*E.g. The reference genome for this workspace is hg38 (aka GRCh38). Required and optional references and resources for the methods are included in the Workspace Data table (""Workspace Attributes"" in FireCloud).*           
 
**Estimated time and cost to run on sample data**    


| Sample Name | Sample Size | Time | Cost $ |
| :---:  | :---: | :---: | :---: |
| NA12878_24RG_small | 3.11 GB | 1:28:00 | 0.18 |
| NA12878 | 64.89 GB | 22:35:00 | 4.98 |     

For helpful hints on controlling Cloud costs, see this article ([click here to view](https://support.terra.bio/hc/en-us/articles/360029748111)).       

---


### Contact information

*The author of the workspace should provide their own contact method. Comms will include a link to our forum site here because we are responsible for the Featured Workspaces and the forum is how we address user questions.*

### Workspace Citation (Optional)
*Citation to use when other users are referencing your workspace. Details can be found here: [How to cite Terra](https://support.terra.bio/hc/en-us/articles/360035343652)*  

*Citation format:*  
*Last Name, First Initial. (Year, Month Day that your workspace was last modified) name of your workspace [workspace] Retrieved Month Day, Year that workspace was retrieved URL to workspace*  

Example:  
Shifaw, B. (2021, September 2) help-gatk/Featured-Workspace-Template [workspace] Retrieved September 10, 2021,  https://app.terra.bio/#workspaces/help-gatk/Featured-Workspace-Template

### License (Optional)
*License information and links.*

### Workspace Change Log (Optional)
| Date | Change | Author |
| --- | --- | --- |
|  2020-09-03 | Useful for future updates. Add any notable changes made the workspace | Name |
|  2020-09-06 | Renamed Version Updates to Workspace Change Log | Beri Shifaw |
|  2021-09-02 | Added Citation section | Beri Shifaw |
 ",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/Featured-Workspace-Template"
213,"ad-data-platform","International Neuroimmune Consortium Data Collection","READER","https://app.terra.bio/#workspaces/ad-data-platform/International%20Neuroimmune%20Consortium%20Data%20Collection",TRUE,FALSE,NA,NA,NA,"# Summary   
This is the landing page for the International Neuroimmune Consortium Data Collection. This workspace links to data sets and resources created by the Neuroimmune Consortium. Please review the below descriptions for each resource and how to access those resources.   



# Data Sets  
## Idiopathic Normal Pressure Hydrocephalus (iNPH)-Kuopio  
#### Data:
The iNPH-Kuopio data set contains single-nuclei RNA-sequencing data and gene expression matrices on 58 high-quality frontal cortex (Brodmann Area 8 or 9) biopsies from individuals with suspected idiopathic normal pressure hydrocephalus who have variable degrees of local amyloid and tau histopathology. 1-5 libraries were sequenced per individual.  

#### Link:
https://app.terra.bio/#workspaces/broad-cfos-data-platform1/iNPH-Kuopio  

#### Access:  

To access, please request through the DUOS data catalog ([link](https://duos.broadinstitute.org/dataset_catalog)) using the dataset Id 	DUOS-000144 .   
## Human Stem-Cell Differentiated Microglia (iMGL)    

#### Data:
This dataset provides single-cell RNAseq data for iMGLs, microglia-like cells differentiated from human stem cells. We provide scRNAseq data from iMGLs differentiated from H1 after challenging these cells with a variety of CNS substrates (myelin, synthetic amyloid beta, synaptosomes, apoptotic neurons) or control with two replicates per condition. In addition, we perform a further analysis on iMGLs differentiated from three iPSC lines exposed to either control or apoptotic neurons.  
#### Link:
https://app.terra.bio/#workspaces/broad-cfos-data-platform1/CIRM%20-%20iMGL   

#### Access:  
To access, please request through the DUOS data catalog ([link](https://duos.broadinstitute.org/dataset_catalog)) using the dataset Id 	DUOS-000151 .  
## Imperial - UK DRI  
#### Data:

The UKDRI astrocyte and microglia enriched dataset contains single-nuclei RNA-sequencing data and gene expression matrices from 24 high-quality post-mortem brain tissue samples (entorhinal or somato-sensory cortex) obtained from 6 non-diseased control (Braak stage 0–II) and 6 Azheimer’s Disease individuals. Astrocyte and microglia nuclei were selectively enriched during isolation post-mortem. This data has been used for the study: “Diverse human astrocyte and microglial transcriptional responses to Alzheimer’s pathology”.  
#### Link:
https://app.terra.bio/#workspaces/broad-cfos-data-platform1/Imperial%20-%20UK%20DRI  

#### Access:
Please contact Eugene Duff at e.duff@imperial.ac.uk . 

# Workspaces  
## International Neuroimmune Consortium Analysis Playground
#### Data:

This is a workspace intended for collaborative integrative analyses related to the International Neuroimmune Consortium Data Collection. The workspace will collect metadata, unstrutured data, workflows, references and analyses/notebooks being used for joint analysis. The work shared in this workspace is shared as embargoed work within consortia efforts.  
#### Link:
https://app.terra.bio/#workspaces/ad-data-platform/Neuroimmune%20Integrative%20Analysis%20Playground  

#### Access:
Access to this workspace can be requested by emailing Evan Macosko (emacosko@broadinstitute.org), Anna Kane (akane@broadinstitute.org) and Timothy Tickle (ttickle@broadinstitute.org), please include your name and the email you use to log into Terra.  
# Community Resources
## NPH Integrative Analysis
Please visit https://braincelldata.org/resource to learn more about the NPH integrative analysis as well as how to access associated data and analysis.",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","ad-data-platform/International Neuroimmune Consortium Data Collection"
214,"gdscn-exercises","SARS-CoV-2-Genome","READER","https://app.terra.bio/#workspaces/gdscn-exercises/SARS-CoV-2-Genome",TRUE,FALSE,NA,NA,NA,"## SARS-CoV-2 Variant Detection with Galaxy

![""Coronavirus image""](https://drive.google.com/uc?id=1uQ4SmuinnG17b9BJKwy5G2Cas4Gm-_C9)

Tiny changes in DNA called variants help create variety in life. Variants can mean more dangerous disease when they happen in pathogens like viruses. But how do we detect this variation? 

Scientists use bioinformatics (part biology, part data science) to detect variants. Two major steps of detecting variants are quality checks and alignments. We'll be looking at tools for these steps to detect a variant of the SARS-CoV-2 virus.

**Quality checks** make sure the data is suitable for analysis. Data has to be of good quality if we want good results that we can trust.

**Alignments** compare our samples to the original strain of the virus. The alignment tools use the genomic sequence of the original strain. The scientific term for this sequence is the _reference_. Because the nucleotides are compared side-by-side, we can see exactly where the sample sequence differs from the reference.

Coronavirus researchers have used alignments to carefully monitor for new variants of the virus. In some cases, variants could be tied to different epidemiology. For example, some strains are more infectious or have lower host immunity. Performing these analysis make sure epidemiologists, public health officials, and health care professionals are poised to respond to the evolving outbreak.

## Data Files 

**This Workspace contains data files for the variant detection activity for SARS-CoV-2.**

In the lab activity, we'll see if there are genomic differences in the collected sample compared to the original SARS-CoV-2 genome (the reference). We need three files to do this:
* **SARS-CoV-2_reference_genome.fasta** : the reference genome
* **VA_sample_forward_reads.fastq.gz**: 1 of 2 raw read data files
* **VA_sample_reverse_reads.fastq.gz**: 2 of 2 raw read data files

## Original Data

The sample for this activity was derived from data collected at Virginia Commonwealth University in Richmond, VA. The researchers collected the sample with the goal of being able to track the spread and evolution of this virus state-wide, nationally, and internationally. You can download the original data [here](https://www.ncbi.nlm.nih.gov/sra/?term=XGTK449087).

You can download the reference genome [here](https://www.ncbi.nlm.nih.gov/nuccore/1798174254).

## Workflow
You can import the [activity workflow](https://usegalaxy.org/u/nakucher/w/sars-cov-2-genome/json ) used for this tutorial directly into the Galaxy environment you create to run each of the activity steps.

## License

Except where otherwise indicated, The contents of this slide presentation are available for use under the [Creative Commons Attribution 4.0 license](https://creativecommons.org/licenses/by/4.0/). 

You are free to adapt and share the work, but you must give appropriate credit, provide a link to the license, and indicate if changes were made.

Sample attribution: [Title of work] by Johns Hopkins Data Science Lab. [CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/)

![""Institutions""](https://drive.google.com/uc?id=10JHlTp3Zc4JioSohjy-X4i0aGI26Yg5S)  ",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","gdscn-exercises/SARS-CoV-2-Genome"
215,"broad-firecloud-tcga","TCGA_UCS_hg38_OpenAccess_GDCDR-12-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_UCS_hg38_OpenAccess_GDCDR-12-0_DATA",TRUE,TRUE,"Uterine Carcinosarcoma","Tumor/Normal","USA","TCGA Uterine Carcinosarcoma Open-Access Data Workspace

*hg38 TCGA and TARGET workspaces reference files by their GDC UUIDs.  In order to run analyses on the referenced data files, you will need to run workflows that retrieve the files from the GDC and copy them to your workspace bucket.  See   [this forum post](https://gatkforums.broadinstitute.org/firecloud/discussion/10382/populating-hg38-tcga-and-target-workspaces-with-data-files#latest) for instructions on the running of these workflows.*","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients' clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","57","Uterus","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh38/hg38","TCGA_UCS_hg38_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_UCS_hg38_OpenAccess_GDCDR-12-0_DATA"
216,"cgp-terra-outreach-workshop","Human-Pangenome-Giraffe-DeepVariant-AnVIL-ASHG-Jan22","READER","https://app.terra.bio/#workspaces/cgp-terra-outreach-workshop/Human-Pangenome-Giraffe-DeepVariant-AnVIL-ASHG-Jan22",TRUE,FALSE,NA,NA,NA,"This is a demonstration workspace for the ASHG workshop Reproducible Analysis of Human Pangenome Data using the AnVIL. The workspace demonstrates variant calling using the Human Pangenome Reference Consortium's (HPRC) year 1 pangenome with the Giraffe/DeepVariant pipeline for calling germline variants. This workspace is intended to be a demonstration of utilizing a pangenome from the HPRC in AnVIL and Terra. 


The demonstration is structured in two parts:

1. Call variants using a pangenome and Giraffe/DeepVariant (WDL workflow)
2. Inspect results of variant calling in a Python notebook (Jupyter Notebook)

Note that the WDL included in the workspace is still under development and has not been peer-reviewed. For up-to-date best practices for both Giraffe and DeepVariant please see the links included in the relevant sections below.

Detailed instructions for running using this workspace can be downloaded from [here](https://docs.google.com/document/d/1Jynxe-AfinrCyCSgtkmSl2hsRAO-ctG7vBbfo8IE7kM/edit?usp=sharing).
## Data: 
### HG003 Illumina Reads (35X)
The sample-based data we will use in this workspace is loaded into the sample data table. An Illumina dataset (produced by Google and made publicly available) with 35X coverage of HG003 has been imported into the workspace. Additionally, the Genome In A Bottle short variant truth calls and conficent regions have been imported into the workspace. The truth data has been modified by prepending GRCh38 to the contig names for compatibility with the workflow. The sample table has multiple columns and the columns with the prefix input_ (e.g. input_hifi) are used as inputs for the Giraffe/DeepVariant workflow. The workflow has been prerun for convenience, and the outputs have been written to columns with names prepended with ""output_"".

### Minigraph/CACTUS Pangenome
This workspace uses one of the HPRC's year 1 pangenomes created with the [Minigraph/CACTUS pipeline](https://github.com/ComparativeGenomicsToolkit/cactus/blob/master/doc/pangenome.md). For the Giraffe/DeepVariant pipeline, it is recommended to use a filtered version of the GRCh38-based graph. Information about the HPRC's pangenome releases can be found in the HPRC's [publicly available AnVIL workspace](https://app.terra.bio/#workspaces/anvil-datastorage/AnVIL_HPRC) as well as the HPRC's [pangenome resources GitHub repo](https://github.com/human-pangenomics/hpp_pangenome_resources).

### DeepVariant Model
DeepVariant uses a learned model to call variants in aligned sequencing data. A model has been imported into this workspace and can be found in the Data tab under ```Files --> dv-giraffe-model/.```. The model was trained with DV 1.1 on the GRCh38-based Cactus-Minigraph pangenome (the same graph used in this workspace), on ~30-40x coverage read sets.

## Workflow: Variant Calling With a Pangenome & Giraffe/DeepVariant


### [GiraffeDeepVariant](https://dockstore.org/workflows/github.com/vgteam/vg_wdl/GiraffeDeepVariant:giraffedv)

This workflow aligns reads to a pangnome structure and uses Google's DeepVariant caller to produce an output VCF. 

1. Split reads
2. Align reads to pangenome with Giraffe
3. Realign around InDels (optional)
4. Call variants with DeepVariant
5. Compare variants to truth set (optional)


#### Recommended Inputs

**This workflow has a number of inputs. For convenience the recommended workflow inputs have been pre-populated in the workflow inputs tab. Alternatively, users may navigate to the [example json](https://drive.google.com/file/d/1M7iJVHHa8_AJ3kZdABVKtuhuf5la7NKz/view?usp=sharing) included as part of this workspace.**


#### Outputs

* output_vcf: VCF output from DeepVariant
* output_calling_bams: Giraffe aligned bams, split by chromosome
* output_happy_evaluation_archive: comparison results of Giraffe/DeepVariant VCF against truth callset
* output_vcfeval_evaluation_archive: comparison results of Giraffe/DeepVariant VCF against truth callset

#### Time and cost estimates    
Note that actual time and cost may vary due to the use of preemptible instances. 

| Input Coverage | Time | Cost |
| -------- | -------- | ---------- |
| 35X | 10 hours | $15 |


## Notebook: Inspect Results From Giraffe/DeepVariant Run

After running the workflow, we need a way to inspect the results. One of the easiest, and most reproducible, ways to do this is with a Jupyter Notebook. In the example notebook provided users will:

1. Read In the Workspace's Sample Table 
2. Download & Inspect vcfeval Results
3. (Optional) Upload New Data Table

To run the notebook, you will have to create a cloud environment in the NOTEBOOKS section of this workspace. At the creation step, choose the default (GATK) environment. 

## Next Steps
To run this pipeline on your own data you have to upload your data to the Google Cloud bucket for your version of the workspace. You can find the bucket information in the Google Bucket section on the right hand side of this page. In that section you can find the bucket name for gsutil commands, or you can open the bucket in your web browser. You can also (optionally) create a new data table in your workspace which points to the data you uploaded.

----
----
## Authors and contact information

This workspace is a product of the Human Pangenome Reference Consortium [HPRC](https://humanpangenome.org/) and [the AnVIL](https://anvilproject.org/). Contributors include:

* Julian Lucas(juklucas@ucsc.edu) -- UCSC Computational Genomics Platform
* Beth Sheets(esheets@ucsc.edu) -- UCSC Computational Genomics Platform
* Trevor Pesout(tpesout@ucsc.edu ) -- UCSC Paten Lab
* Mobin Asri(masri@ucsc.edu) -- UCSC Paten Lab
* Karen Miga(khmiga@ucsc.edu) -- UCSC Miga Lab

The Giraffe/DeepVariant workflow was developed by the VG team at UCSC and Google. Contributors include:

* Charles Markello (UCSC)
* Jean Monlong (UCSC)
* Adam Novak (UCSC)
* Maria Nattestad (Google)
* Pichuan Chang (Google)
* Andrew Carroll (Google)

## Additional generally helpful resources

* **[HPRC Pangenome GitHub](https://github.com/human-pangenomics/hpp_pangenome_resources)**   
    Description of HRPC's currently available (release) pangenomes.   
		
*  **[Jupyter Notebooks 101](https://app.terra.bio/#workspaces/fc-product-demo/Terra-Notebooks-Quickstart)**    
    This crash course will help you get started using Jupyter notebooks. It integrates hands-on practice to solidify concepts.    
		
* **For helpful hints on controlling cloud costs**, see [this article (https://support.terra.bio/hc/en-
us/articles/360029748111)](https://support.terra.bio/hc/en-us/articles/360029748111).      
 

 ## Citations
 
1. Sirén, Jouni, et al. ""Pangenomics enables genotyping of known structural variants in 5202 diverse genomes."" Science 374.6574 (2021): abg8871.
2. Poplin, Ryan, et al. ""A universal SNP and small-indel variant caller using deep neural networks."" Nature biotechnology 36.10 (2018): 983-987.

### Change log  
```
* Jan 26, 2022 - Initial release
* Feb 22, 2022 - Update links in DASHBOARD
* Mar 04 2022 - Update DASHBOARD to include recommended cloud notebook environment
```",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","cgp-terra-outreach-workshop/Human-Pangenome-Giraffe-DeepVariant-AnVIL-ASHG-Jan22"
218,"broad-firecloud-tcga","TCGA_SKCM_hg38_OpenAccess_GDCDR-12-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_SKCM_hg38_OpenAccess_GDCDR-12-0_DATA",TRUE,TRUE,"Skin Cutaneous Melanoma","Tumor/Normal","USA","TCGA Skin Cutaneous Melanoma Open-Access Data Workspace

*hg38 TCGA and TARGET workspaces reference files by their GDC UUIDs.  In order to run analyses on the referenced data files, you will need to run workflows that retrieve the files from the GDC and copy them to your workspace bucket.  See   [this forum post](https://gatkforums.broadinstitute.org/firecloud/discussion/10382/populating-hg38-tcga-and-target-workspaces-with-data-files#latest) for instructions on the running of these workflows.*","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients' clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","470","Skin","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh38/hg38","TCGA_SKCM_hg38_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_SKCM_hg38_OpenAccess_GDCDR-12-0_DATA"
219,"brain-initiative-bcdc","SnapATAC_Pipeline","READER","https://app.terra.bio/#workspaces/brain-initiative-bcdc/SnapATAC_Pipeline",TRUE,FALSE,NA,NA,NA,"# SnapATAC: Single Cell ATAC-seq Analysis Pipeline

## This workspace is an older version of this pipeline. For the latest scATAC pipeline please go [here](https://app.terra.bio/#workspaces/brain-initiative-bcdc/scATAC).
The SnapATAC workflow reads FASTQ inputs tagged with cell identifiers from scATAC-seq experiments and generates SNAP files. This workspace describes the pipeline and provides a fully reproducible example of the workflow.

The following material is provided by the Broad Pipelines Team. Please send questions and feedback to Kylee Degatano at kdegatano@broadinstitute.org. We'd like to acknowledge and thank Rongxin Fang and Yang Li at Ludwig Cancer Research, Bing Ren at UC San Diego, and Sebastian Preissl at the Center for Epigenomics, UC San Diego for their work on this pipeline.

---

## Sample Data
### Test Data
Links to the expected input types are available in the workspace data model for testing. The snap-atac-v1_0 workflow accepts an R1_fastq.gz and an R2_fastq.gz. This workspace data model contains a demo sized R1 and R2 FASTQ file under the columns `fastq1` and `fastq2`, respectively.      

### Workspace Data
The reference genome for this workspace is mm10.  A pre-built human reference can be found at `gs://hca-dcp-sc-pipelines-test-data/alignmentReferences/snapATAC_BWA/hg38/hg38.tar`.

## Tools 
snap-atac-v1_0.wdl:  This WDL pipeline takes sequencing data in FASTQ format (R1 and R2) and outputs snap files.  The pipeline is composed of five steps:

| step name        | step description                                                                         |
|------------------|------------------------------------------------------------------------------------------|
| AlignPairEnd     | Align the FASTQ files to the genome                                                      |
| SnapPre          | Initial generation of snap file                                                          |
| SnapCellByBin    | Binning of data by genomic bins                                                          |
| MakeCompliantBAM | Generation of a GA4GH compliant BAM                                                      |
| BreakoutSnap     | Extraction of tables from snap file into text format (for testing and user availability) |

## Requirements/Expectations

**Inputs**
The snap-atac-v1_0 workflow accepts two FASTQ files and a reference file. The input data are samples:

* Pair-end sequencing data in FASTQ format - R1 and R2 fastq files.
* A reference bundle file

The pipeline accepts paired reads in the form of FASTQ files. The current version of the pipeline requires that cellular barcodes for reads be appended to the FASTQ read names (for both R1 and R2 FASTQ files). The following is an example of the format of the expected input. The full cell barcode must form the first part of the read name (for both R1 and R2 files) and be separated from the rest of the line by a colon.

```
@CAGTTGCACGTATAGAACAAGGATAGGATAAC:7001113:915:HJ535BCX2:1:1106:1139:1926 1:N:0:0
ACCCTCCGTGTGCCAGGAGATACCATGAATATGCCATAGAACCTGTCTCT
+
DDDDDIIIIIIIIIIIIIIHHIIIIIIIIIIIIIIIIIIIIIIIIIIIII
```

The following table outlines the inputs to the pipeline in more detail.

| input name       | input type    | description                                                                |
|------------------|---------------|----------------------------------------------------------------------------|
| input_fastq1     | File          | FASTQ file of the first reads (R1)                                         |
| input_fastq2     | File          | FASTQ file of the second reads (R2)                                        |
| genome_name      | String        | name of the genomic reference used                                         |
| input_reference  | File          | reference bundle that is generated with bwa-mk-index-wdl                   |
| output_bam       | String        | name of the output bam to generate                                         |


`input_reference` is an input reference bundle that has been generated with the bwa-mk-index-wdl accessory pipeline. This accessory pipeline can be found [here](https://github.com/HumanCellAtlas/skylab/tree/master/library/accessory_workflows/build_bwa_reference).

If your sequencing data is not in FASTQ format, check out this file conversion workspace, [https://app.terra.bio/#workspaces/help-gatk/Sequence-Format-Conversion](https://app.terra.bio/#workspaces/help-gatk/Sequence-Format-Conversion) for prepared workflows to assist in file conversion.

**Outputs**  

The pipeline outputs a .snap file that contains all the output information as well as snap_qc will Quality Control Metrics. Furthermore, it returns the above output in text format files for easier access. For more details regarding working with these files please refer to the SnapATAC pipeline [here (external link)](https://github.com/r3fang/SnapTools)  The format of the snap file, as well as what the different section contains, is described in more detail [here (external link)](https://github.com/r3fang/SnapTools) and [here](https://github.com/r3fang/SnapTools/blob/master/docs/snap_format.docx). Furthermore the pipeline returns a GA4GH compliant BAM file, where the cell identifiers are extracted from the read names and into the CB tags.

The pipeline outputs the following files:

| output file name              | description            |
|-------------------------------|------------------------|
| output_snap_qc                | Quality control file corresponding to the snap file |
| output_snap                   | Output snap file (in hdf5 container format) |
| output_aligned_bam            | Output BAM file, compliant with GA4GH |
| breakout_barcodes             | Text file containing the 'Fragments session' barcodeLen, barcodePos fields           |
| breakout_fragments            | Text file containing the 'Fragments session' fragChrom, fragLen and fragStart fields |
| breakout_binCoordinates       | Text file with the AM section ('Cell x bin accesibility' matrix), binChrom and binStart fields |
| breakout_binCounts            | Text file with the AM section ('Cell x bin accesibility' matrix), idx, idy and count fields |
| breakout_barcodesSection      | Text file with the data from the BD section ('Barcode session' table) |

The output bins for summarization of the data are of size 10kb in the snap file.
 
## Workflow
The source code for this workflow is found on [Github](https://github.com/HumanCellAtlas/skylab/releases/tag/snap-atac_v1.0.0). The pipeline is tagged with versioned releases, and this workspace will be updated periodically as the pipeline updates.

---

## Time/Cost Analysis

| Sample Name | R1.fastq Size | R2.fastq Size | Time | Cost $ |
| :---:  | :---: | :---: | :---: | :---: |
| demo1l | 42.33 MB | 44.92 MB |0:10:00 | 0.18 |   

**For helpful hints on controlling Cloud costs**, see this article ([click here to view](https://support.terra.bio/hc/en-us/articles/360029748111)).       

---

### Contact information  

This material is provided by the Broad Pipelines Team. Please send questions and feedback to Kylee Degatano at kdegatano@broadinstitute.org.

---

### License  

**Copyright Broad Institute, 2019 | BSD-3**  
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at https://github.com/openwdl/wdl/blob/master/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","brain-initiative-bcdc/SnapATAC_Pipeline"
221,"waldronlab-terra-rstudio","pathml_stain_normalization_template","PROJECT_OWNER","https://app.terra.bio/#workspaces/waldronlab-terra-rstudio/pathml_stain_normalization_template",TRUE,FALSE,NA,NA,NA,"",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","waldronlab-terra-rstudio/pathml_stain_normalization_template"
222,"fc-product-demo","Terra-Notebooks-Quickstart","READER","https://app.terra.bio/#workspaces/fc-product-demo/Terra-Notebooks-Quickstart",TRUE,TRUE,NA,NA,NA,"Learn how to run a notebook analysis in Terra, from setting up a Cloud Environment VM to learning Jupyter basics to opening and running your first analysis.   

### What's in the Quickstart?
This workspace includes an interactive tour that walks through setting up a Jupyter Cloud Environment in Terra. If you aren't familiar with Jupyter notebooks, you can learn the basics by running the Jupyter Notebooks 101 tutorial. Four optional notebooks demonstrate analyzing data in a notebook, depending on where your data is stored:  
1. A workspace data table    
2. Workspace storage (i.e. Google bucket)
3. The Terra Data Library 
4. Public data stored in BigQuery      
<br>  

### Quickstart flow   
The Quickstart has four steps (scroll down for more details). 
![Diagram of steps in Notebooks Quickstart](https://storage.googleapis.com/terra-featured-workspaces/QuickStart/Notebooks-Quickstart-tutorial_diagram.png)    

### How long will it take to run? How much will it cost?     
**Time:** Expect to spend 10 minutes on the interactive tour and 20 minutes for the Jupyter 101 tutorial. Each of the optional notebooks should take only a few minutes to run all cells.      

**Cost:** Using the default configuration, the Jupyter Cloud Environment cost is $0.06/hour for Google Cloud services. It should cost less than a quarter to complete all the notebooks in the Quickstart.            
<br>

## How do I get started? Step-by-step instructions
|  ![read more icon](https://storage.googleapis.com/terra-featured-workspaces/QuickStart/icon_read_more%401x.png) | [**Open step-by-step instructions**](https://support.terra.bio/hc/en-us/articles/360059009571) |     
|---| ---------|       

### Step 1: Create (i.e. clone) your own editable workspace    
In order to run the notebooks in the Quickstart, you need to have edit and execute permissions in the workspace. Cloning the workspace makes an exact copy with you as owner. You will be able to run the notebooks and all costs will be charged to the Terra Billing project you assign when creating the workspace.

**1.** Click on the round circle with three dots in the upper right corner of this page. 
    ![Clone_ScreenShot](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/Clone_workspace_Screen%20Shot.png)
		
**2.** Select ""Clone"" from the dropdown menu.     

**3.** Rename your new workspace something memorable.     

**Note:**  It may help to write down or memorize the name of your workspace.          

**4.**  Choose your billing project from the dropdown menu.          

**5.** Do not select an authorization domain as this workspace uses open-access data.      

**6.** Use the default (US central1) workspace storage location. 

**7.** Click the ""Clone Workspace"" button to make your own copy.    
<br>

### Work from your copy of the workspace     

You need to be able to set up and run a Jupyter Cloud Environment VM in order to do the interactive tour and hands on exercises. The featured workspace is ""read only"", so you will need to make your own copy with your own Terra Billing project. 

### Step 2: Take the interactive Cloud Environment setup tour

The Quickstart workspace has an interactive tour to walk you through the process of setting up your first Cloud Environment VM. When you go to the Notebooks tab, the interactive setup tour will start automatically. Follow the instructions in green. You can choose to dismiss the tour, but it will only appear once in each copy of the workspace. If you haven't run a Cloud Environment (interactive) application before, this is a great, short introduction. 

### Step 3: Explore the Jupyter Notebooks 101 tutorial

At the end of the interactive tour, if you're brand new to Jupyter Notebooks, start with the Jupyter_Notebooks_101 primer! You can open the Jupyter 101 notebook in either ""Edit"" or ""Playground"" mode to learn more about: 

* Why notebooks are used in biomedical research.    
* The relationship between the notebook and the workspace.    
* Jupyter Notebook basics: how to use a notebook, install packages, and import modules.    
* Common libraries in data analysis and popular tutorial notebooks.       
<br>
Read through the documentation cells and follow the instructions.  
<br>

### Step 4 (optional): Run a demo analysis
Each of the four optional notebooks is a self-guided deeper dive into a different use-case depending on where the data you want to analyze is stored. 

**Option 1: Data in a Terra data table**   
In this notebook, you'll bring phenotypic data from the workspace data table to the notebook virtual machine (VM)M. Plotting functions (included) demonstrate the kind of analysis you might then do on this data type. 

**Option 2: Data from workspace storage**    
In this notebook, you'll bring in the same phenotypic data - this time stored in a CSV file from the workspace bucket - to the notebook VM. Plotting functions (included) demonstrate the kind of analysis you might then do on this data type. 


**Option 3: Data from the Terra Data Library**    
Find the 1,000 Genomes data explorer in the Terra Data Library and use it to create a custom cohort. You'll import the cohort to your workspace and bring it into the notebook VM for analysis. Find step-by-step instructions in Terra Support.       

|![PDF icon](https://storage.googleapis.com/terra-featured-workspaces/QuickStart/icon_tip%401x.png) | Read the [step-by-step instructions](https://support.terra.bio/hc/en-us/articles/360059009571-Notebooks-QuickStart-Guide) |  
| --------| ------------------|    

**Option 4: Public data in BigQuery**   
Learn two different ways to access public data stored in BigQuery, Google's structured cloud storage.      
<br>
<br>


## Additional resources  

### How can I learn more about Terra?

To learn more about Terra and its functionality, see [Terra Support](https://support.terra.bio/hc/en-us):       
* [Getting started on Terra](https://support.terra.bio/hc/en-us/categories/360005881492)      
* [Doing research on Terra](https://support.terra.bio/hc/en-us/categories/360001399872)    
* [Pipelining with workflows](https://support.terra.bio/hc/en-us/sections/360004147011)      
* To learn more about cloning workspaces, see [this article](https://support.terra.bio/hc/en-us/articles/360026130851)     
* For more information about how to manage data with a data table, see [this article](https://support.terra.bio/hc/en-us/articles/360025758392)      
* To learn more about troubleshooting and monitoring your workflow submission on Terra, see [this article](https://support.terra.bio/hc/en-us/articles/360027920592)          
* [Understanding Cloud costs and tips and tricks for reducing costs](https://support.terra.bio/hc/en-us/articles/360029748111)          
* [Sample use cases Cloud costs](https://support.terra.bio/hc/en-us/articles/360029772212)         
<br>
<br>

### How can I learn more about how to do reproducible research in a notebook?      
* For an example of doing the first, exploratory part of a GWAS in a notebook, see this [Featured Workspace](https://app.terra.bio/#workspaces/fc-product-demo/2019_ASHG_Reproducible_GWAS).
* For a real-world example of using a Jupyter Notebook in Terra to reproduce a cluster analysis from a publication, see [this Featured Workspace](https://app.terra.bio/#workspaces/help-gatk/Reproducibility_Case_Study_Tetralogy_of_Fallot).       
* To learn more about Best Practices when creating a notebook for reproducible analysis, see [this article](https://towardsdatascience.com/jupyter-notebook-best-practices-f430a6ba8c69)     
<br>
<br>

### How can I learn more about  using BigQuery for research?
* Additional instructions on using BigQuery in a notebook: [Google Cloud Platform intro](https://cloud.google.com/blog/products/gcp/google-cloud-platform-for-data-scientists-using-jupyter-notebooks-with-apache-spark-on-google-cloud)     
* Intro to controlling BigQuery costs: [https://cloud.google.com/bigquery/docs/controlling-costs](https://cloud.google.com/bigquery/docs/controlling-costs)
* Google blog on controlling BigQuery costs: [https://cloud.google.com/blog/products/data-analytics/cost-optimization-best-practices-for-bigquery](https://cloud.google.com/blog/products/data-analytics/cost-optimization-best-practices-for-bigquery)    
* BigQuery Bioinformatics SQL Query Lessons: [https://github.com/lynnlangit/gcp-for-bioinformatics/blob/master/1_Files_%26_Data/6a_SQLQuestions.md](https://github.com/lynnlangit/gcp-for-bioinformatics/blob/master/1_Files_%26_Data/6a_SQLQuestions.md) 
* Data Management in Bioinformatics: SQL Exercises [https://en.wikibooks.org/wiki/Data_Management_in_Bioinformatics/SQL_Exercises](https://en.wikibooks.org/wiki/Data_Management_in_Bioinformatics/SQL_Exercises)      
* BigQuery best practices: [https://cloud.google.com/bigquery/docs/best-practices-costs](https://cloud.google.com/bigquery/docs/best-practices-costs)
* BigQuery cost calculator: [https://cloud.google.com/products/calculator/](https://cloud.google.com/products/calculator/)
* BigQuery genomics: [https://cloud.google.com/genomics/](https://cloud.google.com/genomics/) 
* Analyzing variants: [https://cloud.google.com/genomics/docs/how-tos/analyze-variants](https://cloud.google.com/genomics/docs/how-tos/analyze-variants)
* On GitHub: [https://github.com/googlegenomics/](https://github.com/googlegenomics/)
<br>
<br>

## Contact information  
This material is provided by the Terra Team. Please post any questions or concerns to our forum site: [Terra](https://support.terra.bio/hc/en-us/community/topics/360001603491-Featured-Workspaces) 
<br>
<br>

## Workspace Citation 
Details on citing Terra workspaces can be found here: [How to cite Terra](https://support.terra.bio/hc/en-us/articles/360035343652)

Data Sciences Platform, Broad Institute (*Year, Month Day that this workspace was last modified*) fc-product-demo/Terra-Notebooks-Quickstart [workspace] Retrieved *Month Day, Year that workspace was retrieved*,  https://app.terra.bio/#workspaces/fc-product-demo/Terra-Notebooks-Quickstart
<br>
<br>

## License  
**Copyright Broad Institute, 2020 | BSD-3**  
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at https://github.com/openwdl/wdl/blob/master/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","fc-product-demo/Terra-Notebooks-Quickstart"
223,"help-gatk","Germline_variant_discovery_b37_v2","READER","https://app.terra.bio/#workspaces/help-gatk/Germline_variant_discovery_b37_v2",TRUE,TRUE,NA,NA,NA,"### GATK Best Practices for germline variant discovery (b37 reference)
The purpose of this workspace is to provide a fully reproducible example of the GATK Best Practices workflows for germline variant discovery. 

#### Workspace attributes
All required and optional resources for the preconfigured methods included. The reference genome is b37 (aka GRCh37), the Broad Institute's version of hg19. Some resource files may be named hg19 for historical reasons.

#### Data 
A set of three BAMs of NA12878 WGS data downsampled to 20% (med), 5% (small) and 20k reads, respectively. 

#### Method configs
This workspace contains the following preset method configurations:

- **HaplotypeCallerGvcf_GATK3_MC**
Runs HaplotypeCaller in GVCF mode on a single sample according to ## the GATK Best Practices (June 2016) scattered across intervals. Usage instructions: launch this on any qualifying Sample or Set of Samples (each sample must reference an analysis-ready BAM). See workflow for additional input requirements and version notes.

### Version notes

v1 -> v2 updates the reference files to use https://console.cloud.google.com/storage/browser/broad-references/hg19/v0/ as source, for the purpose of consolidating resource versions across multiple workspaces. Input data were regenerated and updated accordingly. The previous version of the workspace used a slightly different version of the b37 genome reference (the one with decoys). ",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/Germline_variant_discovery_b37_v2"
224,"broad-firecloud-tcga","TCGA_SARC_hg38_OpenAccess_GDCDR-12-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_SARC_hg38_OpenAccess_GDCDR-12-0_DATA",TRUE,TRUE,"Sarcoma","Tumor/Normal","USA","TCGA Sarcoma Open-Access Data Workspace

*hg38 TCGA and TARGET workspaces reference files by their GDC UUIDs.  In order to run analyses on the referenced data files, you will need to run workflows that retrieve the files from the GDC and copy them to your workspace bucket.  See   [this forum post](https://gatkforums.broadinstitute.org/firecloud/discussion/10382/populating-hg38-tcga-and-target-workspaces-with-data-files#latest) for instructions on the running of these workflows.*","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients' clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","261","Soft tissue","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh38/hg38","TCGA_SARC_hg38_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_SARC_hg38_OpenAccess_GDCDR-12-0_DATA"
225,"wsi-pam-s10005-vectorhap","MalariaGEN_vector_phasing_ag3","READER","https://app.terra.bio/#workspaces/wsi-pam-s10005-vectorhap/MalariaGEN_vector_phasing_ag3",TRUE,FALSE,NA,NA,NA,"### This workspace is currently under construction

Please refer to the information on the official [Ag1000G phase 3 SNP data release](https://www.malariagen.net/data/ag1000g-phase3-snp) before downloading any of the data linked in this workspace",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","wsi-pam-s10005-vectorhap/MalariaGEN_vector_phasing_ag3"
226,"broad-firecloud-tcga","TCGA_PCPG_OpenAccess_V1-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_PCPG_OpenAccess_V1-0_DATA",TRUE,TRUE,"Pheochromocytoma and Paraganglioma","Tumor/Normal","USA","TCGA Pheochromocytoma and Paraganglioma Open-Access Data Workspace","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients’ clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","179","Adrenal gland","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh37/hg19","TCGA_PCPG_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_PCPG_OpenAccess_V1-0_DATA"
228,"anvil-dash-research","AnVIL_Ape_T2T_chrXY","READER","https://app.terra.bio/#workspaces/anvil-dash-research/AnVIL_Ape_T2T_chrXY",TRUE,FALSE,NA,NA,NA,"# T2T-GreatApes

The T2T-GreatApes is one of the Telomere-to-Telomere (T2T) consortium projects that aims at advancing our understanding of great ape genomics. Utilizing advanced sequencing technologies such as PacBio HiFi and Oxford Nanopore ultra-long reads, the first version of T2T chrXY assemblies for great ape species have been released. 

## Genomics Analysis
Here, we evaluate the impact of T2T-chrXY assemblies on read alignments and variant calling across 129 individuals from 11 great ape subspecies. We applied a consistent analytical pipeline for previous and T2T reference genomes, ensuring that any observed differences are attributed to the reference improvements rather than technology discrepancies. To optimize variant detection accuracy and completeness, we adopt the masking strategy proposed by the T2T-CHM13v2.0 human chrY study in which the pseudoautosomal regions (PARs) and/or Y chromosome are masked in a sex-specific manner (Rhie et al., 2023). After generating karyotype-specific references for XX and XY samples, we realign the reads of each sample to the updated references and performed the variant calling. 

## Available Results and Resources
We provide three types of reference genomes: the previous reference, the T2T reference, and the masked-T2T reference and CRAM files generated using these reference. For variant calling, we offer detailed results at both subspecies and species levels. Below is a list of subspecies with their respective abbreviations:
* Western Lowland Gorilla (GGG)
* Mountain Gorilla (GBB)
* Eastern Lowland Gorilla (GBG)
* Cross River Gorilla (GGD)
* Bonobo (PP)
* Eastern Chimpanzee (PTS)
* Central Chimpanzee (PTT)
* Western Chimpanzee (PTV)
* Nigeria Chimpanzee (PTE)
* Bornean Orangutan (PPY)
* Sumatran Orangutan (PAB)

Additionally, for species-level variant calling, we have included data for both variant and invariant sites.

## Paper

[The Complete Sequence and Comparative Analysis of Ape Sex Chromosomes](https://www.biorxiv.org/content/10.1101/2023.11.30.569198v2)<br>
Mokova et al. (2023) bioRxiv doi.org/10.1101/2023.11.30.569198",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","anvil-dash-research/AnVIL_Ape_T2T_chrXY"
229,"waldronlab-terra","Tumor_Only_CNV","PROJECT_OWNER","https://app.terra.bio/#workspaces/waldronlab-terra/Tumor_Only_CNV",TRUE,FALSE,NA,NA,NA,"## Reliable analysis of tumor CNV/SNV without matching normal 

This workspace provides a fully reproducible example of copy number variation (CNV) and single nucleotide variants (SNV) analysis of tumor samples without matching normal profile, described in the recent publication [[link](https://ascopubs.org/doi/10.1200/CCI.19.00130)]:    

**Reliable analysis of clinical tumor-only whole exome sequencing data**       
Oh *et al.*, JCO Clin Cancer Inform. 2020 Apr;4:321-335. doi: 10.1200/CCI.19.00130.   

Synthetic dataset from [BioIT-Hackathon-2019-Synthetic-Data-Team](https://app.terra.bio/#workspaces/bioit-hackathon/BioIT-Hackathon-2019-Synthetic-Data-Team) workspace is loaded in this workspace. Data model for current workspace is based on synthetic dataset, so please **modify input/output attributes** based on the data model of your own datasets.
 
---
 
## Overview 
Allele-specific copy number alteration (CNA) analysis is essential to study the functional impact of single nucleotide variants (SNV) and the process of tumorigenesis. Most commonly used tools in the field rely on high quality genome-wide data with matched normal profiles, limiting their applicability in clinical settings. 

This workflow, based on the open-source [PureCN](http://www.bioconductor.org/packages/release/bioc/html/PureCN.html) Bioconductor package in conjunction with widely used variant-calling and copy number segmentation algorithms, is optimized for allele-specific CNA analysis from whole exome sequencing (WES) without matched normals. This workflow can further classify SNVs by somatic status and then uses this information to infer somatic mutational signatures and tumor mutational burden (TMB).   


![Workflow Diagram](https://raw.githubusercontent.com/shbrief/CNVworkflow_Terra_WDL/master/Terra_workflow_diagram.png)

---

## For production
If you are collecting tumor samples using the same protocol (with the same capture kit and sequencing read length), you need to process BED and process-matched normal samples only once to build a normal database. Here, the 'normal database' includes `normalDB`, `normal_panel`, `intervals`, `intervalweightfile`, `snpblacklist` files. At the end, you need to run three workflows for each new tumor samples: *1_MuTect1_Variants_Calling*, *2_PureCN_Coverage*, and *4_PureCN_PureCN* workflows. 

---

## Workflows    
This workspace contains the below workflows. Prefix indicates the order of workflows being run. Three workflows with prefix *'1_'* can be run at the same time because they don't require any input from other workflows.

### 1_PureCN_IntervalFile
- BED file needs to be properly formatted before processed by this workflow. Check the notebook *'3_Format_BED'*.
- This workflow annotates the regions defined by the baits BED file with mean GC-content, mean mappability, and gene symbols.
- Input data example:
| Variable | Attribute | Data Model | Description |
|----|----|----|----|
| inputBED | workspace.BED | Select 'Process single workflow from files' |  formated through *'3_Format_BED'* notebook. |

### 1_MuTect1_Variants_Calling
- requires **only tumor** (you can use the corresponding matching normal, if it's available)
- Input data example:
| Variable | Attribute | Data Model | 
|----|----|----|
| tumor_bam | this.mutSynthExomeBam | Select '8_participants_for_case' set from 'Participant' |
| tumor_bai  | this.mutSynthExomeBamIndex | Select '8_participants_for_case' set from 'Participant' | 
- '8_participants_for_case' set is manually picked in the data model ('Choose specific rows to process') and does NOT behave as a 'set' entity, which we need for this workflow.

### 1_MuTect1_PON
- applied to **only normal** BAM files --> Select `neutral` from 'Participant Set'
- VCF were combined into a single multi-sample VCF with GATK `CombineVariants`.
- Input data example:
| Variable | Attribute | Data Model | 
|----|----|----|
| normal_bams | this.participants.synthExomeBam | Select 'neutral' from 'Participant Set' |
| normal_bais  | this.participants.synthExomeBamIndex | Select 'neutral' from 'Participant Set' | 

### 2_PureCN_Coverage 
- **both tumor and normal** samples are processed through this workflow
- This workflow normalizes on- and off-target coverages independently for GC-content. 
- Input data example:
| Variable | Attribute | Data Model | 
|----|----|----|
| bam | this.synthExomeBam | Process multiple workflows from 'Participant' (all 100 participants) |
| bai  | this.synthExomeBamIndex | Process multiple workflows from 'Participant' (all 100 participants) | 

### 3_PureCN_NormalDB 
- applied to **only normal**
- Normal samples are run in artifact detection mode (`--artifact_detection_mode`) and then combined into a single multi-sample VCF (`normal.panel.vcf.file`) using GATK `CombineVariants` with argument `--minimumN 5`, which specifies the minumum number of normal VCF files containing the variant call to be included in the normal database.
- Input data example:
| Variable | Attribute | Data Model | 
|----|----|----|
| loess | this.participants.loess | Select 'neutral' from 'Participant Set' |

### 4_PureCN_PureCN
-  In the current workspace, [*'SynthData'*](https://github.com/shbrief/4_purecn_purecn/tree/dbbdd1b6ec81b83e6f37c7497783dc71f2da5e1c) version of the workflow is recommended, because synthetic data doesn't have proper gene annotation, so the outputs from `PureCN.R` other than `.rds` can't be created. So this version of the workflow saves only the `.rds` file. For the patient data where gene annotation is available, use the [*'PatientData'*](https://github.com/shbrief/4_purecn_purecn/tree/dbbdd1b6ec81b83e6f37c7497783dc71f2da5e1c) version of the workflow. You can choose different version of this workflow in the workflow's version tab.
-  This workflow contains the main copy number calling step that includes tumor purity and ploidy inference as well as classification of somatic status and clonality for all variants.
-  Variants in the UCSC simple repeat track were excluded (`--snpblacklist`). Proper formatting of snpblacklist file can be done through a notebook, *'4_Format_SNP_Blacklist'*.
- Input data example:
| Variable | Attribute | Data Model | 
|----|----|----|
| tumor_loess | this.loess | Select 'case' set from 'Participant' |
| var_calls | this.output_mutect_vcf | Select 'case' set from 'Participant' | 
| var_calls_idx | this.output_mutect_vcf_index | Select 'case' set from 'Participant' | 
| var_calls_stats | this.output_mutect_stats_txt | Select 'case' set from 'Participant' | 

- 'case' set does NOT behave as a 'set' entity as far as we select it under 'Participant' root entity type, which we need for this workflow.

### 5_PureCN_Dx
-  In the current workspace, [*'SynthData'*](https://github.com/shbrief/5_purecn_dx/tree/79d7a9c33caca954eb50e11ac7d9777d8a768ea7) version of the workflow is recommended, because there is not enough somatic calls to deconstruct signatures for synthetic data, so this version of the workflow doesn't save `_signatures.csv`output. For the patient data, use the [*'PatientData'*](https://github.com/shbrief/5_purecn_dx/tree/8879c53f85555a44ab26b55fa653a3e88a8fa2e1) version of the workflow. You can choose different version of this workflow in the workflow's version tab.
- This workflow extracts copy number and mutation metrics. (e.g. *Tumor Mutation Burden (TMB)* and *COSMIC Mutational Signature (mutSig)*)
- GATK `CallableLoci` was used to collect callable regions with sufficient coverage (with a minimum read depth of 30), mappability, and sequence quality.
- Input data example:
| Variable | Attribute | Data Model | 
|----|----|----|
| tumor_bam | this.mutSynthExomeBam | Select 'case' set from 'Participant' |
| tumor_bai | this.mutSynthExomeBamIndex | Select 'case' set from 'Participant' | 
| resRDS | this.res | Select 'case' set from 'Participant' | 

- 'case' set does NOT behave as a 'set' entity as far as we select it under 'Participant' root entity type, which we need for this workflow.

---

## Notebooks
#### 1_Annotate_Manifest
In this notebook, you can build a manifest file of [GDC data](https://portal.gdc.cancer.gov/), using [GenomicDataCommons](http://www.bioconductor.org/packages/release/bioc/html/GenomicDataCommons.html) Bioconductor package. Manifest file is further annotated with metadata using [TCGAutils](http://www.bioconductor.org/packages/release/bioc/html/TCGAutils.html) Bioconductor package.

#### 2_Build_Data_Table
In this notebook, you start from a sample entity data table and build a new, custom 'sample set' entity data table. 

#### 3_Format_BED
This notebook contains the pre-processing step of BED file, which is a required input for *'1_PureCN_IntervalFile'* workflow.

#### 4_Download_SNP_Blacklist
This notebook contains the pre-processing step of `snpblacklist` file before you use it for  *'4_PureCN_PureCN'* workflow.

#### 5_Downstream_Analysis
This notebook demonstrates how to access/extract below information from PureCN outputs.

- Purity and ploidy
- Loss of heterozygosity (LOH)
- Tumor mutation burden (TMB)
- Mutational signature (mutSig)

---

## Data
### 1. Reference data
You should use the same reference build as the one that BAM files are aligned to. 

### 2. BAM files
Tumor bam files and their indexed files (`.bai`) are required for the analysis. To build a normal database, you don't need a matching normal (normal samples from the corresponding tumor samples), but you need at least 'process-matched normal'. Using normal samples collected from different capture kit is NOT recommended. 

### 3. BED file
BED file has information on the capture regions. For a proper formating, process your BED file through *'3_Format_BED'* notebook. 

### 4. COSMIC
COSMIC data used in *'1_MuTect1_Variants_Calling'* and *'1_MuTect1_PON'* are free for academic use but **requires license for commercial use**, priced according to organization size. You can download `VCF/CosmicCodingMuts.vcf.gz` from [COSMIC website](https://cancer.sanger.ac.uk/cosmic/download).

### 5. Intermediate files
#### Mappability   
You can find the *kmer* information by opening aligned file (`.bam`) in the visualization tool such as [IGV](http://software.broadinstitute.org/software/igv/). Zoom-in the locus with exomes and hover your mouse on the grey read-fragment box, which will display the *Read length* information.

- For hg19/b37 built, you can download `.bigWig` mappability files with different kmers [here](http://hgdownload.soe.ucsc.edu/goldenPath/hg19/encodeDCC/wgEncodeMapability/).   
- For GRCh38 built, you can download `.bigWig` mappability files for [76-mer](https://s3.amazonaws.com/purecn/GCA_000001405.15_GRCh38_no_alt_analysis_set_76.bw) and [100-mer](https://s3.amazonaws.com/purecn/GCA_000001405.15_GRCh38_no_alt_analysis_set_100.bw) from each link.   

This workspace use the synthetic dataset from [BioIT-Hackathon-2019-Synthetic-Data-Team](https://app.terra.bio/#workspaces/bioit-hackathon/BioIT-Hackathon-2019-Synthetic-Data-Team) workspace, where the read-length is 100bp and the reference built is hg19. So we are using the `wgEncodeCrgMapabilityAlign100mer.bigWig` downloaded from [here](http://hgdownload.soe.ucsc.edu/goldenPath/hg19/encodeDCC/wgEncodeMapability/).   

--- 
## Runtime
### Docker containers
#### [PureCN](https://quay.io/repository/shbrief/pcn_docker)
- `docker pull quay.io/shbrief/pcn_docker`
- Docker image for PureCN R/Bioconductor Package (PureCN_1.12.2)
- Built on `bioconductor/release_base2` image (R version 3.5.3)

#### [MuTect1](https://hub.docker.com/r/jvivian/mutect)
- `docker pull jvivian/mutect`
- Broad Institute's MuTect version 1.1.7

#### [GATK3](https://hub.docker.com/r/broadinstitute/gatk3/tags)
- `docker pull broadinstitute/gatk3:3.8-1`
 
#### [htslib](https://hub.docker.com/r/miguelpmachado/htslib)
- `docker pull miguelpmachado/htslib:1.9`
- `bgzip` and `tabix` for PoN

---
 
## Notes 
1. Workflows in this workspace is hard-coded with the identical parameters used in the publication. We will add a link if we launch a new workspace with optional parameters added.    
2. We recommend to use [AnVIL package](http://www.bioconductor.org/packages/release/bioc/html/AnVIL.html) for R-based analysis in Notebook.

---

### Contact information    
Sehyun Oh (Sehyun.Oh@sph.cuny.edu or shbrief@gmail.com) 

### License    
License information and links
 ",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","waldronlab-terra/Tumor_Only_CNV"
231,"landmarkanvil2","curatedAnVILDataProto6","OWNER","https://app.terra.bio/#workspaces/landmarkanvil2/curatedAnVILDataProto6",TRUE,FALSE,NA,NA,NA,"# Use MultiAssayExperiment to interface to AnVIL data resources

NOTE: curatedAnVILData github repo in which this package is defined is PRIVATE.  Please contact maintainer if access is desired.

Author:

- Vince Carey [aut, cre] (<https://orcid.org/0000-0003-4046-0063>)

Maintainer:

- Vince Carey <stvjc@channing.harvard.edu>

Source package: curatedAnVILData

Licence: Artistic-2.0

Version: 0.0.13

## Description

Use MultiAssayExperiment to interface to AnVIL data resources.  Work from
PFB extracted to Terra data model.  Use virtual SummarizedExperiments, etc., to
interrogate and analyze data that are materialized on need.

## Notebook setup

- Use the '00-curatedAnVILDataProto3' notebook to configure your runtime for
  this workspace.

## Vignettes


curatedAnVILData -- early sketch (file: [survgtex.ipynb](https://anvil.terra.bio/#workspaces/use-strides/curatedAnVILDataProto3/notebooks/launch/survgtex.ipynb))

- Vincent J. Carey, stvjc at channing.harvard.edu



## Citation

Carey V (????). _curatedAnVILData: Use MultiAssayExperiment to
interface to AnVIL data resources_. R package version 0.0.13.

R version 4.2.0, Bioconductor version 3.15

Workspace dashboard created: 2022-03-01 10:18:01
 # curatedAnVILData
prototype for AnVIL-Bioc symbiosis

## Overview

This workspace illustrates structuring and filtering of Gen3 PFB:Terra exports with AnVIL functions.

This started out as a jupyter-oriented workspace, and a jupyter
notebook is available in the workspace.  But in August 2021 the jupyter notebook
was manually migrated to R markdown, and then to an R package, managed in github.

We started out with a selection of records using the explorer at
gen3.theanvil.io, making selections from the ""CF-GTEx"" project, and
'exporting to Terra'.

Here's an example of some of the R programming underlying resources
in this workspace:.

```
seqtab = avtable(table=""sequencing"")
table(seqtab$`pfb:data_type`, seqtab$`pfb:data_format`)
##                             
##                                bai   bam bigWig  crai  cram fastq  junc   svs
##   Aligned Reads              15512 15783  13803   667   667     0     0     0
##   Allele Specific Expression     0     0      0     0     0     0     0     0
##   Histology                      0     0      0     0     0     0     0 18781
##   Junction Files                 0     0      0     0     0     0 12132     0
##   Unaligned Reads                0     0      0     0     0   351     0     0
##                             
##                                tab   tsv   txt
##   Aligned Reads                  0     0     0
##   Allele Specific Expression     0  1236  1236
##   Histology                      0     0     0
##   Junction Files                 0     0     0
##   Unaligned Reads            14781     0     0
```

Vignettes will demonstrate, emulation of curatedTCGAData
with respect to convenience of identifying and filtering key data components of AnVIL.

## Resources for R programming

### A MultiAssayExperiment

#### Concept

The MultiAssayExperiment class unites diverse 'assays' collected on
a collection of samples.

<img alt=""wordcloud"" title=""pathology notes wordcloud"" src=""https://storage.googleapis.com/bioc-anvil-images/MAEschema.png"" width=900>

That's from the [MultiAssayExperiment vignette](https://bioconductor.org/packages/release/bioc/vignettes/MultiAssayExperiment/inst/doc/MultiAssayExperiment.html).  See also
the [JCO CCI paper](https://ascopubs.org/doi/full/10.1200/CCI.19.00119).

#### An example

After `curatedAnVILData` has been installed, acquire metadata
about a selection of GTEx samples via
```
suppressPackageStartupMessages(
  library(curatedAnVILData)
	)
data(GTExMAE)
GTExMAE
```

The `GTExMAE` has 30 ""experiments"" corresponding to types
of tissue that were subjected to RNA sequencing.

```
A MultiAssayExperiment object of 30 listed
 experiments with user-defined names and respective classes.
 Containing an ExperimentList class object of length 30:
 [1] Skin: SummarizedExperiment with 0 rows and 7206 columns
 [2] Lung: SummarizedExperiment with 0 rows and 3055 columns
 [3] Thyroid: SummarizedExperiment with 0 rows and 2743 columns
 [4] Adipose Tissue: SummarizedExperiment with 0 rows and 4853 columns
 [5] Blood Vessel: SummarizedExperiment with 0 rows and 5770 columns
 [6] Esophagus: SummarizedExperiment with 0 rows and 6226 columns
 [7] Blood: SummarizedExperiment with 0 rows and 6787 columns
 [8] Heart: SummarizedExperiment with 0 rows and 3623 columns
 [9] Nerve: SummarizedExperiment with 0 rows and 2532 columns
 [10] Colon: SummarizedExperiment with 0 rows and 3420 columns
 [11] Brain: SummarizedExperiment with 0 rows and 9807 columns
 ...
 ```
 
 #### Interrogating an MAE
 
 The variety of sequencing tasks for lung:
 
 ```
 GTExMAE[,,""Lung""][[1]] |> colData() |> as.data.frame() |> group_by(sequencing_assay) |> summarise(n=n())
 ```
 
 The result is chatty:
 ```
 harmonizing input:
  removing 72685 sampleMap rows not in names(experiments)
  removing 72685 colData rownames not in sampleMap 'primary'
 A tibble: 4 × 2
  sequencing_assay     n
  <chr>            <int>
1 RNA-Seq           2737
2 WES                 24
3 WGS                 20
4 NA                 274
Warning message:
'experiments' dropped; see 'metadata' 
 ```
 
 #### Filtering an MAE

We can create a selection of bigWig files measuring
mRNA abundance in lung via:

```
lungbw = GTExMAE[,,""Lung""][[1]] |> colData() |> filter(data_category==""Transcriptome Profiling"" & data_format == ""bigWig"") 
```


Some interesting sample-level metadata is available in free text:
```
head(lungbw |> select(pathology_notes_prc) |> unlist() |> unname(),10)
```
```
 [1] ""2 pieces, mild congestion, partially sloughing bronchial mucosa delineated""                                                                                
 [2] ""2 pieces  ~1x7mm.  Abundant vessels, bronchial/bronchiolar epithelial elements present, encircled.  Minute focus of bronchial cartilage in one, delineated""
 [3] ""2 pieces; emphysema; mild fibrosis; congestion; bronchus with cartilage in 1 piece""                                                                        
 [4] ""2 pieces; patchy emphysema; pleura sampled [annotated]""                                                                                                    
 [5] ""2 pieces, moderate congestion/alveolar edema""                                                                                                              
 [6] ""2 pieces; mild emphysema""                                                                                                                                  
 [7] ""2 pieces, 9x7 & 8x8mm; mild congestion; bronchioles well visualized, rep ones delineated, fairly well preserved""                                           
 [8] ""2 pieces, marked congestion. Hyaline cartilage foci present, rep delineated""                                                                               
 [9] ""2 pieces; atelectasis, focal smooth muscle hypertrophy and increased mucosal goblet cells""                                                                 
[10] ""2 pieces; bronchus and large blood vessel comprise 50% of 1 piece"" 
```

With the wordcloud2 R package and some hints from a [blog post](https://towardsdatascience.com/create-a-word-cloud-with-r-bde3e7422e8a),
we can summarize the content of these pathology notes:

<img alt=""wordcloud"" title=""pathology notes wordcloud"" src=""https://storage.googleapis.com/bioc-anvil-images/lungWCloud.png"" width=500>



### Working with DRS URI

```
> lungbw$ga4gh_drs_uri[1]
[1] ""drs://dg.ANV0:dg.ANV0/c19b5318-4b0b-4585-b833-cdf927f0ddfc""
> AnVIL::drs_stat(.Last.value)
# A tibble: 1 × 10
  fileName         size contentType  gsUri          timeCreated  timeUpdated  bucket   name         googleServiceAc… hashes
  <chr>           <int> <chr>        <chr>          <chr>        <chr>        <chr>    <chr>        <list>           <list>
1 GTEX-13X6K-1…  1.76e8 application… gs://fc-secur… 2020-07-08T… 2020-07-08T… fc-secu… GTEx_Analys… <named list [1]> <name…
> .Last.value |> as.data.frame()
                                                              fileName      size      contentType
1 GTEX-13X6K-1626-SM-7EWCX.Aligned.sortedByCoord.out.patched.md.bigWig 175687253 application/json
                                                                                                                                                                     gsUri
1 gs://fc-secure-ff8156a3-ddf3-42e4-9211-0fd89da62108/GTEx_Analysis_2017-06-05_v8_RNAseq_bigWig_files/GTEX-13X6K-1626-SM-7EWCX.Aligned.sortedByCoord.out.patched.md.bigWig
               timeCreated              timeUpdated                                         bucket
1 2020-07-08T18:46:07.637Z 2020-07-08T18:46:07.637Z fc-secure-ff8156a3-ddf3-42e4-9211-0fd89da62108
                                                                                                                  name
1 GTEx_Analysis_2017-06-05_v8_RNAseq_bigWig_files/GTEX-13X6K-1626-SM-7EWCX.Aligned.sortedByCoord.out.patched.md.bigWig
```
We can use `AnVIL::gsutil_cp` on the `gsUri` obtained above, to materialize the bigWig for local analysis.

Eventually region-based queries can be conducted over HTTP within AnVIL.

### Visualizing imported bigWig

We produced the following display (it is interactive when produced in RStudio) using the tntplot function in curatedAnVILData.
This takes a GRanges as input; the regions must have width 1 and an mcols field 'value' must be present.

<img alt=""coverage"" title=""TnT coverage sketch"" src=""https://storage.googleapis.com/bioc-anvil-images/tntplotAnVIL.png"" width=800>

### Exercise 1 for Sept. 7

Form a MAE-level colData with selected variables, bind to GTExMAE, and demonstrate its use for cross-tissue selection.

#### Solution

Here is a function for this task;

```
add_cd = function(mae, kpvar=NULL) {
  allcds = lapply(experiments(mae), colData)
  fullm = do.call(rbind, allcds)
	if (!is.null(kpvar)) {
	     fullm = fullm[,kpvar]
			 }
	colData(mae) = fullm
	mae
}
```

Add complete colData to GTExMAE:

```
> colData(GTExMAE)
DataFrame with 75740 rows and 0 columns
> gt2 = add_cd(GTExMAE)
> dim(colData(gt2))
[1] 75740   270
```

Limit to samples for which ischemic time exceeds 1400 minutes:

```
> gt2[, which(gt2$ischemic_time_in_minutes>1400), ]
harmonizing input:
  removing 9 sampleMap rows with 'colname' not in colnames of experiments
A MultiAssayExperiment object of 30 listed
 experiments with user-defined names and respective classes.
 Containing an ExperimentList class object of length 30:
 [1] Skin: SummarizedExperiment with 0 rows and 31 columns
 [2] Lung: SummarizedExperiment with 0 rows and 10 columns
 [3] Thyroid: SummarizedExperiment with 0 rows and 20 columns
 [4] Adipose Tissue: SummarizedExperiment with 0 rows and 35 columns
 [5] Blood Vessel: SummarizedExperiment with 0 rows and 42 columns
 [6] Esophagus: SummarizedExperiment with 0 rows and 50 columns
 ...
 ```
 

### Exercise 2 for Sept. 7

Compare expression patterns for selected genes (provide a shiny app) in lung samples annotated with either fibrosis or emphysema.

<img alt=""coverage"" title=""TnT coverage sketch"" src=""https://storage.googleapis.com/bioc-anvil-images/tracks.png"" width=800>
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","landmarkanvil2/curatedAnVILDataProto6"
232,"inform-africa-dsi","SARS-CoV-2 genomic surveillance across Africa","READER","https://app.terra.bio/#workspaces/inform-africa-dsi/SARS-CoV-2%20genomic%20surveillance%20across%20Africa",TRUE,FALSE,NA,NA,NA,"An open access workspace based on publicly available data and analyses from the Africa CDC Pathogen Genomics Initiative (PGI) community publication led by Tegally et al, Science 378 (6115),15 Sept 2022


# **PUBLICATION**
Houriiyah Tegally, James E. San, Matthew Cotten, Monika Moir, Bryan Tegomoh,  et. al. The evolving SARS-CoV-2 epidemic in Africa: Insights from rapidly expanding genomic surveillance https://doi.org/10.1126/science.abq5358

# **OVERVIEW**
Investment in severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) sequencing in Africa over the past year has led to a major increase in the number of sequences that have been generated and used to track the pandemic on the continent, a number that now exceeds 100,000 genomes. Our results show an increase in the number of African countries that are able to sequence domestically and highlight that local sequencing enables faster turnaround times and more-regular routine surveillance. Despite limitations of low testing proportions, findings from this genomic surveillance study underscore the heterogeneous nature of the pandemic and illuminate the distinct dispersal dynamics of variants of concern—particularly Alpha, Beta, Delta, and Omicron—on the continent. Sustained investment for diagnostics and genomic surveillance in Africa is needed as the virus continues to evolve while the continent faces many emerging and reemerging infectious disease threats. These investments are crucial for pandemic preparedness and response and will serve the health of the continent well into the 21st century.

# **NOTES ON DATA IN THIS WORKSPACE**
To reproduce **Figure 1** of the paper, the following datasets located directly in the Files folder are required:

1. metadata_2022-03-30_23-13.tsv   with a size of** 3GB**
2. owid-covid-data_1April2022.xlsx

Additionally, to reproduce **Figure 4** of the paper, the datasets (all lightweight-sized)  located in the ImportExport folder  and the metadata_2022-03-30_23-13.tsv are required.


# **AUTHORS AND CONTACT INFORMATION**
This workspace is a product of the Centre for Epidemic and Response Innovation (CERI), School of Data Science and Computational Thinking, at the Stellenbosch University, South Africa, in collaboration with the KwaZulu-Natal Research Innovation and Sequencing Platform (KRISP), Nelson R Mandela School of Medicine,University of KwaZulu-Natal, Durban, South Africa.",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","inform-africa-dsi/SARS-CoV-2 genomic surveillance across Africa"
233,"warn-id","ACEGID NGS Workshop June 2024","READER","https://app.terra.bio/#workspaces/warn-id/ACEGID%20NGS%20Workshop%20June%202024",TRUE,FALSE,NA,NA,NA,"Viral alignment, variant calling, assembly, and metagenomics.

Workshop tutorial: https://broadinstitute.github.io/viral-workshops",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","warn-id/ACEGID NGS Workshop June 2024"
234,"broad-firecloud-dsde-methods","workflow-resource-and-cost-monitoring-guide-v1","READER","https://app.terra.bio/#workspaces/broad-firecloud-dsde-methods/workflow-resource-and-cost-monitoring-guide-v1",TRUE,TRUE,NA,NA,NA,"
## Overview 
Want to understand how much CPU, memory, and disk space the tasks in your workflows use?  This workspace guides you through methods to capture resource usage and cost data for your executed workflows and even provides notebooks for visualizing this information.

The workspace focuses on two key aspects: workflow resource usage and cost.  For both, you'll have the option to drill down and see details at the individual task level within your workflows.

As an example this workspace runs Theiagen's  TheiaCoV_Illumina_PE workflow (from the [TheiaCoV_MPXV](https://app.terra.bio/#workspaces/theiagen-demos/TheiaCoV_MPXV) workspace) with monitoring setting enabled. The monitoring data is retrieved and plotted using jupyter notebooks. 

*Please use default VM image environment when running notebooks***


### Resource Usage

When you run a workflow on Terra, you can track the resources used by each task within the virtual machine (VM) using your own monitoring scripts. Terra offers two monitoring options: script monitoring and image monitoring. This workspace utilizes image monitoring for resource usage tracking.

The container image used for monitoring is called cromwell-task-monitor-bq, on GitHub: https://github.com/broadinstitute/cromwell-task-monitor-bq. This image feeds resource usage data for each task directly to a BigQuery database. You can then retrieve and analyze this data for insights into resource consumption. This workspace even provides example notebooks to help you retrieve and visualize the data, sourced from the [cromwell-task-monitor-bq-vis](https://github.com/broadinstitute/cromwell-task-monitor-bq-vis) Git repository.

Workflow Duration Summary Example Plot:  
<img src=""https://github.com/broadinstitute/cromwell-task-monitor-bq-vis/blob/main/docs/images/workflow-duration-summary.png?raw=true"" alt=""drawing"" width=""600""/>

Task Resource Usage Example Plot:  
<img src=""https://github.com/broadinstitute/cromwell-task-monitor-bq-vis/blob/main/docs/images/task-resource-summary.png?raw=true"" alt=""drawing"" width=""600""/>

#### Requirements:

1. Your Terra account must have [bigquery editor](https://cloud.google.com/bigquery/docs/access-control-basic-roles) permission to a Google billing project with BigQuery Enabled  (NOT the Google project associated with the workspace e.g. terra-1b24549). The following roles are required for you account on the billing project:  
	1. BigQuery Data editor
	2. BigQuery Job User
	3. BigQuery Read Session User
	4. BigQuery Resource Viewer
	5. BigQuery Studio User

3. Save a source file named `cromwell-monitor-script.sh` in your workspace bucket. The content of the file should be as follows but replace ""my-google-project"", with your Google project which you have BigQuery editor permission. 
> \# Set the project the monitor script will use when creating the bq data table or sending stats.  
> export PROJECT_ID_OVERRIDE=""my-google-project""  
> \# Run the monitoring script  
> monitor  


#### Running a Workflow with Monitoring:  

1. Select Workflow: Navigate to the ""Workflows"" tab and click on the workflow you want to monitor, this sends you to the workflow configuration page.  
2. Provide Input: Select your data and configure inputs for the workflow as usual.  
3. Disable Caching: Uncheck ""Use call caching"" to ensure all tasks run (avoiding skipped tasks).  
4. Enable Resource Monitoring:   
   - Check mark ""Resource monitoring"".  
   - In the image textbox, enter `us.gcr.io/broad-dsp-lrma/cromwell-task-monitor-bq:bs-project_override`.  
   - Provide the gs:// path to your edited `cromwell-monitor-script.sh` mentioned in the requirements above (e.g. `gs://fc-8c9aa514-727d-4ec1-94cc-3eb484df2083/cromwell-monitor-script.sh`)
5. Run the Workflow: Start the workflow with monitoring enabled.  


You should now be able to run the resource monitoring notebook (located in this workspace Notebook tab) which will query the BigQuery database that is collecting data on your executed workflow and plot the data. 


#### Troubleshooting:  

While resource monitoring typically runs smoothly, here are some checks to ensure everything works as expected, especially for the first time.

1. Verify Image Execution:  In the Job Manager, open a log file for one of your workflow tasks. The first lines should indicate if the monitoring image (""cromwell-task-monitor-bq"") ran successfully.  

Example of Successful Startup:  
> 2024/03/19 20:54:02 Starting container setup.
> 2024/03/19 20:54:03 Done container setup.
> 2024/03/19 20:54:04 Running localizing monitoring image script action: docker run -v /mnt/local-disk:/cromwell_root --entrypoint=/bin/sh gcr.io/google.com/cloudsdktool/cloud-sdk:461.0.0-alpine -c python3\ -c\ \'import\ base64\;\ print\(base64.b64decode\(\""IyEvYmluL2Jhc2g...RcIHJldHJ5aW5nCiAgICBzbGVlcCA1CiAgZmkKZG9uZQpleGl0ICIkUkMi\""\).decode\(\""utf-8\""\)\)\;\'\ \>\ /tmp/12ea4bf3-fe3d-47aa-9f36-caf99bed286b.sh\ \&\&\ chmod\ u+x\ /tmp/12ea4bf3-fe3d-47aa-9f36-caf99bed286b.sh\ \&\&\ sh\ /tmp/12ea4bf3-fe3d-47aa-9f36-caf99bed286b.sh
> 2024/03/19 20:54:19 Running monitoring action: docker run -v /mnt/local-disk:/cromwell_root -e TASK_CALL_NAME:version_capture -e TASK_CALL_ATTEMPT:1 -e WORKFLOW_ID:e1ca88e9-d902-4dd9-a9eb-89d1d6d935e1 -e DISK_MOUNTS:/cromwell_root -e TASK_CALL_INDEX:NA --pid=monitoring --entrypoint=/bin/sh us.gcr.io/broad-dsp-lrma/cromwell-task-monitor-bq:bs-project_override -c cd\ \'/cromwell_root\'\ \&\&\ chmod\ +x\ \'/cromwell_root/monitoringImage.sh\'\ \&\&\ \'/cromwell_root/monitoringImage.sh\'
> 2024/03/19 20:54:22 Starting localization.   

Example of a Terra account not having sufficient BigQuery permissions for a Google project. In this case make sure you contact your administrator to get proper permissions.  
> 2024/05/01 04:22:06 Starting container setup.
> 2024/05/01 04:22:08 Done container setup.
> 2024/05/01 04:22:08 Running monitoring action: docker run -v /mnt/local-disk:/cromwell_root -e TASK_CALL_NAME:FinalizeHCGVcf -e TASK_CALL_ATTEMPT:1 -e WORKFLOW_ID:19e1fe6b-6ef4-4cb0-9f6b-f2574a0ca6e8 -e DISK_MOUNTS:/cromwell_root -e TASK_CALL_INDEX:NA --pid=monitoring us.gcr.io/broad-dsp-lrma/cromwell-task-monitor-bq:bs-project_override
> 2024/05/01 04:22:11 Starting localization.
> 2024/05/01 04:22:12 googleapi: Error 403: Access Denied: Project example-billing-project: User does not have bigquery.datasets.create permission in project example-billing-project., accessDenied 

2. Check that Bigquery received monitoring data:
 - The monitoring image should have created a dataset in the Bigquery project in your project on your behalf. The dataset will be labeled ""cromwell-monitoring"", and the monitoring image will always send data to this dataset for all workflow submissions. Make sure this exists.  
 - The dataset will have two tables, `runtime` and `metrics`. If you are running this for the first time then simply checking whether the tables are empty or not is sufficient.   
 - If others are also submitting workflow monitoring data to the same bigquery project it would be good to check your workflow ID is in the table. The runtime table will have the workflow ID so its best to query this table to check if your workflow ID was recorded.  
   - Template Query:  
		 ```
		 SELECT *   
		 FROM `<your-billing-project>.cromwell_monitoring.runtime`   
		 WHERE TIMESTAMP_TRUNC(start_time, DAY) = TIMESTAMP(""<year-month-day>"")   
		 AND workflow_id = ""<workflow id>""   
		 LIMIT 1000  
		 ```  


### Cost Report

While Terra provides the total cost of a workflow, this guide helps you identify the tasks contributing the most to that cost.  
The cost details is stored on Bigquery database, this workspace even provides example notebooks to help you retrieve and visualize the data.

Example Workflow Cost:  
<img src=""https://github.com/broadinstitute/cromwell-task-monitor-bq-vis/blob/main/docs/images/workflow-cost-plot.png?raw=true"" alt=""drawing"" width=""600""/>

Example Task Cost:  
<img src=""https://github.com/broadinstitute/cromwell-task-monitor-bq-vis/blob/main/docs/images/task-cost-plot.png?raw=true"" alt=""drawing"" width=""600""/>

#### Requirement:  

Enable BigQuery Billing Exports:  Billing exports must be enabled for the billing project associated with your Terra project. See Terra instructions here: [How-much-did-my-workflow-cost](https://support.terra.bio/hc/en-us/articles/360037862771)

#### Running a Workflow with Cost Report:

There's no need for special instructions! Simply run your workflow as usual. Google will automatically capture cost data for each task if billing exports are enabled. 

After waiting a minimum of 24 hours (the time required for cost data to be exported to the BigQuery data table), you can run the cost report notebook located in this workspace. The notebook will query the BigQuery and plot the data. 


## Notebooks
The previous steps guide you through capturing resource usage and cost information in BigQuery.  This workspace goes a step further: it provides notebooks to help you retrieve and visualize that data with plots and charts using Jupyter Notebooks.


**A. example-interactive-resource-monitoring:** Retrieves resource monitoring data from BQ database and plots data. 


Computer Power: 

| Runtime  | Value |
| --- | --- |
| Environment | Default (GATK4..) |
| CPU Minimum| 4|
| Disksize Minimum | Default |
| Memory Minimum | Default |
 
 **B. example-interactive-cost-report:** Retrieves resource cost data from BQ database and plots data. 


Computer Power: 

| Runtime  | Value |
| --- | --- |
| Environment | Default (Gatk4...) |
| CPU Minimum| 4|
| Disksize Minimum | Default |
| Memory Minimum | Default |

## Contact information

Please post questions or problems to the git repository page : https://github.com/broadinstitute/cromwell-task-monitor-bq-vis/issues


## Workspace Change Log
| Date | Change | Author |
| --- | --- | --- |
|  2024-04-19 | Workspace creation | Beri Shifaw |

 ",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","broad-firecloud-dsde-methods/workflow-resource-and-cost-monitoring-guide-v1"
235,"help-gatk","Reproducibility_Case_Study_Tetralogy_of_Fallot","READER","https://app.terra.bio/#workspaces/help-gatk/Reproducibility_Case_Study_Tetralogy_of_Fallot",TRUE,FALSE,NA,NA,NA,"## Reproducing the paper: Variant analysis of Tetralogy of Fallot

### Overview  
This workspace reproduces the work described by Matthieu Miossec and collaborators in the bioRxiv preprint ""[Deleterious genetic variants in NOTCH1 are a major contributor to the incidence of non-syndromic Tetralogy of Fallot (ToF)](https://www.biorxiv.org/content/early/2018/04/13/300905).”


The original ToF study is a classic example of a study to understand the genetics that underlie a particular phenotype. The workspace reproduces all steps in the study as closely as possible, from processing the raw data (BAM) files, to calling variants, to the clustering analysis that led to the final result.  

The workspace serves as a template of best practices for making your own work easily reproducible with a detailed explanation of how we reproduced the ToF study using a cloud-based analysis platform. Sample data and notebooks allow users to reproduce the process themselves.  


-----
### Summary of original ToF study  
By analysing high-throughput exome sequence data from 867 cases and 1252 controls, the authors identified 49 deleterious variants within the NOTCH1 gene that appeared to correspond with the ToF congenital heart disease. Others had previously identified NOTCH1 variants in families with congenital heart defects, including ToF. However, the work by Miossec et al. is the first to scale variant analysis of ToF to a cohort of nearly a thousand case samples and show that NOTCH1 is a significant contributor to ToF risk.

- **Paper URL:** https://www.ahajournals.org/doi/abs/10.1161/CIRCRESAHA.118.313250 (This workspace is based on the [preprint](https://www.biorxiv.org/content/early/2018/04/13/300905))
- **Workflows:** see [Workflows](https://app.terra.bio/#workspaces/help-gatk/Reproducibility_Case_Study_Tetralogy_of_Fallot/workflows) tab
- **Notebook:** [download from Google Cloud Storage](https://storage.cloud.google.com/firecloud-workshops/181017-ashg18/notebooks/cluster_analysis.ipynb)
- **Precomputed results:** [view in Google Cloud Storage](https://console.cloud.google.com/storage/browser/firecloud-workshops/181017-ashg18 )

   
-----
### Adapting the original experimental design  
Our goal was to recapitulate the ToF study results by using similar methodology and data - to run the same analysis on the same data and get the same results. To tackle this, we divide the study into three main phases - data input, processing, and analysis - as described by Justin Kitzes et al. in their book, ""[The Practice of Reproducible Research](http://www.practicereproducibleresearch.org/)."" To reconstruct these phases, we used information provided in the preprint and its supplemental materials, as well as input from the study author.

We made the following adjustments to the original study, which used private medical data that was processed and analyzed locally. Our guiding principle was to be as close to the original study as possible. We assumed 1) basic data processing should be fairly robust to adaptations as long as it is applied the same way to all of the data and 2) the annotations and clustering analysis at the core of the study were critical to reproduce exactly.

Specifically:  
* For the **data input phase**, we created a synthetic dataset to get around the lack of appropriate public data to use as input.  
* For the **processing phase**, we applied a variant discovery workflow that we judged equivalent (see assumption 1, above) to the original study workflow.  
* For the **analysis phase**, we obtained the original scripts and commands from Dr. Miossec (following assumption 2, above). With his assistance, we reimplemented the original scripts in two parts: the prediction of variant effects as a workflow in WDL [Workflow Description Language](http://www.openwdl.org) and the clustering analysis as R code in a [Jupyter notebook](http://jupyter.org). We did all the work in the Broad Institute’s open-source analysis platform, FireCloud (now Terra).
 
  
#### Note on synthetic dataset creation  
We chose not to use the existing read alignments from 1000G due to technical shortcomings of that dataset (file validation errors, insufficient coverage, etc.). Instead, we opted to generate synthetic sequence data from the Phase 3 callset using the NEAT toolkit. We then introduced the desired variant alleles into reads in those synthetic BAM files using BAMSurgeon. For control samples, we used a synonymous SNP as neutral variant to ensure that the processing applied to controls would mimic cases as closely as possible.
  
Additional details will be provided in an upcoming revision.
  
	
 -----   
### Results & Discussion  
We were able to reproduce the core analysis using our synthetic data set and adapted processing workflows. Encouragingly, we found that the size of the cohort, not the use of synthetic data, had the most impact on the results. Using a synthetic cohort size closer to that of the original study yielded better agreement with the published results. This underscores the importance of developing community resources to support reproducing this kind of work at the appropriate scale.

**Demonstrating the importance of large cohorts for statistical analysis** 
In a test run on a synthetic cohort of 100 samples (of which 8 were case samples), our reimplementation of the original methods identified the spiked-in NOTCH1 variants among the highest scoring hits in the clustering analysis.

However, the NOTCH1 variants were not the top hit, suggesting that the small cohort test run was statistically underpowered. Analyzing a larger synthetic cohort of 500 samples confirmed this hypothesis, as NOTCH-1 rose to the top. The greater number of samples provided sufficient statistical power to reproduce the original results more closely. The workspace notebook highlights this effect by contrasting the clustering analysis for the 100- and 500-sample cohorts using precomputed data.


-----
### How to use this workspace
We have included the six WDLs used to reproduce the ToF study end-to-end in this workspace. However, running the full complement of tools is time consuming and expensive, and is not necessary to understand the final results and conclusions. To that end, we provide pre-computed data sets to use as input for the clustering analysis for both the 100- and 500-sample synthetic cohorts. To reproduce the ToF study results, and to see how the larger (i.e. statistically significant) cohort size successfully reproduces the original ToF study, you can run the notebook “cluster.ipynb” in the Notebook tab.



### Clustering Analysis in interactive notebooks  
This workspace contains a Jupyter notebook that reproduces the clustering analysis at the core of the ToF study results. Annotations explain each analysis step and the cell blocks contain R code. The notebooks contained in the Featured Workspace are set to run on the 100-sample cohort, and running the notebook will identify the NOTCH-1 gene as a candidate for deleterious variants responsible for ToF, but not number one. Repeating the analysis on the 500-sample cohort delivers the analysis from the original paper, identifying NOTCH-1 as the top candidate. 

The critical clustering analysis reveals how boosting the statistical power of the synthetic sample set allows us to more closely emulate the ToF study. 

**Note** Because of the time and expense of running the computations for the cohort of 500, we have included links to precomputed data for the notebook analysis. 

 
 
-----     
### Best Practices for “Reproducible” Research  
Several features highlighted in this workspace enabled us to reproduce the ToF study. Incorporating these will help reproduce and validate your own work precisely and exactly.

- **Version-controlled code** Capturing the version used for processing and analysing data is critical for exact reproducibility.

- **Open data** Open -access data is of course the first choice for a reproducible study. If that option isn’t available, as in this case, creating an equivalent synthetic data can make the study reproducible.

- **Automation** Both batch processing (WDLs) and interactive analysis (in Jupyter notebooks) can be automated , with shared code to ensure consistency throughout the processing and analysis stages.

- **Built-in documentation** Comments in Jupyter notebooks are a convenient way to ensure a transparent and understandable  analysis process.

- **Free and open tools** Similar in theme to open data, free and open tools remove many barriers to an exact reproduction.



----- 
### Detailed contents of this workspace  
#### Data  
The workspace contains precomputed per-sample output files for two synthetic participant_sets derived from participants in the 1000 Genomes project : Test_cohort_A100 with 100 samples and Test_cohort-B500 with 500 samples. The workflows take a long time to run on the larger set (see Time and Cost below), but the larger numbers provide greater statistical muscle and agreement with the original study (noted for its large cohort size).

The workspace data model also contains IDs for all 3,500 participants of the 1000 Genomes Project, the foundation of our synthetic data. The data are stored in an external Google bucket that is fully public. The data sharing policy has yet to be defined formally, but will be largely inherited from the progenitor dataset, the 1000 Genomes Project.

#### Workspace attributes  
All required and optional references and resources for the methods are included in the Workspace Data section in the Data tab. Of note for this workspace: we use the reference genome from the original study, hg19 (aka GRCh37, or b37), to ensure the most accurate reproduction.

#### Workflows
We provide preconfigured workflows :

| Name | Entity Type| Synopsis |
|---|---|---|
| 1_Collect-1000G-participant | participant | Collect the variants for a single participant from 1000G Phase 3 |
| 2_Generate-synthetic-reads | participant | Generate synthetic read data based on intervals and a VCF of variants |
| 3_Mutate-reads-with-BAMSurgeon | participant | Introduce specific mutations into an analysis-ready BAM file with BAMSurgeon |
| 4_Call-single-sample-GVCF-GATK4 | participant | Call variants per-sample and produce a GVCF file with GATK4 HaplotypeCaller |
| 5_Joint-call-and-hard-filter-GATK4 | participant_set | Apply joint variant discovery analysis and hard filtering |
| 6_Predict-variant-effects-GEMINI | participant_set | Predict and annotate the functional effects of variants using SnpEff and GEMINI

#### Time and Cost

| Workflow Name                  	| 1 file (range) 	| 100 files 	| Time to Run 1 file 	| Time to Run 100 files 	|
|--------------------------------	|----------------	|-----------	|--------------------	|-----------------------	|
| 1_Collect-1000G-participants     	| $1.64 to $2.90 	| $193.75   	| 4.5 hours          	| 12 hours              	|
| 2_Generate-synthetic-reads       	| $2.40 to $3.44 	| $405.67   	| 4.5 hours          	| 12 hours              	|
| 3_Mutate-reads-with-BAMSurgeon   	| $0.02 to $0.15 	| $5.72     	| .5 hours           	| 2.5 hours             	|
| 4_Call-single-sample-GVCF-GATK4  	| $0.32 to $0.52 	| $39.82    	| 1.75 hours         	| 2.75 hours            	|
| 5_Joint-call-and-hard-filter     	|                	| $10.09    	|                    	| 4 hours               	|
| P6_redict-variant-effects-GEMINI 	|                	| $1.00     	|                    	| 4 minutes             	|

**Note that the size range** for a single 1000 genome vcf file is 669.5 MB to 843.5 MB. The total size of the 100-sample cohort is 72.12 GB.


Throughout this project, we strove to apply the same versions of all the tools used in the original study. For the GATK variant calling portion, however, it is technically much more straightforward to work with more recent versions and avoid redeveloping a lot of pipeline code available (at the time) in FireCloud (now available in Terra). Based on our team's experience with GATK we estimated that the results should be functionally equivalent for the purposes of this project. The rest of the tools used the same versions as in the paper, including the custom-built **Predict-variant-effects-GEMINI** WDL workflow and Jupyter Notebook, which were developed from scratch. The table below lists the tools and their version.

| Tool Name | Version  |
|---|---|
| GATK | 4.0.9.0 |
| SNPEff | 4.0e |
| Gemini | 0.18.2 |
| CADD | 1.3 |

### Workspace Change Log
| Date | Change | Author |
| --- | --- | --- |
|  2020-01-17 | Updated the relative notebook paths for data imorted to the notebook 2. Updated markdown documentation for clarity and consistency. | Allie Hajian |
|  2020-05-12 | Replaced ""Tools"" with ""Workflows"" in dashboard| Beri Shifaw|

### Credits and acknowledgments  
This workspace was developed for an ASHG2018 Invited Workshop. The work was carried out as a collaboration between our team in the Data Sciences Platform at the Broad Institute in Cambridge, MA, USA and Dr. Matthieu J. Miossec of the Center for Bioinformatics and Integrative Biology (CBB) at Universidad Andrés Bello, Santiago, Chile.
​
### License  
#### Copyright Broad Institute, 2018 | BSD-3  
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at https://github.com/openwdl/wdl/blob/master/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.
​
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/Reproducibility_Case_Study_Tetralogy_of_Fallot"
237,"help-terra","Terra on GCP Quickstart","READER","https://app.terra.bio/#workspaces/help-terra/Terra%20on%20GCP%20Quickstart",TRUE,TRUE,NA,NA,NA,"Hands on practice working in Terra on the Google Cloud Platform (GCP). This Quickstart will introduce you to basic data and analysis functionality as you walk through a mock study of the correlation between height and GPA for a cohort of 7th, 8th, and 9th graders.   

## Quickstart learning objectives 
**After working through this tutorial, you should know how to**       
1. Add and modify input data and metadata in a data table in your workspace
2. Set up and run a workflow analysis on data in a table   
3. Set up and launch a Jupyter Cloud Environment VM       
4. Run a notebook to plot and visualize processed data in a table 

## How long will it take? How much will it cost?    
#### Time     
You should be able to complete the Quickstart tutorial in about an hour.    

#### Cost      
Running the tutorial will cost  much less than $1 (Google Cloud data storage and VM costs). You will need to have a Terra Billing project and your own copy of the Quickstart workspace to complete the tutorial.     

## Quickstart summary    

The mock study asks the question ""Is there a relationship between a student's height and their total GPA across three subjects. The mock study has four steps (data visualization has two parts), much like a real research journey you might do in Terra.      

<img alt=""Diagram of steps in Quickstart"" src=""https://storage.googleapis.com/terra-featured-workspaces/QuickStart/Unified-Quickstart-flow.png"" width=""900"">     


#### 1. Data exploration (data tables)     
Explore and manipulate synthetic data for a mock study of 86 middle school students.  Study data includes  student **names**, **height**, and **language**, **math**, and **science grades**.      


|   ![read more icon](https://storage.googleapis.com/terra-featured-workspaces/QuickStart/icon_read_more%401x.png) | <a href=""https://support.terra.bio/hc/en-us/articles/23740086603547-Terra-GCP-Quickstart-1-Data-tables"" target=""_blank"">**Click for step-by-step instructions**</a> |      
|---| ---------|       


#### 2. Data processing (workflows)    
Set up and run a workflow on raw data in the student table. The workflow calculates the average GPA for each student from the three subject grades and writes the average GPA back to the data table.      

|   ![read more icon](https://storage.googleapis.com/terra-featured-workspaces/QuickStart/icon_read_more%401x.png) | <a href=""https://support.terra.bio/hc/en-us/articles/23740210297115-Terra-GCP-Quickstart-2-Workflow"" target=""_blank"">**Click for step-by-step instructions**</a> |      
|---| ---------|       


#### 3. Data visualizing (Jupyter notebook)       
Set up a Cloud Environment (software + VM + persistent disk) to run an interactive Jupyter notebook analysis. Cloud Environments in Terra can be customized to fit your analysis needs.      

Once you're VM is running, you'll import the height and average GPA data from the `student` table and generate plots to see if there is a correlation. You'll compare results from an 8-person subset and the full dataset to explore how small number statistics can skew your results, and why it's important to use large datasets.  

|   ![read more icon](https://storage.googleapis.com/terra-featured-workspaces/QuickStart/icon_read_more%401x.png) | <a href=""https://support.terra.bio/hc/en-us/articles/23740310333851-Terra-GCP-Quickstart-3-Notebooks"" target=""_blank"">**Click for step-by-step instructions**</a> |      
|---| ---------|     


**Jupyter 101.jpynb**  (optional).       
If you're new to notebooks, you can explore this introduction to Jupyter notebooks to learn more about:      
- Why notebooks are used in biomedical research.
- The relationship between the notebook and the workspace.
- Jupyter Notebook basics: how to use a notebook, install packages, and import modules.
- Common libraries in data analysis and popular tutorial notebooks.


## Contact information  
This material is provided by the Terra Team. Please post any questions or concerns to our forum site: <a href=""https://support.terra.bio/hc/en-us/community/topics/360001603491-Featured-Workspaces"" target=""blank"">Terra community forum</a>. 
<br>
<br>

## Workspace Citation 
Details on citing Terra workspaces can be found in the article <a href=""https://support.terra.bio/hc/en-us/articles/360035343652"" target=""blank"">How to cite Terra</a>.

Data Sciences Platform, Broad Institute (*Year, Month Day that this workspace was last modified*) help-terra/T101-Data-Tables-Quickstart [workspace] Retrieved *Month Day, Year that workspace was retrieved*,  https://app.terra.bio/#workspaces/help-terra/Terra%20on%20GCP%20Quickstart
<br>
<br>

## License  
**Copyright Broad Institute, 2023 | BSD-3**  
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at <a href=""https://github.com/openwdl/wdl/blob/master/LICENSE"" target=""blank"">github.com/openwdl/wdl/blob/master/LICENSE</a>). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-terra/Terra on GCP Quickstart"
238,"broad-firecloud-dsde-methods","joint-calling-FE-versus-dragen-batch-effect-exp","READER","https://app.terra.bio/#workspaces/broad-firecloud-dsde-methods/joint-calling-FE-versus-dragen-batch-effect-exp",TRUE,FALSE,"n/a","n/a",NA,"> A ""batch effect experiment"" to explore the differences between two different variant calling pipelines using three SNP and indel joint callsets: 1) all samples from the first pipeline, 2) all samples from the second pipeline and 3) half samples from each pipeline
> TAGS - 1000 Genomes, DRAGEN, Functional Equivalence, GWAS, Hail


### Overview Paragraph
As time goes on, tools get updates, new human references get released, and even new sequencing technologies come on the market. We know that we can get the best statistical power from our genomic analyses by combining more samples, but combining samples that don't look alike could also potentially lead to ""batch effects"". Here we'll look at the types of batch effects that arise in case-control studies using data from different batches, in this case different variant calling pieplines.  The batch effects we want to minimize are false positive results in case-control studies like GWAS. 

This is a companion workspace to the two GATK blogs on this topic:  [Joint Calling and the Batch Effect Boogeyman](https://gatk.broadinstitute.org/hc/en-us/articles/18440261559195) and Part Two to be released soon!
 
### Experimental Design
The two single-sample pipelines to be compared are the Functional Equivalence (FE) pipeline (as described in the <a href=""https://www.nature.com/articles/s41467-018-06159-4"" target = ""_blank"">FE paper</a>) and the DRAGEN-GATK variant calling (WholeGenomeGermlineSingleSample.wdl, equivalent to DRAGEN 3.4.12). Both pipelines were run from BWA-aligned reads and output ""reblocked"" GVCFs. GVCFs from each pipeline were joint called with GenomicsDBImport, GenotypeGVCFs, and VQSR. The differences between the two pipelines were limited to variant calling, contrasting GATK 3.5 HaplotypeCaller with the DRAGEN-equivalent version including an STR-specific indel model.  (Read more details in our <a href=""https://gatk.broadinstitute.org/hc/en-us/articles/360039984151-DRAGEN-GATK-Update-Let-s-get-more-specific"" target = ""_blank"">DRAGEN-GATK Update blog post</a>.)

Here we compare joint callsets made from all FE samples, all DRAGEN samples, and a mixed callset with half of each. You can compare two different conditions of your own with analogous callsets made of all samples processed under conditon 1, all condition 2, and a 50-50 mixed cohort composed of the two conditions in equal proportion.
 

## Data 
*PCR-free, NYGC 1000 Genomes phase 3 samples sequenced to high coverage processed via Functional Equivalence pipeline and GATK-DRAGEN 3.4.  GVCFs were reblocked and then joint called using GenotypeGVCFs and VQSR.  See the Workflows section for more details.*

 
## Workflows
Functionally Equivalent GVCFs are provided for the 1000Genomes samples in the NYGC public data. GATK-DRAGEN 3.4 variant calling was performed for those samples using the GATK-DRAGEN pipeline provided <a href = ""https://app.terra.bio/#workspaces/warp-pipelines/DRAGEN-GATK-Whole-Genome-Germline-Pipeline/workflows/warp-pipelines/WGS_Functional_Equivalence"" taret = ""_blank"">here</a>.

The callsets shared here were generated using the GATK Best Practices joint genotyping workflow: https://app.terra.bio/#workspaces/warp-pipelines/Whole-Genome-Analysis-Pipeline/workflows/warp-pipelines/3-JointGenotyping

Because the NYGC 1000G GVCFs are external GVCFs, they needed to be run through the [ReblockGVCF](https://github.com/broadinstitute/warp/blob/ReblockGVCF_develop/pipelines/broad/dna_seq/germline/joint_genotyping/reblocking/ReblockGVCF.wdl) workflow before generating a sample map and joint calling.    
 
 Novel workflows included in this workspace that are specific to the batch effect experiment are as follows:

### determine_PCA_sites_*

There is a method configuration for each joint callset: all FE, all DRAGEN, and mixed, but they each use the same workflow.

**What does it do?**  
* Removes multi-allelic variants
* Filters to >= 0.1% allele frequency
* Excludes variants with callrate < 99%
* Excludes filtered variants
* Uses hail to prune variants to choose a single variant from each LD block (default 500Kb window, r^2 = 0.1)

**What does it require as input?**       
* One or more input VCFs with genotypes   
* Interval list to restrict results, here with telomeres and centromeres excluded

**What does it return as output?**  
* Filtered and LD-pruned variants, sites-only VCF with index
* Filtered and LD-pruned variants with genotypes, VCF with index

**Reference/Resource data description and location**  
*The reference genome for this workspace is hg38 (aka GRCh38). Required and optional references and resources for the methods are included in the Workspace Data table (""Workspace Attributes"" in FireCloud).*           
 
**Estimated time and cost to run on sample data**    


| Callset | File Size | # Samples | Time | Cost $ |
| :---:  | :---: | :---: | :---: | :--: |
| chr18 all-DRAGEN | 23.19 GB | 2392 samples | 3:30:00 | 0.69 |

For helpful hints on controlling Cloud costs, see <a href = ""https://support.terra.bio/hc/en-us/articles/360029748111"" target = ""_blank""> this article </a>.       

---

## Notebooks
**batch_comparison**: Compare callsets, evaluate Functional Equivalence metrics, and measure batch effects

Environment: Hail (updated July 7, 2023)

Computer Power: 

| Runtime  | Value |
| --- | --- |
| Environments | Hail (Python3.10,11, Spark 3.3.0, Hail 0.2.120) |
| CPU Minimum| 4|
| Disksize Minimum | 150 GB |
| Memory Minimum | 15 GB |
| Compute type | Spark cluster |
| Workers | 2 |
| Preemptibles | 20 |
| Startup Script | None |

**Estimated time and cost to run on sample data**    
The notebook requires three input VCFs, which are between 20 and 25GB in the data provided here.

| # Samples | # Variants | Time | Cost $ |
| :---:  | :---: | :---: | :---: |
| 2392 | 3,341,526 | ~4 hours | ~$10 |

Run time for long running cells is noted in the notebook.  The Mendelian violation calculate takes about an hour and can be excluded by setting the pedigree file to 'None'.


### Contact information

*The data and analysis presented here was prepared by Laura Gauthier (gauthier AT broadinstitute DOT com). Please direct all questions to the Terra User Forum for the benefit of other users.*

### Workspace Citation
Gauthier, L. (2023, September 27) broad-firecloud-dsde-methods/joint-calling-FE-versus-dragen-batch-effect-exp [workspace] Retrieved September 28, 2023, https://app.terra.bio/#workspaces/broad-firecloud-dsde-methods/joint-calling-FE-versus-dragen-batch-effect-exp

### Workspace Change Log
| Date | Change | Author |
| --- | --- | --- |
|  2023-10-27 | Live as Featured Workspace | Anthony Dias-Ciarla |
|  2023-09-1 | Draft Featured Workspace | Laura Gauthier |
 ","AnVIL Team","AnVIL Team","High Coverage 1000G Genomes and VCFs made available through the NHGRI Anvil.","3202","n/a","1000Genomes",NA,"Public",NA,"1000 Genomes","Whole Genome;VCF","broad-firecloud-dsde-methods/joint-calling-FE-versus-dragen-batch-effect-exp"
239,"broad-fc-getzlab-workflows","CGA_WES_Characterization_OpenAccess","READER","https://app.terra.bio/#workspaces/broad-fc-getzlab-workflows/CGA_WES_Characterization_OpenAccess",TRUE,TRUE,NA,NA,NA,"## CGA WES Characterization Workflow for the identification of somatic variants (SNVs, INDELs, CNVs) in tumor-normal paired WES data.

The purpose of this ""template"" workspace is to provide a starting point for the running of the CGA WES Characterization workflow on FireCloud.  Users may clone this workspace,  upload their data and metadata to the clone, and then adapt the clone's method configuration for the running of the WES characterization workflow on their own dataset.

The workflow is described in detail in the [CGA WES Characterization Pipeline User Guide](https://docs.google.com/document/d/1VO2kX_fgfUd0x3mBS9NjLUWGZu794WbTepBel3cBg08).

This workspace contains (1) a method configuration that may be used for configuring and launching the CGA WES Characterization pipeline and (2) workspace attribute definitions pointing to reference files used by the workspace's method configuration.

To use this template workspace, you must reference one or more Token PoNs that are not constructed from TCGA controlled access data.  We provide a public workflow for creating your own Token PoN File (see Appendix B of the workflow's user guide).  You will need to create your own TSV file listing your Token PoNs and set the MAF_PON_FILTER_LIST workspace attribute to the path of that TSV file on Google Cloud Storage.

If you have dbGaP authorization for TCGA controlled access data, you may use a Token PoN created from 8334 normal TCGA samples.  Rather than cloning this workspace, clone the workspace *[broad-fc-getzlab-workflows/CGA_WES_Characterization_TCGAControlledAccess](https://portal.firecloud.org/#workspaces/broad-fc-getzlab-workflows/CGA_WES_Characterization_TCGAControlledAccess)* which is in the TCGA-dbGaP-Authorized authorization domain and is set up to use the TCGA_8334 Token PoN.

Use of the workflow is subject to the following license: [CGA WES Characterization Pipeline License](https://docs.google.com/document/d/1D3UYYIBk-iIicrYhzqOVt4B51Apd_EZr9WCJuuaCJis).

To cite this workflow, you must cite both the workflow and its constituent tools.  The [CGA WES Characterization Pipeline User Guide](https://docs.google.com/document/d/1VO2kX_fgfUd0x3mBS9NjLUWGZu794WbTepBel3cBg08) contains detailed instructions on how to site this workflow.",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","broad-fc-getzlab-workflows/CGA_WES_Characterization_OpenAccess"
240,"broad-firecloud-tcga","CPTAC_Colorectal_OpenAccess_V1_DRAFT","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/CPTAC_Colorectal_OpenAccess_V1_DRAFT",TRUE,TRUE,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","broad-firecloud-tcga/CPTAC_Colorectal_OpenAccess_V1_DRAFT"
241,"terra-outreach","VRS-demo-notebooks","READER","https://app.terra.bio/#workspaces/terra-outreach/VRS-demo-notebooks",TRUE,FALSE,NA,NA,NA,"## Quickstart instructions
This workspace contains a set of Jupyter notebooks that demonstrate the functionality of [`vrs-python`](https://github.com/ga4gh/vrs-python), a library that provides Python language support for the [GA4GH Variation Representation Specification (VRS)](https://github.com/ga4gh/vrs). The original notebooks are located in the [`vrs-python` repository](https://github.com/ga4gh/vrs-python/tree/main/notebooks) on GitHub.


### To use these notebooks in Terra:
1. Either clone the public [`VRS-demo-notebooks`](https://app.terra.bio/#workspaces/terra-outreach/VRS-demo-notebooks) workspace or copy the notebooks' `.ipynb` files to the Analyses tab of a new or existing Terra workspace.
2. Launch a Python cloud environment from the Analyses tab of your Terra workspace. The default Terra environment configuration should be sufficient.
3. Open and execute the `Terra.ipynb` notebook. This will install the `vrs-python` packages and copy a resource file to your environment's persistent disk.

Once you have completed those steps, you can open and execute the VRS demo notebooks in your cloud environment in any order. 

*For more detailed instructions about launching a Cloud Environment and using Jupyter Notebooks in Terra, see the [Cloud Environments Basics](https://support.terra.bio/hc/en-us/sections/5485227699867-Cloud-Environments-Basics) documentation and the [Notebooks Quickstart Guide](https://support.terra.bio/hc/en-us/articles/360059009571-Notebooks-Quickstart-Guide).*


---


## About vrs-python

[`vrs-python`](https://github.com/ga4gh/vrs-python) is a library that provides Python language support for the [GA4GH Variation Representation Specification (VRS)](https://github.com/ga4gh/vrs). It includes the following packages:

* **ga4gh.core**   
  Python language support for certain nascent standards in GA4GH.  Eventually, this package should be moved to a distinct repository.

* **ga4gh.vrs**   
  Python language support for VRS. 

* **ga4gh.vrs.extras**   
  Python language support for additional functionality, including translating from and to other variant formats and a REST service to similar functionality. `ga4gh.vrs.extras` requires access to supporting data, as described in the project documentation.
	
More information about the packages can be found in the [`vrs-python`](https://github.com/ga4gh/vrs-python) project repository on GitHub.
	

---


## Contact information

For questions or problems with the demo notebooks, please use the [`vrs-python`](https://github.com/ga4gh/vrs-python) project's [issue tracker on Github](https://github.com/ga4gh/vrs-python/issues).


### License
The [`vrs-python`](https://github.com/ga4gh/vrs-python) library code and demo notebooks are open-source under an [Apache 2.0 license](https://github.com/ga4gh/vrs-python/blob/main/LICENSE).

 ",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","terra-outreach/VRS-demo-notebooks"
243,"help-gatk","Terra-201-Gen3-Module","READER","https://app.terra.bio/#workspaces/help-gatk/Terra-201-Gen3-Module",TRUE,FALSE,NA,NA,NA,"Welcome to Terra! We developed this hands-on tutorial to help researchers working with Gen3 data in Terra. The tutorial includes step-by-step instructions to cover the entire analysis process:     
- How to link your Gen3 and Terra accounts    
- How to find and export Gen3 data to a Terra workspace    
- Understanding the structure of Gen3 data in Terra
- How to consolidate Gen3 data tables with a Jupyter Notebook
- How to configure and run a bulk analysis using workflow tools    

Reading the documentation and following the step-by-step instructions will help familiarize you with Terra, so you can hit the ground running.

# Tutorial Overview

You should run through the steps below **in order**. Time estimates are conservative - many steps will take less time. **Note that you don’t have to go through all the steps in one sitting, only in the correct order.**           

![Tutorial flow diagram](https://storage.googleapis.com/terra-featured-workspaces/Terra-201-Gen3-workspace/Terra-201-flow.png)

### How long does the tutorial take and how much does it cost?
Working through all the tutorial steps should take just over an hour total (you can break it up and do at different times). The total computation cost of running the workflow will be less than $1.00.    


### Science overview
The tutorial models steps typical in a research journey using public-access 1,000 Genomes genomic data stored in both the BioData Catalyst and AnVIL instances on the Gen3 platform. This tutorial covers how to export Gen3 and run a bulk analysis workflow on the genomic data exported from Gen3. You would use the same process to do any bulk analysis on genomic data, such as aligning reads or calling variants. 
 

### Data disclaimer
The tutorial uses synthetic public-access dataset from the 1,000 Genomes data and stored in the BioData Catalyst and AnVIL instances of Gen3. 

# Instructions 

## Set up tutorial workspace
(1 minute)     

Before you begin, you will need to create your own editable copy (clone) of this WORKSPACE. Click the round circle with three dots in the upper right corner of this page and choose ""Clone"": 
    ![Clone_ScreenShot](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/Clone_workspace_Screen%20Shot.png)
		

### Work from your copy of the workspace following these step-by-step instructions 
|![PDF icon](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/PDF-icon_scaled.png) | Download a clickable PDF of step-by-step instructions [here](https://storage.googleapis.com/terra-featured-workspaces/Terra-201-Gen3-workspace/Terra-201-Gen3-Module.pdf) |  
| --------| ------------------|    

![white space](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/white-space.jpg)    
  
## Step 1. Link your Terra profile with Gen3
(2 minutes)     
If you haven't linked your Terra and Gen3 accounts yet,  you need to follow these [steps](https://support.terra.bio/hc/en-us/articles/360038086332) before you can analyze genomic data from Gen3 inside your Terra workspace.        

![white space](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/white-space.jpg)    

## Step 2. Access the synthetic 1000 Genomes public dataset on Gen3 platform
(5 minutes)     
This section walks through the process of accessing the public-access tutorial dataset (**tutorial-synthetic_data_set_1**) by going to the [Gen3 platform](https://gen3.biodatacatalyst.nhlbi.nih.gov).      

![white space](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/white-space.jpg)    

## Step 3. Export data to workspace    
(10 minutes - mostly exporting time)     
In this step you will practice exporting a Gen3 dataset to Terra. This process will be the same no matter what Gen3 data you are using. As part of this step, you will also learn about the structure of the exported data in a Terra workspace. 
       

## Gen3 data overview
To understand what you will see when you export Gen3 data to a Terra workspace, it helps to first understand how the data is organized in Gen3. An example diagram of a Gen3 Data Dictionary graph structure is shown below on the left. These graphs will vary depending on which Gen3 project instance you're using.

Each box in the tree is a Gen3 data node and each line represents UUIDs connecting the metadata of the two nodes.  Administrative nodes are purple, clinical data nodes are blue, and genomic data nodes are green. When you export a project's data to a Terra workspace, each node in the graph (and all the metadata within that node) gets imported as its own Terra data table. Your data page will look similar to the figure on the right, with each table corresponding to a node in the Dictionary tree.    

| Gen3 graph structure | Gen 3 tables in Terra |    
|-----|--------|      
|![Synthetic Gen3 data overview](https://storage.cloud.google.com/terra-featured-workspaces/BioData%20Catalyst/Gen3-graph-structure-synthetic-data_scaled.png)  | ![Screenshot of tables of Gen3 synthetic data imported to Terra](https://storage.cloud.google.com/terra-featured-workspaces/BioData%20Catalyst/Gen3-synthetic-data-tables_Screen%20Shot.png)  |

You might see from this example several features specific to the Gen3 data structure that can be challenging to work with in Terra:     
* There are separate tables from the export - not all of them include data that you will need        
* There is not a single, human-readable Subject ID that is used across levels. Rather, a UUID connects each of the metadata in each Data Dictionary node to the node directly above it.            

To learn more about the structure of Gen3 data, see [this article](https://support.terra.bio/hc/en-us/articles/360038087312).  

![white space](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/white-space.jpg)    

## Step 4. Consolidate and format exported Gen3 data
(20 minutes)     
In this step you'll run the Notebook **1-Consolidate-Gen3-data-in-Terra**. The tutorial Notebook resolves some of the challenges of Gen3 data exported to Terra. In particular, it consolidates the tables that contain phenotypic data into one Terra table and reorganizes the tables and subject IDs to be more familiar. The Notebook is also a good introduction to interactive analysis in a Jupyter Notebook.     

**Notebook outline**    

* Set up the Notebook environment
* Discover all of the metadata available from the Gen3 export
* Define and use a set of custom functions to      
 * Merge the metadata in a consolidated Pandas dataframe
 * Push the consolidated dataframe to a workspace data table       
     
![white space](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/white-space.jpg)    

## Step 5. Tidy up data tables
(3 minutes)     
Some data tables imported from Gen3 have a LOT of columns, particular data tables containing phenotypic data like age, BMI, height, etc.. To help make a table more manageable to read without scrolling left and right, you can select the columns you want to see when you expand the table. This will allow you to focus on the information you need, without having to scroll across tens of columns. Your selections will persist even when you leave the workspace.          

![white space](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/white-space.jpg)    

## Step 6. Run a workflow on genomic data 
(15 minutes)     

Another mode of analysis in the Terra platform is bulk analysis with workflows. This mode is what you would typically use to do automated analysis steps such as processing genomic data. Bulk analysis uses workflow tools, which you can add to your workspace from another workspace, or by searching Dockstore or the Broad Methods Repository. In this step you will learn how to run a bulk analysis workflow to process genomic data in the synthetic dataset from the Gen3 platform. 

![white space](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/white-space.jpg)    

## Next steps and additional resources
Congratulations! You've accessed and analyzed your first Gen3 data in Terra. Now that you have an idea of the basics of working with Gen3 data in Terra, here are some additional resources to explore.     

1. **[BioDataCatalyst-Gen3-data-on-Terra-Tutorial](https://app.terra.bio/#workspaces/fc-product-demo/BioDataCatalyst-Gen3-data-on-Terra-Tutorial)** 
This workspace is the basis of this Gen3 data tutorial. It contains an example of manipulating Gen3 data using the BioData Catalyst instance of Gen3. It also contains an additional Notebook for analyzing consolidated phenotypic data

2. **[BioData Catalyst Collection](https://app.terra.bio/#workspaces/biodata-catalyst/BioData%20Catalyst%20Collection)** 
This workspace includes the`terra_data_util` notebook that you covered in detail in this tutorial. After you complete this introduction tutorial, we suggest you copy the latest version of `terra_data_util` to your analysis workspace.

![white space](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/white-space.jpg)      

-------
## Authors
Workspace authors: Allie Hajian, Beth Sheets, Liz Kiernan 
 

![white space](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/white-space.jpg)    

## Workspace change log 

| Date | Change | Author | 
| -------  | -------- | -------- |
| October 1, 2020 | This workspace was created based on [BioDataCatalyst-Gen3-data-on-Terra-Tutorial](https://app.terra.bio/#workspaces/fc-product-demo/BioDataCatalyst-Gen3-data-on-Terra-Tutorial) | Allie Hajian, Beth Sheets, and Liz Kiernan |

",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/Terra-201-Gen3-Module"
244,"broad-firecloud-tcga","TCGA_OV_OpenAccess_V1-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_OV_OpenAccess_V1-0_DATA",TRUE,TRUE,"Ovarian serous cystadenocarcinoma","Tumor/Normal","USA","TCGA Ovarian serous cystadenocarcinoma Open-Access Data Workspace","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients’ clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","600","Ovary","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh37/hg19","TCGA_OV_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_OV_OpenAccess_V1-0_DATA"
245,"terra-outreach","CHIP-Detection-Mutect2","READER","https://app.terra.bio/#workspaces/terra-outreach/CHIP-Detection-Mutect2",TRUE,TRUE,NA,NA,NA,"### Introduction
This workspace aims to build upon the GATK4 somatic variant WDL workflow, Mutect2, enabling investigators to perform variant calling and filtering for CHIP data in a consistent and reproducible manner. Users who would benefit from this workspace include investigators interested in the biological implications of CHIP including its role in both malignant and non-malignant disease.

**Background & Experimental Design**

The biological implications of clonal hematopoiesis of indeterminate potential (CHIP) have grown rapidly over the last five years. Somatic variant calling and CHIP specific filtering can be performed from whole genome, whole exome, or targeted sequencing data. 

The setup of this workspace is to run somatic variant calling using the Mutect2 workflow in tumor-only mode with a panel of normals (PON) with the mutect2_pon workflow. Ideally PON samples are from  young patients unlikely to have CHIP variants and have technically similar genome or exome preparation and sequencing technology as the experimental samples. There is no definitive rule for the number of samples to include in a PON, but we recommend at least 40. If a study-specific PON is not available the publically available 1000 Genomes can be used. 

Somatic variant calling for single nucleotide variants (SNVs) and indels is performed by Mutect2 on BAM files along with the PON. General population frequencies are incorporated from gnomAD. Raw somatic SNVs and indels are then filtered using FilterMutectCalls. We recommend carrying forward all variants with filters labeled as references for specific mutations. Except for direct match hotspot driver mutations, we recommend only using variants with the PASS filter for further in silico variant filtering. 

Functotator is applied to these variants on Terra to generate functional annotations. Canonical, best effect or all transcripts can be considered for annotation. Selecting all transcripts allows for a more flexible long term option so that output VCF files may be used for additional analyses. 
It should be noted that there is an existing Funcotator logic [bug](https://github.com/broadinstitute/gatk/issues/6289) that is unresolved and in our experience may impact up to 8% of samples. Alternatives to using Funcotator are VEP or annovar for functional annotation.

Variants are annotated as having CHIP if the output VCF contains one or more of a pre-specified list of putative CHIP variants (see: [Bick et al, 2020](https://www.nature.com/articles/s41586-020-2819-2))

Future iterations of this workflow will include resources for automating the extraction of pathogenic mutations from the annotated variants.


To see the full details about the application of this GATK4 workflow, see: [Bick et al, 2020](https://www.nature.com/articles/s41586-020-2819-2)

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

#### Get Started

For first time Terra users, the following Quickstart videos are recommended for a high-level introduction to the platform:

1. [Introduction to Data Tables](https://www.youtube.com/watch?v=IeLywroCNNA)
2. [Workflows in Terra - Quickstart Demo](https://www.youtube.com/watch?v=rUBrJNqLyfU)

For a full list of videos to learn more about the platform, please refer to this [playlist](https://www.youtube.com/playlist?list=PLh_zJaZ9uQ7P0w6bMLWgL8oDul2EiNlv6).


To actively participate in the workspace, make edits, run analyses, etc, a few prerequisite tasks are required:

1. [Set up your billing](https://support.terra.bio/hc/en-us/articles/360026182251-How-to-set-up-billing-in-Terra)
2. [Clone the workspace](https://support.terra.bio/hc/en-us/articles/360026130851-How-to-clone-a-workspace)

Cloning a workspace (2) creates a copy that can be edited and therefore allows for analysis actions (such as running the `1-Mutect2-GATK4`) which will be billed to the above designated source (1). Continue to the Dashboard in the cloned workspace and follow the steps below to run the `1-Mutect2-GATK4` workflow with the provided example data.

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

#### Data

**What is the example data?**

The data provided in this workspace are hg38 samples taken from the 1000 Genomes Project High Coverage phase 3 panel. There are 10 samples that can be viewed in the Data tab in an individual data tables - if there are multiple tables, they are collectively referred to as the “Data Model”*.

Below is the breakdown of the samples listed in the `sample` table:
*Samples SRS000030-SRS000034 have been synthetically mutated with single nucleotide variants, insertions, and deletions. You can find the details in the Synthetic Data Variants spreadsheet [here.](https://console.cloud.google.com/storage/browser/terra-featured-workspaces/Detecting_CHIP?pageState=(%22StorageObjectListTable%22:(%22f%22:%22%255B%255D%22))&prefix=&forceOnObjectsSortingFiltering=false) BAMSurgeon was used to mutate the data. The WDL can be found [here](https://portal.firecloud.org/?return=terra#methods/methods_from_Tiffany_at_Broad/Mutate-reads-with-BAMSurgeon_ALLvariants/5), snapshot 5.  
*Samples SRS000035-SRS000040 are not mutated.

These ten 1000 Genomes samples can be run with the publically available 1000 Genomes PON. 

**What is the Data Model?**

The Data Model* refers to the set of data tables listed in the Data tab of a Workspace; it is a Terra-centric method to organize, cross-reference, and access your data within the workspace. This workspace is pre-defined with 1 data table:

		1. `sample`: a single entry, or sample, identified by a unique name, which references the associated files that can be used as inputs to the `1-Mutect2-GATK4` Workflow.

*Full documentation (and video) on how to set up your own data tables with your own data can be found [here](https://www.youtube.com/watch?v=IeLywroCNNA&t=20s). Additional information on understanding the types of tables (entities) and how they relate to each other and workflow set-up can be found [here](https://support.terra.bio/hc/en-us/articles/360047046131-Data-Tables-Quickstart-Part-1-Intro-to-data-tables-).

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

#### Workflow
This workspace contains a single Workflow, named `1-Mutect2-GATK4` - accessible from the Workflows tab. Clicking into the workflow will open the “workflow configuration” page from which various required and optional definitions can be set prior to launching a workflow**.

**How to launch the workflow?**

Outlined below are general steps to launch a workflow - full details can be found [here](https://support.terra.bio/hc/en-us/articles/360026521831-How-to-set-up-a-workflow-analysis-):

	1. Select the data table containing your desired input data (“Step 1”).
	2. Select the specific rows/data from the input data table (“Step 2”).
	3. Set the inputs from the “Inputs'' tab. Definitions of all `1-Mutect2-GATK4` inputs can be found here.
	4. Set the outputs from the “Outputs” tab. Definitions of all `1-Mutect2-GATK4` outputs can be found here.
	5. Click Run Analysis.

Once your workflow has been launched, monitor its status from the Job History tab. [This document](https://support.terra.bio/hc/en-us/articles/360037096272-Monitor-and-troubleshoot-in-the-Job-History-tab) outlines how to navigate the Job History tab and learn about the information it provides (workflow status, error logs, etc).

When the workflow has completed, you can view the output files from the `sample` table - visible as hyperlinks in columns denoted with the output variable name.

**In this example, all the definitions have been pre-set as a saved configuration so no modifications are required to run the workflow as is.

**What is the `1-Mutect2-GATK4 workflow`?**

The Mutect2-GATK4 workflow is designed to perform somatic variant calling on a single tumor-normal pair. In this CHIP calling workspace we have it configured to run in tumor-only mode with a PON. Users can supply raw sequencing data in BAM or CRAM format and perform somatic variant calling with pre-specified parameters and filtering. 

The reference genome for this workspace is hg38 (aka GRCh38). Required and optional references and resources for the methods are included in the Workspace Data table.           

Below are the required inputs and generated outputs:

| Inputs | Outputs |
| :---:  | :---: |
| gatk_docker | bamout|
| ref_dict | bamout_index|
| ref_fai | contamination_table|
| ref_fasta | filtered_vcf|
| scatter_count | filtered_vcf_idx|
| tumor_reads | filtering_stats|
| tumor_reads_index | funcotated_file|
|  | funcotated_file_index|
|  | maf_segments|
| | mutect_stats|
| | oncotated_m2_maf|
| | read_orientation_model_params|


*Below are optional inputs and parameters set in workflow configuration:

		1. `list funco_transcript_selection_list`: CHIP_transcripts_74genes_ENST.txt which has the Ensembl transcript IDs for the 74 genes of interest**.
				Users can provide an alternative transcript list or specify ""ALL"", ""CANONICAL"" or ""BEST_EFFECT"" in `funco_transcript_selection_mode`.
				**Jaiswal et al 2017 *N Eng J Med* and Bick et al 2020 *Nature*. 
		2. `filter_funcotations` = `FALSE` and `funco_filter_funcotations` = `FALSE` so all variants are pulled through in the Mutect2 output, not just those that ""PASS"".
					Some large CHIP clones or hotspot variants may be labeled ""germline"" by Mutect2 but may be true CHIP variants that should be considered for downstream analyses.
		4. `funco_default_output_format` = ""VCF"" rather than ""MAF"" to produce an output VCF for downstream filtering and analysis.
		5. `make_bamout` attribute in the workflow config = TRUE. 
 
*Below are estimates on time and cost to run workflow:

| Sample Name | Sample Size | Time | Cost $ |
| :---:  | :---: | :---: | :---: |
| SRS000030 (Mutated) | 59.84 GB | 1:50:00 | $0.10 |
| SRS000035  | 37.53 GB | 1:49:00 | $0.10 |     


**What is the `2-CHIP-Annotation-And-Filtering workflow`?**

The CHIP-Annotation-And-Filtering workflow is designed to annotate the VCF file output from the Mutect2-GATK4 worfklow with annovar followed by a step that filters the VCF file for CHIP mutations using R code that identifies chip driver mutations.

The reference genome for this workspace is hg38 (aka GRCh38). Required and optional references and resources for the methods are included in the Workspace Data table.

| Inputs | Outputs |
| :---:  | :---: |
| annovar_zip | annovar_annotated_file_table |
| whitelist_filter_zip | annovar_annotated_file_vcf |
| annovar_vcf_input | whitelist_filter_output_allvariants |
| sample_id | whitelist_filter_output_manual_review |
| | whitelist_filter_output_varcount |
| | whitelist_filter_output_wl |

*Below are estimates on time and cost to run workflow:

| Sample Name | Sample Size | Time | Cost $ |
| :---:  | :---: | :---: | :---: |
| SRS000030 (Mutated) | 59.84 GB | 0:07:00 | < $0.01 |
| SRS000035  | 37.53 GB | 0:08:00 | < $0.01 |


For helpful hints on controlling Cloud costs, see this article ([click here to view](https://support.terra.bio/hc/en-us/articles/360029748111)).

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

#### Notebooks
This workspace contains a single Jupyter notebook, named `Join Output for Mutect2 (with ANNOVAR and Whitelist Filter).ipynb` - accessible from the Notebooks tab. Clicking into the notebook will show a preview of the notebook along with an option to ""Edit"" the contents (also to execute the code). The ""Edit"" option will reveal a widget to ""Create"" a runtime environement with either Default configurations or Custom options.

**How to launch the notebook?**

Outlined below are general steps to launch a notebook - full details can be found [here](https://support.terra.bio/hc/en-us/articles/360024898671-Interactive-statistics-and-visualization-with-Jupyter-notebooks):

	1. Select the Notebook by clicking the title.
	2. Click on the ""Edit"" button along the top of the notebook interfact.
	3. When the Runtime Environment widget appears, press ""Create"" next to the ""Use Default Environement""..
	5. Wait for the runtime environment to finish setting up after which you can execute or edit the notebook contents.

Once your notebook runtime environment has been launched, you can execute the cells of the notebook individually with (`ctrl + shift`) or execute the full notebook (using `Run All` from the `Cell` menu).


**What is the `Join Output for Mutect2 (with ANNOVAR and Whitelist Filter) notebook`?**

The Join Output for Mutect2 (with ANNOVAR and Whitelist Filter) notebook Jupyter notebook is designed to combine/concatenate the csv output files generated by the CHIP-Annotation-And-Filtering workflow across multiple samples.

The notebook cells also contain detailed instructions on set-up/information that is required to run the cells of the notebook as well as how to find and replace the example input variable. The outputs of the notebook will be placed into the directory that is used as input.

| Inputs | Outputs |
| :---:  | :---: |
| main_id_submission_id_path | filtered_annovar_wl.csv |
| | filtered_annovar_manualreview.csv |
| | filtered_annovar_varcount.csv |
| | filtered_annovar_allvariants.csv |

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

#### License + Contact Information
Copyright Broad Institute, 2020 | BSD-3
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at https://github.com/openwdl/wdl/blob/master/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.

To cite this workspace/workflow:

			Bick, A., Regan, J. (2020, December 16) CHIP-Detection-Mutect2, https://app.terra.bio/#workspaces/terra-outreach/CHIP-Detection-Mutect2

For general questions about getting started and working in Terra, please contact us via:

1. email: support@terra.bio
2. post on our [Community Forum](https://support.terra.bio/hc/en-us/community/topics)
3. use our Contact Us in-app feature (accessible from the main menu).

For questions that are specific to the algorithm or science, please contact:

1. Alex Bick (alexander.bick@vumc.org)
2. Jessica Regan (jessica.a.regan@duke.edu).


Acknowledgements:

The workflow developers 
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","terra-outreach/CHIP-Detection-Mutect2"
246,"featured-workspaces-hca","HCA_Optimus_Pipeline_v3-0-0","READER","https://app.terra.bio/#workspaces/featured-workspaces-hca/HCA_Optimus_Pipeline_v3-0-0",TRUE,FALSE,NA,NA,NA,"# Optimus Pipeline for Analysis of 3’ Single Cell Transcriptomic Data

The Optimus pipeline, developed by the Data Coordination Platform team of the [Human Cell Atlas](https://www.humancellatlas.org/) (HCA DCP), processes 3 prime single-cell transcriptome data from the [10x Genomics v2 or v3](https://www.10xgenomics.com/solutions/single-cell/) assay. This workspace currently describes `v3.0.0` of the Optimus pipeline and provides a fully reproducible example of the workflow. You can access previous versions of this pipeline by cloning the workspace and choosing a version in the version dropdown. For more details about the pipeline, see the [Optimus README](https://github.com/HumanCellAtlas/skylab/tree/master/pipelines/optimus) in GitHub.  A high-level overview of the pipeline can also be found on the [HCA Data Portal](https://prod.data.humancellatlas.org/).

Optimus has been validated for analyzing both [human](https://github.com/HumanCellAtlas/skylab/blob/master/benchmarking/optimus/optimus_report.rst) and [mouse](https://docs.google.com/document/d/1_3oO0ZQSrwEoe6D3GgKdSmAQ9qkzH_7wrE7x6_deL10/edit) datasets. More details about the human validation can be found in the [in the original report](https://docs.google.com/document/d/158ba_xQM9AYyu8VcLWsIvSoEYps6PQhgddTr9H0BFmY/edit).

Scroll down for details on the workflow, including input and output descriptions and requirements, estimated run times and costs, and information on sample data.       

**Note that cost and time estimates will vary** with the use of [Preemptibles](https://gatkforums.broadinstitute.org/firecloud/discussion/11786/preemptibles#latest).  
You can also use [Google's cost calculator](https://cloud.google.com/products/calculator/) to estimate cost to run.  **For helpful hints on controlling Cloud costs**, see [this article](https://support.terra.bio/hc/en-us/articles/360029748111).  

---
---

##  Optimus 

### What does it do?   

This Optimus workflow has a quality control, alignment and transcriptome quantification module. It corrects Cell Barcodes (CBs) and Unique Molecular Identifiers (UMIs), aligns reads to the genome, generates an expression matrix in a UMI-aware manner, detects empty droplets, calculates summary metrics for genes and cells, detects empty droplets, returns read outputs in BAM format, and returns cell gene expression in numpy matrix and Loom file formats. Special care is taken to avoid the removal of reads that are not aligned or that do not contain recognizable barcodes. This design provides flexibility to the downstream user and allows for alternative filtering or leveraging the data for novel methodological development.

### What does it require as input?

The Optimus workflow requires the following input:

| Name       |  Description      |
| ------------ | ------------------- |
| r1_fastq | Forward read, contains the unique molecular identifier (UMI) and cell barcode sequences |
| r2_fastq | Reverse read, contains the alignable genomic information from the mRNA transcript |
| sample_id | Unique name describing the biological sample or replicate that corresponds with the original FASTQ files (must be entered as a String) |
| tar_star_reference | TAR file containing a species-specific reference genome and gtf; it is generated using the[ StarMkRef.wdl](https://github.com/HumanCellAtlas/skylab/blob/master/library/accessory_workflows/build_star_reference/BuildStarReferenceBundle.wdl)  |
| annotations_gtf |  GTF containing gene annotations used for gene tagging (must match GTF in STAR reference) |
| ref_genome_fasta | Genome fasta file (must match star reference and organism) |
| whitelist | [10x Genomics](https://www.10xgenomics.com/) cell barcode whitelist for 10x v2 or v3 | 
| i1_fastq | Index read, optional when analyzing v2 samples but required for v3 samples | 


### Optional Parameters 

The Optimus workflow offers optional inputs such as the following:

| Name       |  Description      |
| ------------ | ------------------- |
| i1_fastq | Index read,  optional  for v2 samples | 
| fastq_suffix | Optional suffix for files that allows automatic detection of compression in some execution environments |
| output_bam_basename | Optional string used as a prefix to the output BAM file; default is <sample_id> |
| chemistry | Optional string description of whether data was generated with 10x v2 or v3 chemistry; the default is ""tenX_V2"" |

### What does it return as output?

In this workspace, the  metadata for all outputs (including the optional Loom file) are written to the workspace data table. These outputs are described in the following table:  

| Name       |  Description      |
| ------------ | ------------------- |
| pipeline_version | Version of the pipeline |
| bam | Merged and sorted BAM file |
| matrix | Sparse count matrix in numpy format |
| matrix_row_index | Sparse count matrix row names in numpy format |
| matrix_col_index | Sparse count matrix column names in numpy format |
| cell_metrics  | Cell metrics table in text format |
| gene_metrics | Gene metrics table in text format | 
| cell_calls | Cell metadata from empytDrops |
| loom_output_file | Count matrix and cell and gene metrics |

\
The final Loom file contains UMI-corrected counts, as well as the cell and gene metrics detailed in the [Loom schema documentation](https://github.com/HumanCellAtlas/skylab/blob/master/pipelines/optimus/Loom_schema.md).  Although the expression matrices are UMI-corrected, they are unfiltered in order to provide all possible data to the end researcher. 

**Zarr array deprecation notice**: As of June 2020 (Optimus v.3.0.0), the previously used Zarr array has been deprecated. The Loom file is now the new default output.

### Sample data description and location    

This workspace contains 10x v2 test data for both human (pbmc4k_human) and mouse (neurons2k_mouse) as well as 10x v3 test data for human (pbmc_human_v3). These test data  downsampled FASTQs from the [10x Genomics](https://www.10xgenomics.com/) [PBMC4k human dataset](https://support.10xgenomics.com/single-cell-gene-expression/datasets/2.0.1/pbmc4k), the mouse [neuron 2k dataset](https://support.10xgenomics.com/single-cell-gene-expression/datasets/2.1.0/neurons_2000), and the [PMBC_10K_V3 human dataset](https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/pbmc_10k_v30). The mouse dataset is restricted to genic regions of chromosome 19 and the human datasets  are restricted to chromosome 21.

The location and description of sample inputs (sample_id, r1_fastq, r2_fastq, and i1_fastq) are provided in the Sample table of the workspace Data tab. The Optimus workflow  accepts sample-demultiplexed unaligned FASTQ files as input. The pipeline is a single sample pipeline, but can take in multiple sets of FASTQs for a sample that has been split over lanes of sequencing. 

### Reference data description and location  

The required reference genomes (human and mouse) and additional resources for the tools in this workspace are included in the Workspace Data table. The reference genome for human is hg38 (GRCh38), the [GENCODE v27](https://www.gencodegenes.org/human/release_27.html) primary assembly gene annotation list. The reference for mouse is GRCm39, the [GENCODE M21](https://www.gencodegenes.org/mouse/release_M21.html). There are two 10x cell barcode whitelists: ""whitelist_v2"" which is compatible with the 10x Genomics v2 chemistry and ""whitelist_v3"" which is compatible with 10x Genomics v3 chemistry. 


### Estimated time and cost to run on sample data 

The following estimates are based on three sets of data, human and mouse, each containing different numbers of samples. All details of each set are listed to give insight into time and cost.
 
| Sample Set Name | Set Size | Sample Set R1.fastq Size | Sample Set R2.fastq Size | Time | Cost $ |
| :---:  | :---: | :---: | :---: | :---: | :---: |
| neurons2k_mouse | 6 entities | 88.26 MB | 277.58 MB | 2:20:00 | 0.96 | 
| pbmc4k_human | 2 entities | 26.84 MB | 59.58 MB | 1:53:00 | 0.68 |
| pbmc_human_v3 | 2 entities | 106.95 MB | 220.04 MB | 4:03:00 | 1.31 |

 
**For helpful hints on controlling Cloud costs, see this article ([click here to view](https://support.terra.bio/hc/en-us/articles/360029748111)).**   

### Versions

All versions listed here are available by cloning this workspace and selecting the version on the Optimus method. Other versions are listed as available, but only the versions below will be compatible with Terra. For a complete version list, please read the [Optimus changelog in GitHub](https://github.com/HumanCellAtlas/skylab/blob/master/pipelines/optimus/Optimus.changelog.md)

| Terra Compatible Version Name | Optimus Release Version | Date | Release Note | 
| :---:  | :---: | :---: | :--- |
| optimus_v3.0.0_terra | v3.0.0 (current) | 06/10/2020 | Removed Zarr arrays as default output, added mitochondrial read metrics, added code to be used for snRNAseq function, added an optional string input to name the output BAM file (default is <sample_id>. |
| optimus_v2.0.0_terra | v2.0.0 (current) | 03/05/2020 | Updated the workflow to WDL 1.0. Fixed an emptyDrops bug causing incorrect emptyDrops output. |
| optimus_v1.4.0_terra | v1.4.0 | 11/08/2019 | Added support for V3 chemistry, addition of input parameter validation step, greatly improved documentation, improvements to Zarr output. |
| optimus_1.3.6_terra_patch2 | v1.3.6 | 09/23/2019 | Added the option to output a Loom formatted count matrix, which is turned on in this workspace's method configuration. |
| optimus_v1.3.3_terra | v1.3.3 | 08/29/2019 | This version and newer have been validated to additionally support mouse data using the GENCODE M21 reference. The gene expression per cell is now counted by GENCODE geneID instead of gene name. There is an additional output mapping geneID to gene name provided. This is a breaking change. | 
| terra-optimus | v1.0.0 |03/30/2019 | Initial pipeline release. Validated on hg38 GENCODE v27. | 

---
---

### Questions and Feedback
Please post workspace questions and feedback to the [Featured Workspaces community forum](https://support.terra.bio/hc/en-us/community/topics/360001603491) (login required). Tag @Kylee Degatano in the “Details” section of your post.   

---
---

### License
**Copyright Human Cell Atlas Authors, https://humancellatlas.org, 2019 | BSD-3**

All rights reserved. Full license text at https://github.com/HumanCellAtlas/skylab/blob/master/LICENSE. Note that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.

---




",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","featured-workspaces-hca/HCA_Optimus_Pipeline_v3-0-0"
247,"broad-firecloud-tcga","TCGA_BRCA_hg38_OpenAccess_GDCDR-12-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_BRCA_hg38_OpenAccess_GDCDR-12-0_DATA",TRUE,TRUE,"Breast Invasive Carcinoma","Tumor/Normal","USA","TCGA Breast invasive carcinoma Open-Access Data Workspace

*hg38 TCGA and TARGET workspaces reference files by their GDC UUIDs.  In order to run analyses on the referenced data files, you will need to run workflows that retrieve the files from the GDC and copy them to your workspace bucket.  See   [this forum post](https://gatkforums.broadinstitute.org/firecloud/discussion/10382/populating-hg38-tcga-and-target-workspaces-with-data-files#latest) for instructions on the running of these workflows.*","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients' clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","1098","Breast","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh38/hg38","TCGA_BRCA_hg38_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_BRCA_hg38_OpenAccess_GDCDR-12-0_DATA"
248,"use-strides","bioc-osca-3-14","OWNER","https://app.terra.bio/#workspaces/use-strides/bioc-osca-3-14",TRUE,FALSE,NA,NA,NA,"We provide resources to compute with the [Orchestrating Single Cell Analysis book](http://bioconductor.org/books/3.14/OSCA/).  Use at least an 8 core machine to
acquire packages and run workflows described in this workspace.  Information on execution profiles (how much time and memory  book-based jobs consume) can be seen at the bottom of the description.

## About the book

The book has a few large sections:

- [Introduction](http://bioconductor.org/books/3.14/OSCA.intro)
- [Basics](http://bioconductor.org/books/3.14/OSCA.basic)
- [Advanced tasks](http://bioconductor.org/books/3.14/OSCA.advanced)
- [Workflows](http://bioconductor.org/books/3.14/OSCA.workflows)
- [Multisample tasks](http://bioconductor.org/books/3.14/OSCA.multisample)

Each section has multiple chapters and code chunks that you will be able to evaluate in this workspace, once the
package environment is properly set up.  See `Acquire all packages...` below to accomplish this setup.

Many code chunks require certain antecedent computations to be completed before they will run.
You may have to hunt back through preceding chunks to find the antecedents.  If this proves
impossible, please file a [detailed bug report](https://github.com/vjcitn/osca4anvil/issues/new).

## Taster

After following the setup instructions below, you can run the code snippet given towards the end of this
description in Rstudio.  In about 3 minutes, you will have acquired data on 4000 PBMCs published by TENxGenomics,
performed basic QC and filtering, and computed projections and cluster assignments to yield

![4000 cells](https://storage.googleapis.com/bioc-anvil-images/demoTSNE.png)

## Simple setup: custom environment

Use `vjcitn/osca4anvil:3.14.0` as a custom environment in the AnVIL environment widget.

![cloudenv](https://storage.googleapis.com/bioc-anvil-images/cloudenv.png)



## Example

Once we have installed all the packages using the code chunk just above, we can get the following
example done in about 3 minutes.  This is from [OSCA.workflows](http://bioconductor.org/books/3.14/OSCA.workflows/unfiltered-human-pbmcs-10x-genomics.html#unfiltered-human-pbmcs-10x-genomics) demonstrating end-to-end
analysis of 4000 unfiltered PBMCs.

```
# acquire data (caching if necessary) and create SingleCellExperiment

library(DropletTestFiles)
raw.path <- getTestFile(""tenx-2.1.0-pbmc4k/1.0.0/raw.tar.gz"")
out.path <- file.path(tempdir(), ""pbmc4k"")
untar(raw.path, exdir=out.path)

library(DropletUtils)
fname <- file.path(out.path, ""raw_gene_bc_matrices/GRCh38"")
sce.pbmc <- read10xCounts(fname, col.names=TRUE)

# identify cells with high abundance of mitochondrial genes

library(scater)
rownames(sce.pbmc) <- uniquifyFeatureNames(
  rowData(sce.pbmc)$ID, rowData(sce.pbmc)$Symbol)

library(EnsDb.Hsapiens.v86)
location <- mapIds(EnsDb.Hsapiens.v86, keys=rowData(sce.pbmc)$ID, 
                   column=""SEQNAME"", keytype=""GENEID"")

# perform cell detection

set.seed(100)
e.out <- emptyDrops(counts(sce.pbmc))
sce.pbmc <- sce.pbmc[,which(e.out$FDR <= 0.001)]

unfiltered <- sce.pbmc

# remove high-mito cells

stats <- perCellQCMetrics(sce.pbmc, subsets=list(Mito=which(location==""MT"")))
high.mito <- isOutlier(stats$subsets_Mito_percent, type=""higher"")
sce.pbmc <- sce.pbmc[,!high.mito]

# normalize

library(scran)
set.seed(1000)
clusters <- quickCluster(sce.pbmc)
sce.pbmc <- computeSumFactors(sce.pbmc, cluster=clusters)
sce.pbmc <- logNormCounts(sce.pbmc)

# identify highly variable genes

set.seed(1001)
dec.pbmc <- modelGeneVarByPoisson(sce.pbmc)
top.pbmc <- getTopHVGs(dec.pbmc, prop=0.1)

# project; cluster

set.seed(10000)
sce.pbmc <- denoisePCA(sce.pbmc, subset.row=top.pbmc, technical=dec.pbmc)

set.seed(100000)
sce.pbmc <- runTSNE(sce.pbmc, dimred=""PCA"")

set.seed(1000000)
sce.pbmc <- runUMAP(sce.pbmc, dimred=""PCA"")

g <- buildSNNGraph(sce.pbmc, k=10, use.dimred = 'PCA')
clust <- igraph::cluster_walktrap(g)$membership
colLabels(sce.pbmc) <- factor(clust)

# visualize

plotTSNE(sce.pbmc, colour_by=""label"")

```

## Execution profile

The following graphic illustrates the use of CPU, memory, disk, and network in the
execution of the script given above on an 8 core, 30GB Terra instance.

![exec profile](https://storage.googleapis.com/bioc-anvil-images/prof1.png)

Of note is the total memory consumption of around 4GB (units are kilobytes), and the occurrence of
short events in which the baseline CPU consumption rose from around 1/8 (one of 8 cores) to around 1 (all 8 cores mobilized).
The analysis of RAM usage by the specific tasks undertaken involves subtracting the maximum from minimum
displayed.   This indicates that RAM usage over the baseline required by Rstudio and
basic package loading never exceeded 1.5.GB.

## Using the default environment (instead of custom container)

This approach avoids the use of a custom container but requires significant setup efforts.

### Acquire helper package

```
BiocManager::install(c(""AnVIL"", ""vjcitn/osca4anvil""), ask=FALSE)

library(osca4anvil)
```

### Investigate the packages needed to run computations from OSCA book

The book is defined by five high-level R packages.  The DESCRIPTION files of these packages
define section-specific dependencies, and when these and their downstream dependencies
are satisfied,  we require hundreds of packages to be available.

```
show_packs()
```

This produces a hyperlinked table with 436 entries.  Links point to landing pages at bioconductor.org or CRAN.

Examples of interest are the landing page for [scRNAseq](https://bioconductor.org/packages/3.14/data/experiment/html/scRNAseq.html) and the
associated [vignette](https://bioconductor.org/packages/3.14/data/experiment/vignettes/scRNAseq/inst/doc/scRNAseq.html), both specific to
Bioconductor 3.14.

### Acquire all packages needed to run computations from the book

Strictly speaking, the process defined below is not necessary.  A reader can attempt a computation,
and it may fail owing to the lack of a package.  One can then use `AnVIL::install` to obtain
the missing package.  But this process may recur frequently.  Thus we advocate the following
steps to be taken, which will take some time to complete, but will provide all packages
needed to perform any computational example in the book.

```
clone_osca(""RELEASE_3_14"") # be sure the argument is consistent with the cloud environment in use

lapply(packs(), get_deps, installer=AnVIL::install)  # Ncpus argument?
```

",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","use-strides/bioc-osca-3-14"
249,"help-gatk","GATKTutorials-Somatic-July2019","READER","https://app.terra.bio/#workspaces/help-gatk/GATKTutorials-Somatic-July2019",TRUE,FALSE,NA,NA,NA,"# What's in this workspace?
Welcome to Day 3 of the Genome Analysis Toolkit (GATK) workshop at the University of Cambridge in Cambridge, U.K.! Today we will focus on Somatic Variant Discovery.

Earlier today you received introductions to GATK tools and Best Practices pipelines. In this workspace we will be going over two forms of Somatic Analysis: one comparing tumor and normal samples using Mutect2 workflow for variant differences, and another using the Copy Number Alterations (CNA) workflow for copy number variations. 

Your instructor will guide you through this workspace, but you should find the documentation within the notebooks sufficient to run through them again on your own or to share with a friend later (and we encourage you to do so!).

### Data
Data associated with this workspace is located in the [gs://gatk-tutorials/workshop_1907/3-somatic](https://console.cloud.google.com/storage/browser/gatk-tutorials/workshop_1907/3-somatic/?project=broad-dsde-outreach&organizationId=548622027621) google bucket. It contains both input and resource files for the Mutect2 and CNA workflows. Along with the inputs are precomputed outputs generated by each step in the tutorial. This can be used as input for any step in the workflow, in case your generated outputs are not correct or you are unable to complete a step in the workflow. 

### Tools
There are no tools in this workspace. The tutorials are notebook-based, allowing you to run each step manually and view the intermediate outputs of the workflow. This is a great way to understand each step in the workflow and give you the chance to manipulate the parameters to see what happens with the output.

If you are interested in a WDL based workflow of these analyses, check out the [Showcase](https://app.terra.bio/#library/showcase) area in Terra, which features many of our popular GATK workflows in workspaces ready to run your data.


### Notebooks
 **1-somatic-mutect2-tutorial :**
In this hands-on tutorial, we will call somatic short mutations, both single nucleotide and indels, using the GATK4 Mutect2 and FilterMutectCalls. If you need a primer on what somatic calling is about, see the following [GATK forum Article](https://software.broadinstitute.org/gatk/documentation/article?id=11127).


| Runtime Environments | Value |
| --- | --- |
| CPU Minimum| 4|
| Disksize Minimum | 100 GB |
| Memory Minimum | 15 GB |
| Startup Script | gs://gatk-tutorials/scripts/install_gatk_4110_with_igv.sh |

**2-somatic-cna-tutorial :**
This hands-on tutorial outlines steps to detect alterations with high sensitivity in total and allelic copy ratios using GATK4's ModelSegments CNV workflow. The workflow is suitable for detecting somatic copy ratio alterations, more familiarly copy number alterations (CNAs), or copy number variants (CNVs) for whole genomes and targeted exomes.

| Runtime Environments | Value |
| --- | --- |
| CPU Minimum| 2|
| Disksize Minimum | 100 GB |
| Memory Minimum | 13 GB |
| Startup Script | gs://gatk-tutorials/scripts/install_gatk_4110_with_igv.sh |

 
### Software versions
GATK4.1.1.0

### Time and cost 
This workspace focuses on the use of notebooks, thus the time and cost of completing the tutorial depends on the runtime environment of your notebook and the length of time you spend actively working in the notebook. With the given runtime environments above, users can expect the cost to be much less than a dollar an hour for each notebook. The specific cost will be displayed on the top right corner of your screen, next to the notebook machine options. 

## Appendix

### GATK @ Cambridge 2019 in the Terra Support Center

Following the conclusion of the workshop, the workshop materials will be made available in the Terra Support Center. For now, you can find these materials in a google drive folder at [broad.io/GATK1907](https://broad.io/GATK1907)


### GATK documentation and support

Detailed documentation, tutorials, and more are available at the [GATK web site](https://software.broadinstitute.org/gatk/). If you are still running into issues after consulting the User Guide, have a question, or just want to say hello, reach out to the GATK team via the [GATK Support Forum](https://gatkforums.broadinstitute.org/gatk)!

### Terra documentation and support

Documents and helpful guides to support your journey through Terra are available in the Terra Support Center. Navigate there by following the “Learn About Terra” link in the left-side menu. At the time of this workshop, we are still actively writing and publishing documents but we’ve got a good set of docs in the Quick Start Guide to get you going.


",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/GATKTutorials-Somatic-July2019"
250,"help-gatk","Whole-Genome-Analysis-Pipeline","READER","https://app.terra.bio/#workspaces/help-gatk/Whole-Genome-Analysis-Pipeline",TRUE,FALSE,NA,NA,NA,"### GATK Best Practices for Germline SNPs and Indels as used at the Broad Institute

**Note: This workspace is superseded by the [Whole-Genome-Analysis-Pipeline](https://app.terra.bio/#workspaces/warp-pipelines/Whole-Genome-Analysis-Pipeline) workspace and is no longer being updated. Please visit the new workspace for the latest version of the pipeline**

A fully reproducible example of data pre-processing and germline short variant discovery. This workspace holds Broads production sequence processing pipeline which contains several quality control tasks within the workflow in addition to regular data processing tasks. The workflow takes unmapped pair-end sequencing data ([unmapped BAM format](https://software.broadinstitute.org/gatk/documentation/article?id=11008)) and returns a GVCF and other metrics read for joint genotyping. 

**Workspace Notes:**  

- This workspace is supersedes the [five-dollar-genome-analysis-pipeline](https://app.terra.bio/#workspaces/help-gatk/five-dollar-genome-analysis-pipeline) workspace.
- Users viewing this workspace may also be interested in viewing the [Sequence-Format-Conversion](https://app.terra.bio/#workspaces/help-gatk/Sequence-Format-Conversion) workspace which contains several file formatting workflows 
- User may also be interested in the [Joint Genotyping](https://dockstore.org/workflows/github.com/gatk-workflows/gatk4-germline-snps-indels/JointGenotyping:2.2.0?tab=info) workflow which creates a VCF using 50 or more whole genome GVCFs.

A description of the workflow is available in [Gatk's Best Practices Document](https://gatk.broadinstitute.org/hc/en-us/sections/360007226651-Best-Practices-Workflows).

## Workflows

There are two workflows listed in the workflows tab, their wdl scripts are identical but their workflow configurations are different. WholeGenomeGermlineSingleSample_file_path is configured so that the input files or parameters need to be directly entered into configurations, which is useful when needing to quickly process a single sample. WholeGenomeGermlineSingleSample_Data_Table is configured so that its possible to use samples listed in the data tables in the Data Tab, which is useful when processing several samples. 

### How to run the workflow?

The workflow is written in WDL1.0 and imports structs to organize and use inputs. To run WholeGenomeGermlineSingleSample_file_path users will need to manually enter the workflow attributes for each variable. The easiest way to configure the workflow would be to download the following [input json template](https://github.com/gatk-workflows/gatk4-genome-processing-pipeline), edit the variables to point to the proper input and reference files as needed, and upload the json in the workflow configuration window. The option to upload the json will be on the right side of the screen as seen in this example:  
![image](https://storage.googleapis.com/terra-featured-workspaces/Whole-Genome-Analysis-Pipeline/Upload_json.png)  
Once the json file has been uploaded click on ""Run Analysis"" to launch the workflow. 

To run  WholeGenomeGermlineSingleSample_Data_Table workflow version you'll need to first add ubam files to the read_group data table. The columns in this table is described as such: 
* Read_group = Read group + sample name (There is no strict naming convention but each row should be uniquely named)
* flowcell_unmapped_bams_list = File paths to the unmapped bam files
* Sample = The sample the unmapped bam files belongs to

Once the ubam files are in the Read_group data table select the samples to be processed in the workflow  configurations by selecting them as a read_group set.  
![](https://storage.googleapis.com/terra-featured-workspaces/Whole-Genome-Analysis-Pipeline/Select_Data.png)  
Then select ""Create a new set from selected read_groups"" and select all rows belonging to one sample.  
![](https://storage.googleapis.com/terra-featured-workspaces/Whole-Genome-Analysis-Pipeline/Create_a_new_set.png)  
Once the input files have been selected click on ""Run Analysis"" to launch the workflow. 

### WholeGenomeGermlineSingleSample

**What does it do?**     
This WDL pipeline implements data pre-processing and initial variant calling (GVCFgeneration) according to the GATK Best Practices for germline SNP and Indel discovery in human whole-genome sequencing data .

**What data does it require as input?**    
This workflow accepts a sample of human, whole-genome paired-end sequencing data in unmapped BAM (uBAM) format:    
- One or more read groups, one per uBAM file, all belonging to a single sample (SM)
- Input uBAM files must additionally comply with the following requirements:
  - filenames all have the same suffix (we use "".unmapped.bam"")
  - files must pass validation by ValidateSamFile
  - reads are provided in query-sorted order
  - all reads must have an RG tag
- Reference genome must be Hg38 with ALT contigs

**What does it return as output?**     
The following files are stored in the workspace Google bucket.    
- CRAM file, CRAM index, and CRAM md5 
- GVCF and its gvcf index 
- BQSR Report
- Several Summary Metrics       

**Sample data description and location**    
The workflow in this workspace is preconfigured with the small unaligned test data below, you can use this as a template when setting up the workflow to run on your own data. 

* Small unaligned bam list of NA12878
* Unaligned bam list of NA12878

**Reference data description and location**     
Required and optional references and resources for the workflow are set in the workflow configurations. The reference genome for this workspace is hg38 (aka GRCh38).     

**Time and cost estimates**    

Below is an example of the time and cost for running the workflow.

| Sample Name | Sample Size | Time | Cost $ |
| ---  | :---: | :---: | :---: |
| NA12878_24RG_small | 3.11 GB | ~4:55:00 | ~$0.96 |
| NA12878 | 64.89 GB | ~45:01:00 | ~$7.71 |

**Workflow Note:** 
- Cost and time will vary with the use of [Preemptibles](https://cromwell.readthedocs.io/en/stable/RuntimeAttributes/#preemptible).    
  - For more information about controlling Cloud costs, see [this article](https://support.terra.bio/hc/en-us/articles/360029748111).
- By default, HaplotypeCaller will perform variant calling using GATK 3.5, which is what is used in Broad Production. To use GATK4, specify use_gatk3_haplotype_caller=false in the workflow configurations.


**Software Version**      
- GATK 4 or later
- GATK 3 for variant calling but option exist to enable GATK4
- BWA 0.7.15-r1140
- Picard 2.16.0-SNAPSHOT
- Samtools 1.3.1 (using htslib 1.3.1)
- Python 2.7

---


---

### Contact information  
For questions about this workspace please visit the Featured Workspaces Topic topic on the [Terra Community Forum](https://broadinstitute.zendesk.com/hc/en-us/community/topics). Use the search box to see if other users have asked the same question previously. If not, post and tag **@Beri Shifaw** so that we get notified.

Please post any questions or concerns regarding the GATK tool to the [GATK forum](https://gatk.broadinstitute.org/hc/en-us/community/topics)

### License  
**Copyright Broad Institute, 2019 | BSD-3**  
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at https://github.com/openwdl/wdl/blob/master/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.

### Workspace Change Log
| Date | Change | Author |
| --- | --- | --- |
|  2020-01-26 | Updated workflow, Release notes found [here](https://github.com/gatk-workflows/gatk4-genome-processing-pipeline/releases/tag/1.0.0) | Beri Shifaw |
|  2020-04-23 | Updated workflow, Release notes found [here](https://github.com/gatk-workflows/gatk4-genome-processing-pipeline/releases/tag/1.1.0) | Beri Shifaw |
|  2020-07-22 | Updated workflow, Release notes found [here](https://github.com/gatk-workflows/gatk4-genome-processing-pipeline/releases/tag/1.2.0) | Beri Shifaw |
|  2020-10-09 | Updated workflow, Release notes found [here](https://github.com/gatk-workflows/gatk4-genome-processing-pipeline/releases/tag/1.3.0). Added ""How to Run Workflow"" section in dashboard. Added two ways to run the workflow. Added read_group tables in Data tab  | Beri Shifaw |
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/Whole-Genome-Analysis-Pipeline"
251,"kco-tech","Cumulus","READER","https://app.terra.bio/#workspaces/kco-tech/Cumulus",TRUE,FALSE,NA,NA,NA,"## Single-Cell RNA-Seq Analysis using Cumulus



This workspace is a showcase of [Cumulus](https://cumulus.readthedocs.io), a cloud-based single-cell/single-nucleus data analysis framework. It uses a large-scale single-cell dataset, and demonstrates Cumulus on both workflow and interactive analysis.

Users can simply clone this workspace, and reproduce this showcase on their own by following the instructions below.
 

------------------

## Contents   
(Scroll down to find the following sections)     

* Data Table
* Workflow
* Interactive Analysis
* Data Visualizer
* More Materials & Contact Info
* License & Change Log

------------

### Data  Table

The bone marrow dataset in the [Census of Immune Cells](https://data.humancellatlas.org/explore/projects/cc95ff89-2e68-4a08-a234-480eca21ce79) study consists of 63 10x Genomics channels (v2 chemistry) collected from 8 donors.

We picked 8 channels (one channel per donor) from the bone marrow dataset as our test dataset.

Information of  FASTQ files of the 8 channels are listed in `sample` table of `DATA` tab of this workspace. This table is critical for running workflows.

If you create your own workspace by cloning from this one, you'll need to update the Google bucket URLs in `sample` table to refer to your own workspace. Below are the steps:

1. Look for Google bucket ID of your workspace. You can get it from the bottom of the right panel of this page (CLOUD INFORMATION -> Bucket Name), which starts with ""fc-"" and follows by a sequence of heximal numbers.
2. Open `sample` table in `DATA` tab, then replace the GS URLs in `Cellranger_output_directory` and `Analysis_output_directory` by yours. For example, if your workspace's GS ID is `gs://xxx`, then replace `Cellranger_output_directory` from `gs://yyy/cellranger_output` to `gs://xxx/cellranger_output`. Similarly for `Analysis_output_directory`.

--------------

### Workflow

There are 2 steps for the analysis:

* Generate RNA gene-count matrices. (Cellranger)
* Process the gene-count matrices for single-cell RNA-seq analysis, including data aggregation, quality-control, dimension reduction, clustering analysis, visualization, differential expression analysis, cell-type annotation, etc. (Cumulus)

Each step uses one Cumulus workflow.

In this section, we'll first provide a quick guide on running workflows, then discuss advanced settings on each step.

#### Quick Guide

1. Follow the two steps in ""Data Table"" section to redirect the output to Google bucket of your own workspace.
2. Open `Cellranger` configuration page from `WORKFLOWS` tab. Then click `SELECT DATA` button and select the only entry in the opened table. After that, click `RUN ANALYSIS` botton. Then the workflow will use the information from `sample` table to run the job.
3. Open `Cumulus` configuration page from `WORKFLOWS` tab. Then click `SELECT DATA` button and select the only entry in the opened table. After that, click `RUN ANALYSIS` botton. Then the workflow will use the information from `sample` table to run the job.

When finished, you can find Cellranger workflow output at `cellranger_output` folder on your workspace, and Cumulus output at `analysis_output` folder.

Notice that by using the preset configurations to run these workflows, you are actually using input files from the original public workspace.

A runtime summary is the following:

|Step|CPU|Memory|Time|Cost|
|---|---|---|:---:|---|
|cellranger_workflow|32 * 8|120 GB * 8|1h34min|$2.65|
|cumulus|32|200 GB|22min|$0.21|

In the output directory, there is a subfolder with the sample name ""MantonBM_subset"". All the output files are stored there. Below is a brief description of these files:
* `MantonBM_subset.aggr.zarr.zip`: The aggregated gene-count matrix of all 8 channels. Notice that Cumulus workflow default setting has already filtered out cell barcodes with less than 100 genes expressed in data aggregation.
* `MantonBM_subset.zarr.zip`: Count matrix containing clustering result.
* `MantonBM_subset.GRCh38-rna.h5ad`: Analysis result in `h5ad` format. This example only generates one h5ad file, as there is only one modal data with key `GRCh38-rna`.
* `MantonBM_subset.GRCh38-rna.filt.*.pdf`: Quality-control plots.
* `MantonBM_subset.GRCh38-rna.filt.xlsx`: Quality-control stats spreadsheet.
* `MantonBM_subset.GRCh38-rna.fitsne.pdf`: The FIt-SNE embedding plot.
* `MantonBM_subset.GRCh38-rna.umap.pdf`: The UMAP embedding plot.
* `MantonBM_subset.GRCh38-rna.*.composition.pdf`: The composition plot regarding cluster labels and channels.
* `MantonBM_subset.GRCh38-rna.de.xlsx`: The differential expression analysis result in spreadsheet.
* `MantonBM_subset.GRCh38-rna.anno.txt`: The putative cluster-specific cell type annotation result.
* `MantonBM_subset.log`: Analysis log file.

#### Advanced on Step 1

This step uses Cellranger to generate a gene-count matrix for each of the 8 channels from FASTQ files.

Since those FASTQ files are stored in the original public workspace, you don't have to fetch them to your own workspace. In this way, it's fine to use the sample sheet located at `Cellranger_input_csv` of your `sample` table, which refers to the one at the original public workspace.

The Cellranger workflow by default uses [Cellranger v7.0.1](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/release-notes), which is released on 2022/08/18.

The Cellranger sample sheet has the following structure:

|Sample|Reference|Flowcell|
|---|---|---|
|MantonBM1_HiSeq_1|GRCh38-2020-A|`Flowcell`|
|MantonBM2_HiSeq_1|GRCh38-2020-A|`Flowcell`|
|MantonBM3_HiSeq_1|GRCh38-2020-A|`Flowcell`|
|MantonBM4_HiSeq_1|GRCh38-2020-A|`Flowcell`|
|MantonBM5_HiSeq_1|GRCh38-2020-A|`Flowcell`|
|MantonBM6_HiSeq_1|GRCh38-2020-A|`Flowcell`|
|MantonBM7_HiSeq_1|GRCh38-2020-A|`Flowcell`|
|MantonBM8_HiSeq_1|GRCh38-2020-A|`Flowcell`|

where
* `Flowcell` is short for the value of `Flowcell` in `sample` table. The Flowcell column specifies the Google bucket URL to top-level folders containing all the samples' FASTQ files.
* The human genome reference `GRCh38-2020-A` is chosen from a list of [prebuilt references](https://cumulus.readthedocs.io/en/stable/cellranger/index.html#sample-sheet).
* As this example starts from FASTQ files, sample names cannot be chosen arbitrarily, but should be consistent with subfolders within `Flowcell`.

Moreover, since we start from FASTQ files, we only need to run `cellranger count`, so setting `run_mkfastq` to `false` in cellranger_workflow configuration page is necessary.

See [here](https://cumulus.readthedocs.io/en/stable/cellranger/index.html#run-cellranger-count-only) for details on this step.

#### Advanced on Step 2

This step uses the output of Step 1 for downstream analysis. So we first need to wait for Step 1 to finish.

After that, we need to create a sample sheet for cumulus workflow. Let `gs://xxx/cellranger_output` be the output directory of Step 1. Then your sample sheet should be the following (notice that CSV file uses comma to seprate values):

|Sample|Location|
|---|---|
|MantonBM1_HiSeq_1|gs://xxx/cellranger_output/MantonBM1_HiSeq_1/raw_feature_bc_matrix.h5|
|MantonBM2_HiSeq_1|gs://xxx/cellranger_output/MantonBM2_HiSeq_1/raw_feature_bc_matrix.h5|
|MantonBM3_HiSeq_1|gs://xxx/cellranger_output/MantonBM3_HiSeq_1/raw_feature_bc_matrix.h5|
|MantonBM4_HiSeq_1|gs://xxx/cellranger_output/MantonBM4_HiSeq_1/raw_feature_bc_matrix.h5|
|MantonBM5_HiSeq_1|gs://xxx/cellranger_output/MantonBM5_HiSeq_1/raw_feature_bc_matrix.h5|
|MantonBM6_HiSeq_1|gs://xxx/cellranger_output/MantonBM6_HiSeq_1/raw_feature_bc_matrix.h5|
|MantonBM7_HiSeq_1|gs://xxx/cellranger_output/MantonBM7_HiSeq_1/raw_feature_bc_matrix.h5|
|MantonBM8_HiSeq_1|gs://xxx/cellranger_output/MantonBM8_HiSeq_1/raw_feature_bc_matrix.h5|

The sample name this time can be changed for your convenience, as long as they are distinct to each other. These will be the labels of `Channel` cell attribute in the analysis.

Once done with creating the sample sheet, upload it to Google bucket of your workspace. Here are two ways:
1. Click ""Open in browser"" link at bottom-right of this page, then click ""Upload Files"" button in the new page.
2. Use [gsutil](https://cloud.google.com/sdk) to upload from command-line on your local machine:

```
gsutil cp count_matrix.csv gs://xxx/
```

where `count_matrix.csv` should be replaced by your sample sheet filename on your local machine, and `gs://xxx` be placed by GS URL to the target location.

Now go to `sample` table in `DATA` tab, change `Analysis_input_csv` to location of your sample sheet file. 

In Cumulus configuration page in `WORKFLOWS` tab, several input fields are preset:

* `input_file`: link to input file. In this example, we use a CSV format sample sheet. In other cases, Cumulus also accepts a single count matrix file, or multiple files via data table. See [details](https://cumulus.readthedocs.io/en/latest/cumulus.html#prepare-input-data).
* `output_directory`: Google bucket URL of the top-level output directory. If there are multiple samples analyzed seperately, each of them will have a dedicated subfolder within this output directory.
* `output_name`: Name of subfolder and filename prefix of analysis result files of of this sample.

These 3 input fields above are required. All the others are optional:

* **Aggregate_matrix**: Use default settings. So aggregate gene-count matrices of 8 channels into one, while filtering out cells with fewer than 100 genes expressed.
* **Cluster**:
	* Preprocessing: with default settings, keep cells with 500 < number of genes <= 6000 expressed, generate quality-control plots and stats, select 2000 highly variable features, calculate 50 PCs in PCA, and construct nearest neighbor graph of 100 neighbors. Fields of non-default settings are below:
		* `percent_mito`: Set to `10`, meaning to keep cells with mitochondrial ratio less than 10% of total counts.
		* `correct_batch_effect`: Set to `true`. By default, use [Harmony](https://www.nature.com/articles/s41592-019-0619-0) algorithm for batch correction.
	* Clustering:
		* `run_louvain`: Set to `false`. Louvain is the default clustering algorithm, but we won't use it in this example.
		* `run_leiden`: Set to `true`. We use Leiden for clustering.
	* Visualization: By default, UMAP embedding of cells is calculated.
		* `run_tsne`: Set to `true`. We also want to see FIt-SNE embedding of cells.
* **Differential Expression (DE) analysis**: Run the default Mann-Whitney U test on clusters, calculate AUROC, and annotate cluster-specific cell types based on DE result.
	* `cluster_labels`: Set to `""leiden_labels""`. Use Leiden clustering result for DE analysis. Its default is `""louvain_labels""`, the Louvain clustering result which we don't have here.
	* `annotate_cluster`: Set to `true`. For cell type annotation, by default, use t test result and preset `""human_immune""` cell markers, report putative cell types with scores higher than `0.5`.
* **Plotting**:
	* `plot_composition`: Set to `""leiden_labels:Channel""`. We want to see the composition of each cluster regarding samples.
	* `plot_tsne`: Set to `""leiden_labels,Channel""`. We want to see FIt-SNE plots regarding clusters and samples, respectively.
	* `plot_umap`: Set to `leiden_labels,Channel`. We want to see UMAP plots regarding clusters and samples, respectively.

See [here](https://cumulus.readthedocs.io/en/stable/cumulus.html) for details on input data preparation and description of Cumulus workflow inputs.

-------------

### Interactive Analysis

Besides running cumulus workflow, users can also analyze the data interactively via Terra Notebook.

First, follow [this instruction](https://pegasus.readthedocs.io/en/latest/terra_notebook.html) to create your own Cloud Environment with Pegasus environment selected from *CUSTOMIZE* menu. The tutorials introduced in this section use the cloud compute profile as the following:

| CPUs | Memory (GB) | Compute type | Persistent disk size (GB) | GPU type | GPUs |
|:---:|:---:|:---:|:---:|:---:|:---:|
| 8 | 30 | Standard VM | 50 | NVIDIA Tesla T4 | 1 |

where **GPU type** and **GPUs** are only used in Pegasus GPU Benchmark (see list below).

When Runtime is created and started, open any of the following notebooks in `NOTEBOOKS` tab, and click `EDIT` to enter the edit mode:
* **Pegasus Analysis Tutorial**: Show how to use Pegasus API to analyze single-cell data interactively.
* **Pegasus Plotting Tutorial**: Show how to use Pegasus plotting functions to generate plots for analysis.
* **Pegasus Batch Correction Tutorial**: Show how to use batch correction / data integration methods provided by Pegasus.
* **Pegasus Doublet Detection Tutorial**: Show how to detect doublets by Pegasus.
* **Pegasus Regress Out Tutorial**: Show how to regress out effects such as cell cycle using Pegasus.
* **Pegasus GPU Benchmark**: A benchmark on Pegasus core functions in GPU mode.

Then follow the notebook tutorial. Pegasus is the analysis module of Cumulus, which can be used as an independent Python package. See [here](https://pegasus.readthedocs.io) for its documentation.

When finished, before leaving, don't forget to stop your Cloud Environment.

---------------

### Data Visualizer

Cirrocumulus is a cloud-based interactive data visualizer. Below is how to use it on Terra:

1. Start your Terra notebook runtime, then click ""Open terminal"" button on its panel.
2. In terminal, copy the data file you want to visualize (must in `h5ad` format) from Google bucket to the environment. For example:
```
gsutil -m cp gs://xxx/result.h5ad .
```
3. Launch Cirrocumulus with this data file:
```
cirro launch result.h5ad &
```
This will run Cirrocumulus on `http://localhost:5000`.
4. However, we can't directly view it by typing the address above in our web browser. We need to export it to a public URL using ngrok:
```
ngrok http 5000
```
And in the pop-up terminal, click the URL ending with "".ngrok.io"" to open Cirrocumulus in your web browser.

Now enjoy using Cirrocumulus to go over the dataset.

-------------

### More Materials

Cumulus is published in Nature Methods as an [article](https://www.nature.com/articles/s41592-020-0905-x). You can access to a view-only version of this paper [here](https://rdcu.be/b5R5B).

We have [tutorial videos](https://www.youtube.com/watch?v=-azWohOas7g&list=PLIv12jQJ5nmaZprdUzBie0OkKU7yEPTyk), introducing Cumulus workflows, Cirrocumulus visualizer, and Pegasus analysis package.


More tutorials:
* Pegasus: [link](https://pegasus.readthedocs.io/en/latest/tutorials.html).
* PegasusIO: Module on data I/O and manipulation for Cumulus. [link](https://pegasusio.readthedocs.io/en/latest/tutorials.html).
* Cirrocumulus: Data visualizer. [link](https://cirrocumulus.readthedocs.io/en/latest/tutorial.html).

Documentation:
* Cumulus: [link](https://cumulus.readthedocs.io).
* Pegasus: [link](https://pegasus.readthedocs.io).
* PegasusIO: [link](https://pegasusio.readthedocs.io).
* Cirrocumulus: [link](https://cirrocumulus.readthedocs.io).

GitHub Repository:
* Cumulus: [link](https://github.com/lilab-bcb/cumulus).
* Pegasus: [link](https://github.com/lilab-bcb/pegasus).
* PegasusIO: [link](https://github.com/lilab-bcb/pegasusio).

### Contact Information

This workspace is documented by Bo Li (<libo@broadinstitute.org>), Yiming Yang (<yyang@broadinstitute.org>), and Asma Bankapur.

For questions regarding Cumulus workflows, please [contact us](https://cumulus.readthedocs.io/en/stable/contact.html).

Readers can also direct questions to the Terra [Forum](https://support.terra.bio/hc/en-us/community/topics) for generic Terra questions.

--------------

### License

**Copyright Broad Institute, 2018 - 2022 | BSD-3**

*Cumulus* is under BSD-3 lincense (see https://github.com/lilab-bcb/cumulus/blob/master/LICENSE). All the workflows in Cumulus project are also under WDL's BSD-3 open-source code license (see https://github.com/openwdl/wdl/blob/master/LICENSE).

### Workspace Change Log

|Date|Changes|Author|
|----|----|----|
| 2022/11/18 | Update workflows to v2.3.0; update documentation | Yiming Yang |
| 2022/04/19 | Update workflows to v2.0.0; update notebooks with Pegasus v1.6.0 | Asma Bankapur, Yiming Yang |
| 2021/08/30 | Update tutorials in NOTEBOOKS tab with Pegasus v1.4.3 | Asma Bankapur |
| 2021/08/14 | Update the documentation to make it clear that cellranger v6.0.1 is used. In addition, update update `2-Cumulus` to `V.43`  | Bo Li |
| 2021/06/25 | Update workflows to Cumulus v1.4, i.e. `1-Cellranger_Workflow` in `V.26` and `2-Cumulus` to `V.41`. | Yiming Yang |
| 2020/08/18 | Add notebooks on plotting and batch correction tutorials. | Hui Ma |
| 2020/08/06 | Use a subset of human bone marrow dataset for demonstration instead. Update `1-Cellranger_Workflow` to `V.12`, `2-Cumulus` to `V.29`. | Hui Ma, Yiming Yang, Bo Li |
| 2020/05/12 | Initialize workspace. Provide mouse cortex single-nucleus hashing dataset for demonstration. | Yiming Yang, Bo Li |






",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","kco-tech/Cumulus"
252,"aryee-lab","bisulfite-seq-tools","READER","https://app.terra.bio/#workspaces/aryee-lab/bisulfite-seq-tools",TRUE,FALSE,NA,NA,NA,"### bisulfite-seq-tools

Methods from this workspace can be used for alignment and quality control analysis for various DNA methylation protocols including Whole Genome Bisulfite Sequencing (WGBS), Reduced Representation Bisulfite Sequencing (RRBS) and Hybrid Selection Bisulfite Sequencing (HSBS).

More information could be find in the github repo here: https://github.com/aryeelab/dna-methylation-tools

---

### Data

**Sample Data**  
All preprocessing methods require forward and reverse  fastq or fastq.gz files entered into a sample data table.  The tables below outline the format and content of the **Sample** data model tables (tab separated files with “tsv” extension).

   **Example 1:** samples.tsv for RRBS and WGBS

| entity:sample_id 	| bs_fastq1                          	| bs_fastq2                          	|
|-----------------------	|------------------------------------	|------------------------------------	|
| Sample1               	| gs://location/sample_1_R1.fastq.gz 	| gs://location/sample_1_R2.fastq.gz 	|
| Sample2               	| gs://location/sample_2_R1.fastq.gz 	| gs://location/sample_2_R2.fastq.gz 	||

   **Example 2:** Participants.tsv for HSBS requires an additional input, the target_region.

| entity:sample_id 	| bs_fastq1                          	| bs_fastq2                          	| target_region                         	|
|-----------------------	|------------------------------------	|------------------------------------	|---------------------------------------	|
| Sample1               	| gs://location/sample_1_R1.fastq.gz 	| gs://location/sample_1_R2.fastq.gz 	| gs://location/hg38_targetCoverage.bed 	|
| Sample2               	| gs://location/sample_2_R1.fastq.gz 	| gs://location/sample_2_R2.fastq.gz 	| gs://location/hg38_targetCoverage.bed 	|


   **Example 3:** sample_set_membership.tsv can be created by grouping sample_ids using this format:

| membership:sample_set_id 	| sample 	|
|-------------------------------	|-------------	|
| Human_single_cell_Expt_1      	| Sample1     	|
| Human_single_cell_Expt_1      	| Sample2     	|


This workspace includes test datasets to help you  become familiar with the workflows

| sample_set_id 	| participants 	| cell type                                             	|
|--------------------	|--------------	|-------------------------------------------------------	|
| HES                	| 3            	| Human single cell                                     	|
| HES_set2           	| 2            	| Human single cell                                     	|
| Human_wgbs         	| 1            	| Human whole genome                                    	|
| MES                	| 4            	| Mouse single cell                                     	|
| Mouse_wgbs         	| 1            	| Mouse whole genome                                    	|
| test_set_mouse_sc  	| 3            	| Small mouse single cell data set(for program testing) 	|


---

### Tools

This workspace contains the following preset method configurations, already set up for grch38, hg19 and mm10.  Other genomes can be loaded by changing the json inputs.  

* Preprocessing tools are run on a ""sample"", selecting one sample or a set of samples.  
* The aggregation tool is run as a ""sample set"" on a set of preprocessed samples aligned to the same genome.  
* Aggregation can only be done after preprocessing.      
* Both preprocessing and aggregation generate html reports (see links to examples below).  

**Preprocessing:**

* wgbs-grch38: Preprocess Whole Genome Bisulfite Sequencing (WGBS) data for GRCh38
* rrbs-grch38: Preprocess Reduced Representation Bisulfite Sequencing (RRBS) data for  GRCh38
* hsbs-grch38: Preprocess Hybrid Selection Bisulfite Sequencing (HSBS) data for  GRCh38

[Example bismark processing report](https://storage.googleapis.com/terra-featured-workspaces/methylation-preprocessing/example_output/test_HES_sample_1_R1.fastq.gz_bismark_report.html)

Similar workflows exist for mm10 and hg19.

**Aggregation:**

* aggregate_bismark_output: Aggregates outputs from preprocessing pipelines and produces an aggregated data structure for further downstream analysis

[Example output from scmeth R package](https://storage.googleapis.com/terra-featured-workspaces/methylation-preprocessing/example_output/qcReport.html)

Based on the reference genome different method configurations could be selected. So far, we have hg38, hg19 and mm10 reference genome versions of all preprocessing and aggregation tools.

---

### Example Time and Cost to Run Workflow

| Sample size |     Per-sample preprocessing |     Aggregation and QC | Total |
| :---:    | :---: | :---: | :---: |
|     | (Hours / $)                  | (Hours / $)            | (Hours / $)        |
| ..................................... | ............................................................................. | .......................................................... | ......................... |
| 10              | 0.98 ($0.93)                 | 0.97 ($0.28)           | 1.95 ($1.21)       |
| 100             | 1.47 ($8.99)                 | 6.00 ($0.86)           | 7.47 ($9.85)       |
| 1000            | 4.48 ($52.48)                | 58.01 ($13.74)         | 62.49 ($66.22)     |


To obtain these estimates, we ran the workflows in FireCloud on the default n1-highmem-4 compute nodes (26 GB RAM with 4 CPUs). Test-run samples were 1000 single-cell RRBS samples with a median of 872,223 reads.

---

### Contact information  

Divy Kangyen, 
Department of Biostatistics
Harvard T.H. Chan School of Public Health
Email address: divyswar01@g.harvard.edu

Issues and feature requests can be submitted to issue tracker in the [dna-methylation-tools github repo](https://github.com/aryeelab/dna-methylation-tools/issues)

Paper associated with the workspace can be found here: https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-2750-4

---

### License  

**Copyright Broad Institute, 2019 | BSD-3**
All code provided in this workspace is released under the WDL open source code license (BSD-3) [full license text here](https://github.com/openwdl/wdl/blob/master/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.

",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","aryee-lab/bisulfite-seq-tools"
253,"broad-firecloud-dsde-methods","Model_Segments_PostProcessing_Home","READER","https://app.terra.bio/#workspaces/broad-firecloud-dsde-methods/Model_Segments_PostProcessing_Home",TRUE,FALSE,NA,NA,NA,"## Overview
This is the home workspace for the Model_Segments_PostProcessing method configurations.  In other words, clone the method configurations from here.


**This workspace and the contained method configurations are totally unsupported.**

The method configurations are used for:
- Improving the accuracy of GATK ModelSegments.  These improvements tend to be modest.
- Converting results from GATK ModelSegments to files that can be read by ABSOLUTE (https://software.broadinstitute.org/cancer/cga/absolute)
- Converting results from GATK ModelSegments to files can be read by GISTIC2 (https://genomebiology.biomedcentral.com/articles/10.1186/gb-2011-12-4-r41)
- Aggregating results from GATK ModelSegments for easy visualization in IGV.

## Important Notes
- *Cannot be run in tumor-only mode.*  A matched normal is required.
- The GATK ModelSegments workflow (i.e. the input to the method configurations here) should be run with `calling_copy_ratio_z_score_threshold` of `3.05`

## Important Notes for WGS
- **It is more effective to also specify the blacklist used here as an input to the pair workflow** (`blacklist_intervals`/`-XL`).  This is probably also true for capture samples (e.g. WES), but the performance of this workflow on capture samples has not been evaluated well.  You must use one of the files listed below.

Unified blacklists that can be used with `blacklist_intervals` or `-XL` can be found at:

 hg19: gs://gatk-best-practices/somatic-b37/CNV_and_centromere_blacklist.hg19.list

 hg38: gs://gatk-best-practices/somatic-hg38/CNV_and_centromere_blacklist.hg38liftover.seg


## Important Notes for WES (or any other capture)
- Set the min_hets_acs_results to 1
- See the WGS note about `-XL`


## Detailed Description

### Model_Segments_PostProcessing (the main workflow)
- Does conversion of output seg files to a format compatible with IGV.  This may be removed soon, since the GATK tools have been updated to produce files that can be easily read by IGV.
- Tags possible germline events that appear in the tumor results.  This is done by comparing breakpoints and reciprocal overlaps.  This algorithm takes into account any hypersegmentation.  This workflow can tag regions that appear as CNLoH in the normal.
- Tags tumor events that appear in the given (and provided) blacklist
- Removes any events that have been tagged in the previous two steps.  Any resulting gaps are imputed, so long as the segment mean would remain constant and the distance is less than max_merge_distance (default 1Mb as of this writing)
- *Cannot be run in tumor-only mode.*  A matched normal is required.
- seg file that is has germline events removed (and gaps merged).  Same columns as a called seg file.  This should be considered the final output.  IGV compatible.
- seg file that can be used as input to ABSOLUTE.  This uses an identical format as AllelicCapSeg.  This is NOT compatible with IGV.  Please note that the balanced-segment calling in this file has not been evaluated heavily.
- seg file that can be used as input to GISTIC2.  Note that since this workflow is meant for a single pair, users will need to aggregate the GISTIC2 seg files from this workflow for all pairs.
- skew file for the skew parameter in ABSOLUTE
- skew as a float for the skew parameter in ABSOLUTE
- Intermediate files for browsing various steps of the workflow.

### Aggregate_Model_Segments_PostProcessing
- Aggregates some of the IGV files, so that a user can view all samples together.
- Aggregates the single GISTIC2 seg files into one GISTIC2 seg file.  This input can be fed directly into GISTIC2.

## Known Issues
We are unlikely to fix the following:
- If your data is not denoised well, you can have large segments pruned during the germline tagging step.  GISTIC crashes if a sample is missing an entire contig.  For example, if chromosome 5 is (mistakenly) tagged as amplified, all segments on chromosome 5 will be removed from the output of this workflow.    Usually, the fix is logistic (I.e. ""Your PoN needs to match your case samples better""), but if you are sure that you cannot then you can try to run the GATK ModelSegments workflow with a larger `calling_copy_ratio_z_score_threshold`.  Fixing your PoN is almost always the better approach.



",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","broad-firecloud-dsde-methods/Model_Segments_PostProcessing_Home"
255,"veme-training","VEME NGS","READER","https://app.terra.bio/#workspaces/veme-training/VEME%20NGS",TRUE,FALSE,NA,NA,NA,"**Viral alignment, variant calling, assembly, and metagenomics.**

Workshop tutorials [https://broadinstitute.github.io/viral-workshops](https://broadinstitute.github.io/viral-workshops):
* [Alignment, variant calling, and consensus calling](https://broadinstitute.github.io/viral-workshops/veme-ngs/alignment.html)
* [De novo assembly](https://broadinstitute.github.io/viral-workshops/veme-ngs/denovo.html)
* [Viral metagenomics](https://broadinstitute.github.io/viral-workshops/veme-ngs/metagenomics.html)",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","veme-training/VEME NGS"
256,"bayer-pcl-single-cell","AHA_single_cell_scanpy","READER","https://app.terra.bio/#workspaces/bayer-pcl-single-cell/AHA_single_cell_scanpy",TRUE,FALSE,NA,NA,NA,"workspace for AHA single cell tutorial for vascular discovery and scientific sessions 2021.",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","bayer-pcl-single-cell/AHA_single_cell_scanpy"
258,"ga4gh-cnest-test","cnest-terra","READER","https://app.terra.bio/#workspaces/ga4gh-cnest-test/cnest-terra",TRUE,FALSE,NA,NA,NA,"### CNest - Copy Number Estimation for Large NGS Cohorts
The CNest workflow runs [CNest](https://github.com/tf2/CNest) which is a copy number estimator and variant caller that has been specifically developed for large scale analysis of copy number from NGS data. 

It primarily uses read depth information to generate robust copy number estimates for individual samples and is most appropriate for use in very large cohorts (minimum of 1000 samples). 

The main objective of the CNest pipeline is the calculation of copy number estimates that are robust enough to allow genome wide association analysis for CNVs from NGS datasets.
 
### Background and Experimental Design 
The [CNest](https://github.com/tf2/CNest) methods are most appropriate for use on large NGS cohorts and work under the assumption that most samples within a cohort have similar properties in terms of the total amount of variation (CNV) within their genomes. It is therefore not suitable for use on relatively small cohorts or e.g. cancer genomes. 

CNest generalises a 'dynamic' reference based approach for copy number estimation where a predicted set of 'ideal internal samples' are used for dynamic baselines across the genome for every sample, meaning that each sample has a different reference containing different baseline coverage levels for all target regions across the genome. 

It is important to note that these copy number estimate are likely to be a good represetation of the underlying copy number distributions but should not be considered as 'real' copy numbers rather they are relative copy number estimates across a population.

To avoid doubt the CNest pipeline should not be applied to multiple BAMs spanning multiple cohorts with different sequencing designs and platforms. If its is desired to combine multiple studies, each cohort (sequencing design / platform sample set) should be run at the individual cohort level and combined later.

For more details about the CNest methods see [here](https://www.biorxiv.org/content/10.1101/2021.08.19.456963v2) and [here](https://github.com/tf2/CNest/blob/master/src/example_cnest_py.md), and for setup on different infrastructure using GA4GH standards please see [here](https://starterkit.ga4gh.org/docs/starter-kit-in-action/2021/cnest-workflow-ebi/)*
 

## Data 
CNest operates on NGS data and has been tested using both Whole Exome (WES) and Whole Genome (WGS) NGS data. The typical input is multiple BAM files and their BAI indexes which will be used to generate copy number estimates and CNV calls using all specified target regions.

**Target regions:** We require a list of target regions in the format of ""chr:start-end"". These will be used to extract coverage level information and allow copy number estimation and variation calling. For WGS data we suggest you use 1 kb non-overlapping regions across the genome of interest. For targeted sequence such as whole exome the target regions would be e.g. the bait regions from the sequencing platform.

Note: target regions must have ""chr_tags"" in the same format as in the BAMs e.g. ""chr1"" vs. ""1"".* 

**Example Data** 
Performance on small datasets will be limited however we provide 2 different scale tests and example data using  publically available [1000 genomes](https://www.internationalgenome.org/) WGS BAM files.


The data for input into the CNest workflow should be data tables containing the location of BAM files and BAI index files. In the featured workspace we provide two sets of example data. 1) the ""sampleset"" table that contains 50 BAM files and BAI index files from the 1000 genomes project and 2) the bam_index_file_copyset that contains 200 samples.

Note: that the format of the ""chr_tags"" are different between the two example datasets with the BAMs in the ""sampleset"" table having e.g. ""chr1"" and the BAMs in the ""bam_index_file_copyset"" having e.g. ""1"" i.e. the BAMs in the sampleset table include ""chr"" whereas the ""bam_index_file_copyset"" does not. This means that a target regions input file needs to be selected of the two different datasets.

 
## Workflows 
This workflow contains a single workflow ([cnest.wdl](https://dockstore.org/workflows/github.com/tf2/CNest/cnest:master?tab=info)) that combines all the individual steps from the CNest pipeline and allows full end to end analysis to be performed by a single job submission. This workflow wraps 5 CNest steps that each have a seperate wdl and can be found via dockstore [here](https://dockstore.org/search?descriptorType=WDL&entryType=workflows&search=CNest).     

We also include two additional workflows to aid in the generation of the required inputs, both BAM and BAI files for all samples to be run. These include reliable CRAM to BAM convertion [cram-to-bam](https://portal.firecloud.org/?return=terra#methods/gatk/cram-to-bam/3) and a BAM index workflows [IndexBamWithSamtools_4_DRS](https://portal.firecloud.org/?return=terra#methods/bshifaw/IndexBamWithSamtools_copy/32).


To launch the CNest workflow - select the cnest.wdl workflow from the workflow tabs and input the required parameters (detailed below). 

Next, select the data of interest - which should be a ""sampleset"" containing a data table with all BAM files and their associated BAI index files - see [here](https://support.terra.bio/hc/en-us/articles/360026521831-How-to-set-up-a-workflow-analysis-) for further details.

### CNest workflow

This CNest workflow performs all the steps required to go from a list of input BAM files and their indices to copy number estimates and CNV calls across all samples included. It will perform a gender classification based on the coverage observed on chromosome X and generate some quality control statistics on the samples and the genomic regions. It will also generate 3 different copy number estimates for each sample across all targeted regions and output this information into a specific binary format ""rbin"" each allow fast data access across samples and regions. Finally it will output CNV calls for each sample generated with a custom HMM that performs ""joint calling"" across all samples, information on all CNV regions and the definition of copy number states is provided for each sample in the ""cnv_calls"" directory.    

**Input**       
The primary input are the BAM file and BAI index file location which should be contained within a ""sampleset"" table.

Required arguments:

* **bam_file**: location of BAM file 
* **bai_file**: location of BAI index file 
* **batch**: the number of samples to use to generate each reference - we advice this is be set at approximately 10% of the cohort size
* **bedgz**: the ""target_region"" input file in the format of ""chr:start-end"" 
* **project**: a name for the overall project 
* **ref**: the reference genome in FASTA format 
* **samples**: a list of sample identifiers  - this should be an entity in the ""sampleset"" table.
* **cor_cut**: a correlation based cut-off that can be used for certain dynamic reference definitions (Note: for standard runs keep this set to zero)
* **skipem**: an option to skip an expectation maximisation step (Note: for small cohort we suggest you set this to true)
* **wgs**: a variable indicating if the sequence data is whole genome or not,  is used to help calculate the memory needed for certain tasks (see also additional optional arguments for adjusting memory usage)

Optional arguments:

*  **cov_cut**: a cut-off for coverage levels that can be adjusted for the selection of dynamic references
* **part3_addtional_mem_gb**: an additional amount of memory that can be added for step3
* **part4_addtional_mem_gb**: an additional amount of memory that can be added for step4
* **part4_addtional_disk_gb**: an additional amount of disk space that can be added for step4
* **preemptible_tries**: maximum number of preemptible tries  


**Output**  
The main output from the CNest workflow are copy number estimate,  CNV calls and some quality control statistics.

The first main step is the classification of gender for all samples using coverage levels on chromosome X compared to autosomes. This step outputs two main files named:

* **gender_qc.txt**: file containing the normalised coverage levels for chromosome X and autosomes along with some additional statistics
*  **gender_classification.txt**: file containing the classification of male and female samples and sample_ids (Note: this can classifiy sex chromosome aneuploidy)


The other set of important output are the actual copy number estimates and CNV calls for all samples.

* **mixed_calls**: a set of files containing the CNV calls for each sample
* **mixed_stats**: a set of files containing copy number state definitions and estimates for all targeted regions for each sample
* **out_rbin**: a of binary files containing all copy number specific information for each sample (Note: this format is used extensively during further analysis e.g. copy number genome wide association testing (CN-GWAS) 
* **mean_coverage.txt**: a quality control file listing the mean coverage across all samples for each target region


**Reference/Resource data description and location**  
*The reference genome for this workspace is hg38 (aka GRCh38). Required and optional references and resources for the methods are included in the Workspace Data table (""Workspace Attributes"" in FireCloud).*           
 
**Estimated time and cost to run on sample data**    


| Sample Name | Sample Size | Time | Cost $ |
| :---:  | :---: | :---: | :---: |
| NA12878 | 64.89 GB | 3:05:00 | 0.65 |     

For helpful hints on controlling Cloud costs, see this article ([click here to view](https://support.terra.bio/hc/en-us/articles/360029748111)).       

---


### Contact information

Questions can be directed to Tomas Fitzgerald, email: tomas@ebi.ac.uk

### Citations

Tomas Fitzgerald and Ewan Birney. CNest: A Novel Copy Number Association Discovery Method Uncovers 862 New Associations from 200,629 Whole Exome Sequence Datasets in the UK Biobank https://doi.org/10.1101/2021.08.19.456963.


### License 
GNU GPL

",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","ga4gh-cnest-test/cnest-terra"
260,"pathogen-genomic-surveillance","HCoV-19","READER","https://app.terra.bio/#workspaces/pathogen-genomic-surveillance/HCoV-19",TRUE,TRUE,NA,NA,NA,"This workspace contains COVID-19 genomic data and workflows that will enable you to perform viral genomic analysis. This workspace will be routinely updated with new, additional data as it becomes available.


## **The Data**

The data in this workspace includes raw sequencing data (.fastq and .BAM) available from NCBI's Sequence Read Archive (SRA). 
On a routine basis, the Broad Institute will add new COVID-19 data to this workspace, as it becomes available. These data, from around the world, have been run on the Illumina platform. All .FASTQ files have been processed to raw uBAM files.

### **What data should I run?**

Since this data is collected from various places around the world, from different patients and labs, data quality will vary. We have created two separate sample sets for analysis to help you decide which samples you would like to work with. 

1. ""Successful_Assembly_Group"" - these are samples that successfully ran through assembly
2. ""Failed_Assembly_Group"" - these are samples that did not successfully assemble

If this is your first time running this data through these workflows, we suggest running the ""Successful_Assembly_Group"" samples through the workflows, so that you may see what the results of good quality samples that fully assemble look like, as a means of comparison. 

You may run the ""Failed_Assembly_Group"" samples if you are interested in using all of the data, regardless of if a sample assembled or not. When these samples run through the workflow, you will see some error messages alerting you to their failed assembly. For examples of these error messages, please see the section **'Sample Quality and Sample Failures'** below.

## **Workflows**

The workflows in this worksapce enable you to perform assembly, QC, kraken metagenomics and aggregate statistics. Depending on your starting input file, whether SRA Accession#, .FASTQ, or uBAM, you can utilize the appropriate workflow outlined below to load your data in Terra and prepare it for assembly.

![Overview of the Terra Workflows in this workspace and how to use them](https://drive.google.com/uc?id=16PebfpNoQV175okuzYs83p15omhRU9un)

The following workflows are included in this workspace. Below, the inputs, outputs, example variables and a description of each workflow can be found.

#### **SRA_to_uBAM**

*What does it do?*

This workflow downloads .fastq files from SRA, given an SRA_ID as input. Specifically, the output of the workflow will produce the unaligned BAM file that is needed for viral assembly.

*What does it require for input?*

|Input | Example |
|---|---|
| SRA_ID| SRA123456 or SRR123456 |


*What does it return as output?*

|Output |
|---|
| biosample_accession |
| library_id |
| run_date |
| sample_collected_by |
| sample_collection_date |
| sample_geo_loc |
| sample_strain |
| sequencing_center |
| sequencing_platform |
| sequencing_platform_model |
| sra_metadata |
| srr_uBAM |


#### fastq_to_ubam

*What does it do?*

This workflow accepts paired-end or single-end .fastq files and converts them to uBAM files.


*What does it require for input?*

|Input | Example |
|---|---|
| SRA_ID| SRA12345678 |
| gcs_output_dir  | gs://fc-9e394ffa-2766-4c55-bbb4-e4050069caf4 |
| Fastq_1 | srr123456.1.fastq |
| Fastq_2 | srr123456.2.fastq |
| Library_name | veroSTAT-1KO_illumina |
| Platform_name | Illumina |
| Platform_unit | M01472 |
| Readgroup_name | A |
| Run_date | 03082020 |
| Sample_name | Sample1 |
| Sequencing_center | Broad Institute |


*What does it return as output?*

|Output |
|---|
| uBAM files |
| A list of uBAM files that were generated |



#### assemble_denovo_with_deplete

*What does it do?*

This takes a raw read file (uBAM) and assembles a viral genome.

*What does it require for input?*

|Input | Example |
|---|---|
| raw_reads_unmapped_bam | sample1.bam |
| trim_clip_db | ""contaminants.clip_db.fasta"" |
| lastal_db_fasta | all_COVID19_on_genbank_as_of_2020-02-13.fasta |
| reference_genome_fasta | ref-sarscov2-NC_045512.2.fasta |
| blastDbs | gs://pathogen-public-dbs/v0/GRCh37.68_ncRNA.fasta.zst"" |
| bwaDbs | gs://pathogen-public-dbs/v0/hg19.bwa_idx.tar.zst |


*What does it return as output?*

There are many outputs, but a few important ones include:

|Output |
|---|
| refine_2x_and_plot.final_assembly_fasta (the assembly) |
| refine_2x_and_plot.mean_coverage (float) |
| refine_2x_and_plot.assembly_length_unambiguous (the number of non-N bases in final_assembly_fasta) |



#### classify_krakenuniq

*What does it do?*

This takes a collection of uBAMs and runs the Kraken (v1) taxonomic classifier against a custom built database. 

*What does it require for input?*

|Input | Example |
|---|---|
| krakenuniq_db_tar_lz4 | krakenuniq.full.20180811.tar.zst |
| krona_taxonomy_db_tgz | taxonomy-krona-20160502.tar.zst |
| reads_unmapped_bam | sample1.bam |

*What does it return as output?*

|Output |
|---|
| krakenuniq_classified_reads |
| krakenuniq_summary_reports |
| krona_report_html |
| viralngs_version |
| krakenuniq_aggregate_taxlevel_summary |



## **Sample Quality and Sample Failures**


```PoorAssemblyError: Error: poor assembly quality, chr 1: contig length 19301, unambiguous bases 2066; bases required of reference segment length: 14951.5/29903 (50%)```

This error message occurs in the assemble_denovo_with_deplete workflow when a sample fails assembly. This error can be overcome by changing one specific variable in the workflow. This poor assembly error can be overcome if you change the ```scaffold.min_unambig float``` variable value down from 0.5 to 0.1. In most instances, this will allow the sample to pass. It should be noted, however, that this is at the discretion of the researcher and their acceptance criteria for sample quality.


## **Interactive Analysis (Notebooks)**

#### QC of the Data

In this Jupyter notebook, we walk through how to read in the output data from your data table into the notebook to make a few QC plots. As more data becomes available, we will update the notebook with more analyses.








#### **Contact information**
For questions regarding viral genomics methods, please contact Danny Park (dpark@broadinstitute.org)",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","pathogen-genomic-surveillance/HCoV-19"
261,"help-gatk","cnn-variant-filter","READER","https://app.terra.bio/#workspaces/help-gatk/cnn-variant-filter",TRUE,FALSE,NA,NA,NA,"### GATK Best Practices for CNN Variant Filtration

A fully reproducible example of filtering variants using GATK CNN. Supplemental workflows enable advanced users to generate and evaluate their own training model. 

The workflow is sourced from the [GATK repository](https://github.com/broadinstitute/gatk/tree/master/scripts/cnn_variant_wdl).

## Notebooks
All notebooks in this workspace can use the following runtime settings:

**gatk-cnn-tutorial**: A tutorial on a deep learning method to filter germline variants that is applicable to single sample callsets.

| Option | Value |
| --- | --- |
| Environment | Default |
| Profile | Custom |
| CPU Minimum | 4|
| Disksize Minimum | 100 GB |
| Memory Minimum | 15 GB |

**Time and cost**
This workspace focuses on the use of notebooks, thus the time and cost of completing the tutorial depends on the runtime environment of your notebook and the length of time you spend actively working in the notebook. With the given runtime environments above, users can expect the cost to be much less than a dollar an hour for each notebook. The specific cost will be displayed on the top right corner of your screen, next to the notebook machine options.


## Workflows

The workflows are directly linked to the GATK repo via [Dockstore](https://dockstore.org/organizations/BroadInstitute/collections/GATKWorkflows) so its possible to select a particular workflow version based off the GATK version. If requested by the workflow inputs, be sure to provide the corresponding GATK docker image for the workflow version. For example if using workflow version 4.0.0.0 then the proper GATK docker to use would be us.gcr.io/broad-gatk/gatk:4.0.0.0. 

### 1-CNN-Variant-Filter workflow          
**What does it do?**   
This workflow takes an input CRAM/BAM to call variants with HaplotypeCaller, then filters the calls with the CNNVariant neural net tool using one of the following filtering models (depending on which workflow you choose):

* **1-CNN-Variant-Filter-1d** uses a 1D Model with pre-trained architecture   
* **1-CNN-Variant-Filter-2d** uses a 2D Model with pre-trained architecture      

The site-level scores are added to the `INFO` field of the VCF. The architecture arguments,
`info_key` and `tensor_type` arguments MUST be in agreement (e.g. 2D models must have
`tensor_type` of `read_tensor` and `info_key` of `CNN_2D`, 1D models have `tensor_type` of
`reference` and `info_key` of `CNN_1D`). The `INFO` field key will be `1D_CNN` or `2D_CNN`
depending on the neural net architecture used for inference. The architecture arguments
specify pre-trained networks. New networks can be trained by the GATK tools: CNNVariantWriteTensors 
and CNNVariantTrain. The CRAM could be generated by the [single-sample pipeline](https://dockstore.org/workflows/github.com/gatk-workflows/gatk4-data-processing/processing-for-variant-discovery-gatk4:2.1.0?tab=info).
If you would like test the workflow on a more representative example file, use the following 
CRAM file as input and change the scatter count from 4 to 200: gs://gatk-best-practices/cnn-h38/NA12878_NA12878_IntraRun_1_SM-G947Y_v1.cram.

**What are the input file requirements/expectations?**    
 The workflows accept data in CRAM or BAM format. If the input is a BAM file, it requires a BAM index. 

**What does the workflow output?**   
The workflow outputs a filtered VCF and its index written to the Google bucket associated with the workspace.    

**Sample data description and location**  
Links to the example inputs are provided in the workspace Data Model for testing. A full sized and downsampled file of NA12878 has been provided as input for CNN-Variant-Filter workflow. The following files are listed under the CRAM column of the data model. 

**Workspace data description and location**  
Required and optional references and resources for the methods are included in the Workspace Data table (""Workspace Attributes"" in FireCloud). The reference genome for this workspace is hg38 (aka GRCh38).

**Estimated time and cost to run**    
Note that cost and time will vary, for more information on understanding and controlling Cloud costs  see this article ([click here](https://support.terra.bio/hc/en-us/articles/360029748111)).  

| Sample Name | Sample Size | Time | Cost $ |
| -------  | -------- | -------- | ---------- |
| NA12878 | 19.51 GB | 11:57:00 | 13.49 |
| NA12878_20K_hg38 | 6.84 MB | 00:28:00 | 0.04 |


### optional-CNN-CRAM-2-Trained-Model     workflow
**What does it do?**   
This optional workflow is for advanced users who would like to train a CNN model for filtering variants.

**What are the input requirements/expectations?**     
 - CRAM
 - Truth VCF and its index
 - Truth Confidence Interval Bed

**What are the output files?**     
 - Model HD5
 - Model JSON
 - Model Plots PNG


### optional-Happy-Workflow     
**What does it do?**    
This optional evaluation and plotting workflow runs a filtering model against truth data (e.g. [NIST Genomes in a Bottle](https://github.com/genome-in-a-bottle/giab_latest_release), [Synthic Diploid Truth Set](https://github.com/lh3/CHM-eval/releases) ) and plots the accuracy.

**What are the input requirements/expectations?**     
 - File of VCF Files
 - Truth VCF and its index
 - Truth Confidence Interval Bed

**What are the outputs?** .    
 - Evaluation summary
 - Plots


### Software Version
- GATK 4.1.4.0
- Samtools 1.3.1


------

### Contact information  
For questions about this workspace please visit the Featured Workspaces Topic topic on the [Terra Community Forum](https://broadinstitute.zendesk.com/hc/en-us/community/topics). Use the search box to see if other users have asked the same question previously. If not, post and tag **@Samantha** so that we get notified.

This material is provided by the GATK Team. Please post any questions or concerns regarding the GATK tool to the [GATK forum](https://gatk.broadinstitute.org/hc/en-us/community/topics)

### Workspace Citation 
Details on citing Terra workspaces can be found here: [How to cite Terra](https://support.terra.bio/hc/en-us/articles/360035343652)

Data Sciences Platform, Broad Institute (*Year, Month Day that this workspace was last modified*) help-gatk/cnn-variant-filter [workspace] Retrieved *Month Day, Year that workspace was retrieved*,  https://app.terra.bio/#workspaces/help-gatk/cnn-variant-filter

### License  
**Copyright Broad Institute, 2019 | BSD-3**  
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at https://github.com/openwdl/wdl/blob/master/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.

### Workspace Change Log
| Date | Change | Author |
| --- | --- | --- |
|  2020-04-14 | Workflows now point to Dockstore workflows registered from GATK repo | Beri Shifaw |
|  2020-04-27 | Updated GATK version to 4.1.6.0 | Beri Shifaw |
|  2020-10-15 | Updated GATK version to 4.1.7.0 , Minor adjustments to dashboard. Added the CNN notebook tutorial | Beri Shifaw |
|  2021-04-27 | Updated GATK version to 4.2.0.0 ,  Updated notebook to latest CNN tutorial | Beri Shifaw |
|  2021-09-15 | Addition of Workspace Citation section | Beri Shifaw |",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/cnn-variant-filter"
262,"help-gatk","Sequence-Format-Conversion","READER","https://app.terra.bio/#workspaces/help-gatk/Sequence-Format-Conversion",TRUE,FALSE,NA,NA,NA,"This workspace contains workflows needed to convert various sequencing file formats to GATK analysis ready input formats. Plus a validation tool to confirm that SAM or BAM files are in the proper format.     
1) Interleaved FASTQ to paired FASTQ  
2) Paired FASTQ to unmapped BAM   
3) CRAM to BAM  
4) BAM to unmapped BAM    
5)  Validate BAM
6)  Validate VCF

The Validate BAM tool is also added to confirm proper formatting of SAM or  BAM files.  

Each section below contains       
* Tool name and purpose   
* Input requirements/expectations    
* Outputs       
* Sample data description      
* Time and cost estimates for running the tool    

Note that cost and time estimates will vary with the use of preemptibles. Using preemptibles can save up to 80% on compute costs. For further helpful hints on controlling cloud costs, see [this article](https://support.terra.bio/hc/en-us/articles/360029772212).

For more details on input types typically used by GATK please review the following article: [What Input Files does the GATK Accept/Require?]( https://gatk.broadinstitute.org/hc/en-us/articles/360035889451)


## Workflows
### Interleaved_FASTQ_to_Paired_FASTQ
This WDL takes in a single interleaved(R1+R2) FASTQ file and separates it into separate R1 and R2 FASTQ (i.e. paired FASTQ) files. Paired FASTQ files are the input format for the tool that generates unmapped BAMs (the format used in most GATK processing and analysis tools). 


#### Input requirements/expectations   

Entity type: Sample

- A single interleaved (alternating R1 + R2) FASTQ file 

An example interleaved Fastq is added to the sample table.  
![](https://storage.googleapis.com/terra-featured-workspaces/sequence-format-conversion/dashboard-images/sample-table-interleavedFQ.png)  

 **To run this tool on the sample file**: Select ""Process multiple workflows from Sample"" from the Tool menu.
 
 **To run this tool on your own data**: 
 Option 1: Generate and upload your own sample.tsv, with the file you would like to process under column named fastq_interleaved
 
 Option 2: Select ""Process single workflow from files"" after opening the tool in the Tool menu and insert the gs:// path to the file to be processed in the `input_fastq` configuration
 
 ### Sample data 
 You can find a sample interleaved FASTQ file at: gs://gatk-test-data/wgs_fastq/NA12878_20k/H06JUADXX130110.1.ATCACGAT.20k_interleaved.fastq. Metadata for this sample file can be found in this workspace Data tab (click on the ""Sample"" data table under the fastq_interleaved column). 
 

#### Outputs  
Separate R1 and R2 FASTQ files (i.e. paired FASTQ), ready to input into the Paired_FASTQ_to_unmapped_BAM tool. 

#### Time and cost estimate  
| Sample Name | Sample Total Size | Time | Cost $ |
| :-------:  | :--------: | :--------: | :----------: |
| NA12878_20k.list |  31.02 MB | 00:03:00 | 0.01 |***


### Paired_Fastq_to_Unmapped_BAM  
This WDL converts paired FASTQ to unmapped BAM formats and adds read group information. 


#### Input requirements/expectations 
 
The tool accepts a readgroup list (in tsv format) containing metadata and paths to pair-end sequencing data in FASTQ format (one file per orientation per sample). 
Requirements/expectations :
- Pair-end sequencing data in FASTQ format (one file per orientation)
- The following metadata descriptors per sample:
  - readgroup=<sample_name>_<flowcell_id>_<lane#>
  - sample_name
  - library_name=<library_name> (see note below)
  - platform_unit=<sample_name>.<flowcell_id>.<lane#> (see note below)
  - run_date=yyyy-mm-dd
  - platform_name=ILMN
  - sequencing_center=BI      

**In most cases, each sample only has one library, so the sample name could be used if the library name is not available.**

**Platform unit is the identifier used by the Picard tools (not readgroup), so PU must be unique for each readgroup.**
 
What the inputs will look like in a table in Terra. 
![](https://storage.googleapis.com/terra-featured-workspaces/sequence-format-conversion/dashboard-images/sample-table-paired2bam.png)     

To run the tool on your own data, adjust the Inputs (in the Tool tab) to reference your own list file.  

You may also be interested in running several pairs of the same sample but different readgroups. This would require another data table with the Readgroup being the unique identifier of the row as seen in the image below
![](https://storage.googleapis.com/terra-featured-workspaces/sequence-format-conversion/dashboard-images/readgroup-table-paired2bam.png)

 **To run this tool on the sample file**: Select ""Process multiple workflows from Sample"" from the Tool menu.  
 
 **To run this tool on your own data**: Generate and upload your own sample.tsv, with the all the input files and metadata descriptors in the proper column as seen in the images above.
 

#### Sample Data  

 You can find a sample paired FASTQ file at: gs://gatk-test-data/wgs_fastq/NA12878_20k/


#### Outputs 
- Set of unmapped BAMs, one per read group
- File containing a list of the generated unmapped BAMs  (If `make_fofn` is enabled)


#### Time and cost estimates
| Sample Name | Sample Total Size | Time | Cost $ |
| :-------:  | :--------: | :--------: | :----------: |
| NA12878_20k.list |  31.02 MB | 00:07:00 | <0.01 |


### CRAM_To_BAM    
This script converts files from CRAM to SAM to BAM format, and outputs a BAM, BAM Index, and validation report to a Google bucket. The reason this approach was chosen instead of converting CRAM to BAM directly using Samtools is because Samtools 1.3 produces incorrect bins due to an old version of htslib 
included in the package. Samtools versions 1.4 & 1.5 have an NM issue that causes them to not validate  with Picard.  


#### Input requirements/expectations
Entity type: Sample   

This tool accepts single or multiple CRAM files.  It is configured to accept data from the sample table in the Data tab.    
An example CRAM file is added to the sample table.  
![](https://storage.googleapis.com/terra-featured-workspaces/sequence-format-conversion/dashboard-images/sample-table-cram2bam.png)

**To run this tool**: Select ""Process multiple workflows from Sample"" from the Tool menu.   

**To run on your own data**, you will need to generate and upload your own sample.tsv load file. 


#### Outputs 
- BAM file and index (written to the workspace data table)
- Validation report (written to the workspace data table)


#### Sample data
You can find a sample CRAM file at gs://gatk-test-data/wgs_cram/NA12878_20k_hg38/NA12878.cram.

#### Time and cost estimates
| Sample Name | Sample Total Size | Time | Cost $ |
| :-------:  | :--------: | :--------: | :----------: |
| NA12878_small |  6.84 MB | 00:07:00 | 0.02 |


### BAM_to_Unmapped_BAM 
This tool is important because unmapped BAM files are the file format generally accepted by most GATK analysis workflows. 

#### Input requirements/expectations 

Entity type: sample  

This tool accepts single or multiple BAM files.  It is configured to accept data from a table in the Data tab.    
An example BAM file is added to the sample table.  
![](https://storage.googleapis.com/terra-featured-workspaces/sequence-format-conversion/dashboard-images/sample-table-bam2ubam.png)

**To run this tool**: Select ""Process multiple workflows from Sample"" from the Tool menu.    

**To run on your own data**, you will need to generate and upload your own sample.tsv load file. 


#### Outputs 
- Sorted Unmapped BAMs

#### Sample data
* BAM file. e.g. NA12878_24RG_small.hg38.bam‎

#### Time and cost estimates
| Sample Name | Sample Total Size | Time | Cost $ |
| :-------:  | :--------: | :--------: | :----------: |
| NA12878_small |  5 GB | 01:3:00 | 0.06 |


### Validate_BAM     
This WDL performs format validation on SAM/BAM files in a list.  


#### Input requirements/expectations 
- One or more SAM or BAM files to validate
- Explicit request of either SUMMARY or VERBOSE mode in inputs.json

- Entity type: NA

#### Outputs
Set of .txt files containing the validation reports, one per input file

#### Sample data

A file containing a list of SAM or BAM files to validate. e.g. NA12878_24RG_small.txt

#### Time and cost estimates  
| Sample Name | Sample Total Size | Time | Cost $ |
| :-------:  | :--------: | :--------: | :----------: |
| NA12878_small |  5 GB | 0:19:00 | 0.03 |

### Validate_Variants
This WDL performs format validation on SAM/BAM files in a list.  


#### Input requirements/expectations 
- One VCF file or GVCF file and its index (can be bgzip/tabix)
- A list of intervals to process (for parallelization)
- Genomic resources: reference genome in FASTA format (.fasta) and its accessory files (.fasta.fai and .dict)

#### Outputs
A list of text files (for each scatter chunk)containing the tool's standard output, which will contain the relevant error message if the tool encounters a validation error.

#### Sample data

A file containing a list of SAM or BAM files to validate. e.g. NA12878_24RG_small.txt

#### Time and cost estimates  
| Sample Name | Sample Total Size | Time | Cost $ |
| :-------:  | :--------: | :--------: | :----------: |
| NA12878_small |  5 GB | 0:19:00 | 0.03 |


-----




### Contact information  
For questions about this workspace please visit the Featured Workspaces Topic topic on the [Terra Community Forum](https://broadinstitute.zendesk.com/hc/en-us/community/topics). Use the search box to see if other users have asked the same question previously. If not, post and tag **@Samantha** so that we get notified.

This material is provided by the GATK Team. Please post any questions or concerns regarding the GATK tool to the [GATK forum](https://gatk.broadinstitute.org/hc/en-us/community/topics)

### Workspace Citation 
Details on citing Terra workspaces can be found here: [How to cite Terra](https://support.terra.bio/hc/en-us/articles/360035343652)

Data Sciences Platform, Broad Institute (*Year, Month Day that this workspace was last modified*) help-gatk/Sequence-Format-Conversion [workspace] Retrieved *Month Day, Year that workspace was retrieved*,  https://app.terra.bio/#workspaces/help-gatk/Sequence-Format-Conversion

### License  
**Copyright Broad Institute, 2019 | BSD-3**  
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at https://github.com/openwdl/wdl/blob/master/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.

### Workspace Change Log
| Date | Change | Author |
| --- | --- | --- |
|  2019-01-02 | Updated WDLs release version, added images of the data table for workflow input description | Beri Shifaw |
|  2020-11-17 | Added Validate_Variants Workflow | Beri Shifaw |
|  2021-09-15 | Addition of Workspace Citation section | Beri Shifaw |   
|  2022-06-23 | Added additional information on metadata fields for paired_fastq_to_unmapped_bam | Allie Cliffe |",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/Sequence-Format-Conversion"
264,"convergneuro-mccarroll-anvil","Broad_ConvergentNeuroscience_McCarroll_Nehme_SupplementaryVillageData","READER","https://app.terra.bio/#workspaces/convergneuro-mccarroll-anvil/Broad_ConvergentNeuroscience_McCarroll_Nehme_SupplementaryVillageData",TRUE,FALSE,NA,NA,NA,"# Convergent-Neuroscience Open-Access Village Data

A collection of data from the collaborative work between the [McCarroll Lab](https://mccarrolllab.org),  [Nehme Lab](https://www.broadinstitute.org/labs/nehme), and [Eggan Lab](https://hscrb.harvard.edu/labs/eggan-lab/) at the Broad Institute's Stanley Center funded by the NIMH's Genetics Neuroscience grant. These experiments involved performing a variety of genomic and phenotypic assays on mixed-donor cultures (which we call villages) of stem cells or differentiated stem cells with different genetic backgrounds. The overarching aim of these experiments was to develop a framework by which we can use stem cell models to research the relationship between common genetic variation in humans and cellular phenotypes. The stem cells used are from the CIRM Induced Pluripotent Stem Cell repository or from the NIH registry of human embryonic stem cells.

There are two preprints available from this collaboration, [Mapping genetic effects on cellular phenotypes with “cell villages”](https://www.biorxiv.org/content/10.1101/2020.06.29.174383v1), and [Natural variation in gene expression and Zika virus susceptibility revealed by villages of neural progenitor cells](https://www.biorxiv.org/content/10.1101/2021.11.08.467815v1.full). This workspace organizes data by these papers and then further breaks down by village.

<style>
	.container {
	    width: 40%;
    height: 40%;
    background: #444;
    margin: 0 auto;
	}

</style>
<div style=""display:flex"">
<img src=""https://www.biorxiv.org/content/biorxiv/early/2020/06/29/2020.06.29.174383/F1.medium.gif"" alt=""smn-paper-figure"" height=""500px"" width=""500px"" sizes=""(max-width: 500px) 500px, 50vw""/>
<img src=""https://www.biorxiv.org/content/biorxiv/early/2021/11/09/2021.11.08.467815/F4.medium.gif"" alt=""zika-paper-figure""  height=""500px"" width=""500px"" sizes=""(max-width: 500px) 500px, 50vw""//>
</div>
	
## About CIRM:

The CIRM Induced Pluripotent Stem Cell (iPSC) Repository is the largest publicly available collection of iPSCs (2607 lines) generated from 2184 donors by the California Institute for Regenerative Medicine (CIRM). This repository provides researchers with access to cells to study how a disease develops and progresses and to facilitate the discovery and testing of new drugs or other therapies. The large size of the collection makes it a powerful resource for studying genetic variation between individuals, helping scientists to understand how disease and treatment may vary in a diverse population. The CIRM iPSC lines are available through the [Fujifilm Cellular Dynamics iPSC Repository](https://www.fujifilmcdi.com/cirm-ipsc-products/).

## Controlled access data

For researchers who wish to access read-level sequencing data for this project, all data is available under controlled access. For data using induced pluripotent stemcells from the CIRM collection, users can apply for access via [dbGAP](https://www.ncbi.nlm.nih.gov/gap/) accession [phs002032](https://www.ncbi.nlm.nih.gov/projects/gap/cgi-bin/study.cgi?study_id=phs002032.v1.p1). Once access users are granted via dbGAP, these data can be found in our corresponding [AnVIL Workspace](https://anvil.terra.bio/#workspaces/anvil-datastorage/AnVIL_NIMH_Broad_ConvergentNeuro_McCarroll_Eggan_CIRM_GRU_VillageData). Controlled access data for experiments using human embryonic stem cell lines can be applied for access through DUOS using the ID [DUOS-000121](https://duos.broadinstitute.org/dataset_statistics/168) and once approved, the data itself can be accessed at [this workspace](https://app.terra.bio/#workspaces/convergneuro-mccarroll-anvil/Broad_ConvergentNeuro_McCarroll_Nehme_hESC_HMB_VillageData).
## Directory Structure


```
zika_paper/
    QueenBasic108_iPSCs/
        QueenBasic.sample_list.txt
        QueenBasic.donor_covariates.txt
        pseudobulk_expression/
            QueenBasic_*.pseudobulk_expression.txt
        filtered_single_cell_expression/
            QueenBasic_*.donors.digital_gene_expression.txt.gz
        full_single_cell_expression/
            QueenBasic_*.digital_gene_expression.txt.gz
        cell_donor_assignment/
            QueenBasic_*.donor_cell_map.txt
        eQTL/
            QueenBasic.eQTL_index_SNPs.txt
            QueenBasic.eQTL_results.txt.gz
		        Genotypes (not uploaded but described)
            Normalized expression data
            Donor covariates + PEER factors
    SNaP44_neural_progenitor_hESCs/
        SNaP44.sample_list.txt
        SNaP44.donor_covariates.txt
        pseudobulk_expression/
            SNaP44_*.pseudobulk_expression.txt
        filtered_single_cell_expression/
            SNaP44_*.donors.digital_gene_expression.txt.gz
        full_single_cell_expression/
            SNaP44_*.digital_gene_expression.txt.gz
        cell_donor_assignment/
            SNaP44_*.donor_cell_map.txt
        eQTL/
            SNaP44.eQTL_index_SNPs.txt
            SNaP44.eQTL_results.txt.gz
        census_seq_summary/
        census_seq_comparison/
    Arrayed_zikv_infectivity/
        SNaP24Arrayed_neural_progenitor_hESCs.zika_infectivity.txt
        SNaP36Arrayed_neural_progenitor_iPSCs.zika_infectivity.txt
sma_paper/
```



## File types

All files are tab-delimited text (with gzip compression if ending in .gz). Files may have comments on lines starting with `#`. Unless stated otherwise,the first non-comment line of the file provides the names of columns.


### Sample List [`sample_list.txt`]

Set of IDs for donors expected in each village, written out as text and separated by newlines. Every ID in this list should be unique and must be a subset of the samples in the VCF file for the cohort. This file contains a single column without  column names.


### Donor Covariates [`donor_covariates.txt`]

Donor metadata for village members. First column IID specifies donor ID, following columns are various covariates for each cohort.


### Pseudobulk Expression [`pseudobulk_expression.txt`]

The first line is a header that begins with GENE followed by donor IDs. The entries of the matrix are integers representing the sum  of transcripts found in cells belonging to donor d (column) for gene g (row). These values can be generated by using the filtered DGE and donor cell map file for the same library. 


### Full Single Cell Expression [`digital_gene_expression.txt.gz`]

This file contains a matrix of cell level expression measured as integer counts of transcripts for all cell barcodes with at least 20 transcripts from a single sequencing reaction. This file includes both true cells, as well as cell barcodes where droplets captured a bead and cell free RNA, but did not capture a cell. The first line is a header line that begins with GENE followed by cell barcode labels. Subsequent lines of the matrix are integers representing the total number of transcripts found in each cell c (column) for gene g (row).


### Filtered Single Cell Expression [`donors.digital_gene_expression.txt.gz`]

This file is identical to the full single cell expression file, but has been filtered by both cell calling and donor assignment QC to produce a subset of high quality cell barcodes for downstream analysis. In order to determine which cell barcodes are kept in the filtered DGE, we first draw a threshold for minimum total transcripts per cell based on the two-dimensional density plot of log10(number of transcripts per cell) vs log10(number of reads per cell / number of transcripts per cell). This selects cell barcodes that contain a cell, and discards cell barcodes that only measure expression of cell free RNA.  Additionally, we select for cells which can unambiguously be assigned to a known donor and are unlikely to be a doublet - that is, a droplet that captured two cells that belonged to different donors.


### eQTL Files 


#### eQTL Results [`eQTL_results.txt.gz,eQTL_index_SNPs.txt`]

There are two types of eQTL outputs reported with each village: a full cis-eQTL results file that contains every SNP by gene interaction for all SNPs within a 10kb window of a given gene, and an index SNPs file, which has been filtered using a hierarchical FDR correction.  This correction involves two steps: first, FDR is controlled for each gene individually by controlling for the number of independent SNPs tested via [eigenMT](https://github.com/joed3/eigenMT).  After all genes have been individually corrected, we further apply an FDR control across the distribution of genes using Benjamini-Hochberg correction to control for the number of genes tested.  Uncorrected and corrected p-values at each level of correction are available.

The columns of the full cis-eQTL results file are as follows:

|                  |                                                                                                                      |
|------------------|----------------------------------------------------------------------------------------------------------------------|
|SNP               |  SNP identifier in the format {CHR}:{POS}:{REF}:{ALT}                                                                |
|gene              | gene name                                                                                                            |
|beta              | value from regression of SNP against gene expression                                                                 |
|t-stat            | t-statistic from regression                                                                                          |
|p-value           | unnormalized p-value estimate calculated by eigenMT                                                                  |
|r2                | squared pearson correlation coefficient                                                                              |
|beta_se           | standard error of beta coefficient                                                                                   |
|snp_pos           |                                                                                                                      |
|snp_end           |                                                                                                                      |
|id                | rsid for SNP (if available)                                                                                          |
|gene_start_pos    |                                                                                                                      |
|gene_end_pos      |                                                                                                                      |
|dist_to_gene      | distance of SNP from the closer of gene_start_pos and gene_end_pos if not gene_start_pos &lt;= POS &lt;= gene_end_pos|
|MAF               | frequency of minor (alternate) allele in the population of donors being tested                                       |
|median_expression | median normalized expression of gene across donors (in transcripts per 100,000)                                      |
|effect_size       | The effect size as the beta of the regression / median_expression of the gene                                        |


The columns of the index SNPs results file have all the same fields as the full eQTL results data but also contains the additional fields:

|                     |                                                                                                                                                                  |
|---------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|gene_permuted_pvalue | adjusted p-value generated by eigenMT after correcting for the number of independent tests [TESTS].  This corrects for the number of SNPs tested for by each gene|
|TESTS                | the number of independent SNPs as generated by eigenMT used for the gene level FDR correction                                                                    |
|qvalue               | The final p-value for the eGene.  This is a correction of the distribution of eigenMT corrected p-values to control for the number of genes tested               |


#### eQTL Gene expression [`gene_expression.txt,gene_expression_peer.txt`]

gene_expression.txt would be the normalized expression: pseudobulked donor-level expression, normalized by the gene/sum(all genes) * 1e5.  gene_expression_peer.txt then runs through [PEER](https://www.sanger.ac.uk/tool/peer/) to extract latent factors, and the residuals of the expression after fitting both known covariates and those latent factors.  gene_expression_peer.txt is the residuals from the regression plus the mean gene expression from the input (normalized) data.


#### eQTL covariates [`covariates.txt`]

Donor-level information that can be used for incorporating covariates into eQTL analyses. Plain text file containing tab-separated values. The first line is a header line that begins with ‘id’ followed by donor IDs. Each subsequent line is a variable, with the  name in the first column and values for each donor in subsequent columns. These covariates are used for eQTL analysis. [PEER](https://www.sanger.ac.uk/tool/peer/) is used to generate latent factors that are also included among these covariates, which we regress out in eQTL analysis in order to focus on true biological signal.


#### Genotype matrix [`genotype_matrix.txt.gz`]

Matrix of SNPs (by {CHR}:{POS}:{REF}:{ALT} format in first column, ""id"") per row by donors (column names are donor IDs) per column. This file is not included in this workspace, but is available for controlled access through our controlled access workspace.


### Cell Donor Assignment [`donor_cell_map.txt`]

Each file has a header with two columns: ""cell"" and ""bestSample"". The first column contains the cell barcode string and the second column  the donor ID of the donor from which the cell originated.


### Zika Infectivity [`zika_infectivity.txt`]

Columns the donor ID, rs34481144 genotype, and Zikv infectivity (i.e. percent of cells in sample infected with Zika virus) as detected by immunostaining. The file SNaP24Arrayed_neural_progenitor_hESCs.zika_infectivity.txt contains zika virus infectivity values for 24 hESC-derived SNaP neural progenitor cell  lines (21 of which are also found in the SNaP44 dataset) using only the Ugandan (Ug) strain of the virus.The file SNaP36Arrayed_neural_progenitor_iPSCs.zika_infectivity.txt contains zika virus infectivity values for 36 iPSC-derived SNaP neural progenitor cell  lines using both the Ugandan (Ug) and the Puerto Rican (Pr) strains of the virus.


### Census-Seq Summary [`census.txt`]

Each census file contains a multi-line comment prefixed by ""#"", followed by a two column tabular output.  The first column DONOR contains the donor ID. The second column REPRESENTATION contains the results of the CensusSeq algorithm’s estimate of the fraction of DNA that can be attributed to that donor. The total representation of all donors sums to 1.  

There are a total of 14 census files included, providing donor representation for 14 different pellets of cells. There are two MOCK pellets: ZikvMOCK_rep1 and ZikvMOCK_rep2. These MOCK pellets are cells not treated with any Zika virus but put through all of the same steps as remaining samples, including processing through the FACS machine. There were also 3 different samples that were sorted into a total of 12 different pellets by FACS. Each of rep1, rep2, and rep3 first had non-infected cells sorted out into a NEG fraction, then of the remaining infected cells, was sorted  into thirds by Zikv viral load. This sorting of infected cells formed a HIGH fraction with the top third of viral load, a MID fraction containing the middle third, and a LOW fraction containing the lower third.

There are 4 census-seq experiments containing results for each of the fractions tested: ""HIGH"", ""LOW"", ""NEG"", and ""MOCK"". The ""MOCK"" values are calculated from running census on an unfiltered pellet of cells not treated with any virus. The ""NEG'' values are calculated from a pellet of cells sorted out by FACS to have negligible levels of Zika virus. The ""LOW"" values are calculated from cells selected by FACS to be in the bottom third of infected cells by viral load. The ""HIGH"" values are calculated from cells selected by FACS to be in the top third of infected cells by viral load. The ""MID"" values are calculated from cells selected by FACS to be in the middle third of infected cells by viral load. There are 2 replicates of MOCK and 3 replicates of each of the other fractions. For the paper, only the first mock replicate was used and the 2nd replicates of the other two were used for analysis.


### Census-Seq Comparisons

Comparisons between donors across different fractions (using MOCK_rep1, NEG_rep2, LOW_rep2, MID_rep2, and HIGH_rep2). Many of these tables were used to generate various figures in the paper, as specified by their filenames.",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","convergneuro-mccarroll-anvil/Broad_ConvergentNeuroscience_McCarroll_Nehme_SupplementaryVillageData"
265,"broad-firecloud-tcga","TCGA_THCA_hg38_OpenAccess_GDCDR-12-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_THCA_hg38_OpenAccess_GDCDR-12-0_DATA",TRUE,TRUE,"Thyroid carcinoma","Tumor/Normal","USA","TCGA Thyroid carcinoma Open-Access Data Workspace

*hg38 TCGA and TARGET workspaces reference files by their GDC UUIDs.  In order to run analyses on the referenced data files, you will need to run workflows that retrieve the files from the GDC and copy them to your workspace bucket.  See   [this forum post](https://gatkforums.broadinstitute.org/firecloud/discussion/10382/populating-hg38-tcga-and-target-workspaces-with-data-files#latest) for instructions on the running of these workflows.*","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients' clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","507","Thyroid","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh38/hg38","TCGA_THCA_hg38_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_THCA_hg38_OpenAccess_GDCDR-12-0_DATA"
266,"help-gatk","Pre-processing_b37_v3","READER","https://app.terra.bio/#workspaces/help-gatk/Pre-processing_b37_v3",TRUE,FALSE,NA,NA,NA,"### GATK Best Practices for Germline SNPs Indels
The workflow takes unmapped pair-end sequencing data [unmapped BAM format](https://software.broadinstitute.org/gatk/documentation/article?id=11008) and returns a CRAM file and a GVCF file and index. A fully reproducible example of the data pre-processing portion of the GATK Best Practices for Germline SNP & Indel Discovery on human whole-genome sequence data.         

Scroll down for details on the workflow, including input and output descriptions and requirements, estimated run times and costs, and information on sample data.  A detailed description of the workflow is available in [Gatk's Best Practices Document](https://software.broadinstitute.org/gatk/best-practices/workflow?id=11145).       

**Note:** This workspace is superseded by the [Whole-Genome-Analysis-Pipeline](https://app.terra.bio/#workspaces/help-gatk/Whole-Genome-Analysis-Pipeline) workspace and is no longer being updated. 

Cost and time estimates will vary with the use of [Preemptibles](https://gatkforums.broadinstitute.org/firecloud/discussion/11786/preemptibles#latest).  
You can also use [Google's cost calculator](https://cloud.google.com/products/calculator/) to estimate cost to run.  For helpful hints on controlling Cloud costs, see [this article](https://support.terra.bio/hc/en-us/articles/360029748111).       

The following material is provided by the GATK Team. Please post any questions or concerns to one of our forum sites : [GATK](https://gatkforums.broadinstitute.org/gatk/categories/ask-the-team/) , [Terra](https://broadinstitute.zendesk.com/hc/en-us/community/topics/360000500432-General-Discussion) , or [WDL/Cromwell](https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team).

---

## The Pre-Processing_B37 workflow
**What does it do?**   
The workflow takes an unmapped pair-end sequencing data (unmapped BAM format) and returns a CRAM file and a GVCF file and index. **Note**: If your data are not in unmapped BAM format, see [this workspace with helpful file format conversion tools](https://app.terra.bio/#workspaces/help-gatk/Sequence-Format-Conversion). This WDL pipeline implements data pre-processing according to the GATK Best Practices on human whole-genome sequence data.  

**What are input data requirements/expectations?**     
The Pre-Processing_B37 workflow accepts a file containing a list of human whole-genome pair-end sequencing data in unmapped BAM (uBAM) format. In particular:    
- One or more read groups, one per uBAM file, all belonging to a single sample (SM)
- Input uBAM files must comply with the following requirements:
  - Filenames all have the same suffix (we use "".unmapped.bam"")
  - Files must pass validation by ValidateSamFile
  - Reads are provided in query-sorted order
  - All reads must have an RG tag
- Reference genome must be B37

**If your sequencing data is not in uBAM format**, check out this file conversion workspace, [https://app.terra.bio/#workspaces/help-gatk/Sequence-Format-Conversion](https://app.terra.bio/#workspaces/help-gatk/Sequence-Format-Conversion) for workflows to convert:    

1. Interleaved FASTQ to paired FASTQ
2. Paired FASTQ to unmapped BAM
3. BAM to unmapped BAM
4. CRAM to BAM files from sequencer output for use in GATK analysis tools     

**What outputs does the workflow return?**  
Metadata for all outputs are written to the workspace data table, and include:
- CRAM, CRAM index, and CRAM md5
- GVCF and its gvcf index
- BQSR Report
- Several Summary Metrics     

**Sample data description and location**  
This workspace data sample table contains metadata for both a full sized and downsampled version of NA12878's of unaligned BAM list file under the column flowcell_unmapped_bams_list. Links to the expected input types are available in the workspace data model for testing.      
 
**Reference data description and location**    
Required and optional references and resources for the methods are included in the Workspace Data table (""Workspace Attributes"" in FireCloud). The reference genome for this workspace is B37.

**Example time and cost to run**   
Below is an example of the time and cost for running the workflow. Note that cost and time will vary with the use of [preemptibles](https://gatkforums.broadinstitute.org/firecloud/discussion/11786/preemptibles#latest).  
You can also use [Google's cost calculator](https://cloud.google.com/products/calculator/). 

| Sample Name | Sample Size | Time | Cost $ |
| ---  | :---: | :---: | :---: |
| NA12878_24RG_small | 3.11 GB | 4:18:00 | 0.77 |
| NA12878 | 64.89 GB | 47:16:00 | 7.23 |

**For more information on understanding and controlling Cloud costs**, see this article ([click https://support.terra.bio/hc/en-us/articles/360029748111](https://support.terra.bio/hc/en-us/articles/360029748111)).

---
---

### Software Versions  
- GATK 4.0.11.0
- BWA 0.7.15-r1140
- Picard 2.16.0-SNAPSHOT
- Samtools 1.3.1 (using htslib 1.3.1)
- Python 2.7

----

### Contact information  
This material is provided by the GATK Team. Please post any questions or concerns to one of our forum sites : [GATK](https://gatkforums.broadinstitute.org/gatk/categories/ask-the-team/) , [Terra](https://broadinstitute.zendesk.com/hc/en-us/community/topics/360000500432-General-Discussion) , or [WDL/Cromwell](https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team).

----

### License  
**Copyright Broad Institute, 2019 | BSD-3**  
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at https://github.com/openwdl/wdl/blob/master/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/Pre-processing_b37_v3"
267,"terra-billing-account-on-gcp","Viral AI","READER","https://app.terra.bio/#workspaces/terra-billing-account-on-gcp/Viral%20AI",TRUE,FALSE,NA,NA,NA,"[Viral AI](https://viral.ai) is a global network for genomic variant surveillance and infectious disease research, developed by [DNAstack](https://dnastack.com). Viral AI was designed to deliver equitable access to software infrastructure for genomic variant surveillance, accelerate international data sharing, and empower scientists and public health officials with globally representative datasets they need to mitigate COVID-19 and future infectious disease outbreaks. 

SARS-CoV-2 genome sequences from international databases have been added with corresponding genome assemblies, variant calls, and lineage assignments, after being re-processed through a harmonized open source bioinformatics pipeline. 

This workspace provides worked examples of how to perform interactive and scalable analyses on data from Viral AI in Terra.

More documentation is available [here](https://docs.viral.ai/analysis/integrations/terra/).

![Viral AI Landing Page](https://i0.wp.com/dnastack.com/wp-content/uploads/2021/12/viral-ai-explorer-01.png?fit=4000%2C2221&ssl=1)",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","terra-billing-account-on-gcp/Viral AI"
268,"featured-workspaces-hca","HCA_Optimus_Pipeline_archive","READER","https://app.terra.bio/#workspaces/featured-workspaces-hca/HCA_Optimus_Pipeline_archive",TRUE,FALSE,NA,NA,NA,"## Optimus Pipeline for Analysis of 3’ Single Cell Transcriptomic Data

The Optimus pipeline, developed by the Data Coordination Platform of the [Human Cell Atlas](https://www.humancellatlas.org/) (HCA DCP), processes 3 prime single cell transcriptome data from the [10x Genomics v2 or v3](https://www.10xgenomics.com/solutions/single-cell/) assay. This workspace currently describes `v1.3.6` of the Optimus pipeline and provides a fully reproducible example of the workflow. You can access previous versions of this pipeline by cloning the workspace and choosing a version in the version dropdown.

The following material is provided by the HCA DCP Pipelines Team.  Please send questions and feedback to data-help@humancellatlas.org, cc Kylee Degatano.  

Scroll down for details on the workflow, including input and output descriptions and requirements, estimated run times and costs, and information on sample data.       

**Note that cost and time estimates will vary** with the use of [Preemptibles](https://gatkforums.broadinstitute.org/firecloud/discussion/11786/preemptibles#latest).  
You can also use [Google's cost calculator](https://cloud.google.com/products/calculator/) to estimate cost to run.  **For helpful hints on controlling Cloud costs**, see [this article](https://support.terra.bio/hc/en-us/articles/360029748111).  

---
---

###  Optimus 
**What does it do?**    

This WDL has a quality control, alignment and transcriptome quantification module. It corrects Cell Barcodes (CBs) and Unique Molecular Identifiers (UMIs), aligns reads to the genome, generates an expression count matrix in a UMI-aware manner, detects empty droplets, calculates summary statistics, and returns outputs in BAM and Zarr file formats. The Zarr file format can optionally be output in Loom format. Special care is taken to avoid the removal of reads that are not aligned or that do not contain recognizable barcodes. This design allows the use of the entire dataset by those who may want to use alternative filtering or leverage the data for methodological development associated with the data processing.

**What does it require as input?** 

The Optimus workflow requires as input the following:

| Name       |  Description      |
| ------------ | ------------------- |
| r1_fastq | forward read, contains cell barcodes and molecule barcodes |
| r2_fastq | reverse read, contains cDNA fragment generated from captured mRNA |
| sample_id | name of sample matching this file, inserted into read group header (must be entered as a String) |
| tar_star_reference | star genome reference (human or mouse), generated with the wdl [here](https://github.com/HumanCellAtlas/skylab/blob/master/library/accessory_workflows/build_star_reference/BuildStarReferenceBundle.wdl)  |
| annotations_gtf | gtf containing annotations for gene tagging (must match star reference and organism) |
| ref_genome_fasta | genome fasta file (must match star reference and organism) |
| whitelist | [10x Genomics](https://www.10xgenomics.com/) cell barcode whitelist for 10x v2 or v3 | 


**Optional Parameters** 

The Optimus workflow offers optional inputs such as the following:

| Name       |  Description      |
| ------------ | ------------------- |
| i1_fastq | index read, not currently used in the test Workflow submissions | 
| fastq_suffix | optional suffix for files that allows automatic detection of compression in some execution environments |
| output_loom | Optionally output a loom formatted count matrix. In versions v1.3.6 and newer. Set to false by default, but set to true in the method config of this workspace | 
| (deprecated) output_zarr | Only in versions prior to v1.3.6, when true the outputs are converted to Zarr format. In v1.3.6, Zarr is always output. See [here](https://github.com/HumanCellAtlas/skylab/blob/optimus_v1.3.6/docs/matrix_format_spec.md) for more information. |

**What does it return as output?**

Metadata for all outputs are written to the workspace data table, and include the following:

| Name       |  Description      |
| ------------ | ------------------- |
| pipeline_version | Version of the pipeline |
| bam | Merged and Sorted BAM file |
| matrix | Sparse count matrix in numpy format |
| matrix_row_index | Sparse count matrix row names in numpy format |
| matrix_col_index | Sparse count matrix column names in numpy format |
| cell_metrics  | Cell metrics table in text format |
| gene_metrics | Gene metrics table in text format | 
| cell_calls | Cell metadata from empytDrops |
| zarr_outputs_file | Count matrix and cell and gene metrics | 
| loom_output_file | Optional output of loom formatted count matrix and metadata |

**Sample data description and location**     

Links to the expected input types/test data are provided in the Sample table of the Data model for testing. The Optimus Workflow tool accepts sample-demultiplexed unaligned FASTQ files as input. The pipeline is a single sample pipeline, but can take in multiple sets of fastqs for a sample that has been split over lanes of sequencing. 

Test data for both human (pbmc4k_human) and mouse (neurons2k_mouse)  are linked in the Sample table of the Data model for testing. The Data model contains downsampled FASTQs from the [10x Genomics](https://www.10xgenomics.com/) PBMC4k human dataset, described [here](https://support.10xgenomics.com/single-cell-gene-expression/datasets/2.0.1/pbmc4k) as well as downsampled FASTQs from the mouse [neuron 2k dataset](https://support.10xgenomics.com/single-cell-gene-expression/datasets/2.1.0/neurons_2000). The data have been downsampled to genic regions of chromosomes 21 and 19 respectively.


**Reference data description and location**  

The required reference genomes (human and mouse) and additional resources for the tools in this workspace are included in the Workspace Data table. The reference genome for human is hg38 (GRCh38), the [GENCODE v27](https://www.gencodegenes.org/human/release_27.html) primary assembly gene annotation list. The reference for mouse is GRCm39, the [GENCODE M21](https://www.gencodegenes.org/mouse/release_M21.html). The cellbarcode barcode whitelist is compatible with the V2 and V3 10x Genomics system.

The source code is available from [GitHub](https://github.com/HumanCellAtlas/skylab/blob/master/pipelines/optimus/Optimus.wdl), an overview of the pipeline can be found on the [HCA Data Portal](https://prod.data.humancellatlas.org/), and the benchmarking that was performed on the pipeline can be found [here](https://docs.google.com/document/d/158ba_xQM9AYyu8VcLWsIvSoEYps6PQhgddTr9H0BFmY/edit#heading=h.calfpviouwbg).


**Estimated time and cost to run on sample data** 

The following estimates are based on two sets of data, human and mouse, each containing different numbers of samples. All details of each set are listed to give insight into time and cost.
 
| Sample Set Name | Set Size | Sample Set R1.fastq Size | Sample Set R2.fastq Size | Time | Cost $ |
| :---:  | :---: | :---: | :---: | :---: | :---: |
| neurons2k_mouse | 6 entities | 88.26 MB | 277.58 MB |2:52:00 | 0.82 |
| pbmc4k_human | 2 entities | 26.84 MB | 59.58 MB |2:04:00 | 0.61 |

**For helpful hints on controlling Cloud costs**, see this article ([click here to view](https://support.terra.bio/hc/en-us/articles/360029748111)).   

**Versions**

All versions listed here are available by cloning this workspace and selecting the version on the Optimus method. Other versions are listed as available, but only the versions below will be compatible with Terra.

| Terra Compatible Version Name | Optimus Release Version | Date | Release Note | 
| :---:  | :---: | :---: | :---: |
| optimus_1.3.6_terra_patch2 | v1.3.6 (current) | 09/23/2019 | Optimus now optionally outputs a Loom formatted count matrix, which is turned on in this workspace's method configuration. |
| optimus_v1.3.3_terra | v1.3.3 | 08/29/2019 | This version and newer have been validated to additionally support Mouse data using the GENCODE M21 reference. The gene expression per cell is now counted by GENCODE geneID instead of gene name. There is an additional output mapping geneID to gene name provided. This is a breaking change. | 
| terra-optimus | v1.0.0 |03/30/2019 | Initial pipeline release. Validated on hg38 GENCODE v27. | 

---
---

### Contact Information
This material is provided by the HCA DCP Pipelines Team.  Please send questions and feedback to data-help@humancellatlas.org, cc Kylee Degatano.  

---
---

### License
**Copyright Human Cell Atlas Authors, https://humancellatlas.org, 2019 | BSD-3**

All rights reserved. Full license text at https://github.com/HumanCellAtlas/skylab/blob/master/LICENSE. Note that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.

---",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","featured-workspaces-hca/HCA_Optimus_Pipeline_archive"
270,"broad-firecloud-cptac","PANOPLY_Production_Modules_v1_3","READER","https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_Production_Modules_v1_3",TRUE,FALSE,NA,NA,NA,"# PANOPLY: A cloud-based platform for automated and reproducible proteogenomic data analysis
#### Version 1.3

PANOPLY is a platform for applying state-of-the-art statistical and machine learning algorithms to transform multi-omic data from cancer samples into biologically meaningful and interpretable results. PANOPLY leverages [Terra](http://app.terra.bio)—a cloud-native platform for extreme-scale data analysis, sharing, and collaboration—to host proteogenomic workflows, and is designed to be flexible, automated, reproducible, scalable, and secure. A wide array of algorithms applicable to all cancer types have been implemented, and available in PANOPLY for analysis of cancer proteogenomic data.


| ![PANOPLY Overview ](https://raw.githubusercontent.com/broadinstitute/PANOPLY/dev/panoply-overview-v2.png) |
|:--:|
|  *Figure 1. Overview of PANOPLY architecture and the various tasks that constitute the complete workflow. Tasks can also be run independently, or combined into custom workflows, including new tasks added by users. Inputs to PANOPLY consists of (i) externally characterized genomics and proteomics data (in gct format); (ii) sample phenotype and annotations (in csv format); and (iii) parameters settings (in yaml format). Panoply modules are grouped into Data Preparation Modules (green box), Data Analysis Modules (blue box) and Report Modules (red box). Data preparation modules perform quality checks on input data followed by optional normalization and filtering for proteomics data. Analysis ready data tables are then used as inputs to the data analysis modules. Results from the data analysis modules are summarized in interactive reports generated by appropriate report modules. Modules under development are listed in grey text.* |


PANOPLY consists of the following components:

* A Terra production workspace on [PANOPLY_Production_Pipelines_v1_3](https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_Production_Pipelines_v1_3) with a preconfigured unified workflow to automatically run all analysis tasks on proteomics (global proteome, phosphoproteome, acetylome, ubiquitylome), transcriptome and copy number data; and an [additional workspace](https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_Production_Modules_v1_3) that includes separate methods for each analysis component. 
* An interactive Jupyter notebook (included in the Terra workspaces) that provides step-by-step instructions for uploading data, identifying data types, specifying parameters, and setting up the PANOPLY workspace for analyzing a new dataset.
* A GitHub [repository](https://github.com/broadinstitute/PANOPLY) that contains code for all PANOPLY tasks and workflows, including R code for implementing analysis algorithms, task module creation, and release management. A GitHub [wiki](https://github.com/broadinstitute/PANOPLY/wiki) includes documentation and description of algorithms. 
* A [tutorial](https://github.com/broadinstitute/PANOPLY/wiki/PANOPLY-Tutorial) illustrating the application of PANOPLY to a published breast cancer dataset and demonstrating the practical relevance of PANOPLY by regenerating many of the results described in (Mertins et al) (1) with minimal effort. The [PANOPLY_Tutorial](https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_Tutorial) workspace contains data and results from running the tuorial.
* Terra workspaces showing case studies of applying PANOPLY to the analysis of [CPTAC BRCA](https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_CPTAC_BRCA) (7) and [CPTAC LUAD](https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_CPTAC_LUAD) (8) datasets. The workspaces include all data, parameter settings and results.


PANOPLY provides a comprehensive collection of proteogenomic data analysis methods including sample QC (sample quality evaluation using profile plots and tumor purity scores (1), identify sample swaps, etc.), association analysis, RNA and copy number correlation (to proteome), connectivity map analysis (1 ,2), outlier analysis using BlackSheep (3), PTM-SEA (4), GSEA (5) and single-sample GSEA (6), consensus clustering, and multi-omic clustering using non-negative matrix factorization (NMF). Most analysis modules include a report generation task that outputs a HTML interactive report summarizing results from the respective analysis tasks. Mass spectrometry-based proteomics data amenable to analysis by PANOPLY includes isobaric label-based LC-MS/MS approaches like iTRAQ, TMT and TMTPro profiling the proteome and multiple PTM-omes including phospho-, acetyl-and ubiquitylomes.

Users can also [customize or create new pipelines and add their own tasks](https://github.com/broadinstitute/PANOPLY/wiki/Customizing-PANOPLY) and integrate them into the PANOPLY.


## Quick Start

* For a quick introduction and tour of PANOPLY, follow the [**tutorial**](https://github.com/broadinstitute/PANOPLY/wiki/PANOPLY-Tutorial). 
* Detailed **documentation** can be found [here](https://github.com/broadinstitute/PANOPLY/wiki).

### Contact

Email proteogenomics@broadinstitute.org with questions, comments or feedback.


### Citation

Mani, D. R. et al. PANOPLY: a cloud-based platform for automated and reproducible proteogenomic data analysis. *Nature Methods* 1–3 (2021) doi:10.1038/s41592-021-01176-6.
  

### References

1. Mertins, P. et al. Proteogenomics connects somatic mutations to signalling in breast cancer. *Nature* 534, 55–62 (2016).
2. Subramanian, A. et al. A Next Generation Connectivity Map: L1000 Platform and the First 1,000,000 Profiles. *Cell* 171, 1437–1452.e17 (2017).
3. Blumenberg, L. et al. BlackSheep: A Bioconductor and Bioconda package for differential extreme value analysis. *J of Proteome Research* 20(7), 3767-3773 (2021).
4.	Krug, K. et al. A Curated Resource for Phosphosite-specific Signature Analysis. *Mol. Cell. Proteomics* 18, 576–593 (2019).
5.	Subramanian, A. et al. Gene set enrichment analysis: a knowledge-based approach for interpreting genome-wide expression profiles. *Proc. Natl. Acad. Sci.* 102, 15545–15550 (2005).
6.	Barbie, D. A., Tamayo, P., Boehm, J. S., Kim, S. Y. & Moody, S. E. Systematic RNA interference reveals that oncogenic KRAS-driven cancers require TBK1. *Nature* (2009).
7. Gillette, M. A. et al. Proteogenomic Characterization Reveals Therapeutic Vulnerabilities in Lung Adenocarcinoma. *Cell* 182, 200–225.e35 (2020).
8. Krug, K., Jaehnig, E. J., Satpathy, S., Blumenberg, L., et al. Proteogenomic Landscape of Breast Cancer Tumorigenesis and Targeted Therapy. *Cell* 183, 1–21 (2020).
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","broad-firecloud-cptac/PANOPLY_Production_Modules_v1_3"
271,"kco-incubator","COVID-19_cross_tissue_analysis","READER","https://app.terra.bio/#workspaces/kco-incubator/COVID-19_cross_tissue_analysis",TRUE,FALSE,NA,NA,NA,"The COVID-19 pandemic, caused by the novel coronavirus SARS-CoV-2, underscores the urgent need to identify molecular mechanisms that mediate viral entry, propagation, and tissue pathology in distinct cell types across organs. The surface receptor angiotensin-converting enzyme 2 (**ACE2**) and the associated proteases, transmembrane protease serine 2 (**TMPRSS2**) and Cathepsin L (**CTSL**), were previously identified mediators of SARS-CoV cellular entry. 

### Experimental Overview

We use single-cell RNA-seq (scRNA-seq) across diverse tissues to assess the cell-type-specific expression of ACE2, TMPRSS2, and CTSL. We identify specific subsets of respiratory epithelial cells as putative targets of viral infection, including subsets of epithelial cells in the nasal passages, lung and airways. Additionally, we detect expression in other tissues that may serve as routes of viral transmission, including the gut and corneal epithelia, and in cells potentially associated with COVID-19 clinical pathology including cardiomyocytes, olfactory sustentacular cells, and renal epithelial cells.

For more details about the resources presented in this Terra Workspace, please use [this link](https://www.biorxiv.org/content/10.1101/2020.04.19.049254v1.full.pdf) to refer to the official manuscript hosted on Biorxiv.

### Contact Information

For questions about this workspace, please contact the appropriate channel listed below based on your type of inquiry:
* questions about the content of the workspace: Christoph Muus -- muus@broadinstitute.org
* questions about the Terra platform: Terra Support -- Terra-support@broadinstitute.zendesk.com
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","kco-incubator/COVID-19_cross_tissue_analysis"
272,"broad-firecloud-wupo1","CLLmap_Methods_Apr2021","READER","https://app.terra.bio/#workspaces/broad-firecloud-wupo1/CLLmap_Methods_Apr2021",TRUE,FALSE,NA,NA,NA,"# Terra workflows used in the CLL-map project
### The CLL-map project: Assembly and analysis of genetic, transcriptomic, epigenetic and clinical data from over 1100 CLL patients
### Citation: Knisbacher, Lin, Hahn, Nadeu, Duran-Ferrer et al., Nature Genetics, 2022

#### CLL-map Data Portal: https://data.broadinstitute.org/cllmap/

#### CLL-map GitHub repository: https://github.com/getzlab/CLLmap

For questions, please contact:  bknisbac@broadinstitute.org (Binyamin Knisbacher)

### **IMPORTANT NOTES:**
#### 1. Workspace description:
This Terra workflow-only workspace is primarily intended for review and educational purposes.
It is a snapshot in time of the workflows and individual tools as used for the CLLmap project, but many tools have been evolving.
We, therefore, recommend that users refer to the source GitHub repositories to find the most updated
versions of the tools and search https://portal.firecloud.org/?return=terra#methods for updated Terra workflows.

#### 2. Credit and citation:
Most Terra workflows are wrappers of existing software. Therefore, if you use these workflows, **please cite the respective tools** that the workflow runs.
We provide references to GitHub repos and publications to aid in doing so.

**If this repo enabled or accelerated your research, please **also** cite the CLL-map (Knisbacher et al, Nature Genetics, 2022)**

#### 3. Code license:

Please refer to GitHub repos per tool for license, or contact authors. Most Terra workflows are under BSD-3 license (Broad Institute). See the LICENSE.txt file in Workspace Attributes.

See at bottom of this page the source workflows from which workflows here were derived. This has implications for credit and licensing.

# WORKFLOWS

## Whole-exome sequencing processing and mutation calling
**Workflow: Realign_hg38_bam_to_hg19_bam_WES** -  Realignment workflow to lift up/down a genome version.

**Workflow: WES_pipeline_fastq_bwamem_bqsr_coclean_MarkDuplicates** - Whole exome sequencing (WES) alignment pipeline (from FASTQ to BAM) using the BWAMEM algorithm, together with co-clean and mark duplicates tasks.

**Workflow: CGA_WES_Characterization_Pipeline_v0.2_Jun2019_fix_detin_bug_add_strelka2_indels_for_detin** - Somatic short variant detection using matched tumor and normal WES BAMs. Applies QC and various characterization tools.
For details and a comprehensive user manual, see the official Broad Cancer Genome Analysis WES pipeline Firecloud/Terra Method: https://portal.firecloud.org/?return=terra#methods/getzlab/CGA_WES_Characterization_Pipeline_v0.2_Jun2019/5

**Workflow: Mutect1_ForceCall_extract** - A variant discovery tool, currently used for somatic SNV discovery and outputs all alternative alleles (cite: Cibulskis et al, Nature, 2013).

## Downstream analyses of genetic alterations
**Workflow: mutation_mutsig2cv_hg19** - Somatic driver gene detection with MutSig2CV (cite: Lawrence, Nature, 2014). https://github.com/getzlab/MutSig2CV

**Workflow: CopyNumber_Gistic2_hg19** - Somatic copy number alteration driver detection with GISTIC 2.0 (cite: Mermel, Genome Biology, 2011).

**Note: CLUMPS for driver detection using 3D protein structures was not run in Terra. Find it here: https://github.com/getzlab/CLUMPS

**Workflow: PhylogicNDT** - Tumor evolutionary trajectory inference (cite: Gruber, Nature, 2019). https://github.com/broadinstitute/PhylogicNDT

**Workflow: Mutation_Signature_Analyzer_Config** - Mutational signature analysis (cite: Kim, Nature Genetics, 2016; Kasar, Nature communications, 2015). https://github.com/broadinstitute/SignatureAnalyzer-GPU

## Whole-genome sequencing processing
Pipeline is essentially identical to WES pipeline.
The most updated WGS pipeline developed in the Getz lab can be viewed here: https://github.com/getzlab/hg19_WGS_pipeline_wolF

## Structural variants pipeline

SV pipeline developed by Chip Stewart, Broad Institute.

For pipeline details, see supplementary material of Morton et al, Science, 2021 at https://www.science.org/doi/10.1126/science.abg2538
Morton et al should be cited if you use this SV pipeline.

Official methods for this SV pipeline were released in: https://app.terra.bio/#workspaces/rebc-oct16/REBC_methods_only

### Step 1 - SV tools
**Workflow: dRanger_pipette_WGS** - dRanger SV detection method (cite one or all: Chapman, Nature, 2011; Berger, Nature, 2011; Bass, Nat Genet, 2011)

**Workflow: manta** - Manta SV detection method (cite: Chen, Bioinformatics, 2016)

**Workflow: SvABA_xtramem** - SvABA SV detection method (cite: Wala, Genome Res, 2018)

### Step 2 - SV method format harmonization
**Workflow: extract_dRanger_intermediates** - reformat dRanger output

**Workflow: mantavcf2dRangerForBP** - reformat Manta output

**Workflow: svaba_snowmanvcf2dRangerForBP** - reformat SvABA output

### Step 3 - SV tool result integration and post-filtering
**Workflow: SV_cluster_forBP** - cluster SV calls before Breakpointer

**Workflow: breakpointer** - Run breakpointer on aggregate results

**Workflow: Breakpointer_fix_sample** -Breakpointer fix sample

**Workflow: SV_consensus_filter** - Filter based on tool agreement on SV calls

## B-cell receptor characterization
**Workflow: mixcr_bam_flexible** - MIXCR method for BCR characterization (cite: Bolotin, Nature Methods, 2015; Bolotin, Nature Biotech, 2017)

For details: https://github.com/milaboratory/mixcr

**Workflow: IgCaller** - IgCaller method for BCR characterization. Includes IGHV sequence assembly used in IMGT/VQUEST to determine IGHV mutation status (cite: Nadeu, Nat Comm, 2021; Knisbacher, Nat Genet, 2022)

For details: https://github.com/ferrannadeu/IgCaller

## RNA-seq processing (GTEx pipeline)
For details: https://github.com/broadinstitute/gtex-pipeline

**Workflow: samtofastq_v1-0_BETA_cfg** - Preprocessing - convert BAM to fastq before re-alignment with STAR

**Workflow: star_v1-0_BETA_cfg** - RNA alignment using STAR (cite: Dobin, Bioinformatics, 2013)

**Workflow: markduplicates_v1-0_BETA_cfg** - Mark duplicate reads in the aligned BAM (used in RNA-SeQC)

**Workflow: rnaseqc2_v1-0_BETA_cfg** - Quality Control for aligned RNA-seq BAMs. (cite: Graubert, Bioinformatics, 2021)

Counts and TPMs generated by RNA-SeQC were used for gene expression analyses in the CLL-map project.

## Methylation (RRBS processing)

Three workflows were used for the RRBS (Reduced-representation bisulfite sequencing) data, based on the sequencing protocol and consequential trimming required.

For details: https://github.com/getzlab/RRBS_tools (specifically the BSMAP and Docker directories).

We thank Helene Kretzmer and Sven Klages (Max Planck Institute for Molecular Genetics) for contributing code used at the core of this pipeline.

cite: Knisbacher, Nature Genetics, 2022 (CLLmap); Xi, BMC bioinformatics, 2009 (BSMAP), Sun, Genome Biology, 2014 (mcall module of MOABS)

Each workflow includes:

(1) FASTQC for FASTQ quality control

(2) FASTQ trimming

(3) extracting BAM statistics

(4) alignment with BSMAP

(5) MarkDuplicates

(6) MCALL for methylation calling

(7) MULTIQC for BAM QC

**Workflow: bsmap_mcall_SE_uniq** - Single-end RRBS processing workflow

**Workflow: bsmap_mcall_PE_trim5p2R2** - Paired-end RRBS processing workflow for samples that needed 2 bases trimmed at 5' of Read2

**Workflow: bsmap_mcall_PE_trimRRBS_trim5p6R1_trim5p6R2** - Paired-end RRBS processing workflow for samples that needed 6 bases trimmed at 5' of Read1 and Read2

## Multiomic workflows

**Workflow: CrossCheckLaneFingerprints_WESvsRNA** - CrossCheckFingerprint to rule out sample swaps in multiomic data (cite: Javed, Nature comm, 2020). The example here is for WES vs RNA-seq.

https://gatk.broadinstitute.org/hc/en-us/articles/360037594711-CrosscheckFingerprints-Picard

**Workflow: mutation_validator** - uses multiple data types to confirm presence of a pre-defined set of mutations. Applied in CLL-map to assist in manual review of driver mutation calls

https://github.com/chipstewart/MutationValidator

# SOURCE WORKFLOWS
The following list maps CLLmap workflows to the original 'source' workflow from which they were derived (refer to source method for license and credit):

**Source namespace/workflow -> CLLmap workflow**

GPTAG/BamRealigner -> Realign_hg38_bam_to_hg19_bam_WES

WES-Utils/WES_pipeline_fastq_bwamem_bqsr_coclean_MarkDuplicates -> WES_pipeline_fastq_bwamem_bqsr_coclean_MarkDuplicates

getzlab/CGA_WES_Characterization_Pipeline_v0.2_Jun2019 -> CGA_WES_Characterization_Pipeline_v0.2_Jun2019_fix_detin_bug_add_strelka2_indels_for_detin

getzlab/Mutect1 -> Mutect1_ForceCall_extract

broadgdac/mutation_mutsig2cv -> mutation_mutsig2cv_hg19

broadgdac/copy_number_gistic2 -> CopyNumber_Gistic2_hg19

jcha/PhylogicNDT -> PhylogicNDT

broadgdac/mutation_signature_analyzer -> Mutation_Signature_Analyzer_Config
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","broad-firecloud-wupo1/CLLmap_Methods_Apr2021"
273,"biodata-catalyst","TOPMed Aligner Gen3 Data","READER","https://app.terra.bio/#workspaces/biodata-catalyst/TOPMed%20Aligner%20Gen3%20Data",TRUE,FALSE,NA,NA,NA,"# TOPMed Aligner (Template Workspace Version)

In this workspace, you will be running the TOPMed alignment workflow as you learn how to use the BioData Catalyst platform powered by Terra, Dockstore, and Gen3. This is beginner-oriented tutorial walks you through how each of these platforms interact with one another, specifically, how to find and hand off data from Gen3 to Terra, and then set the data up for use with a Dockstore workflow. 

The workflow in this tutorial is the TOPMed aligner. This workflow is a containerized version of the pipeline University of Michigan developed to process all CRAM files available in the TOPMed program. You can use this aligner workflow to prepare data for comparison with TOPMed CRAM files. 

## Data Models Covered in this Tutorial
One aim of this tutorial is to help you learn the Gen3 data model for TOPMed and how to interact with it. This can help you bring your own data to the BioData Catalyst platform and compare it with the TOPMed data that is currently hosted in this system.

You will learn how to import TOPMed data from Gen3, which uses a more complex graph-based data model than the data model used for what's already hosted on Terra, such [as Terra's version of the 1000 Genomes](https://app.terra.bio/#library/datasets/public/1000%20Genomes/data-explorer). However, in order to use TOPMed data from Gen3, you must be approved to use TOPMed data via [dbGaP](https://www.ncbi.nlm.nih.gov/books/NBK99225/). If you don't have dbGaP access, worry not -- you'll still be able to run this workflow on 1000 Genomes Data, as long as you have an NIH credential such as [eRA Commons](https://era.nih.gov/faqs.htm#II) that can get you into Gen3.

## Cost Estimate
Your costs may vary depending on how your data is formatted and what parameters you use. In addition, if you are using preemptibles, there is some element of randomness here -- a preemptible may or may not be stopped by Google at any given time, causing an in-progress task to need to restart.

When running the aligner workflow on 10 full-size CRAMs from the PharmaHD study imported from Gen3, using the aligner's default settings, the cost was $80.38 as reported by Terra. The most expensive of those ten files cost $10.82 and the least expensive cost $5.74.
## Setting Up
Because Gen3 hosts controlled-access data, you will need to set up your Terra account for the linkage to Gen3 to work properly. Please see [Terra's documentation on the subject](https://support.terra.bio/hc/en-us/articles/360037648172-Accessing-TCGA-Controlled-Access-workspaces-in-Terra).

If you are planning to work with controlled access data, you should set up an Authorization Domain to protect your work. This will prevent you from inadvertently sharing data that shouldn't be shared. Learn more in this [article](https://support.terra.bio/hc/en-us/articles/360039415171).

# Using Gen3
If you are new to using Gen3 and Terra, you may want some background information. A good overview can be found in [Understanding and Using Gen3 Data in Terra](https://support.terra.bio/hc/en-us/articles/360038087312), which will introduce you to the Gen3 data model and how it is stored in Terra. Those seeking more specific information may want to use these resources:
* [How Terra stores data in tables](https://support.terra.bio/hc/en-us/articles/360025758392-Managing-data-with-tables-)
* [How Gen3 stores data](https://bdcatalyst.gitbook.io/biodata-catalyst-documentation/explore_data/gen3-discovering-data)
* [Gen3's data dictionary](https://gen3.biodatacatalyst.nhlbi.nih.gov/DD)

Head on over to [Gen3](https://gen3.biodatacatalyst.nhlbi.nih.gov/) and log in by clicking ""Profile"" in the top right hand corner. You will need to log in using your NIH eRA Commons ID. When you click on ""Exploration"" you will see all subjects and studies you have access to. Use filters on the right hand side of the screen to select what you are interested in. 

#### Selecting by study/project
If you're interested in a particular study, click the ""subject"" tab on the leftmost toolbar. You will see a dropdown menu to select by project ID. In this example, we will search by Project ID for ""1000 Genomes"".

![censored view of Gen3 when you are logged in, with ""export to Terra"" button highlighted](https://github.com/aofarrel/tutorials/blob/master/gen3_overview_with_text_resized.png?raw=true)

#### Importing to Terra
After selecting a project, click the red ""Export To Terra"" button. Bear in mind that exporting may take a few minutes and you shouldn't navigate away from the page while this is happening. Once it completes, you will be taken to a Terra page where you can choose which workspace to put your data into. From here, you should select ""template workspace"" and import into the TOPMed aligner workspace.

![screenshot showing the workspace import, with the topmost selection on the right annotated with a circle as it's the option that allows us to import to a template workspace](https://raw.githubusercontent.com/aofarrel/tutorials/master/bdc_template_import_resized.png)

What happens next? A copy of this exact workspace will be created, with several data table that hold administrative, and any clinical and biospecimen data associated with the project. Biospecimen data tables will hold DRS that point to the location of the genomic data hosted by NHLBI. You can learn more about DRS [here](https://support.terra.bio/hc/en-us/articles/360039330211-Data-Access-with-the-GA4GH-Data-Repository-Service-DRS-). It will take a few minutes for the data to fully import, but once it does, you'll see the workspace has been populated with several data tables.

One of the data tables you'll see is called ""Submitted Aligned Reads."" If you scroll across that in Terra's UI, you will see a column named ""data_format,"" indicating that these are CRAM files. But where are those files actually? Keep scrolling and you will see ""pfb:object_id"" as a column header, and under that, several drs:// URIs. This is what Terra will be using to locate the files. Thankfully, you don't need to remember these URIs. When running a workflow, if we want to run a WDL on this data, we can reference that pfb:object_id column in order to enter dozens (hundreds, even) of URIs into a workflow with just a few clicks.

# Running the Aligner
The links at the top of this section should serve as an explanation as to how Gen3 data is stored. But even with that background, it may still look a little odd when imported into Terra, so let's walk through how to use these tables. If you want a more complete explanation of how these tables relate to each other, please see the optional section towards the end of this workspace.

Go to this workspace's workflows tab, and you will see a WDL has already been imported from Dockstore: a Dockerized version of the [University of Michigan's aligner for TOPMed data](https://dockstore.org/workflows/github.com/DataBiosphere/topmed-workflows/UM_aligner_wdl:1.32.0?tab=info). Select it. 

![the two buttons in your workspace's data section, with the leftmost allowing you to make a new workflow, and the righmost one being for the TOPMed aligner workflow](https://raw.githubusercontent.com/aofarrel/tutorials/master/aligner_workflow_resized.png)

You will see two buttons. Select the bubble labeled ""Run workflow(s) with inputs defined by data table"", and in the drop down menu, select ""Submitted Aligned Reads"". 

To select only particular participants, click ""Select Data"" which is located to the right of the Step 2 heading. This will open a new menu where you can select precisely which rows you want to run your workflow on. In this case, each row represents a participant.

![screenshot showing the workflow input page on Terra, as described in the text proceeding this image](https://raw.githubusercontent.com/aofarrel/tutorials/master/submitted%20aligned%20reads%20redo.png)

Below this, you will see several arguments that can be used for this workflow. If it's not already filled out, type out `this.pfb:object_id` as the `input_cram_file`.

But what is ""this""? In Terra, we can use ""this"" to represent the data table that we selected from the drop down menu. So, in this case, ""this"" refers to the Submitted Aligned Reads data table. As for "".pfb:object_id"", that part means that Terra is looking at the column called ""pfb:object_id"" in said data table. 


For more information on using workflow inputs on Terra, please see [Terra's documentation on setting inputs.](https://support.terra.bio/hc/en-us/articles/360026521831-Configure-a-workflow-to-process-your-data)

### To Preempt or Not to Preempt?

*If you already know what preemptibles are and don't want to use them, you can skip this section, as this template workspace is set up to avoid them by default.*

When computing on Google Cloud, you have the option of using preemptible virtual machines. These virtual machines work essentially the same as what you expect from Google Cloud, but they are significantly cheaper (sometimes less than half the price!). There is a catch, however -- they may shut down in the middle of a task and only exist for 24 hours at most. You can find more information [from Google](https://cloud.google.com/preemptible-vms/), but let's talk about the specifics of this workspace. 

This isn't a concern if you were running on small test CRAM files such as the ones on the aligner's Dockstore sample JSON. But when running on full-size CRAM files such as ones imported from Gen3, there is a chance that any given step (pre-alignment, alignment, or post-alignment) might get shut down before it completes if it's run on a preemptible VM. This is especially true of the post-align step, which tends to take the longest, often more than 24 hours when running on full-sized CRAM files.

We've broken down some possibilities here so you can balance risk, run time, and cost:


| Setting        | Max # of Tries On Preemptible VM           | Max # of Non-Preemptible Tries  |  Total Max Tries |
| ------------- |:-------------:|:-------------:|  -----:|
| `PostAlign_preemptible_tries`​ = 0 and `PostAlign_max_retries`​ = 1 |  0  | 1 | 1 |
| `PostAlign_preemptible_tries`​ = 3 and `PostAlign_max_retries`​ = 3 |  3  | 3 | 6 |
| You don't set `PostAlign_preemptible_tries`​ nor `PostAlign_max_retries`​, ie, default values are used |  0  | 3 | 3 |

Whenever `preemptible_tries` is a positive integer, the task will exhaust all preemptible tries before trying on the more expensive non-preemptible VM. The same logic applies for `PreAlign_preemptible_tries` and `PreAlign_max_retries`, as well as `Align_preemptible_tries` and `Align_max_retries`. Terra handles all this on its own; you don't have to manually restart any of these tasks as long as the workflow itself is still running.

Once you have decided if you want to change the default behavior for preemptible VMs, press ""save"" and run your workflow.

### Tracking the Aligner's Progress
With your workflow now running, click on the ""JOB HISTORY"" tab at the top of this workspace. You will see a table detailing every submission you have made in your workspace.

Let's click on the top row, which shows the most recently submitted workflow. You will see something like this -- metadata on the top, and below that, a one-row table. If you click on ""view"" in the leftmost tab of the table (marked with a square below), you will more information about the workflow, including how long it's running certain tasks. In this case, that means pre-alignment, alignment, and post-alignment. This can come in handy of a workflow fails, as this screen will tell you how long each task has run for, how many attempts it had, and (if applicable) any logs created in the process. You can also click ""workflow ID"" in the leftmost column (circled) to be taken to Google Cloud's own website to explore the bucket, although you can do the same on Terra itself.

Take note of the submission ID and workflow ID, or at least the first few digits of each. They'll come in handy in a moment.

![screenshot of job history page with view, workflow ID, and submission ID circled](https://github.com/aofarrel/tutorials/blob/master/resized_smaller_workflow_history.png?raw=true)

### Examining Output
As the aligner does its thing, it will begin writing files to the Google Cloud bucket your workspace is hosted on. You can find this in the ""Files"" section of your workspace's DATA tab. Note that this ""Files"" section will be the lefthand side below all of the data tables you imported from Gen3, so you'll probably have to scroll a bit to see it.

[//]: # ({Will need to change this if screenshots are retaken with diff IDs})

Remember that workflow ID we took note of earlier? That workflow ID is also the name of the folder that your workflow is writing to. Keep in mind *every time* you run a workflow, a new folder will be created, even if you are just running the same workflow on the same data with the same parameters. In your workspace's file system, your output data for the aligner will be stored in `Files/{submission ID}/TopMedAligner/{workflow ID}/call-PostAlign/`. So, for instance, in the screenshot above my submission ID started with ""b17b9937-ff17-46e8-b20b-798dc3d4ebf2"" and my workflow ID started with ""2368abf3-0e2a-4cd7-b10a-89d4d1f9002d."" So my output files are located in `Files / b17b9937-ff17-46e8-b20b-798dc3d4ebf2 / TopMedAligner / 2368abf3-0e2a-4cd7-b10a-89d4d1f9002d / call-PostAlign /`. Of course, these files will only be created once the aligner finishes -- if your workflow is running but hasn't finished yet, the `call-PostAlign` folder might not exist yet.

# Running on More Than One Data Table
### This is optional to run the aligner included in this workspace, however, certain other workflows may require you to draw upon more than one table, so this is good information to learn.
Go back to the workflow tabs and again select the aligner. However, this time, select ""Aligned Reads Index"" instead of ""Submitted Aligned Reads"". Yes, this is a table of CRAI files, not CRAM files -- but like the CRAM table, it too links to its files using DRS URIs in the pfb:object_id column. Don't worry, we'll get back to the CRAMs in a moment.

![screenshot of Terra's workflow UI page with the submitted aligned reads index table selected via dropdown menu](https://raw.githubusercontent.com/aofarrel/tutorials/master/aligned%20reads%20index%20redo.png)

Like what was indicated in the section detailing Gen3's data structure, your input will depend on what table you select. In our case, we selected Aligned Reads Index. So that means that now anything that says ""this"" for an input will be looking at the Aligned Reads Index table.

In this version of the TOPMed aligner, CRAI files optional, but for the sake of learning more about Gen3's data structure we will be using them. In Gen3's data structure, CRAI files are considered a child of CRAM files, or in other words, ""submitted aligned reads"" table (which includes the CRAM files and their metadata) is the parent of ""aligned reads index"" table (the CRAI files and their metadata). When dealing with Gen3 data, children know their parents, but not vice versa. **In summary, the table containing CRAI files also links to the table that contains CRAM files, but not vice versa.** In this particular example, that link is stored in the CRAI table as a column named submitted_aligned_reads. Note that it does not link to the table overall, but rather a given row of that table: Each row on the CRAI table points to the row of the CRAM table that its associated with. This can be a bit confusing to wrap your head around, so feel free to click through your data tables on Terra or review Gen3's documentation if you're getting a headache at this point.

Why does this matter? Because you can use this to effectively use two different data tables in your analysis, even though at first glance it looks like you can only select one in the drop down menu. You simply enter that table's link to the CRAI files, ie, `this.pfb:object_id` as the input for `input_crai_file`. (You will have to scroll to find `input_crai_file` in the aligner as Terra puts optional arguments below all required arguments.) And for `input_cram_file`, the correct entry is `this.submitted_aligned_reads.pfb:object_id`. Handy, isn't it?


# Final Notes
We've now gone over how to set up the TOPMed alignment workflow to work with the Gen3 graph-model . We now leave you with some final notes if you want to align your data from other sources using this workflow.
* [How to run WDLs from Dockstore](https://bdcatalyst.gitbook.io/biodata-catalyst-documentation/community_tools/dockstore-example) -- useful if you want to preform further analysis on your newly aligned data
* [TOPMed Variant Caller workflow on Dockstore](https://dockstore.org/workflows/github.com/DataBiosphere/topmed-workflows/UM_variant_caller_wdl:feature/checker-optional-crai?tab=info)

### If Your Data Was/Will Be Aligned to Something that Isn't HG38

Something that may be easy to miss in the aligner's documentation is the following note:
> The CRAM to be realigned may have been aligned with a different reference genome than what will be used in the alignment step. The pre-align step must use the reference genome that the CRAM was originally aligned with to convert the CRAM to a SAM.

There's two important things to take away from this:
1. If what you already aligned your data to (remember, CRAM/SAM/BAM files are already aligned!) does not match the reference genome you are aligning to with this workflow, you must include what you aligned them to as the argument for `PreAlign_reference_genome` and its associated index as the argument for `PreAlign_reference_genome_index`. For instance, if your CRAM files were generated using HG19 as your reference and you are using this workflow to align them HG38, you must set HG38 as your argument for `ref_fasta` and HG19 as `PreAlign_reference_genome`.
2. TOPMed data is aligned to HG38. If you are running this workflow to compare your own data to TOPMed data, you must align to HG38.


------

# Authors
Workspace author: Ash O'Farrell  
WDL script and bug fixing: Walt Shands  
Dockerized TopMED Aligner: Jonathon LeFaive  
Edits and bug-squashing: Michael Baumann, Beth Sheets  
  
This workspace was completed under the NHLBI BioData Catalyst project.

### Workspace Changelog 

| Date | Change | Author | 
| -------  | -------- | -------- |
| Feb 4, 2020 | initial draft | Ash |
| Feb 6, 2020 | updated dashboard text | Beth |
| Feb 10, 2020 | gen3, job tracking, output, major revisions | Ash |
| Mar 27, 2020 | major reorganization, removal of unneeded info, better data structure explanation | Ash |
| Oct 28, 2020 | updated to reflect pfb prefix of Geb3 tables + minor edits | Ash, Beth |
| Dec 7, 2020 | added cost estimate | Ash |
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","biodata-catalyst/TOPMed Aligner Gen3 Data"
274,"ctat-firecloud","ctat-VirusIntegrationFinder","READER","https://app.terra.bio/#workspaces/ctat-firecloud/ctat-VirusIntegrationFinder",TRUE,FALSE,NA,NA,NA,"## VirusIntegrationFinder

VirusIntegrationFinder (ctat-VIF) is a module of the Trinity Cancer Transcriptome Analysis Toolkit (CTAT) and used to identify virus insertion sites within the human genome. VIF leverages the STAR aligner to identify chimeric NGS read alignments involving the human genome and a database of viral sequences.

ctat-VIF operates to:

- capture evidence of virus-matching reads (consistent with an infection)
- identify and quantify evidence for viral insertion sites in the human genome
- provide interactive visualizations for evidence of virus-mapped reads and virus insertion sites

Complete documentation for STAR-Fusion is available on the [ctat-VIF  Wiki](https://github.com/broadinstitute/CTAT-VirusIntegrationFinder/wiki).


### Input

- RNA-seq Fastq files (include paths to files in the data sample table)
- use either the provided hg19 or hg38 pre-configured workflows.

### Output

ctat-VIF includes tab-delimited summaries of counts of reads corresponding to the virus insertion sites, counts of reads mapping to the different viral genome targets, and various plots and interactive genome views for interrogating these data. Outputs are detailed below, all based on the included sample data and involve human papillomavirus (HPV).

An example interactive virus-insertion visualization is shown below:

![insertion_view](https://raw.githubusercontent.com/wiki/broadinstitute/CTAT-VirusIntegrationFinder/images/VIF_insertion_example.png)

Virus content and viral genome coverage visualization example is below:

![virus_content_view](https://raw.githubusercontent.com/wiki/broadinstitute/CTAT-VirusIntegrationFinder/images/virus_genome_view.png)


### Sample data description and location

The workflow in this workspace is preconfgured with a small sample of fastq reads that provide evidence for HPV virus integrations. 



### Time and cost estimates
Below is an example of the time and cost for running the workflow.

| Sample Name        | Cost    | Time |       
| ------------- |:-------------:| ----------:| 
| vif_sm_example      | $1.00 | 2.5 hours


Note: Cost and time will vary with the use of preemptible instances.


### Contact Information
Questions can be directed to the Trinity CTAT email list: trinity_ctat_users@googlegroups.com

### License
Copyright Broad Institute, 2022 | BSD-3
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at https://raw.githubusercontent.com/STAR-Fusion/STAR-Fusion/master/LICENSE).

",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","ctat-firecloud/ctat-VirusIntegrationFinder"
275,"help-terra","LungMap-data-in-Terra","READER","https://app.terra.bio/#workspaces/help-terra/LungMap-data-in-Terra",TRUE,TRUE,NA,NA,NA,"# Explore LungMAP single-cell data
This tutorial workspace is a step-by-step guide to importing, accessing, and analyzing single-cell RNA sequencing data from the LungMAP [Data Browser](https://data-browser.lungmap.net/explore/projects) using community-supported single-cell analysis tools.  

Using this workspace, you will:

1. Import 10x sequencing reads in FASTQ format from the LungMAP [Data Browser](https://data-browser.lungmap.net/explore/projects)
2. Use a Jupyter Notebook to create new data tables to organize data for downstream alignment and preprocessing with the Optimus workflow
3. Preprocess 10x FASTQs with the Optimus workflow
4. Convert the Optimus raw cell-by-gene matrix output to mtx format for downstream Seurat analysis
5. Import mtx files and visualize the count matrix in an example **Seurat** Jupyter Notebook


### How much does this cost?
The table below outlines each workflow and notebook's estimated cost and timing.

| Workflow/Notebook | Cost ($) | Timing | Notes | 
| --- | --- | --- | --- |
| Create_data_table_for_preprocessing notebook | 0.007 | ~ 1 min | Prepares data tables for processing. |
| Optimus workflow | 1.46 | 9 hr 27 min | Runs on a single library preparation. |
| convert_npz_to_mtx notebook | 0.07 | ~10 min | Runs on a single library preparation. |
| Seurat notebook | 0.17 | ~ 24 min | Runs on a single library preparation. |

![white space](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/white-space.jpg)  

# Instructions

## Set up the tutorial workspace 
Before you begin, create your own editable copy (clone) of this workspace. Click the round circle with three dots in the upper right corner of this page and choose ""Clone.""
![](https://storage.googleapis.com/terra-featured-workspaces/LungMAP-data-in-terra/clone-lung.png)

## Step 1. Import Data from the LungMAP
### LungMAP overview
LungMAP is a consortium that includes the LungMAP Data Coordinating Center. It is supported by the National Heart, Lung, and Blood Institute of the National Institutes of Health under Award Number U24HL148865. The LungMAP Browser contains human and mouse data derived from lung tissue.

This workspace already contains example LungMAP data, but you can browse the LungMAP's  [Data Browser](https://data-browser.lungmap.net/explore/projects) and export the same or additional data using the Quick-start instructions below. 

### Quick-start instructions
1. Navigate to the LungMAP's [Data Browser](https://data-browser.lungmap.net/explore/projects).
2. Use the faceted search to identify and select a 10x 3' dataset of interest (this workspace is preloaded with [the project ""Genomic, epigenomic, and biophysical cues controlling the emergence of the lung alveolus.""](https://data-browser.lungmap.net/explore/projects/00f056f2-73ff-43ac-97ff-69ca10e38c89)
3. Choose the `Export Selected Data` icon.
4. Under the `Analyze in Terra` section, select the `Analyze in Terra` icon.
5. Confirm the species (Mus musculus for the example data).
6. Select the files to export; for raw sequencing reads, select `fastq`.
7. Select `Request Link`.
8. When the export is ready, select the link.
9. When prompted to select a destination workspace, choose your cloned version of this workspace from the drop-down.

All imported data will be automatically added across the multiple data tables on the workspace `Data` tab. 

## Step 2. Organize data with a Jupyter Notebook

### Quick-start instructions
1. Select the `Environment Configuration` icon (cloud with lightning bolt)  to the right of the workspace.
2. Under the Jupyter option, select `Settings`.
3. Select the `Create custom environment` option.
4. Under the `Application Configuration` drop-down, choose the `R/Bioconductor` option.
5. In the Compute settings, choose `8` CPUs.
6. To the startup script option under Compute, paste `gs://terra-featured-workspaces/HCA_Featured_Workspace/startup-test3.sh`.
7. Select `Create`. It will take 3-5 min to create the custom computer.
8. Go to the workspace `Analyses` tab and open the  `Create_data_table_for_preprocessing` notebook in Open mode.
9. Follow the instructions listed in the notebook.

After running this notebook, you’ll see an `optimus_data_set` table in the workspace Data tab. For this workspace, we've already created it.

## Step 3. Align and preprocess data with the Optimus workflow

### Optimus Overview   

This Optimus workflow has quality control, alignment, and transcriptome quantification modules. It corrects Cell Barcodes (CBs) and Unique Molecular Identifiers (UMIs), aligns reads to the genome, generates a count matrix in a UMI-aware manner, detects empty droplets (single-cell mode only), calculates summary metrics for genes and cells, returns read outputs in BAM format, and returns raw counts in NPZ, NPY, and Loom file formats. 

For more details about the pipeline, see the [Optimus Overview](https://broadinstitute.github.io/warp/docs/Pipelines/Optimus_Pipeline/README) in the [WARP documentation](https://broadinstitute.github.io/warp/).  

### Sample data
This step uses example FASTQ files from a single library preparation that has been split across multiple lanes of sequencing. The library prep has the ID cell_suspensions.4f35a1fd-61cd-5b46-8f09-9d7065996d05.1 and is listed in the optimus_data_set data table on the workspace Data tab.

### Quick-start instructions
1.  Go to the `optimus_data_set` data table on the `Data` tab.
2.  Select the checkbox left of the `cell_suspensions.4f35a1fd-61cd-5b46-8f09-9d7065996d05.1` file-set.
3.  Select `Open with`.
4. Choose workflow.
5. Choose Optimus.
6. Select `Run Analysis` and then `Launch`.

### Outputs
The Optimus pipeline writes several outputs to the `optimus_data_set` data table, including matrix files with raw counts and quality metrics which you can find in the `matrix`, `matrix_col_index`, and `matrix_row_index`  columns.

## Step 4. Convert Optimus matrix output to mtx format

### Quick-start instructions
This step uses can use the same cloud environment used in Step 2 above. However, if you need to recreate the environment, use steps 1-8 below.

1. Select the `Environment Configuration` icon (cloud with lightning bolt) to the right of the workspace.
2. Under the Jupyter option, select `Settings`.
3. Select the `Create custom environment` option.
4. Under the `Application Configuration` drop-down, choose the `R/Bioconductor` option.
5. In the Compute settings, choose `8` CPUs.
6. To the startup script option under Compute, paste `gs://terra-featured-workspaces/HCA_Featured_Workspace/startup-test3.sh`.
7. Select `Create`. It will take 3-5 min to create the custom computer.
8. Go to the workspace `Analyses` tab and open the  `convert_npz_to_mtx` notebook.
9. Follow the instructions listed in the respective notebook.

### Outputs
This Jupyter Notebook will produce a new Seurat_test folder in your Cloud Environment that contains zipped mtx files: barcodes.tsv, features.tsv, and counts.mtx.

## Step 5. Explore matrices using Seurat

### Quick-start instructions
This step uses can use the same cloud environment used in Step 2 above. However, if you need to recreate the environment, use steps 1-8 below.

1. Select the `Environment Configuration` icon (cloud with lightning bolt) to the right of the workspace.
2. Under the Jupyter option, select `Settings`.
3. Select the `Create custom environment` option.
4. Under the `Application Configuration` drop-down, choose the `R/Bioconductor` option.
5. In the Compute settings, choose `8` CPUs.
6. To the startup script option under Compute, paste `gs://terra-featured-workspaces/HCA_Featured_Workspace/startup-test3.sh`.
7. Select `Create`. It will take 3-5 min to create the custom computer.
8. Go to the workspace `Analyses` tab and open the  `Seurat` notebook.
9. Follow the instructions listed in the respective notebook.

### Outputs
This notebook will save the Seurat analysis as an RDS in the Cloud Environment and will copy the RDS to the workspace Google bucket, which you can access by selecting the `Data` tab and going to the Files section. 

## Next steps
Try this tutorial on your own data or check back for updates to the workspace tutorial. 

## Workspace updates
| Date | Change | 
| --- | --- |
| 10/25/22 | First edition of the workspace. |

## Questions and feedback
Please post workspace questions and feedback to the [Featured Workspaces community forum](https://support.terra.bio/hc/en-us/community/topics/360001603491) (login required).


## References
Butler, A., Hoffman, P., Smibert, P. et al. Integrating single-cell transcriptomic data across different conditions, technologies, and species. Nat Biotechnol 36, 411–420 (2018). https://doi.org/10.1038/nbt.4096


Stuart T, Butler A, Hoffman P, Hafemeister C, Papalexi E, Mauck WM 3rd, Hao Y, Stoeckius M, Smibert P, Satija R. Comprehensive Integration of Single-Cell Data. Cell. 2019 Jun 13;177(7):1888-1902.e21. doi: 10.1016/j.cell.2019.05.031. Epub 2019 Jun 6. PMID: 31178118; PMCID: PMC6687398.

## Acknowledgements
Special thanks to the Morrisey lab, including Michael Morley, the Bioconductor team (Martin Morgan, Vince Carey, and Nitesh Turaga), the Broad Pipelines team, and Sushma Chaluvadi for their feedback and contributions to this workspace.

## License
**Copyright Broad Institute, 2022 | BSD-3**

All rights reserved. Note that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.

---







",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-terra/LungMap-data-in-Terra"
276,"broad-firecloud-tcga","TCGA_TGCT_hg38_OpenAccess_GDCDR-12-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_TGCT_hg38_OpenAccess_GDCDR-12-0_DATA",TRUE,TRUE,"Testicular Germ Cell Tumors","Tumor/Normal","USA","TCGA Testicular Germ Cell Tumors Open-Access Data Workspace

*hg38 TCGA and TARGET workspaces reference files by their GDC UUIDs.  In order to run analyses on the referenced data files, you will need to run workflows that retrieve the files from the GDC and copy them to your workspace bucket.  See   [this forum post](https://gatkforums.broadinstitute.org/firecloud/discussion/10382/populating-hg38-tcga-and-target-workspaces-with-data-files#latest) for instructions on the running of these workflows.*","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients' clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","150","Testes","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh38/hg38","TCGA_TGCT_hg38_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_TGCT_hg38_OpenAccess_GDCDR-12-0_DATA"
277,"broad-firecloud-cptac","PANOPLY_Production_Modules_v1_0","READER","https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_Production_Modules_v1_0",TRUE,FALSE,NA,NA,NA,"# PANOPLY: A cloud-based platform for automated and reproducible proteogenomic data analysis
#### Version 1.0

PANOPLY is a platform for applying state-of-the-art statistical and machine learning algorithms to transform multi-omic data from cancer samples into biologically meaningful and interpretable results. PANOPLY leverages [Terra](http://app.terra.bio)—a cloud-native platform for extreme-scale data analysis, sharing, and collaboration—to host proteogenomic workflows, and is designed to be flexible, automated, reproducible, scalable, and secure. A wide array of algorithms applicable to all cancer types have been implemented, and available in PANOPLY for analysis of cancer proteogenomic data.


![*Figure 1.* Overview of PANOPLY architecture and the various tasks that constitute the complete workflow. Tasks can also be run independently, or combined into custom workflows, including new tasks added by users. Inputs to PANOPLY consists of (i) externally characterized genomics and proteomics data (in gct format); (ii) sample phenotype and annotations (in csv format); and (iii) parameters settings (in yaml format). Panoply modules are grouped into Data Preparation Modules (green box), Data Analysis Modules (blue box) and Report Modules (red box). Data preparation modules perform quality checks on input data followed by optional normalization and filtering for proteomics data. Analysis ready data tables are then used as inputs to the data analysis modules. Results from the data analysis modules are summarized in interactive reports generated by appropriate report modules.](https://raw.githubusercontent.com/broadinstitute/PANOPLY/dev/panoply-overview.png)


PANOPLY v1.0 consists of the following components:

* A Terra production workspace on [PANOPLY_Production_Pipelines_v1_0](https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_Production_Pipelines_v1_0) with a preconfigured unified workflow to automatically run all analysis tasks on proteomics (global proteome, phosphoproteome, acetylome, ubiquitylome), transcriptome and copy number data; and an [additional workspace](https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_Production_Modules_v1_0) that includes separate methods for each analysis component. 
* An interactive Jupyter notebook (included in the Terra workspaces) that provides step-by-step instructions for uploading data, identifying data types, specifying parameters, and setting up the PANOPLY workspace for analyzing a new dataset.
* A GitHub [repository](https://github.com/broadinstitute/PANOPLY) that contains code for all PANOPLY tasks and workflows, including R code for implementing analysis algorithms, task module creation, and release management. A GitHub [wiki](https://github.com/broadinstitute/PANOPLY/wiki) includes documentation and description of algorithms. 
* A [tutorial](https://github.com/broadinstitute/PANOPLY/wiki/PANOPLY-Tutorial) illustrating the application of PANOPLY to a published breast cancer dataset and demonstrating the practical relevance of PANOPLY by regenerating many of the results described in (Mertins et al) (1) with minimal effort. The [PANOPLY_Tutorial](https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_Tutorial) workspace contains data and results from running the tuorial.
* Terra workspaces showing case studies of applying PANOPLY to the analysis of [CPTAC BRCA](https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_CPTAC_BRCA) (7) and [CPTAC LUAD](https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_CPTAC_LUAD) (8) datasets. The workspaces include all data, parameter settings and results.


PANOPLY provides a comprehensive collection of proteogenomic data analysis methods including sample QC (sample quality evaluation using profile plots and tumor purity scores (1), identify sample swaps, etc.), association analysis, RNA and copy number correlation (to proteome), connectivity map analysis (1 ,2), outlier analysis using BlackSheep (3), PTM-SEA (4), GSEA (5) and single-sample GSEA (6), consensus clustering, and multi-omic clustering using non-negative matrix factorization (NMF). Most analysis modules include a report generation task that outputs a HTML interactive report summarizing results from the respective analysis tasks. Mass spectrometry-based proteomics data amenable to analysis by PANOPLY includes isobaric label-based LC-MS/MS approaches like iTRAQ, TMT and TMTPro profiling the proteome and multiple PTM-omes including phospho-, acetyl-and ubiquitylomes.

Users can also [customize or create new pipelines and add their own tasks](https://github.com/broadinstitute/PANOPLY/wiki/Customizing-PANOPLY) and integrate them into the PANOPLY.


## Quick Start

* For a quick introduction and tour of PANOPLY, follow the [**tutorial**](https://github.com/broadinstitute/PANOPLY/wiki/PANOPLY-Tutorial). 
* Detailed **documentation** can be found [here](https://github.com/broadinstitute/PANOPLY/wiki).

### Contact

Email proteogenomics@broadinstitute.org with questions, comments or feedback.


### References

1. Mertins, P. et al. Proteogenomics connects somatic mutations to signalling in breast cancer. Nature 534, 55–62 (2016).
2. Subramanian, A. et al. A Next Generation Connectivity Map: L1000 Platform and the First 1,000,000 Profiles. Cell 171, 1437–1452.e17 (2017).
3. Blumenberg, L. et al. BlackSheep: A Bioconductor and Bioconda package for differential extreme value analysis. doi:10.1101/825067.
4.	Krug, K. et al. A Curated Resource for Phosphosite-specific Signature Analysis. Mol. Cell. Proteomics 18, 576–593 (2019).
5.	Subramanian, A. et al. Gene set enrichment analysis: a knowledge-based approach for interpreting genome-wide expression profiles. Proc. Natl. Acad. Sci. U. S. A. 102, 15545–15550 (2005).
6.	Barbie, D. A., Tamayo, P., Boehm, J. S., Kim, S. Y. & Moody, S. E. Systematic RNA interference reveals that oncogenic KRAS-driven cancers require TBK1. Nature (2009).
7. Gillette, M. A. et al. Proteogenomic Characterization Reveals Therapeutic Vulnerabilities in Lung Adenocarcinoma. Cell 182, 200–225.e35 (2020).
8. Krug, K., Jaehnig, E. J., Satpathy, S., Blumenberg, L., et al. Proteogenomic Landscape of Breast Cancer Tumorigenesis and Targeted Therapy. Cell 183, 1–21 (2020).
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","broad-firecloud-cptac/PANOPLY_Production_Modules_v1_0"
278,"malaria-featured-workspaces","Malaria_Plasmodium_Illumina_Whole_Genome","READER","https://app.terra.bio/#workspaces/malaria-featured-workspaces/Malaria_Plasmodium_Illumina_Whole_Genome",TRUE,FALSE,NA,NA,NA,"# Malaria Plasmodium Illumina Whole Genome Workspace for processing of whole genome data

This workspace hosts several workflows for variant discovery and analysis in Plasmodium using short read whole genome sequencing (WGS) data.  Both single-sample and joint-calling of cohorts can be performed in this workspace.  After performing initial variant discovery, there are additional workflows to perform tertiary analysis, including drug resistance screening and the presence or absence of rapid diagnostic test markers.

## Overview of Workflows


 ![Overview of Illumina Short Read Workflows](https://storage.googleapis.com/terra-featured-workspaces/Malaria/Screenshot%202024-09-15%20at%209.39.03%E2%80%AFPM.png) 


There are three primary workflows that are run in order to perform variant discovery:

*  **01_Illumina_Short_Read_Alignment**
*  **02_Plasmodium_Single_Sample_Variant_Calling**
*  **03_Plasmodium_Joint_Variant_Calling**

Once variants have been called in individual samples, three additional workflows can be run to perform drug resistance screening (using known markers for drug resistance) and identify if the HRP2 / HRP3 regions have been deleted:
*  **01a_Plasmodium_HRP2_HRP3_Status_Detection**
*  **02a_Plasmodium_Single_Sample_Drug_Resistance_Marker_Extraction**


## Data

This workspace is populated with publicly available _Plasmodium falciparum_ data from several locations across the globe.  These samples are field isolates, the vast majority of which have been prepared using selective whole genome amplification to enrich for _Plasmodium falciparum_ DNA prior to sequencing.  Details on these samples are as follows:

| Country    | # of Samples | Year of Collection | Dataset | Study |
| -------- | ------- | ------- |------- | --------|
|Cambodia|49|2017|Pf7|1207-PF-KH-CNM-GENRE|
|Cameroon|41|2017|Pf7|1165-PF-CM-APINJOH-SM|
|Guyana|47|2020|PRJNA809659|PRJNA809659|
|Mozambique|29|2013|Pf7|1223-PF-MZ-ROSANAS-URGELL|
|Papua New Guinea|48|2017|Pf7|1233-PF-PG-MITA|
|Senegal|49|2019|PRJNA972644|PRJNA972644|
|Tanzania|51|2014|Pf7|1167-PF-TZ-ISHENGOMA-SM|
|Uganda|15|2010|Pf7|1024-PF-UG-BOUSEMA|


## Workflows

### **01_Illumina_Short_Read_Alignment** 

#### What does it do? 

This workflow utilizes a comprehensive series of genomic processing steps to ensure precise preparation of short read data for variant analysis in *Plasmodium falciparum*. Initially, raw data is converted from BAM to FASTQ format if necessary, followed by sequence alignment using BWA-MEM2. The workflow also includes optional decontamination steps to remove reads from known contaminants, enhancing data integrity. Post-alignment, the workflow marks duplicates and optionally recalibrates base quality scores to correct sequencing errors, using tools like BaseRecalibrator and ApplyBQSR. Final steps involve organizing and finalizing the data into structured directories for easy access and analysis. This preprocessing is crucial for accurate variant calling and is adaptable for other Plasmodium species, supporting a range of genomic studies critical for advancing malaria research and public health strategies.

#### What does it require as input?

- BAM files: BAM files containing reads to be aligned and processed, compressed without spaces or special characters in their names. An index file (`bai`) must also be provided if a BAM file is used. Examples of correct file names are:
 `Sample1.bam`
- BAM Index files: Index files for the BAM files, necessary for processing. These should correspond directly to the BAM files listed:
 `Sample1.bai`
- FASTQ files (Optional): FASTQ files containing paired-end short read data, required in pairs (`fq_end1` and `fq_end2`). File names must be without spaces or special characters. Examples of correct file names are:
`Sample1_S1_L001_R1_001.fastq`
`Sample1_S1_L001_R2_001.fastq`

- Reference Map File: A file containing mappings to the primary reference sequence and auxiliary file locations. This file is essential for alignment and reference-dependent processes.
- Contaminant Reference Map File (Optional): Similar to the `ref_map_file`, but for a contaminant reference. This file is used if decontamination is part of the workflow.
- Sample and Library Names: Text strings identifying the sample (`SM`) and library (`LB`). These identifiers are used for labeling and tracking throughout the workflow. Example:
SM: `Sample1`
LB: `Lib1`

- Sequencing Platform: The sequencing platform identifier, with a default value of ""illumina"", which is used for platform-specific adjustments in the processing steps.

#### What does it return as output?

The output files from the SRFlowcell workflow are automatically stored in a Google Cloud Storage bucket and linked to the dataset used to run the pipeline, ensuring easy access and management of results.

- Aligned and Unaligned Sequence Files:
- FASTQ Files: Paired-end FASTQ files (`fq1`, `fq2`) and optionally unpaired FASTQ files (`fq_unpaired`), providing raw sequence data.
- Aligned BAM File: A BAM file (`aligned_bam`) containing aligned reads along with its index file (`aligned_bai`), ready for further analysis.
- Unaligned BAM Files: BAM (`unaligned_bam`) and BAI (`unaligned_bai`) files containing reads that did not align or were intentionally unprocessed.
- Quality Control and Metrics Reports:
- FastQC Report: A comprehensive quality control report (`fastqc_report`) detailing the quality metrics of the final BAM file.
- Read and Alignment Statistics: Detailed metrics including number of reads, base counts, quality scores, alignment statistics, and more, providing insights into the data quality and alignment efficacy.

If decontamination steps were performed:
- Contaminated BAM File: An optional file (`contaminated_bam`) listing reads identified as contaminants.
- Decontamination Repository: A collection of files in specified directories containing decontaminated reads and associated data, ensuring cleaned data is ready for downstream analysis.

These outputs collectively provide a rich dataset for comprehensive genomic analysis, including direct measurements of sequence quality, alignment accuracy, and contamination control, all crucial for reliable variant detection and genomic research in Plasmodium falciparum and potentially other species.

#### Runtime

The approximate runtime for this workflow on a single sample is: 8 hours.

### **02_Plasmodium_Single_Sample_Variant_Calling**

#### What does it do? 

This workflow is designed for variant calling and analysis in single-sample Illumina sequencing data from Plasmodium falciparum, utilizing the outputs from the 01_Illumina_Short_Read_Alignment workflow. . This workflow uniquely integrates the processing of data across potentially multiple flow cells or replicates, merging them into a unified BAM file for comprehensive analysis. Variant calling is performed using GATK4's _HaplotypeCaller_ for detailed SNP and indel detection.  Post-calling, the workflow employs a robust recalibration system for variant scoring, utilizing custom models trained on provided annotations. This approach ensures high fidelity in variant detection, crucial for studies in malaria genomics where precise genetic characterization informs treatment and resistance management strategies. By maintaining flexibility in processing and variant calling methods, the SRWholeGenome workflow stands as a pivotal tool in the genomics of infectious diseases, particularly adaptable for extending its application to other Plasmodium species.

#### What does it require as input?

02_Plasmodium_Single_Sample_Variant_Calling will require the outputs of 01_Illumina_Short_Read_Alignment (or files in equivalent format) as its primary input for processing. In addition to these outputs, the workflow requires:

- Reference Map File: This file includes mappings to the reference genome sequence and auxiliary file locations essential for variant calling and other analyses.
- Participant Samples: Unique identifiers for the samples being processed, which are used throughout the workflow to label and track outputs.
- Google Cloud Storage Directory: Specifies the directory in a Google Cloud Storage (GCS) bucket where workflow outputs will be stored.

Additional genomic and calibration data required include:

- Genomic Configuration Files: Include files such as a BED file for coverage calculation and a haplotype database file for fingerprinting, if applicable.
- Variant Calibration and Training Data: Arrays of known reference variants, their indices, and identifiers used for training variant scoring models. These also determine which variants are used as training or calibration sets.

Performance settings for variant calling (HaplotypeCaller) are also needed:
- Computational Resources: Specifications for the number of threads and the amount of memory allocated for processing.
- Variant Calling Parameters: Settings such as ploidy and heterozygosity, which affect how genetic variation is modeled during variant calling.

#### What does it return as output?

The output files from the 02_Plasmodium_Single_Sample_Variant_Calling workflow are automatically stored in a Google Cloud Storage bucket and linked to the dataset used to run the pipeline. This workflow comprehensively analyzes genomic data from Plasmodium falciparum, producing a variety of detailed outputs crucial for understanding genetic variations and their implications:
- Variant Call Files: Detailed reports of genomic variants detected, including:
  		- SNPs and indels from HaplotypeCaller.
 		 - Comprehensive gVCF files for extensive genomic coverage analysis.
- Quality Control Reports: FastQC generated HTML files offering insights into the data quality pre-processing, crucial for assessing the integrity of sequencing data.
- Coverage Metrics: Summaries of sequencing coverage across targeted regions, essential for evaluating the depth and uniformity of the sequencing effort.
- Fingerprinting Reports: Verification of sample identity through genomic fingerprinting, ensuring sample accuracy and integrity.
- Recalibration Reports: Detailed recalibration data for SNPs and indels, providing insights into the reliability and accuracy of the variant calls.
- Alignment Files: Final BAM files from variant calling processes, used for detailed review and validation of the alignment and variant calling accuracy.

#### Runtime
The approximate runtime for this workflow on a single sample is: 16 hours.

### **03_Plasmodium_Joint_Variant_Calling**

#### What does it do? 

This workflow is designed for joint variant calling, leveraging gVCF outputs from the 02_Plasmodium_Single_Sample_Variant_Calling workflow, which processes single-sample Plasmodium falciparum Illumina sequencing data. This workflow efficiently handles variant data across multiple samples by integrating them into GenomicsDB, facilitating scalable and robust genotypic analysis. Users can choose between GnarlyGenotyper for rapid joint calling or GenotypeGVCFs for enhanced precision. The process includes advanced recalibration techniques using VETS to refine variant accuracy and employs functional annotations via SnpEff, enhancing the understanding of variant impacts. Tailored for extensive genomic studies, this workflow is essential for comprehensive variant analysis in malaria research, supporting the detailed exploration of genetic variations that influence disease dynamics and treatment responses.

#### What does it require as input?

The 03_Plasmodium_Joint_Variant_Calling workflow requires several key inputs to facilitate joint variant calling:
- gVCFs and Indices: An array of genomic VCF (gVCF) files and their corresponding index files, which are the outputs from the 02_Plasmodium_Single_Sample_Variant_Calling workflow.
- Reference Map File: A file detailing the locations of reference sequences and auxiliary files necessary for analysis.
- Variant Calibration and Training Data: Includes known reference variants for SNPs and INDELs, their indices, and identifiers, used for training and calibration of variant scoring models.
- Annotation Files: BED files for regional annotation of variants, along with their indices and descriptive names.
- Genotyping Settings: Parameters such as heterozygosity rates and calibration sensitivities that influence the genotyping process.

#### What does it return as output?

- Joint VCF Files: Consolidated VCF files that contain variant calls from multiple gVCFs, processed through GenomicsDB.
- Recalibrated VCFs: Variant files that have undergone recalibration for enhanced accuracy, including functional annotations where applicable.
- Annotation Reports: Files that detail the effects and regions of variants based on BED annotations and functional insights from tools like SnpEff.
- GenomicsDB Datastore: The integrated database storing genomic data used in the joint calling process.
- Converted Data Formats: Optional outputs in Hail MatrixTable or Zarr formats for further analysis and integration into larger genomic datasets.

#### Runtime
The approximate runtime for this workflow on a joint call cohort of 3070 samples is: 20 hours.

### **02a_Plasmodium_Single_Sample_Drug_Resistance_Marker_Extraction**

#### What does it do?

This workflow checks an input VCF file (produced by 02_Plasmodium_Single_Sample_Variant_Calling) for known drug resistance mutations.  These mutations are collected and then used and to generate a list of predicted resistance phenotypes for common anti-malarial drugs (Chloroquine, Pyrimethamine, Sulfadoxine, Mefloquine, Artemisinin, Piperaquine).  If such a determination cannot be made for a specific drug, the entry for that drug will be UNDETERMINED. 

#### What does it require as input?

vcf - VCF file containing all variants for a specific sample (produced by 02_Plasmodium_Single_Sample_Variant_Calling).  
snpeff_db - SNPEff database containing _Plasmodium falciparum_ datasources for SNPEff to use to functionally annotate the given VCF file (provided as part of the workspace).  
drug_resistance_list - TSV file containing drug resistance markers to search for in the given VCF file (of the format: gene name, locus, amino acid change - provided as part of the workspace).

#### What does it return as output?

- raw_drug_resistance_predictions - Text file containing information about the presence of known drug resistance mutations in the given sample.  This text file contains data in columnar format for ease of review (columns are: gene name, locus, amino acid change, presence in sample).
- predicted_drug_resistance_summary - Text file report containing a list of common anti-malarial drugs and their effectiveness against the infection in this sample.  Drugs surveyed are Chloroquine, Pyrimethamine, Sulfadoxine, Mefloquine, Artemisinin, Piperaquine, with each labeled as SENSITIVE, RESISTANT, or UNDETERMINED.  Drug resistances are determined using the markers and rubric defined by the MalariaGEN group for the PF7 project (https://www.malariagen.net/wp-content/uploads/2023/11/Pf7_resistance_classification.pdf).
- predicted_drug_status_artemisinin - predicted drug resistance phenotype for artemisinin (sensitive, resistant, or undetermined)
- predicted_drug_status_chloroquine - predicted drug resistance phenotype for chloroquine (sensitive, resistant, or undetermined)
- predicted_drug_status_mefloquine - predicted drug resistance phenotype for mefloquine  (sensitive, resistant, or undetermined)
- predicted_drug_status_piperaquine - predicted drug resistance phenotype for piperaquine (sensitive, resistant, or undetermined)
- predicted_drug_status_pyrimethamine - predicted drug resistance phenotype for pyrimethamine  (sensitive, resistant, or undetermined)
- predicted_drug_status_sulfadoxine - predicted drug resistance phenotype for sulfadoxine  (sensitive, resistant, or undetermined)
- annotated_vcf - VCF file including SNPEff annotations.
- annotated_vcf_index - Index for the annotated_vcf output file.


### **01a_Plasmodium_HRP2_HRP3_Status_Detection**

#### What does it do?

This workflow ingests the aligned reads from a single sample (produced by 01_Illumina_Short_Read_Alignment) and determines whether the regions including the HRP2 and HRP3 genes are deleted.  The results are added to the Terra data table in the HRP2 and HRP3 columns with a “-” indicating the absence of the gene, and a “+” indicating the presence of the gene.  The presence of these regions is determined by examining the read depth over the loci for HRP2 (Pf3D7_08_v3:1373212-1376988) and HRP3 (Pf3D7_13_v3:2840236-2842840) and comparing it to the overall read depth for the contig on which the region is located.  If the read depth for the region is less than 5% of that of its containing contig, that region is marked as absent.  Conversely, if the read depther for the region is 5% or greater than its containing contig, that region is marked as present.

#### What does it require as input?

- bam - BAM file containing aligned reads for a specific sample.
- bai - Index for the given bam file.

#### What does it return as output?

- hrp2 - HRP2 status (+ for present, - for absent), appended to the data table.
- hrp3 - HRP3 status (+ for present, - for absent), appended to the data table.

## Contact Information

Have questions? Contact us at publichealthgenomics@broadinstitute.org
This email address is actively monitored and you will get a response within a business day.

### Workspace Citation

Details on citing Terra workspaces can be found here: [How to cite Terra](https://support.terra.bio/hc/en_us/articles/360035343652_How_to_cite_Terra_and_associated_resources).



### Change Log:

| Date       | Change                                    | Author                     |
|------------|-------------------------------------------|----------------------------|
|2024 May 10 | Set up Workspace and uploaded documentation | Jonn Smith |
|2024 Sept 12 | Updated with new workflow for drug resistance prediction | Jonn Smith |
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","malaria-featured-workspaces/Malaria_Plasmodium_Illumina_Whole_Genome"
279,"broad-firecloud-tcga","TCGA_LGG_OpenAccess_V1-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_LGG_OpenAccess_V1-0_DATA",TRUE,TRUE,"Brain Lower Grade Glioma","Tumor/Normal","USA","TCGA Brain Lower Grade Glioma Open-Access Data Workspace","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients’ clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","516","Brain","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh37/hg19","TCGA_LGG_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_LGG_OpenAccess_V1-0_DATA"
280,"help-gatk","BICCN_Omics_Workshop","READER","https://app.terra.bio/#workspaces/help-gatk/BICCN_Omics_Workshop",TRUE,FALSE,NA,NA,NA,"# BRAIN Initiative Cell Census Network (BICCN) Omics Workshop

This tutorial workspace is a step-by-step guide to analyzing BICCN 10x Genomics single-cell data. Using this workspace, you will:

1. Import an example 10x dataset (FASTQs) from **NeMO**.
2. Align example 10x FASTQs and produce a raw count matrix with quality metrics using the **Optimus workflow**.
3. Filter, normalize, and cluster the raw count matrix with the **Cumulus workflow**.
4. Explore single-cell data in a **Seurat Jupyter Notebook**. 


![](https://storage.googleapis.com/terra-featured-workspaces/BICCN_Workshop_Workspace/BICCN_flow.png)


### Work from your copy of the workspace following these step-by-step instructions 
|![PDF icon](https://storage.googleapis.com/terra-featured-workspaces/BICCN_Workshop_Workspace/Copy%20of%20PDF-icon_scaled.png) | Download a clickable PDF of step-by-step instructions [here](https://storage.googleapis.com/terra-featured-workspaces/BICCN_Workshop_Workspace/BICCN%20Workshop%20Tutorial.pdf) |  
| --------| ------------------|     

### Watch a demo of this workspace
This workspace was developed for the BICCN Omics Workshop. Watch a recording of the workshop [here](https://www.youtube.com/watch?v=PNB4osJQ10c&t=1s).

# Instructions

## Set up the tutorial workspace 

Before you begin, create your own editable copy (clone) of this WORKSPACE. Click the round circle with three dots in the upper right corner of this page and choose ""Clone"".


![](https://storage.googleapis.com/terra-featured-workspaces/BICCN_Workshop_Workspace/clone.png)


## Step 1. Import Data from the NeMO Data Portal
### NeMO Overview
The [Neuroscience Muti-omic Data Archive (NeMO)](https://nemoarchive.org/) is a data repository for the BRAIN Initiative and related brain research initiatives.  You can access NeMO data using the NeMO [Data Portal](https://portal.nemoarchive.org/).  

### Quick-start instructions
1. Navigate to the NeMO [Data Portal](https://portal.nemoarchive.org/).
2. Use the faceted search to identify a 10x data set of interest.
3. Add the selected data to the NeMO cart.
4. From the cart, select `Download` and then `Export to Terra`.
5. Choose the existing cloned version of this workspace.

##  Step 2. Process 10x data with the Optimus workflow

### Optimus Overview   

This Optimus workflow has a quality control, alignment and transcriptome quantification module. It corrects Cell Barcodes (CBs) and Unique Molecular Identifiers (UMIs), aligns reads to the genome, generates a count matrix in a UMI-aware manner, detects empty droplets (single-cell mode only), calculates summary metrics for genes and cells, returns read outputs in BAM format, and returns raw counts in Loom file format. 

For more details about the pipeline, see the [Optimus Overview](https://broadinstitute.github.io/warp/docs/Pipelines/Optimus_Pipeline/README) in the [WARP documentation](https://broadinstitute.github.io/warp/).  

### Sample data
This step uses downsampled mouse data from the NeMO MOp 10x v2 project (`DS10M_L8TX_171026_01_A04`).  

### Quick-start instructions
1.  Go to the `file_set` data table on the `Data` tab.
2.  Select the `DS10M_L8TX_171026_01_A04` file-set.
3.  Open with the `1-Optimus-mouse-v2` workflow.
4.  Go to the workflow Inputs and set the r1_fastq variable to `this.files.r1_fastq`.
5.  Go to the workflow Outputs and set the output-loom-file to `this.my_loom`.
6.  Save the configuration.
7.  Select `Run Analysis` and then `Launch`.

### Outputs
The Optimus pipeline writes several outputs to the `file_set` data table, including a Loom matrix with raw counts and quality metrics which you can find in the ""my_loom"" column you created in Step 5.

## Step 3. Filter, cluster and normalize Optimus output using Cumulus

### Cumulus Overview

Cumulus is a cloud-based analysis workflow for large-scale single-cell and single-nucleus RNA-seq data ([Li et al. 2020](https://doi.org/10.1038/s41592-020-0905-x)). 

See the [Cumulus Featured Workspace](https://app.terra.bio/#workspaces/kco-tech/Cumulus) for additional details on using Cumulus on Terra or read the [Cumulus documentation ](https://cumulus.readthedocs.io/en/stable/) for workflow parameters. 

### Sample data
This step uses the Loom count matrix generated with Optimus as Cumulus input. By default, the workflow is set up to use the output-loom column of the `file_set` data table, which contains a ""pre-baked"" Loom file so that you don't have to run  Optimus prior to trying the Cumulus workflow.  This Loom file should be identical to the one you created with Optimus in Step 2.

### Quick-start instructions
1. Go to the `file_set` data table on the `Data` tab.
2. Select the `DS10M_L8TX_171026_01_A04` file-set.
3. Open with the `Cumulus` workflow.
4. Go to the inputs section of the workflow configuration.
5. On the Inputs tab, paste your Google Bucket ID into  the `output_directory` attribute where it says PASTE_BUCKET_ID_HERE (""gs://PASTE_BUCKET_ID_HERE/Cumulus/""). The final attribute should look similar to ""gs://fc-a8dd893f-84cf-4d28-952d-be4ae5805ebf/Cumulus/"".
6. Update the `output_name` attribute to name your files.

### Outputs

The Cumulus workflow in this workspace is set up to output a Seurat compatible h5ad file as well as visualization and embeddings compatible with [Single Cell Portal](https://singlecell.broadinstitute.org/single_cell), a platform that allows you to find, explore, and share single-cell data.

## Step 4. Explore single-cell data using Seurat
Seurat is an R package designed for single-cell RNA-seq data analysis and exploration ([Butler et al. 2018](https://www.nature.com/articles/nbt.4096); [Stuart, Butler et al. 2019](https://www.cell.com/cell/fulltext/S0092-8674(19)30559-8?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS0092867419305598%3Fshowall%3Dtrue)). This section uses the [Seurat - Guided Clustering Tutorial](https://satijalab.org/seurat/v3.2/pbmc3k_tutorial.html), which walks you through pre-processing, normalizing, dimensionality reduction and clustering for 10x single-cell RNA-seq data . Read more in the [Seurat documentation](https://satijalab.org/seurat/).  

### Sample data

This step uses the Optimus output Loom matrix from Step 2. For timing purposes, the file has been converted to a Seurat object and the resulting RDS file (brain.rds) is stored in a public Google Bucket. 

### Quick-start instructions
1. Select the Cloud Environment widget in the workspace upper right corner and choose the Custom option.
2. Select `R/Bioconductor` image from the Application Configruation drop-down.
3. In the `Cloud compute profile`, select **8 CPUs** from the CPU drop-down.
4. In the startup script section, paste: gs://terra-featured-workspaces/HCA_Featured_Workspace/startup-test3.sh
5. Create the Environment
6. Go to the workspace `Notebooks` page and open the `Seurat` Notebook in edit mode.
7. Follow the instructions listed in the Notebook.

## References
BRAIN Initiative Cell Census Network (BICCN) et al. A multimodal cell census and atlas of the mammalian primary motor cortex. bioRxiv 2020.10.19.343129; doi: https://doi.org/10.1101/2020.10.19.343129

Butler, A., Hoffman, P., Smibert, P. et al. Integrating single-cell transcriptomic data across different conditions, technologies, and species. Nat Biotechnol 36, 411–420 (2018). https://doi.org/10.1038/nbt.4096

Li, B., Gould, J., Yang, Y. et al. Cumulus provides cloud-based data analysis for large-scale single-cell and single-nucleus RNA-seq. Nat Methods 17, 793–798 (2020). https://doi.org/10.1038/s41592-020-0905-x

Stuart T, Butler A, Hoffman P, Hafemeister C, Papalexi E, Mauck WM 3rd, Hao Y, Stoeckius M, Smibert P, Satija R. Comprehensive Integration of Single-Cell Data. Cell. 2019 Jun 13;177(7):1888-1902.e21. doi: 10.1016/j.cell.2019.05.031. Epub 2019 Jun 6. PMID: 31178118; PMCID: PMC6687398.

## Acknowledgements
Special thanks to Brian Herb and the NeMO team, the Bioconductor team (Martin Morgan, Vince Carey, and Nitesh Turaga), the Cumulus team (Bo Li and Yiming Yang), and the Broad Pipelines team for their feedback and amazing contributions to this workspace.

### License
**Copyright Broad Institute, 2021 | BSD-3**

All rights reserved. Full license text available [here](https://github.com/broadinstitute/warp/blob/master/LICENSE).. Note that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.

---






",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/BICCN_Omics_Workshop"
281,"help-gatk","Genomics-in-the-Cloud-v1","READER","https://app.terra.bio/#workspaces/help-gatk/Genomics-in-the-Cloud-v1",TRUE,FALSE,NA,NA,NA,"Companion workspace for Genomics in the Cloud, an O'Reilly animal book by Geraldine A. Van der Auwera and Brian D. O'Connor.

Read it [online in the O'Reilly library](https://learning.oreilly.com/library/view/genomics-in-the/9781491975183/), download the [Kindle version](https://www.amazon.com/dp-B086Q7D47V/dp/B086Q7D47V/ref=mt_kindle) (works on regular computers) or order the [paperback version](https://www.amazon.com/Genomics-Cloud-GATK-Spark-Docker/dp/1491975199/) if you like to read in your bath.


### Book description

Data in the genomics field is booming. In just a few years, organizations such as the National Institutes of Health (NIH) will host 50+ petabytes—or 50 million gigabytes—of genomic data, and they’re turning to cloud infrastructure to make that data available to the research community. How do you adapt analysis tools and protocols to access and analyze that data in the cloud?   

With this practical book, researchers will learn how to work with genomics algorithms using open source tools including the Genome Analysis Toolkit (GATK), Docker, WDL, and Terra. Geraldine Van der Auwera, longtime custodian of the GATK user community, and Brian O’Connor of the UC Santa Cruz Genomics Institute guide you through the process. You’ll learn by working with real data and genomics algorithms from the field.

This book takes you through:

- Essential genomics and computing technology background
- Basic cloud computing operations
- Getting started with GATK
- Three major GATK Best Practices pipelines for variant discovery 
- Automating analysis with scripted workflows using WDL and Cromwell
- Scaling up workflow execution in the cloud, including parallelization and cost optimization
- Interactive analysis in the cloud using Jupyter notebooks
- Secure collaboration and computational reproducibility using Terra

### Detailed table of contents

- Foreword by Dr. Eric Lander, Founding Director of the Broad Institute
- Preface: Purpose, Audience and Scope of this book

1. Introduction
2. Genomics in a Nutshell: A Primer for Newcomers to the Field
3. Computing Technology Basics for Life Scientists
4. First Steps in the Cloud
5. First Steps with GATK
6. GATK Best Practices for Germline Short Variant Discover
7. GATK Best Practices for Somatic Variant Discovery
8. Automating Analysis Execution with Workflows
9. Deciphering Real Genomics Workflows
10. Running Single Workflows at Scale with Pipelines API
11. Running Many Workflows Conveniently in Terra
12. Interactive Analysis in Jupyter Notebook
13. Assembling Your Own Workspace in Terra
14. Making a Fully Reproducible Paper
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/Genomics-in-the-Cloud-v1"
282,"broad-firecloud-tcga","TCGA_GBM_hg38_OpenAccess_GDCDR-12-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_GBM_hg38_OpenAccess_GDCDR-12-0_DATA",TRUE,TRUE,"Glioblastoma multiforme","Tumor/Normal","USA","TCGA Glioblastoma multiforme Open-Access Data Workspace

*hg38 TCGA and TARGET workspaces reference files by their GDC UUIDs.  In order to run analyses on the referenced data files, you will need to run workflows that retrieve the files from the GDC and copy them to your workspace bucket.  See   [this forum post](https://gatkforums.broadinstitute.org/firecloud/discussion/10382/populating-hg38-tcga-and-target-workspaces-with-data-files#latest) for instructions on the running of these workflows.*","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients' clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","617","Brian","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh38/hg38","TCGA_GBM_hg38_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_GBM_hg38_OpenAccess_GDCDR-12-0_DATA"
283,"help-gatk","Germline-SNPs-Indels-GATK4-b37","READER","https://app.terra.bio/#workspaces/help-gatk/Germline-SNPs-Indels-GATK4-b37",TRUE,FALSE,NA,NA,NA,"### GATK Best Practices for Germline SNPs & Indels
Three workflows that perform pre-processing, and SNP and Indel variant and joint calling steps of analysis, configured to run back to back, so that the outputs from the preceding workflow automatically become the inputs for the next. Each workflow implements data processing according to the GATK Best Practices on human whole-genome sequence data, using b37 as the reference genome. 

Scroll down for details on each workflow, including input and output descriptions and requirements, estimated run times and costs, and information on sample data.       

**Note that cost and time estimates will vary** with the use of [Preemptibles](https://gatkforums.broadinstitute.org/firecloud/discussion/11786/preemptibles#latest).  
You can also use [Google's cost calculator](https://cloud.google.com/products/calculator/) to estimate cost to run.  **For helpful hints on controlling Cloud costs**, see [this article](https://support.terra.bio/hc/en-us/articles/360029748111).        

The following material is provided by the GATK Team. Please post any questions or concerns to one of our forum sites : [GATK](https://gatkforums.broadinstitute.org/gatk/categories/ask-the-team/) , [Terra](https://broadinstitute.zendesk.com/hc/en-us/community/topics/360000500432-General-Discussion) , or [WDL/Cromwell](https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team).

**Note:** This workspace is superseded by the [GATK4-Germline-Preprocessing-VariantCalling-JointCalling](https://app.terra.bio/#workspaces/help-gatk/GATK4-Germline-Preprocessing-VariantCalling-JointCalling) workspace and is no longer being updated. 


--------
## 1-Processing-For-Variant-Discovery
**What does it do?**    
This pipeline takes sequencing data in unmapped BAM (uBAM) format and outputs a clean BAM file and its index, suitable for variant discovery analysis. **Note**: If your data are not in unmapped BAM format, see [this workspace with helpful file format conversion tools](https://app.terra.bio/#workspaces/help-gatk/Sequence-Format-Conversion).      

**What does it require as input?**    
The 1-Processing-For-Variant-Discovery workflow accepts a file containing a list of unaligned BAMS. The input data are samples:

* Pair-end sequencing data in unmapped BAM (uBAM) format
* One or more read groups, one per uBAM file, all belonging to a single sample (SM)

Input uBAM files must comply with the following requirements:

* Filenames all have the same suffix (we use "".unmapped.bam"")
* Files must pass validation by ValidateSamFile
* Reads are provided in query-sorted order
* All reads must have an RG tag
* Reference index files must be in the same directory as source (e.g. reference.fasta.fai in the same directory as reference.fasta)    

**If your sequencing data is not in uBAM format**, check out this file conversion workspace, [https://app.terra.bio/#workspaces/help-gatk/Sequence-Format-Conversion](https://app.terra.bio/#workspaces/help-gatk/Sequence-Format-Conversion) for workflows to convert:    

1. Interleaved FASTQ to paired FASTQ
2. Paired FASTQ to unmapped BAM
3. BAM to unmapped BAM
4. CRAM to BAM files from sequencer output for use in GATK analysis tools

**What does it return as output?**    
Metadata for all outputs are written to the workspace data table, and include a clean BAM file and its index, suitable for variant discovery analyses.      

**Sample data description and location**   
This workspace data model contains both a full sized and downsampled version of NA12878's unaligned BAM list file under the column flowcell_unmapped_bams_list. Links to the expected input types are available in the workspace data model for testing.     
**Reference data description and location**  
The reference genome for this workspace is hg37 (aka GRCh37). Required and optional references and resources for the methods are included in the Workspace Data table (""Workspace Attributes"" in FireCloud).      

**Estimated time and cost to run on sample data**      
| Sample Name | Sample Size | Time | Cost $ |
| ---  | :---: | :---: | :---: |
|NA12878_24RG_small|	3.11 GB	|1:28:00	|0.18
|NA12878	|64.89 GB	|22:35:00	 |4.98    | 

**For helpful hints on controlling Cloud costs**, see this article ([click here to view](https://support.terra.bio/hc/en-us/articles/360029748111)).       

## 2-Haplotypecaller-GVCF
**What does it do?**   
The workflow rscatters the HaplotypeCaller tool over a sample (clean BAM file and index, from the previous step) using an intervals list file. In particular, it runs the HaplotypeCaller tool from GATK4 in GVCF mode on a single sample according to GATK Best Practices. The output file produced will be a single gvcf file, which can be used by the joint-discovery workflow.

**What does it require as input?**      
The 2-Haplotypecaller-GVCF workflow accepts an analysis ready BAM file, pre-proccessed using GATK Best Practices. In particular:     
- One analysis-ready BAM file for a single sample (as identified in RG:SM)
- A file containing a set of variant calling interval lists for the scatter     

**What does it return as output?**  
Metadata for all outputs are written to the workspace data table, and include one GVCF file and its index, ready for joint calling

**Sample data description and location**    
Links to the expected input types are available in the workspace data model for testing. The data model lists sample analysis-ready BAM files under the `analysis_ready_bam` column.      

**Reference Data description and location** .  
The reference genome for this workspace is hg37 (aka GRCh37). Required and optional references and resources for the methods are included in the Workspace Data table (""Workspace Attributes"" in FireCloud). 

**Estimated time and cost to run on sample data**        
| Sample Name | Sample Size | Time | Cost $ |
| ---  | :---: | :---: | :---: |
NA12878_24RG_small |	4.66 GB |	02:28:00 |	0.21
NA12878	| 19.55 GB	| 14:05:00 |	2.24

**For helpful hints on controlling Cloud costs**, see this article ([click here to view](https://support.terra.bio/hc/en-us/articles/360029748111)).     

## 3-Joint-Discovery
**What does it do?**     
This WDL implements the joint calling and VQSR filtering portion of the GATK Best Practices for germline SNP and Indel discovery in human whole-genome sequencing (WGS).

**What does it require as input?**  
- One or more GVCFs, produced by HaplotypeCaller in GVCF mode
- Bare minimum: 1 WGS sample or 30 Exome samples. Gene panels are not supported
- When determining disk size in the JSON, use the guideline below
  - small_disk = (num_gvcfs / 10) + 10
  - medium_disk = (num_gvcfs * 15) + 10
  - huge_disk = num_gvcfs + 10

**What does it return as output?**   
Metadata for all outputs are written to the workspace data table, and include a VCF file and its index, filtered using variant quality score recalibration (VQSR), with genotypes for all samples present in the input VCF. All sites that are present in the input VCF are retained. Filtered sites are annotated as such in the FILTER field.

**Sample data description and location**   
Links to the expected input types are available in the workspace data table for testing. 3-Joint-Discovery workflow accepts one or more gvcfs produced by haplotypecaller. The gvcf column in the data table contains a full sized gvcf of NA12878 that will be used for the Joint-Discovery workflow.

**Reference data**  
The reference genome for this workspace is hg37 (aka GRCh37). Required and optional references and resources for the methods are included in the Workspace Data table (""Workspace Attributes"" in FireCloud).    

**Estimated time and cost to run on sample data** .  

| Sample Name | Sample Size | Time | Cost $ |
| ---  | :---: | :---: | :---: |
|NA12878 |	10.75 GB |	02:48:00	| 0.90|   

**For helpful hints on controlling Cloud costs**, see this article ([click here to view](https://support.terra.bio/hc/en-us/articles/360029748111)).      

------
-------

### Important notes on workflow limitations**    
The current version of the posted ""Generic germline short variant joint genotyping"" is derived from the Broad production version of the workflow, which was adapted for large WGS callsets of up to 20K samples.

We believe the results of this workflow run on a single WGS sample are equally accurate, but there may be some shortcomings when the workflow is modified and run on small cohorts. Specifically, modifying the SNP ApplyRecalibration step for higher specificity may not be effective. The user can verify if this is an issue by consulting the gathered SNP tranches file. If the listed truthSensitivity in the rightmost column is not well matched to the targetTruthSensitivity in the leftmost column, then requesting that targetTruthSensitivity from ApplyVQSR will not use an accurate filtering threshold.

This workflow has not been tested on exomes. The dynamic scatter interval creating task was optimized for genomes. The scattered SNP VariantRecalibration may fail because of too few ""bad"" variants to build the negative model. Also, apologies that the logging for SNP recalibration is overly verbose.

The provided tool configurations is meant to be a ready to use example of the workflows. It is the user’s responsibility to correctly set the reference and resource input variables using the GATK Tool and Tutorial Documentations.

### Software Versions
GATK 4.1  
BWA 0.7.15-r1140    
Picard 2.16.0-SNAPSHOT    
Samtools 1.3.1 (using htslib 1.3.1)    
Python 2.7    

------

### Contact information
This material is provided by the GATK Team. Please post any questions or concerns to one of our forum sites : [GATK](https://gatkforums.broadinstitute.org/gatk/categories/ask-the-team/) , [Terra](https://broadinstitute.zendesk.com/hc/en-us/community/topics/360000500432-General-Discussion) , or [WDL/Cromwell](https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team).

## License
Copyright Broad Institute, 2019 | BSD-3
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at https://github.com/openwdl/wdl/blob/master/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.

------------------
----------------
#### Important Notes: 
- The current version of the posted ""Generic germline short variant joint genotyping"" is derived from the Broad production version of the workflow, which was adapted for large WGS callsets of up to 20K samples.  We believe the results of this workflow run on a single WGS sample are equally accurate, but there may be some shortcomings when the workflow is modified and run on small cohorts.  Specifically, modifying the SNP ApplyRecalibration step for higher specificity may not be effective.  The user can verify if this is an issue by consulting the gathered SNP tranches file.  If the listed `truthSensitivity` in the rightmost column is not well matched to the `targetTruthSensitivity` in the leftmost column, then requesting that `targetTruthSensitivity` from ApplyVQSR will not use an accurate filtering threshold.  This workflow has not been tested on exomes.  The dynamic scatter interval creating task was optimized for genomes.  The scattered SNP VariantRecalibration may fail because of two few ""bad"" variants to build the negative model. Also, apologies that the logging for SNP recalibration is overly verbose.
- The provided tool configurations is meant to be a ready to use example of the workflows. It is the user’s responsibility to correctly set the reference and resource input variables using the [GATK Tool and Tutorial Documentations](https://software.broadinstitute.org/gatk/documentation/).


",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/Germline-SNPs-Indels-GATK4-b37"
285,"featured-workspaces-hca","Intro-to-HCA-data-on-Terra","READER","https://app.terra.bio/#workspaces/featured-workspaces-hca/Intro-to-HCA-data-on-Terra",TRUE,FALSE,NA,NA,NA,"# Exploring Human Cell Atlas single-cell data
This tutorial workspace is a step-by-step guide to importing, accessing, and analyzing standardized cell-by-gene count matrices (Loom format) from the Human Cell Atlas (HCA) [Data Portal](https://data.humancellatlas.org/) using community-supported single-cell analysis tools.

Using this workspace, you will:

1. Import 10x cell by gene count matrices (Loom format) from the HCA [Data Portal](https://data.humancellatlas.org/)
2. Filter, normalize, and cluster a raw count matrix with the **Cumulus workflow**
3. Explore and visualize the single-cell data in example **Bioconductor**, **Pegasus**, **Scanpy**, and **Seurat** Jupyter Notebooks

![](https://storage.googleapis.com/terra-featured-workspaces/HCA_Featured_Workspace/intro-to-HCA-Data-on-Terra.png)


### How much does this cost?
The table below outlines the estimated cost and timing of each workflow and notebook, assuming you use the provided sample data.

| Workflow/Notebook | Cost ($) | Timing | Notes | 
| --- | --- | --- | --- |
| Cumulus workflow | 0.15 | 18 min | Runs on entire matrix (5 donors) |
| Bioconductor notebook | 0.09 | ~ 28 min | Runs on matrix subset (1 donor) |
| Pegasus notebook | 0.02 | ~ 5 min | Runs on matrix subset (1 donor) |
| Scanpy notebook | 0.03 | ~ 8 min | Runs on matrix subset (1 donor) |
| Seurat notebook | 0.07 | ~ 21 min | Runs on matrix subset (1 donor) |

![white space](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/white-space.jpg)  

### Work from your copy of the workspace following these step-by-step instructions 
|![doc icon](https://storage.googleapis.com/terra-featured-workspaces/QuickStart/icon_read_more%401x.png) | Read step-by-step instructions [here](https://support.terra.bio/hc/en-us/articles/360060041772) |  
| --------| ------------------|    

![white space](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/white-space.jpg)  

### Workspace data
Each tutorial step uses the standardized project-level count matrix for the project ""[Dissecting the human liver cellular landscape by single cell RNA-seq reveals novel intrahepatic monocyte/ macrophage populations](https://data.humancellatlas.org/explore/projects/4d6f6c96-2a83-43d8-8fe1-0f53bffd4674)"". 

The matrix contains UMI-aware counts and quality metrics produced with the [Optimus workflow](https://data.humancellatlas.org/pipelines/optimus-workflow), a 10x processing pipeline developed by the HCA's [Data Coordination Platform (DCP)](https://data.humancellatlas.org/about). 

Read more about  DCP-generated matrices in the Data Portal's [matrix overview](https://data.humancellatlas.org/guides/consumer-vignettes/matrices).
# Instructions
*Quick-start instructions are provided below, but detailed instructions are provided in the linked [step-by-step guide](https://support.terra.bio/hc/en-us/articles/360060041772) above*.

## Set up the tutorial workspace 
Before you begin, create your own editable copy (clone) of this WORKSPACE. Click the round circle with three dots in the upper right corner of this page and choose ""Clone"".
![](https://storage.googleapis.com/terra-featured-workspaces/HCA_Featured_Workspace/clone.png)

## Step 1. Import Data from the HCA Data Portal
### HCA Data Portal overview
The HCA [Data Portal](https://data.humancellatlas.org/) is the repository for the [HCA project's](https://www.humancellatlas.org/) data and metadata. It is part of the HCA's [Data Coordination Platform](https://data.humancellatlas.org/about) (DCP), a data contribution, access, and analysis service.

You can browse HCA data in the Data Portal's [Data Browser](https://data.humancellatlas.org/explore/projects) and export it to this workspace using the Quick-start instructions below.

### Quick-start instructions
1. Navigate to the Data Portal's [Data Browser]( https://data.humancellatlas.org/explore/projects)
2. Use the faceted search to identify a 10x dataset (with Loom matrix) of interest
3. Choose the `Export Selected Data` icon
4. Under the `Export to Terra` section, select `Start`
5. Select the species
6. Select the files to export; for cell-by-gene count matrices, choose ""loom""
7. Select `Request Export`
8. When the export is ready, select the link
9. Choose your cloned version of this workspace

All imported data will be automatically added to the `participant` data table on the workspace `Data` page. This data table already contains this tutorial's example matrix data, but you can import additional data of interest.

## Step 2. Filter, cluster and normalize 10x count matrix using Cumulus

### Cumulus overview

Cumulus is a cloud-based analysis workflow for large-scale single-cell and single-nucleus RNA-seq data ([Li et al. 2020](https://doi.org/10.1038/s41592-020-0905-x)). 

See the [Cumulus Featured Workspace](https://app.terra.bio/#workspaces/kco-tech/Cumulus) for additional details on using Cumulus on Terra or read the [Cumulus documentation ](https://cumulus.readthedocs.io/en/stable/) for workflow parameters. 

### Sample data
This step uses an example project-level 10x count matrix (sc-landscape-human-liver-10XV2.loom), but the instructions apply to any Loom-formatted project-level matrix imported from Data Portal.

By default, the workflow is set up to pull the matrix from the data table's  `__loom__file_drs_uri` column, which contains the cloud locations for Loom files. 

**This workflow runs on the entire matrix (5 donors), which is not batch corrected. To turn on batch correction, set the `correct_batch_effect` variable to `true`.**

### Quick-start instructions
1. Go to the `participant` data table on the `Data` tab
2. Select the `01335551-3f19-5ce1-9b0a-5828423e9725.2021-02-02T133000.000000Z
` participant
3. Open with the `Cumulus` workflow
4. Go to the inputs section of the workflow configuration
5. On the Inputs tab, paste your Google Bucket ID into  the `output_directory` attribute where it says PASTE_BUCKET_ID_HERE (""gs://PASTE_BUCKET_ID_HERE/Cumulus/""). The final attribute should look similar to ""gs://fc-a8dd893f-84cf-4d28-952d-be4ae5805ebf/Cumulus/""
6. Update the `output_name` attribute to name your files
7. Select `Save`
8. Select `Run Analysis`
9. Select `Launch`


### Outputs

The Cumulus workflow in this workspace is set up to output a Seurat compatible h5ad file as well as visualization and embeddings compatible with [Single Cell Portal](https://singlecell.broadinstitute.org/single_cell), a platform that allows you to find, explore, and share single-cell data.

## Step 3. Explore HCA data using Jupyter Notebooks
There are multiple community tools available to access and analyze HCA data from Jupyter Notebooks with R and python environments. 

This workspace provides four sample notebooks: 
1. [Bioconductor](https://bioconductor.org/)
2. [Convert_Loom_to_Seurat (preparing data for analysis in Seurat)](https://app.terra.bio/#workspaces/featured-workspaces-hca/Intro-to-HCA-data-on-Terra/notebooks/launch/Converting_Loom_to_Seurat.ipynb)
3. [Pegasus](https://pegasus.readthedocs.io/en/stable/)
4. [Scanpy](https://scanpy.readthedocs.io/en/stable/)
5. [Seurat ](https://satijalab.org/seurat/index.html)

Each notebook provides detailed instructions for accessing HCA data in the cloud and performing single-cell analysis using modified tutorials for each tool.

### Sample data
Each Jupyter Notebook uses a subset (~ 87,538  **unfiltered** cells from one donor) of the project matrix from [Dissecting the human liver cellular landscape by single cell RNA-seq reveals novel intrahepatic monocyte/ macrophage populations](https://data.humancellatlas.org/explore/projects/4d6f6c96-2a83-43d8-8fe1-0f53bffd4674).

### Cloud Environment Set-up
| Notebook | Application Configuration | Compute CPUs |
| --- | --- | --- |
| Bioconductor | R/Bioconductor | 4 |
| Convert_Loom_to_Seurat | R/Bioconductor | 4 |
| Pegasus | Pegasus | 4 | 
| Seurat | R/Bioconductor | 4 |
| Scanpy | R/Bioconductor | 4 | 

### Quick-start instructions
1. Select the Cloud Environment widget in the workspace upper right corner and choose the Custom option
2. Select the notebook’s appropriate Application Configuration using the Application Configuration drop-dow
3. In the Compute settings, choose 4 CPUs
4. Go to the workspace `Notebooks` page and open the `Scanpy`(python code), `Bioconductor` (R code),  `Seurat` (R code), or `Pegasus` (python) notebook in edit mode
5. Follow the instructions listed in the respective notebook

## Next steps
Check back for updates to the workspace tutorial. You can find additional Terra and single-cell resources in the workspace [step-by-step tutorial](https://support.terra.bio/hc/en-us/articles/360060041772).

## Workspace updates
| Date | Change | 
| --- | --- |
| 6/04/2021 | Added the Pegasus notebook |
| 5/21/2021 | Updated the Seurat notebook and added [AnVIL Bioconductor package](https://bioconductor.org/packages/release/bioc/html/AnVIL.html)) to all R notebooks |
| 5/07/2021 | Workspace is made public |

## Questions and feedback
Please post workspace questions and feedback to the [Featured Workspaces community forum](https://support.terra.bio/hc/en-us/community/topics/360001603491) (login required).


## References
Amezquita, R.A., Lun, A.T.L., Becht, E. et al. Orchestrating single-cell analysis with Bioconductor. Nat Methods 17, 137–145 (2020). https://doi.org/10.1038/s41592-019-0654-x

Butler, A., Hoffman, P., Smibert, P. et al. Integrating single-cell transcriptomic data across different conditions, technologies, and species. Nat Biotechnol 36, 411–420 (2018). https://doi.org/10.1038/nbt.4096

Li, B., Gould, J., Yang, Y. et al. Cumulus provides cloud-based data analysis for large-scale single-cell and single-nucleus RNA-seq. Nat Methods 17, 793–798 (2020). https://doi.org/10.1038/s41592-020-0905-x

MacParland, Sonya A et al. “Single cell RNA sequencing of human liver reveals distinct intrahepatic macrophage populations.” Nature communications 9,1 4383. 22  (2018). doi:10.1038/s41467-018-06318-7

Stuart T, Butler A, Hoffman P, Hafemeister C, Papalexi E, Mauck WM 3rd, Hao Y, Stoeckius M, Smibert P, Satija R. Comprehensive Integration of Single-Cell Data. Cell. 2019 Jun 13;177(7):1888-1902.e21. doi: 10.1016/j.cell.2019.05.031. Epub 2019 Jun 6. PMID: 31178118; PMCID: PMC6687398.

Wolf, F., Angerer, P. & Theis, F. SCANPY: large-scale single-cell gene expression data analysis. Genome Biol 19, 15 (2018). https://doi.org/10.1186/s13059-017-1382-0

## Acknowledgements
Special thanks to Gary Bader, the Bioconductor team (Martin Morgan, Vince Carey, and Nitesh Turaga), the Cumulus team (Bo Li and Yiming Yang), and the Broad Pipelines team for their feedback and amazing contributions to this workspace.

## License
**Copyright Broad Institute, 2020 | BSD-3**

All rights reserved. Note that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.

---


",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","featured-workspaces-hca/Intro-to-HCA-data-on-Terra"
286,"theiagen-demos","MGen-CDPH-2023","READER","https://app.terra.bio/#workspaces/theiagen-demos/MGen-CDPH-2023",TRUE,FALSE,NA,NA,NA,"# Pathogen Genomics in Public Health Laboratories: Successes, Challenges, and Lessons Learned from California’s SARS-CoV-2 Whole Genome Sequencing Initiative, COVIDNet
This public workspace is used to demonstrate some of the analyses described in a recent manuscript submitted for review to the Microbial Genomics special collection: [Implementation of Genomics in Clinical and Public Health Microbiology](https://www.microbiologyresearch.org/content/implementation-of-genomics-in-clinical-and-public-health-microbiology). 

## Data
All sequence data in this workspace was generated as part of the COVIDNet initiative in California and are available in the NCBI Sequence Read Archive under the accession numbers listed in illumina_specimen and clearlabs_specimen data tables. 

## Workflows
All workflows imported in this workspace (SRA_Fetch, TheiaCoV_ClearLabs, TheiaCoV_FASTA, TheiaCoV_ClearLabs, TheiaCoV_Augur_Prep, TheiaCoV_Augur_Run, and Mercury_Prep_N_Batch) are available on publicly-accessible [Theiagen GitHub repositories](https://github.com/theiagen) in tagged version releases and were imported directly from [Dockstore](https://dockstore.org/organizations/Theiagen). 

### SRA_Fetch
The SRA_Fetch workflow downloads sequence data from NCBI’s Sequence Read Archive (SRA). It requires an SRA run accession to populate the associated read files to a Terra data table. 

#### Inputs
The only input for the SRA_Fetch workflow is an SRA run accession, which begin with “SRR”, or an ENA run accession, which begin with “ERR”. Please see the NCBI Metadata and Submission Overview for assistance with identifying accessions: https://www.ncbi.nlm.nih.gov/sra/docs/submitmeta/.

#### Outputs
This workflow produces output columns for the read data. For paired-end data, these are read1 and read2 columns (for single-end data, only the read1 column). 


### The TheiaCoV Genomic Characterization Workflow Series
The TheiaCoV workflows for genomic characterization are for the assembly, quality assessment, and characterization of viral genomes, especially SARS-CoV-2.

All input reads are processed through “[core tasks](https://theiagen.notion.site/TheiaCoV-Genomic-Characterization-62b8048483d34ae7a9a54bbf4634975e)” in each workflow. These undertake read trimming and assembly, quality assessment, species identification, and some genome characterization. For some taxa identified, “taxa-specific sub-workflows” will be automatically activated, undertaking additional taxa-specific characterization steps. When setting up each workflow, users may choose to use “optional tasks” as additions or alternatives to tasks run in the workflow by default.

<p align=""center"">
	<a href=""https://www.theiagen.com"">
  <img src=""https://storage.googleapis.com/theiagen-public-files/terra/MGEN_2023/TheiaCoV-v2.3.1.png"" width=""1000"" background=""transparent"" class=""center"" alt=""TheiaCoV Genomic Characterization v2.3.1"">
	</a>
</p>

#### Inputs
All TheiaCoV workflows take in a sample name and either Illumina paired-end read data (TheiaCoV_Illumina_PE), Clear Labs-generated read data (TheiaCoV_ClearLabs), or previously assembled genomes in FASTA format (TheiaCoV_FASTA). Other versions of the TheiaCoV workflow exist for Illumina single-end read data (TheiaCoV_Illumina_SE) and Oxford Nanopore Technology read data (TheiaCoV_ONT), but these workflows have not been imported into this workspace. They may be found on [Dockstore](https://dockstore.org/organizations/Theiagen). 

Additionally, if data was generated using an tiled PCR amplicons, the primer sequence coordinates of the PCR scheme used must be provided in [BED file format](https://en.wikipedia.org/wiki/BED_(file_format)#). 

Read file names should end with `.fastq` or `.fq`, with the optional addition of `.gz`. When possible, Theiagen recommends zipping files with [gzip](https://www.gnu.org/software/gzip/) prior to Terra upload to minimize data upload time.

By default, the workflow anticipates 2 x 150bp reads (i.e. the input reads were generated using a 300-cycle sequencing kit). Modifications to the optional parameter for `trimmomatic_minlen` may be required to accommodate shorter read data, such as the 2 x 75bp reads generated using a 150-cycle sequencing kit.

#### Outputs
Please refer to our [technical documentation](https://theiagen.notion.site/TheiaCoV-Genomic-Characterization-62b8048483d34ae7a9a54bbf4634975e) for a full list of the outputs generated by each workflow in the TheiaCoV Genomic Characterization Series.

### The TheiaCoV Genomic Epidemiology Workflow Series
Two TheiaCoV workflows exist for the phylogenetic analysis of SARS-CoV-2 genomes using the tools generated by the NextStrain team, specifically [Augur](https://docs.nextstrain.org/projects/augur/en/stable/). These workflows, TheiaCoV_Augur_Prep and TheiaCoV_Augur_Run must be run sequentially to (1) prepare individual samples and then (2) run Augur on a set of samples. Phylogenetic tree files are generated and can be visualized in [Auspice](https://docs.nextstrain.org/projects/auspice/en/latest/) and [UShER](https://github.com/yatisht/usher).


#### Inputs
The TheiaCoV_Augur_Prep workflow prepares **individual** sample assemblies and their metadata for subsequently running the TheiaCoV_Augur_Run workflow. As input, it takes in an assembly file, collection_date, and other metadata. The assembly FASTA file can be generated with one of the TheiaCoV Genomic Characterization workflows. Metadata must be uploaded by the user.

The TheiaCoV_Augur_Run workflow takes a **set** of sample assemblies and sample metadata files (formatted with TheiaCoV_Augur_Prep) and runs Augur to generate the phylogenetic tree files with the accompanying metadata; SNP distances are also inferred. Please note that there must be at least some sequence diversity among the set of input assemblies, else Augur will fail.

#### Outputs
Please refer to our [technical documentation](https://theiagen.notion.site/TheiaCoV-Genomic-Epidemiology-b53db0b5fa5f4e8cb6d236bd9bba80ef) for a full list of TheiaCoV_Augur_Prep and TheiaCoV_Augur_Run outputs.

The `auspice_input_json` output can be uploaded to [Auspice](https://auspice.us/) for visualization. The `metadata_merged` output can also be uploaded to Auspice to add context to the phylogenetic visualization. The `combined_assemblies` output can be uploaded to [UShER](https://genome.ucsc.edu/cgi-bin/hgPhyloPlace) to view the samples on a global tree of representative sequences from the public repositories. 

**A Note on Using Private Data**: Auspice runs locally, meaning that your data and metadata remain on your computer, enabling the user to view private information. Since UShER performs its calculations on the UCSC server, private data should not be used with UShER. Because of this, UShER offers the [ShUShER](https://shusher.gi.ucsc.edu/) option, which runs entirely in your web browser so no files leave your computer; however, ~4GB of RAM are required.


### Mercury_Prep_N_Batch
Sharing sample read, assembly, and metadata through internationally-accessible databases allows insights to be drawn about how viruses spread and mutate globally. The Mercury Data Submission Workflow Series was developed to allow users to efficiently and accurately prepare files for submission to GISAID, BioSample, SRA, and GenBank. These workflows were developed in accordance with the [PHA4GE SARS-CoV-2 Contextual Data Specifications](https://github.com/pha4ge/SARS-CoV-2-Contextual-Data-Specification).

Recently, the three original workflows that comprised this series and had to be run sequentially were combined into a single workflow called [Mercury_Prep_N_Batch](https://theiagen.notion.site/Mercury-Workflow-Series-77b295720347409fa2aa969e730468dc). This workflow performs all of the steps of the previous three workflows by processing read data, asssembly files, and contextual metadata to prepare submissions for a group of samples in a single workflow.

#### Inputs
This workflow runs on the **set** level and requires an array of sample names that you want ot submit. All other required inputs are String inputs that specify the attributes of your workspace, like the name of the workspace, the name of the Terra project that hosts the workspace, and the name of the Terra table where the samples can be found. In addition, for SRA read data transfer, the workflow requires a `gcp_bucket_uri`, which is a Google bucket location where the SRA reads are temporarily stored before being transferred to SRA.

Several optional inputs control which read files should be used, and if certain quality thresholds (such as a maximum number of Ns in the assembly fasta) should be put in place. 

#### Outputs
Please refer to our [technical documentation](https://theiagen.notion.site/Mercury-Workflow-Series-77b295720347409fa2aa969e730468dc) for a full list of Mercury_Prep_N_Batch outputs.

The outputs include metadata files formatted for GISAID, BioSample, SRA, and GenBank, in addition to concatenated FASTA files for GISAID and GenBank. A list of samples that were excluded from submission is also given as output.

## Contact Information
We are actively seeking feedback on our workflows! Please contact us with any feedback on what can be improved, changed, or added, or for any support requests by emailing support@terrapublichealth.zendesk.com or support@theiagen.com.
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","theiagen-demos/MGen-CDPH-2023"
287,"jump-cellpainting","jump-cellpainting-datasets","READER","https://app.terra.bio/#workspaces/jump-cellpainting/jump-cellpainting-datasets",TRUE,FALSE,NA,NA,NA,"# JUMP Cell Painting data demonstration

This workspace provides Jupyter notebooks demonstrating how to work with data from the collection of [Cell Painting](https://jump-cellpainting.broadinstitute.org/cell-painting) image datasets generated by the [JUMP-CP Consortium](https://jump-cellpainting.broadinstitute.org/).

All the data is hosted on the Cell Painting Gallery on the Registry of Open Data on AWS ([https://registry.opendata.aws/cellpainting-gallery/](https://registry.opendata.aws/cellpainting-gallery/)). If you'd like to take a look at (a subset of) the data interactively, the [JUMP-CP Data Explorer](https://phenaid.ardigen.com/jumpcpexplorer/) by Ardigen and the [JUMP-CP Data Portal](https://www.springdiscovery.com/jump-cp) by Spring Discovery provide portals to do so. More details regarding the current status of the data release can be found on https://github.com/jump-cellpainting/datasets

[![DOI](https://zenodo.org/badge/552371375.svg)](https://zenodo.org/badge/latestdoi/552371375)

## Details about the data

The notebooks in this workspace operation on a principal dataset of 116k chemical and >15k genetic perturbations the partners created in tandem (`cpg0016`), split across 12 data-generating centers. Human U2OS osteosarcoma cells are used.

* Most data components (images, raw CellProfiler output, single-cell profiles, aggregated CellProfiler profiles) are available from 12 sources for the principal dataset. Each source corresponds to a unique data generating center (except `source_7` and `source_13`, which were from the same center).
* Please see  https://github.com/jump-cellpainting/datasets for more details regarding additional data planned for release.

## Notebooks

The following tutorial notebooks were obtained from the [JUMP Cell Painting datasets GitHub repository](https://github.com/jump-cellpainting/datasets):

1. `workspace_setup.ipynb`: run this notebook first to install the Python dependencies and metadata. Takes less than a minute to run.
2. `sample_notebook.ipynb`: demonstrates how to load the data in the principal dataset and visualize it. Takes less than a minute to run.

Recommended environment settings to run the notebooks are below:

Runtime Environment |	Value
------------------------------|-------
CPU Minimum |	1
Disk size Minimum	| 50 GB
Memory Minimum	| 3.75 GB
Environment |  Defalut (GATK, Python, R)

## How to use this workspace
1. Clone this workspace and open the Notebooks tab.
2. Click on notebook `workspace_setup.ipynb` and follow prompts to create a runtime environment Note: if you don't have a runtime environment running, it will take a few minutes to create and start.
3. Run all cells in notebook  `workspace_setup.ipynb` to install the Python dependencies and metadata. Takes less than a minute to run to completion.
4. Open and run all cells in notebook `sample_notebook.ipynb` to load the data in the principal dataset and visualize it. Takes less than a minute to run to completion.

## Citation/license

### Citing the JUMP resource as a whole

All the data is released with CC0 1.0 Universal (CC0 1.0).
Still, professional ethics require that you cite the associated publication.
Please use the following format to cite this resource as a whole:

_We used the JUMP Cell Painting datasets (Chandrasekaran et al., 2022b), available from the Cell Painting Gallery on the Registry of Open Data on AWS ([https://registry.opendata.aws/cellpainting-gallery/](https://registry.opendata.aws/cellpainting-gallery/))._

For applications which require a DOI, this repository is archived at <https://zenodo.org/record/7628768> automatically upon each release.
The permanent DOI is 10.5281/zenodo.7628768; individual versions will also be assigned DOIs, see the badge at the top of this README for the most recent DOI.

Please note that the JUMP whole-project manuscript (Chandrasekaran et al., 2022b) is currently in preparation.

### Citing individual JUMP datasets

To cite individual JUMP Cell Painting datasets, please follow the guidelines in the Cell Painting Gallery citation [guide](https://github.com/broadinstitute/cellpainting-gallery/#citationlicense).
Examples are as follows:

_We used the dataset cpg0001 (Cimini et al., 2022), available from the Cell Painting Gallery on the Registry of Open Data on AWS (<https://registry.opendata.aws/cellpainting-gallery/>)._

_We used the dataset cpg0000 (Chandrasekaran et al., 2022a), available from the Cell Painting Gallery on the Registry of Open Data on AWS (<https://registry.opendata.aws/cellpainting-gallery/>)._

## Gratitude

Thanks to Consortium Partner scientists for creating this data, from Ksilink, Amgen, AstraZeneca, Bayer, Biogen, the Broad Institute, Eisai, Janssen Pharmaceutica NV, Merck KGaA Darmstadt Germany, Pfizer, Servier, and Takeda.

Supporting Partners include Ardigen, Google Research, Nomic Bio, PerkinElmer, and Verily. Collaborators include the Pistoia Alliance, Umeå University, and the Stanford Machine Learning Group. The AWS Open Data Sponsorship Program is sponsoring data storage.

This work was funded by a major grant from the Massachusetts Life Sciences Center and the National Institutes of Health through MIRA R35 GM122547 to Anne Carpenter.

## Questions?

Please ask your questions via issues [https://github.com/jump-cellpainting/datasets/issues](https://github.com/jump-cellpainting/dataset/issues).

Keep posted on future data updates by subscribing to our email list, see the button here: <https://jump-cellpainting.broadinstitute.org/more-info>",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","jump-cellpainting/jump-cellpainting-datasets"
288,"terra-test-bwalsh","vrs_anvil","READER","https://app.terra.bio/#workspaces/terra-test-bwalsh/vrs_anvil",TRUE,FALSE,NA,NA,NA,"# VRS AnVIL
## About

The VRS AnVIL Workspace is designed to bridge the gap between the Terra platform and the Variation Represetation Specification (VRS) supported by the Global Alliance for Genomics and Health (GA4GH). This workspace provides tools to enable the use of GA4GH VRS IDs within the Terra platform. GA4GH VRS identifiers provide a standardized way to represent genomic variations, making it easier to exchange and share genomic information.

The two features of this workspace include the VRS Annotator Workflow and the VRS AnVIL Toolkit.

## Getting Started

To get started, clone the workspace by selecting the menu on the top right (three vertical grey dots)! This will allow you to associate a billing account and begin running analyses and workflows. For more info on the features of the workspace...

## VRS Annotator Workflow

The [VRS Annotator Workflow](https://dockstore.org/my-workflows/github.com/ohsu-comp-bio/vrs-annotator/VRSAnnotator) is a Terra workflow allowing you to annotate Variant Call Format (VCF) files. This makes integration of genomic variant data with downstream evidence data like [MetaKB](https://search.cancervariants.org/) much easier.

To get started, navigate to the workflows and make use of the existing sample URIs to VCFs that already exist in the Workspace. Otherwise, to learn more about workflows, see the docs on [running a Terra workflow](https://support.terra.bio/hc/en-us/articles/360036379771-Overview-Running-workflows-in-Terra) and the Dockstore repository for the [VRS Annotator workflow](https://dockstore.org/workflows/github.com/ohsu-comp-bio/vrs-annotator/VRSAnnotator:main?tab=info). 

## VRS AnVIL Toolkit

### Overview

The [VRS AnVIL Toolkit](https://github.com/ohsu-comp-bio/vrs_anvil_toolkit) is a Python package designed to process Variant Call Format (VCF) files and perform lookup operations on Genomic Variation Representation Service (GA4GH VRS) identifiers.

In addition, this project facilitates the retrieval of evidence associated with genomic alleles by leveraging the Genomic Data Representation and Knowledge Base (GA4GH MetaKB) service. GA4GH MetaKB provides a comprehensive knowledge base that links genomic variants to relevant clinical variant interpretations. To get started, see the GitHub repo [README file](https://github.com/ohsu-comp-bio/vrs_anvil_toolkit/blob/development/README.md).",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","terra-test-bwalsh/vrs_anvil"
289,"canine-cancer-projects","Canine_OSA_cell_line_pipeline","READER","https://app.terra.bio/#workspaces/canine-cancer-projects/Canine_OSA_cell_line_pipeline",TRUE,FALSE,NA,NA,NA,"## Somatic variant calling in canine osteosarcoma cell lines
 <p>This workspace contains the workflows used for data preprocessing and somatic variant calling in canine osteosarcoma cell lines.</p>

<p>Citation: **The genomic landscape of canine osteosarcoma cell lines reveals conserved structural complexity and pathway alterations </p>

### Workflows:
#### Section 1: Preprocessing
**1-Preprocess-GATK_4_preprocess_BQSR_only** - This is a copy of the GATK showcase preprocessing WDL edited to accept an aligned bam file and run MarkDuplicates (optional) and Base Quality Score Recalibration (BQSR).

#### Section 2: Simple somatic mutation (SSM) calling
* Mutect2
* 2-SSM-1-Mutect2_pon
* 2-SSM-2-Mutect2_wdl
* Platypus

#### Section 3: Somatic copy number aberration (SCNA) calling
* GATK CNV PoN
* GATK CNV

#### Section 4: Other structural variant (SV) calling
* Manta tumor
* Manta germline",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","canine-cancer-projects/Canine_OSA_cell_line_pipeline"
290,"help-terra","TCGA_hg38_tools","READER","https://app.terra.bio/#workspaces/help-terra/TCGA_hg38_tools",TRUE,FALSE,NA,NA,NA,"",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-terra/TCGA_hg38_tools"
291,"broad-firecloud-tcga","TCGA_DLBC_hg38_OpenAccess_GDCDR-12-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_DLBC_hg38_OpenAccess_GDCDR-12-0_DATA",TRUE,TRUE,"Lymphoid Neoplasm Diffuse Large B-cell Lymphoma","Tumor/Normal","USA","TCGA Lymphoid Neoplasm Diffuse Large B-cell Lymphoma Open-Access Data Workspace

*hg38 TCGA and TARGET workspaces reference files by their GDC UUIDs.  In order to run analyses on the referenced data files, you will need to run workflows that retrieve the files from the GDC and copy them to your workspace bucket.  See   [this forum post](https://gatkforums.broadinstitute.org/firecloud/discussion/10382/populating-hg38-tcga-and-target-workspaces-with-data-files#latest) for instructions on the running of these workflows.*","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients' clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","50","Lymph Nodes","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh38/hg38","TCGA_DLBC_hg38_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_DLBC_hg38_OpenAccess_GDCDR-12-0_DATA"
292,"broad-firecloud-tcga","TCGA_CHOL_OpenAccess_V1-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_CHOL_OpenAccess_V1-0_DATA",TRUE,TRUE,"Cholangiocarcinoma","Tumor/Normal","USA","TCGA Cholangiocarcinoma Open-Access Data Workspace","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients’ clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","51","Bile Duct","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh37/hg19","TCGA_CHOL_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_CHOL_OpenAccess_V1-0_DATA"
293,"bayer-pcl-single-cell","MPG_primer_R_script_DEG","READER","https://app.terra.bio/#workspaces/bayer-pcl-single-cell/MPG_primer_R_script_DEG",TRUE,FALSE,NA,NA,NA,"workspace to be set to public for the MPG primer on 10/8/2020",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","bayer-pcl-single-cell/MPG_primer_R_script_DEG"
294,"fc-product-demo","Terra-Workflows-Quickstart","READER","https://app.terra.bio/#workspaces/fc-product-demo/Terra-Workflows-Quickstart",TRUE,TRUE,NA,NA,NA,"Learn the basics of running a workflow analysis on data in a workspace table. In this tutorial you'll practice setting up (configuring), launching, and monitoring a bulk process workflow (WDL) analysis.        

**Part 1- Run a pre-configured workflow on a single sample**          

![Part 1 flow - CRAM to workflow to BAM file](https://storage.googleapis.com/terra-featured-workspaces/QuickStart/Workflows-QuickStart_Part1_flow_scaled.png)      

**Part 2 - Set up and run workflow to analyze two samples**   
![Part 2 flow - CRAM in cloud to CRAM in Terra to workflow to BAM file](https://storage.googleapis.com/terra-featured-workspaces/QuickStart/Workflows-QuickStart_Part2_flow_scaled.png)       

**Part 3 - Run an additional workflow on the generated output of the set**     
![Part 3 flow - CRAM in cloud to CRAM in Terra to workflow to workflow to BAM file](https://storage.googleapis.com/terra-featured-workspaces/QuickStart/Workflows-QuickStart_Part3_flow_scaled.png)



### How long will it take to run? How much will it cost?    
**Time to complete**     
If you use the suggested data samples for analysis, it should take less than 10-15 minutes per exercise (excluding time for the queued/submitted job to start. Note that you can click on the ""Timing"" section in the Job Manager to see what is taking time).    
  
**Total cost**      
Total workflow runtime charges (Google Cloud service costs) for all three exercises should be much less than $ 1 USD. **Costs for running the workflows and storing outputs (in the workspace bucket) will be charged to the Terra Billing Project you assign when you clone your own copy of the workspace.**         
<br>

# QuickStart Instructions
## Set up the tutorial workspace
Before you begin, you'll create your own editable copy (clone) of this WORKSPACE to work from. Click on the round circle with three dots in the upper right corner of this page:   
    ![Clone_ScreenShot](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/Clone_workspace_Screen%20Shot.png)

| ![tip icon](https://storage.googleapis.com/terra-featured-workspaces/QuickStart/video-icon-final_scaled-2.png) |[**Right click for a video walkthrough**](https://youtu.be/mYUNQyAJ6WI)  ![white space](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/white-space.jpg)  |     
|-------|-------|    
<br>

 
## Part 1: Run a pre-configured workflow analysis 
  
![Part 1 flow - CRAM to workflow to BAM file](https://storage.googleapis.com/terra-featured-workspaces/QuickStart/Workflows-QuickStart_Part1_flow_scaled.png)      

### Learning objectives
This exercise is to give you a feel for the mechanics of running a workflow successfully on data from the data table. 

### Quickstart steps
1. Navigate to the workflow page
2. Open the `Part_1_CRAM_to_BAM` workflow
3. From the ""Select data"" button, choose the `NA12878` sample to process
4. Confirm preset configuration setup
5. Launch workflow
6. Examine output in the `sample` data table and answer thought questions


|![read more icon](https://storage.googleapis.com/terra-featured-workspaces/QuickStart/icon_read_more%401x.png) | [**Open step-by-step instructions**](https://support.terra.bio/hc/en-us/articles/360043454592) |   ![tip icon](https://storage.googleapis.com/terra-featured-workspaces/QuickStart/video-icon-final_scaled-2.png) |[**Right click for a video walkthrough**](https://youtu.be/rUBrJNqLyfU)  ![white space](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/white-space.jpg)  | 
|---| ---------|------|--------        

### Workflow overview
The workflow is a file format conversion pipeline for converting genomic files from one format
(CRAM) to another (BAM) for downstream analysis.

### Input 
For input you’ll use downsized sample data (from a public Google bucket) already referenced in the workspace ""sample"" table. The workflow is set up to input from and output to the table.      

### Output    
The workflow will generate an output BAM for each input CRAM. 

### Time and cost to complete
The exercise should take a few minutes (plus time your submission is in the queue) and cost a few pennies.      
<br>


## Part 2: Set up a workflow to run on sample data in a table       

![Part 2 flow - CRAM in cloud to CRAM in Terra to workflow to BAM file](https://storage.googleapis.com/terra-featured-workspaces/QuickStart/Workflows-QuickStart_Part2_flow_scaled.png)          

### Learning objectives
In this exercise, you'll use public data in the cloud as a proxy for your own data in a Google bucket. To set up the workflow, you'll fill in a blank configuration form. to run on both samples in the data table, generating a sample set in the process.  

### Quickstart steps
1. Navigate to the workflows page and select the `Part2_CRAM_to_BAM` workflow    
2. Click the ""Select Data"" button to select both samples of data in the data table  
3. Choose a memorable name for the automatically generated set of these two samples    
4. Set up inputs to read from the data table   
5. Set up workspace-level inputs from the workspace data table
6. Set outputs to write to the `sample` data table 
7. Launch and monitor workflow 
8. Examine output files in `sample` data table

|  ![read more icon](https://storage.googleapis.com/terra-featured-workspaces/QuickStart/icon_read_more%401x.png) | [**Open step-by-step instructions**](https://support.terra.bio/hc/en-us/articles/360054121691) |   ![tip icon](https://storage.googleapis.com/terra-featured-workspaces/QuickStart/video-icon-final_scaled-2.png) |[**Right click for a video walkthrough**]( https://youtu.be/k5aGvKljgTk)  ![white space](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/white-space.jpg)  |     
|---| ---------|---|--------|         

### Workflow overview
The workflow is a file same format conversion pipeline from Part 1.

### Input 
For input you’ll use two samples of downsized CRAM data (from a public Google bucket) already in the workspace ""sample"" table.      

### Output   
The workflow will generate an output BAM for each input CRAM. The generated data are stored in the workspace bucket and referenced in the `sample` data table. The workflow also generates a `sample_set` table that includes one set of two samples. You will use this sample_set for downstream analysis. 

### Time and cost estimates
The exercise should take a few minutes (plus time your submission is in the queue) and cost a few pennies.       
<br>


## Part 3: Run further analysis on generated outputs         
   ![Part 3 flow - CRAM in cloud to CRAM in Terra to workflow to workflow to BAM file](https://storage.googleapis.com/terra-featured-workspaces/QuickStart/Workflows-QuickStart_Part3_flow_scaled.png)    

### Learning goals
In this exercise, you'll configure a workflow to use the outputs from Part 2 as inputs for downstream analysis. You will run on the set of two samples created in Part 2.

### Quickstart steps
1. Select the `Part_3_BAM_to_uBAM` workflow from the workflows page
2. Choose the root entity type `sample` 
3. Starting in the ""Select Data"" button, select the group of samples to process from the `sample_set` data table
4. Configure the Inputs in the form
5. Configure the outputs to write to the `sample` table
6. Launch workflow
7. Examine output in the `sample` table


|   ![read more icon](https://storage.googleapis.com/terra-featured-workspaces/QuickStart/icon_read_more%401x.png) | [**Open step-by-step instructions**](https://support.terra.bio/hc/en-us/articles/360053601712) | ![tip icon](https://storage.googleapis.com/terra-featured-workspaces/QuickStart/video-icon-final_scaled-2.png) |[**Right click for a video walkthrough**](https://youtu.be/xOHUeXfMpEU)  ![white space](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/white-space.jpg)  |  
|---| ---------|----|----------|        


### Workflow overview
The workflow takes a genomic data file in BAM format and converts to uBAM format. 

### Input 
The inputs are the BAM files generated in part 2. 

### Output 
The workflow will generate one unmapped BAM file (uBAM) for each BAM file, stored in the workspace bucket. Links to the generated output will be written to the `sample` data table. 

### Time and cost estimate
The exercise should take a few minutes (plus time your submission is in the queue) and cost a few pennies.       
<br>


## How can I learn more?
Now that you know the basics, you are well on the way to using the Terra platform to do your own analysis. Below are some more advanced resources for next steps.     

To learn more about Terra and its functionality, see our [user guide](https://support.terra.bio/hc/en-us)       
* [Getting started on Terra](https://support.terra.bio/hc/en-us/categories/360005881492)      
* [Doing research on Terra](https://support.terra.bio/hc/en-us/categories/360001399872)    
* [Pipelining with workflows](https://support.terra.bio/hc/en-us/sections/360004147011)      
* To learn more about cloning workspaces, see [this article](https://support.terra.bio/hc/en-us/articles/360026130851)     
* For more information about how to manage data with a data table, see [this article](https://support.terra.bio/hc/en-us/articles/360025758392)      
* To learn more about troubleshooting and monitoring your workflow submission on Terra, see [this article](https://support.terra.bio/hc/en-us/articles/360027920592)          
* [Understanding Cloud costs and tips and tricks for reducing costs](https://support.terra.bio/hc/en-us/articles/360029748111)          
* [Sample use cases Cloud costs](https://support.terra.bio/hc/en-us/articles/360029772212)          



## Contact information  
This material is provided by the Terra Team. Please post any questions or concerns to our [Terra forum site.](https://broadinstitute.zendesk.com/hc/en-us/community/topics/360000500432-General-Discussion) 

Help us improve! Let us know what you think of the workflows QuickStart by filling out this [feedback form](https://docs.google.com/forms/d/e/1FAIpQLSeD5gI2CIEgblJ16DdgamE6w4yl3zlt_2xIkgONDaKfRxQxCw/viewform). We value your input.      


## License  
**Copyright Broad Institute, 2020 | BSD-3**  
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at https://github.com/openwdl/wdl/blob/master/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.

### Workspace Citation 
Details on citing Terra workspaces can be found here: [How to cite Terra](https://support.terra.bio/hc/en-us/articles/360035343652).

Data Sciences Platform, Broad Institute (*7/22/2021*), fc-product-demo/Terra-Workflows-Quickstart [workspace] Retrieved *Month Day, Year that workspace was retrieved*,  https://app.terra.bio/#workspaces/fc-product-demo/Terra-Workflows-Quickstart",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","fc-product-demo/Terra-Workflows-Quickstart"
296,"broad-firecloud-cptac","PANOPLY_Production_Pipelines_v1_4","READER","https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_Production_Pipelines_v1_4",TRUE,FALSE,NA,NA,NA,"# PANOPLY: A cloud-based platform for automated and reproducible proteogenomic data analysis
#### Version 1.4

PANOPLY is a platform for applying state-of-the-art statistical and machine learning algorithms to transform multi-omic data from cancer samples into biologically meaningful and interpretable results. PANOPLY leverages [Terra](http://app.terra.bio)—a cloud-native platform for extreme-scale data analysis, sharing, and collaboration—to host proteogenomic workflows, and is designed to be flexible, automated, reproducible, scalable, and secure. A wide array of algorithms applicable to all cancer types have been implemented, and available in PANOPLY for analysis of cancer proteogenomic data.


| ![PANOPLY Overview ](https://raw.githubusercontent.com/broadinstitute/PANOPLY/dev/panoply-overview-v2.png) |
|:--:|
| *Figure 1. Overview of PANOPLY architecture and the various tasks that constitute the complete workflow. Tasks can also be run independently, or combined into custom workflows, including new tasks added by users. Inputs to PANOPLY consists of (i) externally characterized genomics and proteomics data (in gct format); (ii) sample phenotype and annotations (in csv format); and (iii) parameters settings (in yaml format). Panoply modules are grouped into Data Preparation Modules (green box), Data Analysis Modules (blue box) and Report Modules (red box). Data preparation modules perform quality checks on input data followed by optional normalization and filtering for proteomics data. Analysis ready data tables are then used as inputs to the data analysis modules. Results from the data analysis modules are summarized in interactive reports generated by appropriate report modules. Modules under development are listed in grey text.* |


PANOPLY consists of the following components:

* A Terra production workspace on [PANOPLY_Production_Pipelines_v1_4](https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_Production_Pipelines_v1_4) with a preconfigured unified workflow to automatically run all analysis tasks on proteomics (global proteome, phosphoproteome, acetylome, ubiquitylome), transcriptome and copy number data; and an [additional workspace](https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_Production_Modules_v1_4) that includes separate methods for each analysis component. 
* An interactive Jupyter notebook (included in the Terra workspaces) that provides step-by-step instructions for uploading data, identifying data types, specifying parameters, and setting up the PANOPLY workspace for analyzing a new dataset.
* A GitHub [repository](https://github.com/broadinstitute/PANOPLY) that contains code for all PANOPLY tasks and workflows, including R code for implementing analysis algorithms, task module creation, and release management. A GitHub [wiki](https://github.com/broadinstitute/PANOPLY/wiki) includes documentation and description of algorithms. 
* A [tutorial](https://github.com/broadinstitute/PANOPLY/wiki/PANOPLY-Tutorial) illustrating the application of PANOPLY to a published breast cancer dataset and demonstrating the practical relevance of PANOPLY by regenerating many of the results described in (Mertins et al) (1) with minimal effort. The [PANOPLY_Tutorial](https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_Tutorial) workspace contains data and results from running the tuorial.
* Terra workspaces showing case studies of applying PANOPLY to the analysis of [CPTAC BRCA](https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_CPTAC_BRCA) (7) and [CPTAC LUAD](https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_CPTAC_LUAD) (8) datasets. The workspaces include all data, parameter settings and results.


PANOPLY provides a comprehensive collection of proteogenomic data analysis methods including sample QC (sample quality evaluation using profile plots and tumor purity scores (1), identify sample swaps, etc.), association analysis, RNA and copy number correlation (to proteome), connectivity map analysis (1 ,2), outlier analysis using BlackSheep (3), PTM-SEA (4), GSEA (5) and single-sample GSEA (6), consensus clustering, and multi-omic clustering using non-negative matrix factorization (NMF). Most analysis modules include a report generation task that outputs a HTML interactive report summarizing results from the respective analysis tasks. Mass spectrometry-based proteomics data amenable to analysis by PANOPLY includes isobaric label-based LC-MS/MS approaches like iTRAQ, TMT and TMTPro profiling the proteome and multiple PTM-omes including phospho-, acetyl-and ubiquitylomes.

Users can also [customize or create new pipelines and add their own tasks](https://github.com/broadinstitute/PANOPLY/wiki/Customizing-PANOPLY) and integrate them into the PANOPLY.


## Quick Start

* For a quick introduction and tour of PANOPLY, follow the [**tutorial**](https://github.com/broadinstitute/PANOPLY/wiki/PANOPLY-Tutorial). 
* Detailed **documentation** can be found [here](https://github.com/broadinstitute/PANOPLY/wiki).

### Contact

Email proteogenomics@broadinstitute.org with questions, comments or feedback.


### Citation

Mani, D. R. et al. PANOPLY: a cloud-based platform for automated and reproducible proteogenomic data analysis. *Nature Methods* 1–3 (2021) doi:10.1038/s41592-021-01176-6.
  

### References

1. Mertins, P. et al. Proteogenomics connects somatic mutations to signalling in breast cancer. *Nature* 534, 55–62 (2016).
2. Subramanian, A. et al. A Next Generation Connectivity Map: L1000 Platform and the First 1,000,000 Profiles. *Cell* 171, 1437–1452.e17 (2017).
3. Blumenberg, L. et al. BlackSheep: A Bioconductor and Bioconda package for differential extreme value analysis. *J of Proteome Research* 20(7), 3767-3773 (2021).
4.	Krug, K. et al. A Curated Resource for Phosphosite-specific Signature Analysis. *Mol. Cell. Proteomics* 18, 576–593 (2019).
5.	Subramanian, A. et al. Gene set enrichment analysis: a knowledge-based approach for interpreting genome-wide expression profiles. *Proc. Natl. Acad. Sci.* 102, 15545–15550 (2005).
6.	Barbie, D. A., Tamayo, P., Boehm, J. S., Kim, S. Y. & Moody, S. E. Systematic RNA interference reveals that oncogenic KRAS-driven cancers require TBK1. *Nature* (2009).
7. Gillette, M. A. et al. Proteogenomic Characterization Reveals Therapeutic Vulnerabilities in Lung Adenocarcinoma. *Cell* 182, 200–225.e35 (2020).
8. Krug, K., Jaehnig, E. J., Satpathy, S., Blumenberg, L., et al. Proteogenomic Landscape of Breast Cancer Tumorigenesis and Targeted Therapy. *Cell* 183, 1–21 (2020).
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","broad-firecloud-cptac/PANOPLY_Production_Pipelines_v1_4"
297,"amp-pd-public","AMP-PD-In-Terra","READER","https://app.terra.bio/#workspaces/amp-pd-public/AMP-PD-In-Terra",TRUE,FALSE,NA,NA,NA,"# Accelerating Medicines Partnership Parkinson's Disease (AMP PD)

<p>
		<img src=""https://storage.googleapis.com/amp-pd-public-content/featured-workspaces/amp-pd-in-terra/about-icon-image.png"" width=250px align=""left""></img>
AMP PD (Accelerating Medicine Partnership in Parkinson's Disease) was launched in 2018 as a public/private partnership intended to build a database with deep molecular characterization and longitudinal clinical profiling of PD patient data and biosamples with the goal of identifying and validating diagnostic, prognostic and/or disease progression biomarkers for PD.

A critical component of this partnership is broad sharing of the AMP PD data and analyses with the biomedical community to advance research in PD. AMP PD utilizes well characterized cohorts with existing biosamples and clinical data that were collected under comparable protocols and using common data elements.

A list of participating organizations and contributing studies can be found below.
</p>


## Data

### Data Overview
The current release of 
<a href=""https://amp-pd.org/data"" target=""_blank"" rel=""noopener noreferrer"">AMP PD</a> is release 4.0 and of <a href=""https://amp-pd.org/federated-cohorts/gp2"" target=""_blank"" rel=""noopener noreferrer"">GP2</a>, its federated dataset, is 5.0. See below for release history.

#### AMP PD Release 4.0 contains
- 10,908 participants
- 10,490 whole genome sequence (WGS) samples
- 10,609 transcriptomics (RNASeq) samples
- 621 untargeted proteomics participants at various timepoints, including raw and mzML sample level data
- 413 targeted proteomics participants at various timepoints from Release 3.0 in a combined, normalized release data set 
- 100 Single Nucleus participants with data from 5 different brain regions

#### AMP PD Harmonized Data includes:
- Clinical Data (Demographic, Medical History, MDS-UDPRS, MoCA, UPSIT)
- Transcriptomic Data (RNASeq from whole blood and exRNA from CSF and plasma)
- Genomic Data (Whole Genome Sequencing from whole blood)
- Proteomic Data (Targeted and Untargeted proteomics from CSF and plasma)
- Single Nucleus Post Mortem Brain Cohort Data (clinical, WGS and RNA seq)

#### Release 4.0 updates
- New data type:  Untargeted Proteomics dataset for CSF and Plasma 
- New data type: Post Mortem Brain Cohort - Single Nucleus participants with clinical data, WGS, and RNASeq data from 5 brain regions
- New Untargeted Proteomics CSF and Plasma sample level raw files and mzML files
- New bridged release: Targeted Proteomics bridged release proteomics-PPEA-D03
- New Terra Notebook in AMP PD- Proteomics QC and Analysis Workspace: R - Proteomics - Analysis - Untargeted Proteomics PCA 
- Addition of Untargeted Proteomics sections to AMP PD - Proteomics QC and Analysis workspace notebooks : Addition to R - Proteomics - APOE Proteomics Case Study, R - Proteomics - Analysis - NPX Boxplots
- Addition of Single Nucleus RNA seq notebook to Getting Started Tier 2 - Clinical and Omics Access : Py3 - Single Nucleus RNAseq - load H5AD from cloud storage
- Addition of the HBS cohort to the Transcriptomics section of AMP PD, consisting of 2148 whole blood bulk RNA seq samples


### Browse AMP PD Data

There are 3 different data browsers available, depending on your level of approved access.
| Access   | Browser |
| ----------- | ----------- |
| Public | [Summary Data Dashboard](https://amp-pd.org/data/summary-data-dashboard)|
| Tier 1 | [Clinical Data Explorer](https://clinical-data-explorer.amp-pd.org/?filter=&extraFacets=)|
| Tier 2 | [Clinical and Omics Data Explorer](https://data-explorer.amp-pd.org/?filter=&extraFacets=)|

### Release History
- AMP PD Release 1 (October 2019):  [Workspaces](https://app.terra.bio/#workspaces?projectsFilter=amp-pd-release-v1), [Release Notes](https://amp-pd.org/news/release-notes-october-2019)
- AMP PD Release 2 (December 2020):  [Workspaces](https://app.terra.bio/#workspaces?projectsFilter=amp-pd-release-v2), [Release Notes](https://amp-pd.org/news/release-notes-december-2020)
- AMP PD Release 2.5 (May 2021):  [Workspaces](https://app.terra.bio/#workspaces?projectsFilter=amp-pd-release-v2-5), [Release Notes](https://amp-pd.org/release-notes-may-2021) 
- GP2 Release 1 (December 2021):  [Release Notes](https://amp-pd.org/release-notes-december-2021)
- GP2 Release 2 (May 2022):  [Release Notes](https://amp-pd.org/news/gp2-release-notes-may-2022)
- AMP PD Release 2.5 Update 1 (May 2022):  [Workspaces](https://app.terra.bio/#workspaces?projectsFilter=amp-pd-release-v2-5), [Release Notes](https://amp-pd.org/news/release-notes-may-2022)
- GP2 Release 3 (October 2022):  [Release Notes](https://amp-pd.org/news/gp2-release-notes-october-2022)
- AMP PD Release 3 (December 2022):  [Workspaces](https://app.terra.bio/#workspaces?projectsFilter=amp-pd-release-v3), [Release Notes](https://amp-pd.org/news/amp-v3-release-notes-december-2022)
- GP2 Release 4 (February 2023):  [Release Notes](https://amp-pd.org/news/gp2-release-notes-february-2023)
- GP2 Release 5 (May 2023):  [Release Notes](https://gp2.org/the-components-of-gp2s-fifth-data-release/)
- AMP PD Target Explorer (July 2023):  [Release Notes](https://amp-pd.org/news/amp-pd-public-target-explorer-has-launched)
- AMP PD Release 4 (October 2023): [Workspaces](https://app.terra.bio/#workspaces?projectsFilter=amp-pd-release-v4)
- AMP PD Release 4 Update 1 (September 2024): [Workspaces](https://app.terra.bio/#workspaces?projectsFilter=amp-pd-release-v4)

## Registering for Access
AMP PD is a controlled access dataset.

There are two tiers of access to AMP PD data:
- Tier 1 provides access to clinical data.
- Tier 2 provides access to omics data.

To request access, you'll need to register on the [AMP PD website](https://amp-pd.org/register-for-amp-pd):
- Tier 1 access requires submission of the AMP PD Data Use Agreement with an individual's signature.
- Tier 2 access requires submission of the AMP PD Data Use Agreement with an individual and institutional signature.

Complete instructions for registering can be found on the [Registration](https://amp-pd.org/register-for-amp-pd) page and getting set up on the [How To](https://amp-pd.org/how-to) page.

![Diagram illustrating AMP PD access request process](https://storage.googleapis.com/amp-pd-public-content/featured-workspaces/amp-pd-in-terra/AMP-PD-easy-form-workflow.png)

Please be sure to read the [Safe Computing Guidelines](https://amp-pd.org/safe-computing-guidelines) for specific guidance on working with AMP PD data.

## AMP PD Resources in Terra
When you are granted access to AMP PD, you will immediately gain access to a set
of workspaces designed to accelerate your understanding and exploration of the
data. You'll also gain access to workflows and notebooks to help you get
started with your analysis.

### Tier 1 - Clinical Access
| Workspace   | Description |
| ----------- | ----------- |
| [Tier 1 Data Explorer](https://amp-pd-clinical-data-explorers.appspot.com/?filter=&extraFacets=)| Explore Tier 1 data sets, filter and save your desired cohort to Terra for further investigation. |
| [Getting Started Tier 1 - Clinical Access](https://app.terra.bio/#workspaces/amp-pd-release-v4/Getting%20Started%20Tier%201%20-%20Clinical%20Access)| Get started with AMP PD clinical data. Load clinical data into a Python 3 or R Jupyter notebook. Save a cohort using Data Explorer.|

### Tier 2 - Clinical and Omics Access
| Workspace   | Description |
| ----------- | ----------- |
| [Tier 2 Data Explorer](https://amp-pd-data-explorers.appspot.com/?filter=&extraFacets=)| Explore Tier 2 data sets, filter and save your desired cohort to Terra for further investigation. |
| [Getting Started Tier 2 - Clinical and Omics Access](https://app.terra.bio/#workspaces/amp-pd-release-v4/Getting%20Started%20Tier%202%20-%20Clinical%20and%20Omics%20Access)| Get started with AMP PD clinical, RNASeq, and WGS data. Tour the layout of files in Cloud Storage and tables in BigQuery. Load data into a Python 3 or R notebook for analysis and visualization.|
| [Proteomics Getting Started](https://app.terra.bio/#workspaces/amp-pd-release-v4/Getting%20Started%20Tier%202%20-%20Proteomics) | Get started using Terra notebooks to analyze AMP PD proteomics data. |
| [Proteomics QC and Analysis](https://app.terra.bio/#workspaces/amp-pd-release-v4/AMP%20PD%20-%20Proteomics%20QC%20and%20Analysis) | Access QC and analysis notebooks for AMP PD.
| [RNASeq Release Workflows](https://app.terra.bio/#workspaces/amp-pd-release-v4/AMP%20PD%20-%20RNASeq%20Release%20Workflows)| Learn how to run workflows on AMP PD RNASeq data in Terra.|
| [RNASeq QC and PCA ](https://app.terra.bio/#workspaces/amp-pd-release-v4/AMP%20PD%20-%20RNASeq%20QC%20and%20PCA)   | Understand AMP PD RNASeq data, data quality, and analysis tools available. |
| [Polygenic Risk Scores](https://app.terra.bio/#workspaces/amp-pd-release-v2-5/AMP%20PD%20-%20%20Polygenic%20Risk%20Scores%20)   | Calculate Polygenic Risk Scores (PRS), a way to determine risk of developing disease, based on the number of genetic changes   related to the disease. |


## Partner-Provided Workspaces
| Workspace   | Description |
| ----------- | ----------- |
| [Autoantibodies](https://app.terra.bio/#workspaces/amp-pd-partner-workspaces/AMP%20PD%20Autoantibodies)  | Get started using Terra notebooks to get familiar with Sengenics Autoantibodies data and methods provided by AMP PD Partner, AbbVie. |



## Community-Provided Workspaces
[AMP-PD Community Workspaces Dashboard](https://app.terra.bio/#workspaces?projectsFilter=amp-pd-community-workspaces)
| Workspace   | Description |
| ----------- | ----------- |
| [AMP-PD RNA-Seq Explorer](https://app.terra.bio/#workspaces/amp-pd-community-workspaces/AMP-PD%20RNA-Seq%20Explorer)| Visualization tool for individual and summary level data within the Terra-based AMP-PD Knowledge Platform developed by the Craig Laboratory at USC.|
| [Simulated Proteomics Data Set Workspace](https://app.terra.bio/#workspaces/amp-pd-community-workspaces/AMP%20PD%20-%20Simulated%20Proteomics%20Data%20Set)| Work with a simulated proteomics data set. No AMP PD access required! Note, a billing project is necessary.
| [Accelerating Parkinson’s Diagnosis using Multi-omics and Artificial Intelligence](https://app.terra.bio/#workspaces/amp-pd-community-workspaces/AMP%20PD%20-%20Accelerating%20PD%20using%20Multi-omics%20and%20AI)| This workspace created by Dr. Ruifeng Hu for a NINDS funded U01 grant. Within the workspace, there are scripts for data pre-processing, differentially expressed genes analysis and Parkinson's Disease diagnosis classifier models will be introduced. 
| [AMP PD Tandem Repeat Expansion Pipeline Workflows](https://app.terra.bio/#workspaces/sharplab-bj/AMP%20PD%20-%20Tandem%20Repeat%20Expansion%20Pipeline)| Workspace, developed by Bharati Jadhav at the Icahn School of Medicine at Mount Sinai, describes the pipeline to identify short tandem repeat expansions using the [STRetch](https://github.com/Oshlack/STRetch) and [GangSTR](https://github.com/gymreklab/GangSTR) workflows on the Terra platform. These workflows are designed for Whole Genome Sequencing data starting from mapped cram files.
| [Target Explorer Differential Protein Expression](https://app.terra.bio/#workspaces/amp-pd-community-workspaces/AMP%20PD%20-%20Target%20Explorer%20Proteomics%20Differential%20Expression)| See how differential protein lists were generated for AMP PD's newest tool: [Target Explorer](https://target-explorer.amp-pd.org/).
| [Target Explorer Transcriptomics Differential Expression](https://app.terra.bio/#workspaces/amp-pd-community-workspaces/AMP%20PD%20-%20Target%20Explorer%20Transcriptomics%20Differential%20Expression)| See how differential gene expression lists were generated for AMP PD's newest tool:  [Target Explorer](https://target-explorer.amp-pd.org/).

## Getting Help
If you have questions about AMP PD data access or governance, contact the Access Control Team at ACT@amp-pd.org

If you have questions about AMP PD workspaces in Terra or AMP PD data, contact admin@amp-pd.org.

If you have questions about Terra, contact [Terra Support](https://terra.bio/resources/help/#)

## AMP PD Consortium
### Government
- [Food and Drug Administration (FDA)](http://www.fda.gov/)
- [National Institute of Neurological Disorders and Stroke (NINDS)](https://www.ninds.nih.gov/)
- [National Institute on Aging (NIA)](https://www.nia.nih.gov/)
### Industry
- [Bristol-Myers Squibb](https://www.bms.com/)
- [GSK](https://www.gsk.com/)
- [Pfizer](https://www.pfizer.com/)
- [Sanofi](https://www.sanofi.us/)
- [Verily](https://verily.com/)
### Nonprofit
- [Aligning Science Across Parkinson's (ASAP) Initiative](https://parkinsonsroadmap.org/#)
- [The Michael J. Fox Foundation (MJFF)](https://www.michaeljfox.org/)
### Data
- [MJFF and NINDS BioFIND Study](https://amp-pd.org/unified-cohorts/biofind)
- [NIA International Lewy Body Dementia Genetics Consortium Genome Sequencing in Lewy body dementia case-control cohort (LBD)](https://amp-pd.org/unified-cohorts/lbd)
- [MJFF LRRK2 Cohort Consortium (LCC)](https://amp-pd.org/unified-cohorts/llc)
- [Brigham and Women's Hospital/MGH Harvard Biomarkers Study](https://amp-pd.org/unified-cohorts/hbs)
- [NINDS Parkinson's Disease Biomarkers Program](https://amp-pd.org/unified-cohorts/pdbp)
- [MJFF Parkinson’s Progression Markers Initiative](https://amp-pd.org/unified-cohorts/ppmi)
- [NINDS Study of Isradipine as a Disease Modifying Agent in Subjects With Early Parkinson Disease, Phase 3 (STEADY-PD3).](https://amp-pd.org/unified-cohorts/steady-pd-3)
- [The MJFF and NINDS Study of Urate Elevation in Parkinson’s Disease, Phase 3 (SURE-PD3)](https://amp-pd.org/unified-cohorts/sure-pd3)
- [Global Parkinson's Genetics Program (GP2)](https://amp-pd.org/federated-cohorts/gp2)
### Managing
- [Foundation for the NIH (FNIH)](https://fnih.org/)





|Last updated: October 9, 2024|",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","amp-pd-public/AMP-PD-In-Terra"
298,"verily-terra-solutions","Programmatic Notebook Execution Tutorial","READER","https://app.terra.bio/#workspaces/verily-terra-solutions/Programmatic%20Notebook%20Execution%20Tutorial",TRUE,TRUE,NA,NA,NA,"## Programmatic execution of notebooks

Generally, Jupyter notebooks are run interactively in a Terra cloud environment, but it's sometimes useful to run them programmatically.  

### Why you might want to run a notebook programmatically

* To run with a known, clean virtual machine configuration to confirm it has no unresolved dependencies on locally installed Python packages, R packages, or on local files.
* To run a notebook with many different sets of parameters, all in parallel.
* To execute a long-running notebook (e.g., taking hours or even days) on a machine separate from where you are working interactively.
* To run a notebook programmatically without porting it to a workflow.

### Two ways to run notebooks programmatically

* Via a Python script launched via the Terra workspace terminal (`dsub_notebook.py`). See [Using the terminal and interactive analysis shell in Terra](https://support.terra.bio/hc/en-us/articles/360041809272-Using-the-terminal-and-interactive-analysis-shell-in-Terra) for step-by-step instructions for launching a Terra worksapce terminal.

* Via a WDL workflow that launches Jupyter notebooks via the Terra workflows tab (`notebook_workflow.wdl`).  See [Getting started running workflows](https://support.terra.bio/hc/en-us/articles/360036379771-Get-started-running-workflows) for details on how to use workflows on Terra.

### Demonstration notebook (proxy for your own)

This workspace includes an example notebook, `2-explore_sample_qc_results.ipynb` that you can run using either option (Python script or WDL). The notebook produces quality control (QC) reports from detailed QC analyses, which were previously run on three different datasets.

### A note on time and cost estimates

Option 1 (`dsub_notebook.py`) and Option 2 (`notebook_workflow.wdl`) both launch new virtual machine instances to execute notebooks. Generally, it takes a few minutes to provision and configure new machine instances.

**IMPORTANT**: `dsub_notebook.py` and `notebook_workflow.wdl` are tools that enable you to run *your own* notebooks. The cost estimates provided here apply only to the setup and demo notebooks we included in this workspace. The cost of using these tools to run your own notebooks will vary depending on the configuration and number of VM instances you use to run your notebooks.

### Configuring analysis parameters in your notebook 

Both of these options use [Papermill](https://papermill.readthedocs.io/) to run notebooks on clean, newly allocated virtual machines.  You can use  [Papermill parameters](https://papermill.readthedocs.io/en/latest/usage-parameterize.html) to configure your notebooks.  For example, open `2-explore_sample_qc_results.ipynb` and look at the parameters available to you in the Papermill parameters cell:

* Use the `DATASET` parameter to select which dataset you'd like to examine (datasets are described below).
* Use the `CSV_OUTPUT_FILE_NAME` parameter to name your CSV output file. If you leave it unset, it will default to `<DATASET>_problem_summary.csv`.

**Don't edit these parameters directly in the notebook!** 
We'll demonstrate how to use `dsub_notebook.py` and `notebook_workflow.wdl` to set these parameters and customize individual notebook runs.

## Before you begin 

If you're new to Terra, we recommend reading this [introduction to workspaces](https://support.terra.bio/hc/en-us/articles/360046095192-Intro-to-workspaces) before following these instructions.

Clone this workspace by following the directions in the [How to clone your own workspace](https://www.google.com/url?q=https://support.terra.bio/hc/en-us/articles/360026130851-How-to-clone-your-own-workspace&sa=D&source=docs&ust=1665875782230262&usg=AOvVaw0mL4ZWxU-6EWHvsbRo9z54) support article.

## Option 1: Run notebooks with the command `dsub_notebook.py`

You can use `dsub_notebook.py` to execute Jupyter notebooks directly from the Cloud environment terminal.

As noted above, the demo notebook (`2-explore_sample_qc_results.ipynb`) produces QC reports from detailed QC analyses which were previously run on three different datasets.

### Install required dependencies

**Step 1:** In your cloned workspace, navigate to the **Analyses tab** and launch a new Jupyter Cloud Environment.  When creating the Jupyter Cloud Environment, you can use the Default Terra environment [v2.2.8](https://us.gcr.io/broad-dsp-gcr-public/terra-jupyter-gatk:2.2.8).

**Step 2:** Once the Cloud Environment is running, open `1-setup.ipynb` in **playground mode**.

**Step 3:** Select `Cell > Run All` from the **Jupyter menu bar** to install all required dependencies.

### Example and code
In this example, we'll generate a QC analysis report for 1000 Genomes Phase 3 by running this command in the Cloud Environment Terminal:

```
python3 ${HOME}/dsub_notebook.py \
--notebook_to_run=${WORKSPACE_BUCKET}/notebooks/2-explore_sample_qc_results.ipynb \
--parameters ""DATASET thousand_genomes"" \
--nodry_run
```

**Adjusting parameters**    

You can select a dataset by choosing one of these three values for the parameter `DATASET`:
  * `simons` - [The Simons Genome Diversity Project](https://www.simonsfoundation.org/simons-genome-diversity-project/) (As described in the linked document, all data is freely available.)
  * `thousand_genomes` - [1000 Genomes Phase 3](https://www.internationalgenome.org/category/phase-3/) ([Data access policy](https://www.internationalgenome.org/IGSR_disclaimer))
  * `platinum_genomes` DeepVariant Platinum Genomes (Open access)

**Note**: these datasets are all open access

 

**What to expect**

In the command output, you'll see a `dstat` command you can run to see the status of your job.

Once the `dsub` job is  finished, you will find a fully rendered, time stamped copy of the notebook in the `Analyses` tab of your workspace. This fully rendered notebook and any files created by the notebook as part of the analysis will also be written to the output path on Cloud Storage, which defaults to 

`${WORKSPACE_BUCKET}/dsub/results/<NOTEBOOK>/<USER>/<DATE>/<TIME>`.

You can use the `output_notebook` flag to specify a different path for your rendered notebook, and the `output_path` flag to specify a different path for the output files generated by your notebook.

To see a list of all available flags, run:

```
python3 ${HOME}/dsub_notebook.py –help
```

## Option 2: Run notebooks with WDL (`notebook_worfklow.wdl`)

Alternatively, you can execute Jupyter notebooks via a workflow using `notebook_workflow.wdl`.

**Step 1:** Navigate to the Workflows tab and open `notebook_workflow`.

**Step 2:** Configure the following inputs:

* Set `notebookWorkspacePath` using the GCS URL for the notebook. You can find this by first clicking the folder icon on the data tab to browse bucket files and then selecting `notebooks > 2-explore_sample_qc_results.ipynb` to view the GCS URL.
* Set `papermillParameters` using multiple instances of the `-p` flag to specify parameters in a single, space-separated string.  Choose one of the three values for the parameter `DATASET` described in the previous section, for example:

```
-p DATASET simons
```

**Step 3:** Run the WDL by clicking **Run Analysis**. 

**What to expect**

You can follow the progress of your job as you would any other workflow, using the [Job History tab](https://app.terra.bio/#workspaces/verily-terra-solutions/Programmatic%20Notebook%20Execution%20Tutorial/job_history).  To find its outputs, including the fully rendered, timestamped notebook and an HTML preview of the notebook:

1. Click the the job name in the Submission column.
2. Click the Job Manager link on that job's history page.
3. Click the Outputs link on the Job Manager page.

## Additional information about notebooks, environment requirements and cost estimates

For helpful hints on controlling Cloud costs, see this article ([click here to view](https://support.terra.bio/hc/en-us/articles/360029748111)).       

### Notebook 1-setup.ipynb

**What does it do?**

Installs `dsub_notebook.py` and its dependencies, which are required for Option 1.

#### Jupyter Cloud Environment recommendations

**Application configuration**   
Default Terra environment [v2.2.8](https://us.gcr.io/broad-dsp-gcr-public/terra-jupyter-gatk:2.2.8) (GATK 4.2.4.0 Python3.7.12 R 4.2.1)

**Cloud compute profile**

| Parameter | Value|   
| --- | --- |
| CPU Minimum | 1|
| Memory Minimum | 3.75 GB |

**Persistent disk**   

| Parameter | Value|   
| --- | --- |
| Disk size Minimum | 50 GB |

**Time and cost to run**

This notebook runs in a few seconds on the Standard VM configuration, and costs less than US$0.01 to run.

### Notebook 2-explore_sample_qc_results.ipynb

Note that this is a demo notebook only. If you were running your own notebook programmatically, you would replace the information below with your own notebook analysis. 

**What does it do?**

Demonstrates how to execute a notebook using `notebook.wdl` or `dsub_notebook.py` and configure its behavior using Papermill parameters.

This notebook produces QC reports from detailed QC analyses which were previously run on three different datasets:

* [The Simons Genome Diversity Project](https://www.simonsfoundation.org/simons-genome-diversity-project/)
* [1000 Genomes Phase 3](https://www.internationalgenome.org/category/phase-3/)
* DeepVariant Platinum Genomes

**Jupyter Cloud Environment recommendations**

Default Terra environment [v2.2.8](https://us.gcr.io/broad-dsp-gcr-public/terra-jupyter-gatk:2.2.8)

**Compute Power**

Standard VM

| Runtime  | Value |
| --- | --- |
| Environments | Default (GATK 4.2.4.0 Python3.7.12 R 4.2.1) |
| CPU Minimum | 1|
| Disk size Minimum | 50 GB |
| Memory Minimum | 3.75 GB |

**Time and cost to run**
This demo notebook runs in a few minutes on the Standard VM configuration, and costs less than US$0.01 to run.
 
## Additional information about workflows

### notebook_workflow.wdl

**What does it do?**
`notebook_workflow.wdl` executes a Jupyter notebook using Papermill on a new virtual machine instance.

**Inputs and outputs**
Workflow inputs and outputs depend on the notebook you're running. No additional inputs beyond those specified on the workflows page are required for the demonstration notebook `2-explore_sample_qc_results.ipynb`.  This notebook outputs a CSV file detailing the results of the QC analysis.

**Time and cost to complete**
The workflow should complete in under 30 minutes and cost less than US$0.01 to run.

### Contact information

For help with Terra accounts setup, for help navigating the Terra environment, or to submit Terra feedback, please use the main navigation menu at the top left of this page. Click ""Support,"" then ""Terra Support Home"" for Terra documentation.  Alternatively, click ""Contact Us"" to ask a question directly to the Terra support team.

For help with `dsub_notebook.py`, `notebook_workflow.wdl`, or issues with the example notebooks, please file an issue on our [GitHub repository issues page](https://github.com/DataBiosphere/terra-examples/issues). 

### Software license information

Copyright 2022 Verily Life Sciences LLC
Use of this source code is governed by an open-source BSD-style [license](https://developers.google.com/open-source/licenses/bsd).


",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","verily-terra-solutions/Programmatic Notebook Execution Tutorial"
300,"broad-firecloud-cptac","PANOPLY_CPTAC_BRCA","READER","https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_CPTAC_BRCA",TRUE,FALSE,NA,NA,NA,"![](https://raw.githubusercontent.com/broadinstitute/PANOPLY/release-1_0/tutorial/case-studies/images/CPTAC_BRCA_Cover.png)

**NOTE**: Results derived from PANOPLY analysis may be different from those reported in (Krug et al., 2020, *Cell* 183, 1-21) due to variations in input data and analysis methods.",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","broad-firecloud-cptac/PANOPLY_CPTAC_BRCA"
301,"help-terra","BICAN-Omics-Workshop-July-2024","READER","https://app.terra.bio/#workspaces/help-terra/BICAN-Omics-Workshop-July-2024",TRUE,TRUE,NA,NA,NA,"# BRAIN Initiative Cell Atlas Network (BICAN) Omics Workshop July 2024

This tutorial workspace is a step-by-step guide to analyzing BICAN 10x Multiome single-cell data. Using this workspace, you will:

1. Import an example 10x dataset (FASTQs) from **NeMO**.
2. Align 10x Multiome FASTQs and produce a raw gene expression count matrix, ATAC fragment file, and quality metrics using the **Multiome workflow**.
3. Remove gene expression background with **CellBender's `remove-background-v0.3.0` workflow**.
4. Explore single-cell RNA and ATAC data in one of three Jupyter Notebooks:
	*  **10x_analysis_in_Scanpy** 
	* **Exploring_CellBender_Matrices**
	* **ATAC_analysis_with_SnapATAC2** 


![](https://storage.googleapis.com/terra-featured-workspaces/BICAN_July_2024/BIACN_July_2024_Flow.png)


# Instructions

## Set up the tutorial workspace 

Before you begin, [create your own editable copy (clone)](https://support.terra.bio/hc/en-us/articles/360026130851) of this WORKSPACE. 

Click the round circle with three dots in the upper right corner of this page and choose ""Clone"".


![](https://storage.googleapis.com/terra-featured-workspaces/BICCN_Workshop_Workspace/clone.png)


## Step 1. Import Data from the NeMO Data Portal
### NeMO Overview
The [Neuroscience Muti-omic Data Archive (NeMO)](https://nemoarchive.org/) is a data repository for the BRAIN Initiative and related brain research initiatives. You can access NeMO data using the NeMO [Data Portal](https://portal.nemoarchive.org/).  

### Quick-start instructions
1. Navigate to the NeMO [Data Portal](https://portal.nemoarchive.org/).
2. Use the faceted search to identify a 10x data set of interest.
3. Add the selected data to the NeMO cart.
4. From the cart, select `Download` and then `Export to Terra`.
5. When prompted, choose the existing cloned version of this workspace.

##  Step 2. Process 10x RNA and ATAC data with the Multiome workflow

### Multiome Overview   

Multiome is an open-source, cloud-optimized pipeline developed in collaboration with members of the BRAIN Initiative (BICCN and BICAN Sequencing Working Group) and SCORCH. It supports the processing of 10x 3' single-nucleus gene expression (GEX) and chromatin accessibility (ATAC) data generated with the 10x Genomics Multiome assay. 

For more details about the pipeline, see the [Multiome Overview](https://broadinstitute.github.io/warp/docs/Pipelines/Multiome_Pipeline/README) in the [WARP documentation](https://broadinstitute.github.io/warp/).  

### Sample data
This step uses downsampled 10x sequencing data generated from mouse female isocortex.  The raw FASTQ files are listed in the  `multiome_sample` data table on the workspace Data tab. 

Although these example FASTQ files contain only one lane of sequencing, the workflow can take multiple lanes as a set. An example set can be found in the `multiome_sample_set` table with the set name `multiome_set1`. This set has already been processed with the Multiome workflow. In this exercise, you will use the `multiome_set2` example to learn how the data table changes when you run the workflow. 

The example Multiome workflow is set up to use the `multiome_sample_set` table as the workflow's root entity, but parts of the set-up are missing to give you hands-on practice. 

Follow the instructions below to setup and run the workflow.

### Quick-start instructions
1. Select the `multiome_sample_set` data table on the `Data` tab.
1. Click the checkbox next `multiome_set2` data set.
1. Choose “Open with” above the data table.
1. Select “Workflow.”
1. Select with the `1-Multiome-mouse-isocortex` workflow.
1. Under workflow `Inputs`, set the annotations_gtf variable to `workspace.mouse_annotations`.
1. Under workflow `Inputs`, set the gex_r1_fastq variable to `this.multiome_samples.gex_r1_fastq`.
1. Go to the workflow Multiome and set the `h5ad_output_file_gex` to `this.my_gex_h5ad`.
1. Save the configuration.
1. Select `Run Analysis` and then `Launch`.

### Outputs
The Multiome pipeline writes several outputs to the `multiome_sample_set` data table, including a h5ad matrix with raw counts and quality metrics which you can find in the ""my_gex_h5ad"" column you created in Step 5.

## Step 3. Run CellBender's `remove-background-v0.3.0` workflow

### CellBender Overview

The CellBender `remove-background-v0.3.0` workflow models and removes systematic biases and background noise from raw cell-by-gene count matrices and produces improved gene expression estimates.

Overall, the module:
* Removes counts due to ambient RNA molecules and random barcode swapping from (raw) UMI-based scRNA-seq cell-by-gene count matrices. 

* Supports both the count matrices produced by the CellRanger count pipeline and h5ad matrices produced with the [Multiome workflow](https://broadinstitute.github.io/warp/docs/Pipelines/Multiome_Pipeline/README).

For more details about the pipeline, see the [CellBender documentation](https://cellbender.readthedocs.io/en/latest/).

**Note** The remove-background workflow is built into the Multiome workflow and is set up to run by default in this workspace using the Multiome optional `run_cellbender` boolean. However, you can also try CellBender as a standalone workflow, as it is compatible with outputs from the Multiome workflow as well as CellRanger.

### Sample data
This step uses the raw h5ad count matrix generated with Multiome as CellBender input. By default, the workflow is set up to use the h5ad_output_file_gex column of the `multiome_sample_set` data table, which contains a ""pre-baked"" h5ad file so that you don't have to run  Multiome prior to trying the CellBender workflow.  This h5ad file should be identical to the one you created with Multiome in Step 2.

### Quick-start instructions
1. Go to the `multiome_sample_set` data table on the `Data` tab.
1. Select the `multiome_set1` file-set.
1. Select the `Open with` above the data table
1. Choose the `2-Cellbender-remove-background-v0.3.0` workflow.
1. Save the setup.
1. Select `Run Analysis` and then `Launch`. 


### Outputs
The `remove-background` workflow writes outputs to the `multiome_sample_set` data table including an array of H5 cell-by-gene matrix files with corrected counts (one array per false positive rate - FPR).
   *  The first file path in the array is the path to the unfiltered CellBender matrix (suffix is ""out.h5"").
   *  The second file path in the array is the path to the filtered CellBender matrix (suffix is ""out_filtered.h5"").

Additional outputs are detailed in the [CellBender documentation](https://cellbender.readthedocs.io/en/latest/). 

## Step 4. Explore 10x single-cell data with Jupyter Notebooks
This workspace contains three example Jupyter Notebooks to explore the cell-by-gene outputs.

The first notebook, `10x_analysis_in_Scanpy`, is a guided tutorial for filtering, normalizing, and clustering 10x single-cell RNA-seq data that is based on Scanpy’s [Preprocessing and clustering 3k PBMCs
](https://scanpy.readthedocs.io/en/stable/tutorials/basics/clustering-2017.html). 

The second notebook, `Exploring_CellBender_Matrices`, is the companion notebook for the [CellBender featured workspace](https://app.terra.bio/#workspaces/help-terra/CellBender). It contains useful tools and visualizations to compare the raw Multiome gene expression cell-by-gene matrix to a matrix filtered by the CellBender `remove-background` workflow.

The third notebook, `ATAC_analysis_with_SnapATAC2` is a tutorial notebook for performing cell filtering, clustering, cell marker identification, and peak calling with SnapATAC2.

### Sample data

This step uses data linked in the workspace `multiome_sample_set` data table: the unfiltered CellBender output H5 file (first file from the array in the `h5_array` column), the Multiome gex h5ad output file (from the gex_h5ad_output_file column), and the Multiome atac h5ad output (atac_h5ad column). 

### Quick-start instructions
1. Select the Cloud Environment side bar option and choose the Custom option.
2. Select `Custom Enviroment` from the Application Configuration drop-down.
3. Paste the image: `us.gcr.io/broad-dsp-gcr-public/terra-jupyter-python:bican` into the Container image field.
4. In the `Cloud compute profile`, select **8 CPUs** from the CPU drop-down.
5. Go to the workspace `Analyses`  and open  the `10x_analysis_in_Scanpy` notebook, the `Exploring_CellBender_Matrices` notebook, or the `ATAC_analysis_with_SnapATAC2` notebook in edit mode.
6. Follow the instructions listed in the notebook.

## References
Fleming SJ, Chaffin MD, Arduini A, Akkad AD, Banks E, Marioni JC, Philippakis AA, Ellinor PT, Babadi M. Unsupervised removal of systematic background noise from droplet-based single-cell experiments using CellBender. Nat Methods. 2023 Sep;20(9):1323-1335. doi: 10.1038/s41592-023-01943-7. Epub 2023 Aug 7. PMID: 37550580.

Stuart T, Butler A, Hoffman P, Hafemeister C, Papalexi E, Mauck WM 3rd, Hao Y, Stoeckius M, Smibert P, Satija R. Comprehensive Integration of Single-Cell Data. Cell. 2019 Jun 13;177(7):1888-1902.e21. doi: 10.1016/j.cell.2019.05.031. Epub 2019 Jun 6. PMID: 31178118; PMCID: PMC6687398.

Wolf et al. (2018), Scanpy: large-scale single-cell gene expression data analysis, [Genome Biology](https://doi.org/10.1186/s13059-017-1382-0).

Zhang, K., Zemke, N. R., Armand, E. J. & Ren, B. (2024). A fast, scalable and versatile tool for analysis of single-cell omics data. Nature Methods, 1–11. https://doi.org/10.1038/s41592-023-02139-9

## Additional resources

* [CellBender workspace](https://app.terra.bio/#workspaces/help-terra/CellBender)
* [Multiome workspace](https://app.terra.bio/#workspaces/warp-pipelines/Multiome)
* [SnapATAC2 documentation](https://kzhang.org/SnapATAC2/index.html) 


## Acknowledgements
Special thanks to Brian Herb and the NeMO team, the CellBender team (Stephen Flemming and Mehrtash Babadi), the SnapATAC2 team (Kai Zhang) and the Broad Pipeline Development team for their feedback and amazing contributions to this workspace.

### License
**Copyright Broad Institute, 2024 | BSD-3**

All rights reserved. Full license text available [here](https://github.com/broadinstitute/warp/blob/master/LICENSE). Note that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.

---


",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-terra/BICAN-Omics-Workshop-July-2024"
302,"broad-firecloud-tcga","TARGET_RT_hg38_OpenAccess_GDCDR-12-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TARGET_RT_hg38_OpenAccess_GDCDR-12-0_DATA",TRUE,TRUE,"Rhabdoid Tumor","Tumor/Normal",NA,"[TARGET Kidney Tumors Project](https://ocg.cancer.gov/programs/target/projects/kidney-tumors): Rhabdoid Tumor

*hg38 TCGA and TARGET workspaces reference files by their GDC UUIDs.  In order to run analyses on the referenced data files, you will need to run workflows that retrieve the files from the GDC and copy them to your workspace bucket.  See   [this forum post](https://gatkforums.broadinstitute.org/firecloud/discussion/10382/populating-hg38-tcga-and-target-workspaces-with-data-files#latest) for instructions on the running of these workflows.*","NCI","Chet Birger","Rhabdoid Tumor","40",NA,"TARGET",NA,"Open","GRCh38/hg38","TARGET Kidney Tumors Project: Rhabdoid Tumor","Whole Exome","broad-firecloud-tcga/TARGET_RT_hg38_OpenAccess_GDCDR-12-0_DATA"
303,"verily-terra-solutions","ml-on-terra","READER","https://app.terra.bio/#workspaces/verily-terra-solutions/ml-on-terra",TRUE,FALSE,NA,NA,NA,"Examples of using Terra for machine learning.     
The code for these notebooks is [here](https://github.com/DataBiosphere/terra-examples/tree/main/ml_notebooks/image_classification).  Please file an issue in that repo if you have a problem or comment.


# Image Classification notebook examples

This workspace holds a series of image classification machine learning examples.

All of these examples **run best with GPU(s)** -- the training runs fine using only CPUs, but training takes a long time.
The different notebooks have different computational requirements.  See the information at the top of each notebook for details.

- For the **01_keras_pcam** notebook, you'll need at least 2 cores (ideally 4) and a GPU.
- For the other notebooks, the GPU-based config is set up at the [Vertex AI](https://cloud.google.com/vertex-ai) end, and so the notebook environment can use 1 core and does not require GPUs.

## About the dataset and the machine learning task


### PatchCamelyon

The [PatchCamelyon benchmark](https://www.tensorflow.org/datasets/catalog/patch_camelyon) consists of 327,680 color images (96 x 96px) extracted from histopathologic scans of lymph node sections. Each image is annotated with a binary label indicating presence of metastatic tissue.


The examples use Keras, specifically one of Keras' prebuilt model architectures, [Xception](https://keras.io/api/applications/xception/). The training does [_transfer learning_](https://en.wikipedia.org/wiki/Transfer_learning) , bootstrapping from model weights trained on the ['imagenet'](https://en.wikipedia.org/wiki/ImageNet) dataset.


## About the example notebooks

### 'Setup' notebook

You'll need to run the **00_pcam_setup** notebook first, for all notebooks except the **01_keras_pcam** notebook.

### In-notebook training example

The  **01_keras** notebook shows in-notebook model training (rather than calling out to a cloud service to do the training).

It works on all notebook platforms without requiring access to [Vertex AI]((https://cloud.google.com/vertex-ai), or any other Google Cloud Platform (GCP) services aside from Cloud Storage (GCS).  It shows how to build a Keras image classification model, and how to do _transfer learning_.

Given the size of the dataset and model architecture, this example requires a 2-core notebook VM, and the notebook should use an attached GPU to run in a reasonable time frame.

You can use the default GATK image customized to use 2 CPUs and 1 GPU. The default NVIDIA Tesla T4 is fine.

### Using Vertex AI Training, and the Vertex Experiments API

The **02_1_vertex_ai** and **02_2_vertex_ai** notebooks show how to run different types of Vertex AI training jobs.
These notebooks require a ['native' GCP account](https://support.terra.bio/hc/en-us/articles/360051229072-Accessing-advanced-GCP-features-in-Terra).

The notebooks show:

- [Vertex AI Training](https://cloud.google.com/vertex-ai/docs/training/custom-training) via both a training _script_ and a _package_.
- How to set up and use a [Managed Tensorboard](https://cloud.google.com/vertex-ai/docs/experiments/tensorboard-overview) instance to track training progress.
- How to upload and [deploy](https://cloud.google.com/vertex-ai/docs/predictions/deploy-model-api) a trained model to a scalable _Endpoint_ that autoscales up as you hit it with traffic; how to do traffic splitting for A/B testing or canarying.
- How to configure and run a hyperparameter tuning job (and what the Keras-based training code should look like to support this).
- How to configure and run a multi-node distributed training job (and what the training code should do to support this option).
- How to use the **Experiments API** to create an _Experiment_, log info about the training runs (metrics, params, etc.) to the [Metadata Server](https://cloud.google.com/vertex-ai/docs/ml-metadata/introduction), and how to access and analyze the logged data.

### Using Vertex Pipelines

[Vertex Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines) lets you codify your machine learning workflow, supporting workflow reproducibility, composability, collaboration, and more.

This notebook requires a ['native' GCP account](https://support.terra.bio/hc/en-us/articles/360051229072-Accessing-advanced-GCP-features-in-Terra).

The **03_vertex_pipelines** notebook shows, in addition to the concepts introduced in the notebook above:

- how to use the [KFP SDK](https://www.kubeflow.org/docs/components/pipelines/sdk/install-sdk/) to build pipelines
- use of ‘first-party’ components that wrap calls to Vertex AI services
- building and using ‘lightweight’ Python-based custom components; how to generate `yaml` files for the component definitions that can be put under version control and shared.
- step caching (very useful for iterative development)
- pipeline control structures: conditionally deploy a model if its metrics are sufficiently good

### Using the Vizier hyperparameter optimization service

The **04_vizier_pcam_model_training** notebook demonstrates the
use of [Vertex AI Vizier](https://cloud.google.com/vertex-ai/docs/vizier/overview) for
[hyperparameter (HP) tuning](https://en.wikipedia.org/wiki/Hyperparameter_optimization) of an ML
model.
Vertex AI Vizier is a black-box optimization service. You will often see Vertex AI Vizier used to
optimize hyperparameters of ML models, but it can also perform other optimization tasks.

(For another Terra Vizier example, which does multi-objective optimization on a simple non-ML
problem, see [this
notebook](https://github.com/DataBiosphere/terra-example-notebooks/tree/main/ml-notebooks/vizier/vizier-multi-objective-optimization.ipynb).
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","verily-terra-solutions/ml-on-terra"
305,"broad-firecloud-cptac","PANOPLY_CPTAC_LUAD","READER","https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_CPTAC_LUAD",TRUE,FALSE,NA,NA,NA,"![](https://raw.githubusercontent.com/broadinstitute/PANOPLY/release-1_0/tutorial/case-studies/images/CPTAC_LUAD_Cover.png)

**NOTE**: Results derived from PANOPLY analysis may be different from those reported in (Gillette et al., 2020, *Cell* 182, 200-225) due to variations in input data and analysis methods.",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","broad-firecloud-cptac/PANOPLY_CPTAC_LUAD"
306,"ctat-firecloud","STAR-Fusion","READER","https://app.terra.bio/#workspaces/ctat-firecloud/STAR-Fusion",TRUE,FALSE,NA,NA,NA,"## STAR-Fusion

A fully reproducible example workflow for fusion transcript detection.

Complete documentation for STAR-Fusion is available on the [STAR-Fusion Wiki](https://github.com/STAR-Fusion/STAR-Fusion/wiki).

### Workflow Description

STAR-Fusion, a component of the [Trinity Cancer Transcriptome Analysis Toolkit (CTAT)](https://github.com/NCIP/Trinity_CTAT/wiki), uses the STAR aligner to identify candidate fusion transcripts supported by Illumina reads. STAR-Fusion further processes the output generated by the STAR aligner to map junction reads and spanning reads to a reference annotation set.

### Input

- RNA-seq Fastq files (include paths to files in the data sample table)
- use either the provided hg19 or hg38 pre-configured workflows.

### Output
- A tab-delimited file containing fusions, named 'star-fusion.fusion_predictions.tsv'
- An abridged version of the previous file that excludes the identification of the evidence fusion reads, named 'star-fusion.fusion_predictions.abridged.tsv'
- STAR aligned bam file.
- STAR log file.
- Chimeric alignments in 'Chimeric.out.junction'
- High confidence collapsed splice junctions in tab-delimited format named 'SJ.out.tab'
- Interactive html report from named finspector.fusion_inspector_web.html from [FusionInspector](https://github.com/FusionInspector/FusionInspector/wiki)

An example interactive fusion visualization is shown below:

![report](https://raw.githubusercontent.com/wiki/FusionInspector/FusionInspector/images/FusionInspector_igv_report.png)



### Sample data description and location

The workflow in this workspace is preconfgured with a small sample of fastq reads that support fusions from the K562 and BT474 cell lines. These cell lines contain validated fusions, such as  BCR--ABL1 in K562 and  VAPB--IKZF3 in BT474 . Additionally, the workflow is configured to use the GRCh38_gencode_v32 reference, and to run FusionInspector inspect, which provides a more in-depth view of the evidence supporting the predicted fusions. You can use this as a template when setting up the workflow to run on your own data.


### Time and cost estimates
Below is an example of the time and cost for running the workflow.

| Sample Name        | Cost    | Time |       
| ------------- |:-------------:| ----------:| 
| test      | $0.11 | 50 minutes


Note: Cost and time will vary with the use of preemptible instances.


### Contact Information
Questions can be directed to the Trinity CTAT email list: trinity_ctat_users@googlegroups.com

### License
Copyright Broad Institute, 2020 | BSD-3
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at https://raw.githubusercontent.com/STAR-Fusion/STAR-Fusion/master/LICENSE).


",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","ctat-firecloud/STAR-Fusion"
307,"help-terra","CellBender","READER","https://app.terra.bio/#workspaces/help-terra/CellBender",TRUE,FALSE,NA,NA,NA,"# CellBender `remove-background` Workflow 
### For the analysis of 10x or 3’ single-cell transcriptomic data


The CellBender `remove-background` workflow models and removes systematic biases and background noise from raw cell-by-gene count matrices, and produces improved gene expression estimates. 


![cellbender](https://storage.googleapis.com/terra-featured-workspaces/CellBender/CellBenderLogo.png)

**Note that cost and time estimates will vary** using [Preemptibles](https://cloud.google.com/preemptible-vms/).  

You can also use [Google's cost calculator](https://cloud.google.com/products/calculator/) to estimate the cost to run. 

**For helpful hints on controlling Cloud costs**, see [this article](https://support.terra.bio/hc/en-us/articles/360029748111).  

![]()

##  CellBender `remove-background` 

### What does it do?   

CellBender is a software package for eliminating technical artifacts from high-throughput single-cell RNA sequencing (scRNA-seq) data. 

The current version of CellBender contains the `remove-background` module: 

* This module removes counts due to ambient RNA molecules and random barcode swapping from (raw) UMI-based scRNA-seq cell-by-gene count matrices. The module supports both the count matrices produced by the CellRanger count pipeline and h5ad matrices produced with the  [WARP Multiome pipeline](https://broadinstitute.github.io/warp/docs/Pipelines/Multiome_Pipeline/README). 

For more details about the pipeline, see the [CellBender overview](https://cellbender.readthedocs.io/en/latest/introduction/index.html)

![]()

### How to run the workflow

This workspace is preloaded with two **downsampled example datasets and a workflow configuration** (see table below) designed to demo CellBender.

The workflow is configured to leverage the `sample_set` data table in the workspace Data tab. The two data sets are detailed in the table below:

| Data set name | Description |
| --- | --- |
| 10x_set1 | Contains an example unfiltered 10x matrix output from the CellRanger count pipeline. |
| h5ad_set1 | Contains an example unfiltered h5ad matrix output from the Multiome pipeline. |

To run the workflow: 

1. Navigate to the Workflows tab and choose the CellBender `remove-background` workflow. 
1. Make sure the root entity type in Step 1 is set to `sample_set` (selected by default) on the workflow setup page.
1. Choose the respective dataset of interest (Optimus or CellRanger - see table above).
1. For the optional `output_bucket_base_directory` variable, specify a Google bucket (starting with ""gs://...."") where you want output data to be stored. You can use the workspace Google bucket or an alternative bucket.)
1. Save the workflow setup.
1. Select `Run Analysis`. 
1. Select `Launch`.


> This workflow is in development and will be replaced in the near future. Check to workspace changelog for updates.

![]()

### What does CellBender require as input?

The CellBender workflow requires the following input:

| Input variable name |  Description  |
| ---- | ---- |
| input_file_unfiltered | Input 10x H5 file or h5ad file containing raw cell-by-gene matrix. |
| sample_name | String to describe the sample. |
| output_bucket_base_directory | Optional string describing the cloud path to a Google bucket where the data should be stored. This can be the workspace Google bucket or an alternative Google bucket. When this option is used, the data will output to the directory in a new folder named by the same string used for the `sample_name` input. |

Additional optional parameters are detailed in the [CellBender overview](https://cellbender.readthedocs.io/en/latest/usage/index.html#remove-background). 
![]()
### What does CellBender return as output?
CellBender returns outputs described in the table below. These outputs are exported to the optional Google bucket specified in the `output_bucket_base_directory ` input parameter and are additionally accessible from the workspace `sample_set` data table. 

| Output variable name  |  Description | File type |
| --- | --- | --- |
| cell_barcodes_csv | The list of barcodes with a posterior cell probability exceeding 0.5. | File |
| checkpoint_file | A checkpoint file that allows you to save intermediate files in case the workflow fails. | File |
| h5_array | Array of HDF5 files (one per false positive rate) containing a detailed output of the inference procedure, including the normalized abundance of ambient transcripts, contamination fraction of each droplet, a low-dimensional embedding of the background-corrected gene expression, and the background-corrected counts matrix. | Array [File] |
| html_report_array | Summary output HTML report(s) from CellBender that give details about background RNA removal and potentially warn the user of problems with a run. | Array [File] |
| log | CellBender log file. | File |
| metrics_csv_array | Output metrics file(s) from CellBender, potentially to be read by other downstream tools as part of a pipeline, or to be used to automate re-running CellBender. | Array [File] |
| raw_umi_histogram_pdf | Histogram of the input UMI counts per droplet along with CellBender's identification of the empty droplet plateau. | File |
| output_directory | The name of the output directory if output_directory is specified in the output_directory input parameter. | String | 
| summary_pdf | Three quick plots that allow a user to visually diagnose the output of a run at a glance. | File | 



![]()
## Estimated time and cost to run on sample data 

The following estimates are based on three sets of data, human and mouse, each containing different numbers of samples. All details of each set are listed to give insight into time and cost.
 
| Sample  Name | Size (MB) | Time | Cost ($) |
| :---:  | :---: | :---: | :---: |
| 10x_set1 | 31.0 MB | 0 hr 44 min | 0.11 |
| h5ad_set1 | 470.53 MB | 1 hr 46 min | 0.21 |

## Running the companion `Exploring_CellBender_Matrices` Jupyter Notebook
This workspace contains a companion notebook enabling the comparison of cell-by-gene matrices before and after running the CellBender workflow.

To run the notebook:
1. Select the Cloud Environment widget.
1. Under Jupyter option, select **Customize**.
1. Choose the **Default** application configuration.
1. Change the CPUs to **8**.
1. Navigate to the Notebooks tab.
1. Select the companion notebook.
1. Open the Notebook in edit mode.
1. Follow the instructions in the notebook.
 
**For helpful hints on controlling Cloud costs, see this article ([click here to view](https://support.terra.bio/hc/en-us/articles/360029748111)).**   

![]()

## Workspace updates

You can access previous versions of the `remove-background` workflow by cloning the workspace and choosing a version in the version dropdown. 



| CellBender workflow version name and link |  Date | Release note | 
| --- | --- | --- |
| [remove-background-v3-alpha/1](https://portal.firecloud.org/?return=terra#methods/cellbender/remove-background-v3-alpha/1) | 5/2/2022 | First release of workspace with the in-development workflow.  |



![]()
## Questions and feedback
Follow the CellBender team or file an issue on the [CellBender GitHub](https://github.com/broadinstitute/CellBender).



![]()

## License
**Copyright WARP,  2021 | BSD-3**

All rights reserved. Full license text at https://github.com/broadinstitute/warp/blob/master/LICENSE. Note that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.






",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-terra/CellBender"
308,"warp-pipelines","Illumina-Genotyping-Array","READER","https://app.terra.bio/#workspaces/warp-pipelines/Illumina-Genotyping-Array",TRUE,FALSE,NA,NA,NA,"# Illumina Genotyping Array Pipeline

The Illumina Genotyping Array Pipeline was developed by the Broad Pipelines team to process Illumina genotyping array data in the form of IDAT files. This workspace describes `v1.12.13` of the pipeline and provides a fully reproducible example of the workflow. For more details about the pipeline, see the [documentation](https://broadinstitute.github.io/warp/docs/Pipelines/Illumina_Genotyping_Arrays_Pipeline/IlluminaGenotypingArray.documentation) on the WARP documentation site.

Scroll down for details on the workflow, including input and output descriptions and requirements, estimated run times and costs, and information on sample data.       

**Note that cost and time estimates will vary** with the use of [Preemptibles](https://cloud.google.com/compute/docs/instances/preemptible).  
You can also use [Google's cost calculator](https://cloud.google.com/products/calculator/) to estimate cost to run.  **For helpful hints on controlling Cloud costs**, see [Overview: Controlling Cloud costs on Terra](https://support.terra.bio/hc/en-us/articles/360029748111).  




##  Illumina Genotyping Array 

### What does it do?   

Overall, the pipeline performs gender-specific genotyping, sample contamination detection, and summary metric collection. It optionally performs rare variant calling and genotype concordance, creates a fingerprint VCF that can be used for sample verification in parallel processes, and evaluates an existing sample fingerprint to confirm sample identity. 

**For helpful hints on controlling Cloud costs, see [Overview: Controlling Cloud costs on Terra](https://support.terra.bio/hc/en-us/articles/360029748111).**   

### What does it require as input?
A full description of the pipeline’s required and optional inputs can be found in the [Illumina Genotyping Array documentation](https://broadinstitute.github.io/warp/docs/Pipelines/Illumina_Genotyping_Arrays_Pipeline/IlluminaGenotypingArray.documentation).

This workspace is preconfigured to run the required sample-specific inputs listed in the table below, but you can also use several optional inputs that are detailed in the [documentation](https://broadinstitute.github.io/warp/docs/Pipelines/Illumina_Genotyping_Arrays_Pipeline/IlluminaGenotypingArray.documentation), including those for subset variant calling and evaluating an existing fingerprint. 

| Name |  Description |
| --- | --- |
| sample_alias | Name of sample run on barcode-specific chip segment |
| red_idat_cloud_path | Cloud path to the red IDAT file |
| green_idat_cloud_path | Cloud path to the green IDAT file |
| reported_gender | Reported sample sex/gender |
| chip_well_barcode | Unique identifier of the array chip section on which sample was run |

### What does it return as output?

The pipeline outputs annotated VCFs, index files, and summary metrics which are written to the workspace data table. 

Some metrics, such as the `arrays_subset_variant_calling` and `fingerprint` metrics will be empty in the data table unless you provide the optional inputs (subset intervals and fingerprint inputs, respectively) to generate these metrics in the workflow configuration.

All outputs are fully detailed in the [Illumina Genotyping Array documentation](https://broadinstitute.github.io/warp/docs/Pipelines/Illumina_Genotyping_Arrays_Pipeline/IlluminaGenotypingArray.documentation).  

### Sample data description and location    

This workspace contains the NA12878 sample dataset. The sample-specific files and attributes, including the cloud paths to red and green IDAT files are listed in the workspace Sample data table.  

### Reference data description and location  
All required files for the human hg19 reference, including FASTAs, index files, and dictionaries, as well as the VCF for dbSNP_138 (variants from the [dbSNP database](https://www.ncbi.nlm.nih.gov/projects/SNP/snp_summary.cgi?view+summary=view+summary&build_id=138) are listed in the Workspace data table. 

##### Enabling reference disks
We recommend configuring the workflow to use [reference disks](https://support.terra.bio/hc/en-us/articles/360056384631-Reference-Disks-in-Terra). This means that when Terra kicks off the workflow on a virtual computer, it will use a portable disk (kind of like a flash-drive) that is preloaded with the reference files needed for the workflow. This saves time and cost running the workflow. 

To enable reference disks, select the `Use reference disk` option on the workflow config page. You'll need to select this each time you run the workflow.


### Estimated time and cost to run on sample data 

The following table provides time and cost estimates for running the preconfigured workspace dataset with reference disks enabled.
 
| Sample Set Name | IDAT File Size | Time | Cost $ |
| :---:  | :---: | :---: | :---: |
| NA12878 | 3.77 MB |  0h 48 min | 0.02 |


**For helpful hints on controlling Cloud costs, see [Overview: Controlling Cloud costs on Terra](https://support.terra.bio/hc/en-us/articles/360029748111).**   

### Versions

All versions listed here are available by cloning this workspace and selecting the version on the Illumina Genotyping Array workflow. For a complete version list, please read the [IlluminaGenotypingArray changelog](https://github.com/broadinstitute/warp/blob/master/pipelines/broad/genotyping/illumina/IlluminaGenotypingArray.changelog.md) in GitHub.

| Release Version | Date | Release Note | 
| :---: | :---: | :--- |
| v1.12.13 | 08/25/23 | Updated to GATK version 4.3.0.0. Updated CheckFingerprint to allow LOD of 0. |
| v1.12.9 | 09/09/22 | Renamed the CompareVCFs task in VerifyIlluminaGenotypingArray.wdl to CompareVcfsAllowingQualityDifferences, this update has no effect on this pipeline. |
| v1.12.8 | 04/19/22 | Updated to GATK version 4.2.6.1 to address log4j vulnerabilities. Updated to Picard 2.26.11 and addressed obscure bug in GtcToVcf -> VcfToAdp. |
| v1.11.6 | 10/31/21 | Changed the way the version of autocall is returned for the case of arrays that fail gencall; updated Illumina IAAP Autocall and Zcall docker images to address critical vulnerability |
| v1.11.3 | 08/07/21 | Update version of Picard to 2.25.5 in order to allow GtcToVcf to support new enums in that build (updated all Picard tools to use this version), added chip_well_barcode and analysis_version_number available as outputs of the WDL |
| v1.11.0 | 10/14/20 | Added BafRegress to the pipeline. BafRegress detects and estimates sample contamination using B allele frequency data from Illumina genotyping arrays using a regression model. It will run only if a MAF file is input into the pipeline as an optional input |
| v1.9 | 8/14/20 | This is the first Terra release of this pipeline |


 
### Questions and Feedback
* For workspace questions and feedback, email the Broad pipelines team at warp-pipelines-help@broadinstitute.org or create a post on the [Featured Workspaces community forum](https://support.terra.bio/hc/en-us/community/topics/360001603491) (login required). Tag @Alexander Baumann in the **Details** section of your post.

* You can also Contact the Terra team for questions from the Terra main menu. When submitting a request, it can be helpful to include:
    * Your Project ID
    * Your workspace name
    * Your Bucket ID, Submission ID, and Workflow ID
    * Any relevant log information 




### License
**Copyright Broad Institute, 2023 | BSD-3**  
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at https://github.com/openwdl/wdl/blob/master/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.

---",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","warp-pipelines/Illumina-Genotyping-Array"
309,"help-terra","ISMB-Tutorial-Workspace-July-2022","READER","https://app.terra.bio/#workspaces/help-terra/ISMB-Tutorial-Workspace-July-2022",TRUE,FALSE,NA,NA,NA,"# Data processing and visualization in the cloud with Terra, Dockstore, and Galaxy
### ISMB Workshop July 10, 2022

This tutorial workspace is a step-by-step guide to analyzing 10x single-cell data. Using this workspace, you will:

1. Import an example 10x dataset (FASTQs) from **NeMO**.
2. Import the **Optimus workflow** from **Dockstore**.
3. Align example 10x FASTQs and produce a raw count matrix with quality metrics using the **Optimus workflow**.
4. Explore single-cell data in a **Scanpy Jupyter Notebook**. 


![]()


# Instructions

## Set up the tutorial workspace 

Before you begin, create your own editable copy (clone) of this WORKSPACE. Click the round circle with three dots in the upper right corner of this page and choose ""Clone"".

![Picture showing the location of the vertical three dot icon to make an editable copy of the workspace](https://storage.googleapis.com/terra-featured-workspaces/BICCN_Workshop_Workspace/clone.png)


## Follow the instructions in the step-by-step PDF 
|![PDF icon](https://storage.googleapis.com/terra-featured-workspaces/BICCN_Workshop_Workspace/Copy%20of%20PDF-icon_scaled.png) | Download a clickable PDF of step-by-step instructions [here](https://storage.cloud.google.com/terra-featured-workspaces/ISMB_July_2022/Data%20processing%20and%20visualization%20in%20the%20cloud%20with%20Terra%2C%20Dockstore%2C%20and%20Galaxy.pdf) |  
| --------| ------------------|     


## References
BRAIN Initiative Cell Census Network (BICCN) et al. A multimodal cell census and atlas of the mammalian primary motor cortex. bioRxiv 2020.10.19.343129; doi: https://doi.org/10.1101/2020.10.19.343129

Butler, A., Hoffman, P., Smibert, P. et al. Integrating single-cell transcriptomic data across different conditions, technologies, and species. Nat Biotechnol 36, 411–420 (2018). https://doi.org/10.1038/nbt.4096


Stuart T, Butler A, Hoffman P, Hafemeister C, Papalexi E, Mauck WM 3rd, Hao Y, Stoeckius M, Smibert P, Satija R. Comprehensive Integration of Single-Cell Data. Cell. 2019 Jun 13;177(7):1888-1902.e21. doi: 10.1016/j.cell.2019.05.031. Epub 2019 Jun 6. PMID: 31178118; PMCID: PMC6687398.

## Acknowledgements
Special thanks to Brian Herb and the NeMO team, the Bioconductor team (Martin Morgan, Vince Carey, and Nitesh Turaga) and the Broad Pipelines team for their feedback and amazing contributions to this workspace.

### License
**Copyright Broad Institute, 2021 | BSD-3**

All rights reserved. Full license text available [here](https://github.com/broadinstitute/warp/blob/master/LICENSE).. Note that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.

---






",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-terra/ISMB-Tutorial-Workspace-July-2022"
310,"terracontest"," TOSC19-idap","READER","https://app.terra.bio/#workspaces/terracontest/%20TOSC19-idap",TRUE,FALSE,NA,NA,NA," ## What's in this workspace?

### Overview 
---

We demonstrated the reproducible workflow by integrating the analysis results from Mutect2 and Varscan2 for the detection of single nucleotide mutations and small deletions and insertions on paired tumor-normal samples. 

References;
1. [GATK- Somatic short variant discovery](https://gatk.broadinstitute.org/hc/en-us/articles/360035894731-Somatic-short-variant-discovery-SNVs-Indels-)
2. [(How to) Call somatic mutations using GATK4.1.1.0+](https://gatk.broadinstitute.org/hc/en-us/articles/360035531132)
3. [Varscan2 ](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4278659/)
4. [Varscan2-Software ](http://varscan.sourceforge.net/somatic-calling.html)

### Experimental Design     
---
![Two-head together](https://storage.googleapis.com/idap-firecloud-public/Mutect2-Varscan2.jpg)

**Steps**
1. Idap_preprocess_for_analysis: Converts a FASTQ file to an unaligned BAM list.
2.  Preprocessing-for-variant disocvery: Produce analysis-ready BAM files
3.  Mutect2_GATK4: Call somatic short variants both SNVs and indels, and filter for confident somatic calls.
4.  Varscan2: Call somatic mutations, LOH events, and germline variants in tumor-normal pairs, and filter for high confident somatic calls.
5. Annotate variants (on jupyter notebook):  Aggregate the results and analyzes the given variants for their function.

Note:
This example workflow shows an analysis of one normal-tumor sample only. However, if we have multi samples, therefore we will add Mutect2_PON to the workflow.

### Sample Data  
---
We [obtain the  data from EBI website](https://www.ebi.ac.uk/arrayexpress/experiments/E-GEOD-48215/samples/?s_page=2&s_pagesize=25&full=true)(ID GSM1172954 and GSM1172955) and then store them in our google storage. Example inputs are provided in the workspace data model for testing. A given original tumor and normal pair are HCC1143-T and HCC1143-BL paired-end fastq files respectively.

HCC1143-T
* ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR925/SRR925765/SRR925765_2.fastq.gz (4.8 GB)
* ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR925/SRR925765/SRR925765_1.fastq.gz (4.8 GB)

HCC1143-BL
* ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR925/SRR925766/SRR925766_2.fastq.gz (1.86 GB)
* ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR925/SRR925766/SRR925766_1.fastq.gz (1.86 GB)



### Workspace Data    
---
The required and optional references and resources for the tools are included in the Workspace Data table (Workspace Attributes in Terra). The reference genome for this workspace is hg19 (aka GRCh37).

### Tools    
---
This workspace contains the following tools (""Methods"" in Terra):

#### 1. idap_preprocess_for_analysis

Create a uBam (unmapped bam) list from fastq by using [FASTQToSam](https://gatk.broadinstitute.org/hc/en-us/articles/360036351132-FastqToSam-Picard-) and [SplitSamByNumberOfReads](https://gatk.broadinstitute.org/hc/en-us/articles/360037064232-SplitSamByNumberOfReads-Picard-)

**Entity Type**: Sample / Sample set

**Requirements/expectations**

* fastq in paired-end or single file
* Read group name (RG tag)
* Sample name to insert into the read group header (SM tag)

**Outputs**
* File of a list of the generated unmapped BAMs (ubam list)


#### 2.  Preprocessing for variant disocvery

This WDL pipeline implements [data pre-processing](https://gatk.broadinstitute.org/hc/en-us/articles/360035535912-Data-pre-processing-for-variant-discovery) according to the GATK Best Practices.

**Entity Type**: Sample / Sample set

**Requirements/expectations**
* Human genome pair-end sequencing data in unmapped BAM (uBAM) format
* One or more read groups, one per uBAM file, all belonging to a single sample (SM)
* Input uBAM files must comply with the following requirements:
    * filenames all have the same suffix (we use "".unmapped.bam"")
    * files must pass validation by ValidateSamFile
    * reads are provided in query-sorted order
    * all reads must have an RG tag

**Outputs**
* Bam, Bam index, and Bam md5
* BQSR Report
* Several Summary Metrics

**Note**:
This piepline replicated  form [processing-for-variant-discovery-gatk4:version 8](https://portal.firecloud.org/#methods/gatk/processing-for-variant-discovery-gatk4/8). We add  max_retries to the pipeline and fix missing preemptible_tries in SortAndFixTags.

#### 3. Mutect2_GATK4

This WDL workflow runs GATK4 Mutect 2 on a single tumor-normal pair or on a single tumor sample, and performs additional filtering.

**Entity Type**: Pair / Pair set

**Requirements/expectations**
* ref_fasta, ref_fai, ref_dict: reference genome, index, and dictionary
* tumor_bam, tumor_bam_index: BAM and index for the tumor sample 
* normal_bam, normal_bam_index: BAM and index for the normal sample
* (optional but strongly recommended)
    * pon: optional panel of normals in VCF format containing probable technical artifacts (false positves) 
    * gnomad: optional database of known germline variants (see http://gnomad.broadinstitute.org/downloads)
    * variants_for_contamination: VCF of common variants with allele frequencies for calculating contamination

**Outputs**
* One VCF file and its index with primary filtering applied
* A bamout.bam file of reassembled reads if requested

**Note**:
This piepline replicated  form  [gatk/mutect2-gatk4](https://portal.firecloud.org/#methods/gatk/mutect2-gatk4/19).

#### 4. Varscan2  

This WDL workflow perform somatic mutation calling accroding to [basic protocol 2](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4278659/)  and [varscan user's manual](http://varscan.sourceforge.net/using-varscan.html)

**Entity Type**: Pair / Pair set

**Requirements/expectations**
* ref_fasta, ref_fai : reference genome and  index
* tumor_bam, tumor_bam_index: BAM and index for the tumor sample 
* normal_bam, normal_bam_index: BAM and index for the normal sample

**Outputs**
* snp_Somatic_hc_pass_filteredfpfilter_vcf (optional output)
* snp_Somatic_hc_fail_filteredfpfilter_vcf (optional output)
* snp_unfiltered_vcf
* indel_unfiltered_vcf
* snp_Somatic_hc_filtered_vcf (pimary output)
* tumor_readcounts (optional output)
* snp_unfiltered_Germline, snp_unfiltered_LOH, snp_unfiltered_Somatic, snp_unfiltered_Germline_hc, snp_unfiltered_LOH_hc and  snp_unfiltered_Somatic_hc
* indel_unfiltered_Germline, indel_unfiltered_LOH, indel_unfiltered_Somatic, indel_unfiltered_Germline_hc, indel_unfiltered_LOH_hc and indel_unfiltered_Somatic_hc

---- hc (High Confidence), ""filtered"" means output from somaticFilter, ""filteredfpfilter"" means output from fpfilter  

#### Tool Paramters
All tools privide custom and adjustable argument fields that prefixed with ""_extra_args"" 


### Notebooks
---
This notebook script illustrates the processing steps including combine, select, and annotate to the called mutation files, and also allows a user to add their own variants (e.g. vcf) based on a set of data sources (e.g. hg19 , hg38). Each data source can be customized to annotate a variant based on several matching criteria. This allows a user to create their own custom annotations easily.

| Runtime Environment  | Value |
| ------------- | ------------- |
| CPU  | 1  |
| MEM  | 3.75 GB  |
| Disk  | 100 GB  |
| StartUp Script  | gs://idap-firecloud-public/scripts_install_gatk_4120_381_with_condaenv.sh |

### Software versions 
---

* Docker:
    * us.gcr.io/broad-gatk/gatk:4.1.2.0
    * us.gcr.io/broad-gotc-prod/genomes-in-the-cloud:2.3.2-1510681135
    * gcr.io/firecloud-177104/idap-varscan2 ( Including pre-install softwares; Varscan2.4.3, Samtools1.9, and  Bam-readcount0.8)
* GATK-4.1.2.0
* GATK-3.8.1.0 (For Combine and SelectVariant)
* Varscan-2.4.3
* Samtools-1.9
* BAM-ReadCount-0.8


### Time and cost 
---
Below is an example of the time and cost for running the workflows.

##### idap_preprocess_for_analysis
| Sample Name | Sample Size | Time | Cost $ |
| ---  | :---: | :---: | :---: |
| HCC1143-T | 9.6 GB ( .fastq.gz )    | 1h 45m |  
| HCC1143-BL |  3.72 GB ( .fastq.gz )   |49m |  

##### Processing-For-Variant-Discovery
| Sample Name | Sample Size | Time | Cost $ |
| ---  | :---: | :---: | :---: |
| HCC1143-T | 12.16 GB (.ubam)  | 5h 49m| 
| HCC1143-BL | 4.6 GB  (.ubam)  | 3h 25m	|  

#####  Mutect2_GATK4
| Sample Name | Sample Size | Time | Cost $ |
| ---  | :---: | :---: | :---: |
| HCC1143-T ,HCC1143-BL | 10.46 GB (bam) | 1h 31m | 

#####  Varscan2
| Sample Name | Sample Size | Time | Cost $ |
| ---  | :---: | :---: | :---: |
| HCC1143-T ,HCC1143-BL | 10.46 GB (bam) | (2h 17m) | 0.49

##### Notebooks
 | Time | Cost $ |
| ---  | :---: | 
| 15-20 mins | < 0.06 


### Contact information    
---
Bhoom Suktitipat: bhoom.suk@mahidol.ac.th    
Kunaphas Kongkitmanon: kunaphas.kon@gmail.com


## Evaluation form
To rate this workspace as part of the Terra Open Science Contest, please fill out this [seven question survey](https://forms.gle/ma4AembkSLkyAaTE9).
 ",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","terracontest/ TOSC19-idap"
311,"broad-firecloud-dsde-methods","MAS-seq - Data Segmentation and Alignment","READER","https://app.terra.bio/#workspaces/broad-firecloud-dsde-methods/MAS-seq%20-%20Data%20Segmentation%20and%20Alignment",TRUE,FALSE,NA,NA,NA,"Annotate and segment MAS-ISO-seq prepared data as sequenced by a PacBio Sequel IIe instrument.  Produces  both CCS corrected and CCS uncorrected reads from the input MAS-seq dataset which have been aligned to both a reference genome and transcriptome.

This workspace supports the work done in the following preprint:

[High-throughput RNA isoform sequencing using programmable cDNA concatenation](https://www.biorxiv.org/content/10.1101/2021.10.01.462818v1)

Other data and more information on the preprint can be found [here](https://github.com/broadinstitute/mas-seq-paper-data).",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","broad-firecloud-dsde-methods/MAS-seq - Data Segmentation and Alignment"
312,"terra-billing-vir","tf-atlas","READER","https://app.terra.bio/#workspaces/terra-billing-vir/tf-atlas",TRUE,FALSE,NA,NA,NA,"TF-Atlas

This workspace was used to process part of the Transcription factor ChIP-seq datasets available from the ENCODE Consortium.



![TF-Atlas](https://s3-service-broker-live-19ea8b98-4d41-4cb4-be4c-d68f4963b7dd.s3.amazonaws.com/uploads/collections/NSUP_Cov1-8a789cb352586f4994750d169523aa03.jpg)

## Run the workflows in the following order to get all the outputs:
	
1) <a href=""https://anvil.terra.bio/#workspaces/terra-billing-vir/tf-atlas/workflows/terra-billing-vir/run_preprocess""> run_preprocess </a>
	
	We download the BAM files from the ENCODE portal adn convert them to 5' stranded signal tracks.
	
2) <a href=""https://anvil.terra.bio/#workspaces/terra-billing-vir/tf-atlas/workflows/terra-billing-vir/outlier_detection""> outlier_removal </a>

	Remove outlier peaks with very high signal that can affect the training process.
	
3) <a href=""https://anvil.terra.bio/#workspaces/terra-billing-vir/tf-atlas/workflows/terra-billing-vir/gc_matched_negatives""> gc_matched_background_regions </a>

	In addition to the peak regions, BPNet models are also trained on the background regions.
		
	
4) <a href=""https://anvil.terra.bio/#workspaces/terra-billing-vir/tf-atlas/workflows/terra-billing-vir/modeling""> train_bpnet_model </a>
	
	Train a BPNet model that regresses out the input DNA bias and learns the cis-regulatory sequence syntax driving TF binding in the ChIP-seq data. 
	
	
5) <a href=""https://anvil.terra.bio/#workspaces/terra-billing-vir/tf-atlas/workflows/terra-billing-vir/shap""> shap </a>

	Understanding why a model makes a certain prediction can be as crucial as the prediction’s accuracy in many applications. <a href=""https://proceedings.neurips.cc/paper_files/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf""> SHAP</a> (SHapley Additive exPlanations) assigns each feature an importance value for a particular prediction. 
	
	
6) <a href=""https://anvil.terra.bio/#workspaces/terra-billing-vir/tf-atlas/workflows/terra-billing-vir/modisco""> modisco </a>
	
	""TF-MoDISco is a biological motif discovery algorithm that differentiates itself by using attribution scores from a machine learning model, in addition to the sequence itself, to guide motif discovery. Using the attribution scores, as opposed to the signal being predicted by the machine learning model (e.g. ChIP-seq peaks), can be beneficial because the attributions fine-map the specific sequence drivers of biology. Although in many of our examples this model is BPNet and the attributions are from DeepLIFT/DeepSHAP, there is no limit on what attribution algorithm is used, or what model the attributions come from. "" 
	
7) <a href=""https://github.com/austintwang/finemo_gpu/tree/main""> Motif hits </a>
	
	A GPU-accelerated hit caller for retrieving TFMoDISCo motif occurences from machine-learning-model-generated contribution scores.


Pipeline identifies motifs that contribute to the binding of the factors

For example, here are the top motifs for each of these experiments:

|Factor|ENCID|motif|
|-----------|-----------|-----------|
|CTCF|ENCSR000AHD|<img src=""https://mitra.stanford.edu/kundaje/oak/vir/tfatlas/modisco/release_run_1/meanshap/ENCSR000AHD/counts/pos_patterns.pattern_0.cwm.fwd.png"" alt=""motif"" width=""400""/>|
|MAFK|ENCSR000EEB|<img src=""https://mitra.stanford.edu/kundaje/oak/vir/tfatlas/modisco/release_run_1/meanshap/ENCSR000EEB/counts/pos_patterns.pattern_0.cwm.fwd.png"" alt=""motif"" width=""400""/>|
|JUND|ENCSR000BSA|<img src=""https://mitra.stanford.edu/kundaje/oak/vir/tfatlas/modisco/release_run_1/meanshap/ENCSR000BSA/counts/pos_patterns.pattern_0.cwm.fwd.png"" alt=""motif"" width=""400""/>|

",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","terra-billing-vir/tf-atlas"
313,"help-gatk","GATKTutorials-Germline-August2019","READER","https://app.terra.bio/#workspaces/help-gatk/GATKTutorials-Germline-August2019",TRUE,FALSE,NA,NA,NA,"# What's in this workspace?
Welcome to Day 2 of the Genome Analysis Toolkit (GATK) workshop! Yesterday you got an overview of the various tools and pipelines, but today will focus on Germline Variant Discovery. Please note, this workspace is no longer the latest version of our tutorial materials. For our latest materials, please see the Library Showcase section on Terra.

*This workspace was originally developed for the Genome Analysis Toolkit (GATK) workshop in São Paulo, Brazil in August 2019. Feel free to run it on your own, even if you were unable to attend that workshop!*

Here you will be running three different notebooks. In the morning we will cover variant discovery, and in the afternoon we will look at different methods for variant filtering. This workspace is read-only, so clone your own unique copy to work with it.

Your instructor will guide you through this workspace, but you should find the documentation within the notebooks sufficient to run through them again on your own or to share with a friend later (and we encourage you to do so!).

The notebook(s) in this workspace are:
* 1-germline-variant-discovery-tutorial
* 2-gatk-hard-filtering-tutorial
* 3-gatk-cnn-tutorial

All notebooks in this workspace can use the following runtime settings:

| Runtime Environments | Value |
| --- | --- |
| CPU Minimum| 4|
| Disksize Minimum | 100 GB |
| Memory Minimum | 15 GB |
| Startup Script | `gs://gatk-tutorials/scripts/install_gatk_4110_with_igv.sh` |

## Appendix

### GATK @ Brazil 2019 in the Terra Support Center

Following the conclusion of the workshop, the workshop materials will be made available in the Terra Support Center. For now, you can find these materials in a google drive folder at [broad.io/GATK1908](http:/broad.io/GATK1908)

### GATK documentation and support

Detailed documentation, tutorials, and more are available at the [GATK web site](https://software.broadinstitute.org/gatk/). If you are still running into issues after consulting the User Guide, have a question, or just want to say hello, reach out to the GATK team via the [GATK Support Forum](https://gatkforums.broadinstitute.org/gatk)!

### Terra documentation and support

Documents and helpful guides to support your journey through Terra are available in the Terra Support Center. Navigate there by following the “How-to Guides” link under Terra Support in the left-side menu. We are still actively writing and publishing documents but we’ve got a good set of docs in the Quick Start Guide to get you going.



",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/GATKTutorials-Germline-August2019"
314,"help-gatk","GATK-Structural-Variants-Single-Sample","READER","https://app.terra.bio/#workspaces/help-gatk/GATK-Structural-Variants-Single-Sample",TRUE,FALSE,NA,NA,NA,"### GATK Best Practices for Structural Variation Discovery on Single Samples

An integrated structural variation detection and resolution pipeline designed to call
many forms of structural variation in whole genome sequencing data obtained from a single sample. The pipeline will identify,
genotype, and annotate structural variation from the following variant types: 

- Copy number variants, including deletions and duplications
- Insertions 
- Inversions 
- Reciprocal chromosomal translocations
- Additional forms of complex structural variation 

**Pipeline Background**

The Single Sample pipeline is based upon the GATK-SV cohort pipeline, which jointly analyzes WGS data from large research
cohorts and has been used to create SV call sets for [gnomAD-SV](https://www.nature.com/articles/s41586-020-2287-8) and the
[SFARI SSC autism research study](http://dx.doi.org/10.1038/s41588-018-0107-y). 

**Extending SV detection to small datasets**
The Single Sample pipeline in this workspace
is designed to facilitate running the methods developed for the cohort-more GATK-SV pipeline on small data sets or in 
clinical contexts where batching large numbers of samples is not an option. To do so, it uses precomputed data, SV calls,
and model parameters computed by the cohort pipeline on a reference panel composed of similar samples. The pipeline integrates this
precomputed information with signals extracted from the input CRAM file to produce a call set similar in quality to results
computed by the cohort pipeline in a computationally tractable and reproducible manner.

GATK-SV uses [Manta](https://github.com/Illumina/manta), [WHAM](https://github.com/zeeev/wham), GATK gCNV, and [cn.MOPS](https://pubmed.ncbi.nlm.nih.gov/22302147/) 
as raw calling algorithms, and then integrates, filters, refines, and annotates 
the calls from these tools to produce a final output. Please note that most large published joint call sets produced by
GATK-SV, including gnomAD-SV, included the tool MELT, a state-of-the-art mobile element insertion (MEI) detector, as part of the pipeline. 
Due to licensing restrictions, we cannot provide a public docker image or reference panel VCFs for this algorithm. The 
version of the pipeline configured in this workspace does not run MELT or include MELT calls for the reference panel. Therefore, 
the output will be less sensitive to MEI calls that might appear in gnomAD or other joint call sets. 

Source code for all GATK-SV pipelines is stored and documented in the [GATK-SV GitHub repository](https://github.com/broadinstitute/gatk-sv).

Please cite the [gnomAD-SV](https://www.nature.com/articles/s41586-020-2287-8) paper when referring to these methods.

## Data

### Case Sample

This workspace includes a NA12878 input WGS CRAM file configured in the workspace samples data table. This file is part of the
high coverage (30X) [WGS data](https://www.internationalgenome.org/data-portal/data-collection/30x-grch38) for the 1000
Genomes Project samples generated by the New York Genome Center and hosted in
[AnVIL](https://app.terra.bio/#workspaces/anvil-datastorage/1000G-high-coverage-2019). 

### Reference Panel

The reference panel configured in this workspace consists of data and calls computed from 156 publicly available samples
chosen from the NYGC/AnVIL 1000 Genomes high coverage data linked above. 

Inputs to the pipeline for the reference panel include:
- A precomputed SV callset VCF, and joint-called depth-based CNV call files 
- Raw calls for the reference panel samples from Manta and WHAM 
- Trained models for calling copy number variation in GATK gCNV case mode
- Parameters learned by the cohort mode pipeline in training machine learning models on the reference panel samples.

These resources are primarily configured in the ""Workspace Data"" for this workspace. However, several of the resources need 
to be passed  to the workflow as large lists of files or strings. Due to Terra limitations on uploading data containing lists to the
workspace data table, these resources are specified directly in the workflow configuration.

### Reference Resources

The pipeline uses a number of resource and data files computed for the hg38 reference:
- Reference sequences and indices 
- Genome annotation tracks such as segmental duplication and RepeatMasker tracks
- Data used for annotation of called variants, including GenCode gene annotations and gnomAD site allele frequencies. 

## GATKSVSingleSample Workflow

### What does it do?

The workflow calls structural variations on a single input CRAM by running the GATK-SV Single Sample Pipeline end-to-end.

#### What does it require as input?

The workflow accepts a single CRAM or BAM file as input, configured in the following parameters:

|Input Type|Input Name|Description|
|---------|--------|--------------|
|`String`|`sample_id`|Case sample identifier|
|`File`|`bam_or_cram_file`|Path to the GCS location of the input CRAM or BAM file|
|`String`|`batch`|Arbitrary name to be assigned to the run|
|`Boolean`|`requester_pays_cram`|Set to `true` if the case data is stored in a requester-pays GCS bucket|

#### Additional workspace-level inputs

- Reference resources for hg38 
- Input data for the reference panel
- The set of docker images used in the pipeline. 
  
Please contact GATK-SV developers if you are interested in customizing these
inputs beyond their defaults.

#### What does it return as output?

|Output Type|Output Name|Description|
|---------|--------|--------------|
|`File`|`final_vcf`|SV VCF output for the pipeline. Includes all sites genotyped as variant in the case sample and genotypes for the reference panel. Sites are annotated with overlap of functional genome elements and allele frequencies of matching variants in gnomAD|
|`File`|`final_vcf_idx`|Index file for `final_vcf`|
|`File`|`final_bed`|Final output in BED format. Filter status, list of variant samples, and all VCF INFO fields are reported as additional columns.|
|`File`|`metrics_file`|Metrics computed from the input data and intermediate and final VCFs. Includes metrics on the SV evidence, and on the number of variants called, broken down by type and size range.|
|`File`|`qc_file`|Quality-control check file. This extracts several key metrics from the `metrics_file` and compares them to pre-specified threshold values. If any QC checks evaluate to FAIL, further diagnostics may be required.|
|`File`|`ploidy_matrix`|Matrix of contig ploidy estimates computed by GATK gCNV.|
|`File`|`ploidy_plots`|Plots of contig ploidy generated from `ploidy_matrix`|
|`File`|`non_genotyped_unique_depth_calls`|This VCF file contains any depth based calls made in the case sample that did not pass genotyping checks and do not match a depth-based call from the reference panel. If very high sensitivity is desired, examine this file for additional large CNV calls.|
|`File`|`non_genotyped_unique_depth_calls_idx`|Index file for `non_genotyped_unique_depth_calls`|
|`File`|`pre_cleanup_vcf`|VCF output in a representation used internally in the pipeline. This file is less compliant with the VCF spec and is intended for debugging purposes.|
|`File`|`pre_cleanup_vcf_idx`|Index file for `pre_cleanup_vcf`|

#### Example time and cost run on sample data

|Sample Name|Sample Size|Time|Cost $|
|-----------|-----------|----|------|
|NA12878|18.17 GiB|23hrs|~$7.34|

#### To use this workflow on your own data

If you would like to run this workflow on your own samples (which must be medium-to-high coverage WGS data):

- Clone this workspace into a Terra project you have access to
- In the cloned workspace, upload rows to the Sample and (optionally) the Participant Data Table that describe your samples.
  Ensure that the rows you add to the Sample table contain the columns `sample_id`, `bam_or_cram_file`, and `requester_pays_cram`,
  populated appropriately.
- There is no need to modify values in the workspace data or method configuration. If you are interested in modifying the reference
  genome resources or reference panel, please contact the GATK team for support as listed below.
- Launch the workflow from the ""Workflows"" tab, selecting your samples as the inputs.

Please check the `qc_file` output for each sample at the end of the run to screen for data quality issues.

#### Contact information
For questions about this workspace please visit the Featured Workspaces topic on the Terra forum. Use the search box to see if other users have asked the same question previously. If not, post and tag @Chris Whelan or @Mark Walker so that we get notified.

This material is provided by the GATK Team. Please post any questions or concerns regarding the GATK tool to the GATK forum.

#### Workspace Citation 
Details on citing Terra workspaces can be found here: [How to cite Terra](https://support.terra.bio/hc/en-us/articles/360035343652)

Data Sciences Platform, Broad Institute (*Year, Month Day that this workspace was last modified*) help-gatk/GATK-Structural-Variants-Single-Sample [workspace] Retrieved *Month Day, Year that workspace was retrieved*,  https://app.terra.bio/#workspaces/help-gatk/GATK-Structural-Variants-Single-Sample

##### License 

Copyright (c) 2009-2020, Broad Institute, Inc. All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are met:

* Redistributions of source code must retain the above copyright notice, this
  list of conditions and the following disclaimer.

* Redistributions in binary form must reproduce the above copyright notice,
  this list of conditions and the following disclaimer in the documentation
  and/or other materials provided with the distribution.

* Neither the name Broad Institute, Inc. nor the names of its
  contributors may be used to endorse or promote products derived from
  this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""
AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

### Workspace Change Log
| Date | Change | Author |
| --- | --- | --- |
|2023-02-09|Update gatk-sv-single-sample to v0.26.9-beta| Samantha Velasquez
|  2021-09-15 | Addition of Workspace Citation section | Beri Shifaw |


",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/GATK-Structural-Variants-Single-Sample"
315,"broad-firecloud-cptac","PDC_Direct_Data_Import","READER","https://app.terra.bio/#workspaces/broad-firecloud-cptac/PDC_Direct_Data_Import",TRUE,FALSE,NA,NA,NA,"### Workspace for direct import and streamlined processing of raw data files from the Proteomic Data Commons

The **Proteomic Data Commons** ([PDC](https://proteomic.datacommons.cancer.gov/pdc/)) is a data repository within the [NCI Cancer Research Data Commons (CRDC)](https://datacommons.cancer.gov/) and provides access to curated and standardized *proteomic data* along with biospecimen and clinical metadata. The goal of this workspace (notebook+workflow) is to faciliate *automated data download and streamlined analysis* of raw proteomic data imported from the PDC using the [`FragPipe`](https://fragpipe.nesvilab.org/) proteomics pipeline.

**Step-by-step** process for importing and processing raw proteomics data files:
1. Start by navigating to the [PDC](https://proteomic.datacommons.cancer.gov/pdc/). 
2. Browse through the proteomics raw data available there and select files to import and process on Terra. Analyzing TMT or iTRAQ raw data using `FragPipe` requires `mzML` files. On the PDC, `mzML` files are listed in the `Processed Mass Spectra (Open Standard)` data category.
3. Export file manifest for the chosen files using the `PFB` button. This will connect to Terra and prompt for a workspace to put locate the `file` manifest table.
4. Copy the `PDC_Direct_Data_Import` notebook to the workspace with the `file` manifest table and run all the code blocks in sequence, using a Jupyter Cloud Environment. Set `fragpipe_workflow` and `fasta_file` in the `User inputs` section as needed. Running the code blocks will:
    1. Download all the files listed in the `file` table to the `fragpipe` directory in the workspace bucket.
    2. Organize files into subdirectories--one subdirectory for each TMT/iTRAQ plex, including all fractions for that plex.
    3. Create annotation files for each TMT/iTRAQ plex to provide sample IDs.
    4. Generate a `FragPipe` manifest file for processing all the data files.
    5. Download and transfer specified *standard* workflow and FASTA files to the workspace bucket. For custom workflow or FASTA files, see Steps 5 and/or 6. 
5. For *custom workflows*: Once the notebook has been successfully run, create a `FragPipe` [workflow](https://fragpipe.nesvilab.org/docs/tutorial_fragpipe_workflows.html) using the `FragPipe` graphical user interface. Workflows can also be directly [downloaded](https://github.com/Nesvilab/FragPipe/tree/master/MSFragger-GUI/workflows) and/or edited as needed. Upload the workflow to the workspace bucket.
6. For *custom FASTA files*: Obtain a relevant (fasta) sequence database and upload to the workspace bucket. Sequence databases can be downloaded using the `FragPipe` graphical user interface, or copied from other workspaces.
7. If not already present, import the `panoply_fragpipe_search` method from the [FireCloud Method Repository](https://portal.firecloud.org/?return=terra#methods).
8. Customize inputs to the `panoply_fragpipe_search` workflow: By default, the inputs point to respective columns in the `panoply_inputs` TSV table in the workspace. Inputs can be customized if needed:
    1. Point `database` to the sequence database in the workspace bucket (from Step 6).
    2. `files_folder` is the bucket address of the `fragpipe` directory created to host all the downloaded files.
    3. The location of the manifest file `fragpipe-manifest.fp-manifest` created by the notebook should be filled into `fragpipe_manifest`.
    4. The bucket location of the workflow file uploaded in Step 5 goes in the `fragpipe_workflow` input slot.
    5. Set any *optional* inputs if needed. Documentation for the `panoply_fragpipe_search` workflow is available on [GitHub](https://github.com/broadinstitute/PANOPLY/blob/dev/third-party-modules/panoply_fragpipe/README.md).
9. Run the workflow (using inputs defined by the `current` row of the `panoply_inputs` data table) and download the output `zip` files.
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","broad-firecloud-cptac/PDC_Direct_Data_Import"
316,"cgp-terra-outreach-workshop","Human-Pangenome-Assembly-AnVIL-ASHG-Jan22","READER","https://app.terra.bio/#workspaces/cgp-terra-outreach-workshop/Human-Pangenome-Assembly-AnVIL-ASHG-Jan22",TRUE,FALSE,NA,NA,NA,"This is a demonstration workspace for the ASHG workshop Reproducible Analysis of Human Pangenome Data using the AnVIL. The workspace demonstrates how to use the assembly workflow generated and used by the Human Pangenome Reference Consortium.

## Data 
The data we will use in this workspace is loaded into the `sample` data table.  In AnVIL, workflow inputs can be easily synced with data table columns. Currently `sample` has only one row that points to a subset of reads data related to the sample HG002. It has multiple columns and the ones that will be given as inputs to the assembly workflow  have the prefix `input_` (e.g. `input_hifi`) . Similarly the columns containing the output files have the prefix `output_` (e.g. `output_pat_fasta_gz`). In the workshop you learn how to produce the output columns and add them to the table.
  
## Workflows

### [TrioHifiasmAssembly](https://dockstore.org/workflows/github.com/human-pangenomics/hpp_production_workflows/TrioHifiasmAssembly:master?tab=info)

**What does it do?**  
Hifiasm is a fast haplotype-resolved de novo assembler for PacBio HiFi reads. It can assemble a human genome in several hours and assemble a ~30Gb California redwood genome in a few days. Hifiasm emits partially phased assemblies of quality competitive with the best assemblers. Given parental short reads or Hi-C data, it produces arguably the best haplotype-resolved assemblies so far. You can learn more in the [documentation](https://github.com/chhylp123/hifiasm). In this workflow we are calling the trio-binning mode of hifiasm so we we need parental short reads.

**What does it require as input?**       
1. HiFi reads
2. Maternal short reads
3. Paternal short reads

Each of these inputs can be given in one of the formats; FASTQ, gzipped FASTQ, BAM or CRAM

**What does it return as output?**  
It returns two important outputs:

 1. Maternal haplotype-resolved assembly (gzipped FASTA)
 2. Paternal haplotype-resolved assembly (gzipped FASTA)
 
 It also outputs some other files:
1.  Raw and phased GFA files
2.  Hifiasm binary files (can be used for reassembling a sample in less time with a newer version of hifiasm)
3.  Paternal and maternal yak files

**Reference/Resource data description and location**  
The reference genome for this workspace is hg38 (aka GRCh38). Required and optional references and resources for the methods are included in the Workspace Data table (""Workspace Attributes"" in FireCloud).  
 
**Estimated time and cost to run on sample data**    

| Sample Name | HiFi Coverage | Chromosome | Time | Cost $ |
| :---:  | :---: | :---: | :---:|---: |
|HG002| 35X |  chr20 | ~1hour  | < 1$  | 

## Potential Next Steps

### Assemble other samples
After learning how to assemble this small example you will be able to run it on your own data or any HPRC data. To apply it on your own data you have to upload your data to the Google Cloud Platform (GCP) (with [gsutil](https://cloud.google.com/storage/docs/gsutil)) and create a new data table in your workspace which points to your data on GCP. For using HPRC data you can simply clone [the official HPRC workspace](https://app.terra.bio/#workspaces/anvil-datastorage/AnVIL_HPRC).


### Quality Control
After producing the assemblies you can assess them using our comprehensive QC workflow which consists of running a variety of evaluation tools.
The name of the main QC workflow is `StandardQC` and it is available it in [the official HPRC page on Dockstore](https://dockstore.org/organizations/HumanPangenome) under the `Assembly QC` collection. 
For running `StandardQC` you need the inputs below:
1. Maternal assembly
2. Paternal assembly
3. Maternal illumina reads
4. Paternal illumina reads
5. Child illumina reads
6. Reference fasta
7. Gene annotation 

After running the assembly workflow the items 1 to 5 will be all available in your data table. You can find the items 6 and 7 in `Workspace Data` under the `DATA` tab of this workspace. You have to refer to this data while filling the input parameters of the workflow. For example if you want to refer to the reference genome and the gene annotation file you should write `workspace.GRCh38` and `workspace.GeneAnnotation` respectively.      

---
---

## Authors and contact information

This workspace is a product of the Human Pangenome Reference Consortium [HPRC](https://humanpangenome.org/) and [the AnVIL](https://anvilproject.org/). Contributors include:

* [Mobin Asri](masri@ucsc.edu) (UCSC Paten Lab)
* [Julian Lucas](juklucas@ucsc.edu) (UCSC Computational Genomics Platform)
* [Beth Sheets](esheets@ucsc.edu) (UCSC Computational Genomics Platform)
* [Trevor Pesout](tpesout@ucsc.edu ) (UCSC Paten Lab)
* [Karen Miga](khmiga@ucsc.edu) (UCSC Miga Lab)

## Additional generally helpful resources

* **[HPRC Pangenome GitHub](https://github.com/human-pangenomics/hpp_pangenome_resources)**   
    Description of HRPC's currently available (release) pangenomes.   
		
* **For helpful hints on controlling cloud costs**, see [this article](https://support.terra.bio/hc/en-us/articles/360029748111).       

* **For instructions on installing gsutil**, see [this article](https://cloud.google.com/storage/docs/gsutil_install). 

## Citations

Cheng, H., Concepcion, G.T., Feng, X., Zhang, H., Li H. (2021) Haplotype-resolved de novo assembly using phased assembly graphs with hifiasm. Nat Methods, 18:170-175. https://doi.org/10.1038/s41592-020-01056-5

### Workspace License
MIT License

Copyright (c) [2022] [Human-Pangenome-Assembly-AnVIL-ASHG-Jan22]

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the ""Software""), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","cgp-terra-outreach-workshop/Human-Pangenome-Assembly-AnVIL-ASHG-Jan22"
317,"help-gatk","Germline_variant_discovery_hg38_v1","READER","https://app.terra.bio/#workspaces/help-gatk/Germline_variant_discovery_hg38_v1",TRUE,TRUE,NA,NA,NA,"### GATK Best Practices for germline variant discovery (hg38 reference)
The purpose of this workspace is to provide a fully reproducible example of the GATK Best Practices workflows for germline variant discovery. 

#### Workspace attributes
All required and optional resources for the preconfigured methods included. The reference genome is hg38 (aka GRCh38).

#### Data 
A set of three BAMs of NA12878 WGS data downsampled to 20% (med), 5% (small) and 20k reads, respectively. 

#### Method configs
This workspace contains the following preset method configurations:

- **HaplotypeCallerGvcf_GATK3_MC**
Runs HaplotypeCaller in GVCF mode on a single sample according to ## the GATK Best Practices (June 2016) scattered across intervals. Usage instructions: launch this on any qualifying Sample or Set of Samples (each sample must reference an analysis-ready BAM). See workflow for additional input requirements and version notes.",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/Germline_variant_discovery_hg38_v1"
319,"aryee-lab","bisulfite-seq-tools-hg19","READER","https://app.terra.bio/#workspaces/aryee-lab/bisulfite-seq-tools-hg19",TRUE,FALSE,NA,NA,NA,"### bisulfite-seq-tools

Methods from this workspace can be used for alignment and quality control analysis for various DNA methylation protocols including Whole Genome Bisulfite Sequencing (WGBS), Reduced Representation Bisulfite Sequencing (RRBS) and Hybrid Selection Bisulfite Sequencing (HSBS).


---

### Data

**Sample Data**  
All preprocessing methods require forward and reverse  fastq or fastq.gz files entered into a sample data table.  The tables below outline the format and content of the **Sample** data model tables (tab separated files with “tsv” extension).

   **Example 1:** samples.tsv for RRBS and WGBS

| entity:sample_id 	| bs_fastq1                          	| bs_fastq2                          	|
|-----------------------	|------------------------------------	|------------------------------------	|
| Sample1               	| gs://location/sample_1_R1.fastq.gz 	| gs://location/sample_1_R2.fastq.gz 	|
| Sample2               	| gs://location/sample_2_R1.fastq.gz 	| gs://location/sample_2_R2.fastq.gz 	||

   **Example 2:** Participants.tsv for HSBS requires an additional input, the target_region.

| entity:sample_id 	| bs_fastq1                          	| bs_fastq2                          	| target_region                         	|
|-----------------------	|------------------------------------	|------------------------------------	|---------------------------------------	|
| Sample1               	| gs://location/sample_1_R1.fastq.gz 	| gs://location/sample_1_R2.fastq.gz 	| gs://location/hg38_targetCoverage.bed 	|
| Sample2               	| gs://location/sample_2_R1.fastq.gz 	| gs://location/sample_2_R2.fastq.gz 	| gs://location/hg38_targetCoverage.bed 	|


   **Example 3:** sample_set_membership.tsv can be created by grouping sample_ids using this format:

| membership:sample_set_id 	| sample 	|
|-------------------------------	|-------------	|
| Human_single_cell_Expt_1      	| Sample1     	|
| Human_single_cell_Expt_1      	| Sample2     	|


This workspace includes test datasets to help you  become familiar with the workflows

| sample_set_id 	| participants 	| cell type                                             	|
|--------------------	|--------------	|-------------------------------------------------------	|
| HES                	| 3            	| Human single cell                                     	|
| HES_set2           	| 2            	| Human single cell                                     	|
| Human_wgbs         	| 1            	| Human whole genome                                    	|
| MES                	| 4            	| Mouse single cell                                     	|
| Mouse_wgbs         	| 1            	| Mouse whole genome                                    	|
| test_set_mouse_sc  	| 3            	| Small mouse single cell data set(for program testing) 	|


---

### Tools

This workspace contains the following preset method configurations, already set up for grch38, hg19 and mm10.  Other genomes can be loaded by changing the json inputs.  

* Preprocessing tools are run on a ""sample"", selecting one sample or a set of samples.  
* The aggregation tool is run as a ""sample set"" on a set of preprocessed samples aligned to the same genome.  
* Aggregation can only be done after preprocessing.      
* Both preprocessing and aggregation generate html reports (see links to examples below).  

**Preprocessing:**

* wgbs-grch38: Preprocess Whole Genome Bisulfite Sequencing (WGBS) data for GRCh38
* rrbs-grch38: Preprocess Reduced Representation Bisulfite Sequencing (RRBS) data for  GRCh38
* hsbs-grch38: Preprocess Hybrid Selection Bisulfite Sequencing (HSBS) data for  GRCh38

[Example bismark processing report](https://storage.googleapis.com/terra-featured-workspaces/methylation-preprocessing/example_output/test_HES_sample_1_R1.fastq.gz_bismark_report.html)

Similar workflows exist for mm10 and hg19.

**Aggregation:**

* aggregate_bismark_output: Aggregates outputs from preprocessing pipelines and produces an aggregated data structure for further downstream analysis

[Example output from scmeth R package](https://storage.googleapis.com/terra-featured-workspaces/methylation-preprocessing/example_output/qcReport.html)

Based on the reference genome different method configurations could be selected. So far, we have hg38, hg19 and mm10 reference genome versions of all preprocessing and aggregation tools.

---

### Example Time and Cost to Run Workflow

| Sample size |     Per-sample preprocessing |     Aggregation and QC | Total |
| :---:    | :---: | :---: | :---: |
|     | (Hours / $)                  | (Hours / $)            | (Hours / $)        |
| ..................................... | ............................................................................. | .......................................................... | ......................... |
| 10              | 0.98 ($0.93)                 | 0.97 ($0.28)           | 1.95 ($1.21)       |
| 100             | 1.47 ($8.99)                 | 6.00 ($0.86)           | 7.47 ($9.85)       |
| 1000            | 4.48 ($52.48)                | 58.01 ($13.74)         | 62.49 ($66.22)     |


To obtain these estimates, we ran the workflows in FireCloud on the default n1-highmem-4 compute nodes (26 GB RAM with 4 CPUs). Test-run samples were 1000 single-cell RRBS samples with a median of 872,223 reads.

---

### Contact information  

Divy Kangyen, 
Department of Biostatistics
Harvard T.H. Chan School of Public Health
Email address: divyswar01@g.harvard.edu

Issues and feature requests can be submitted to issue tracker in the [dna-methylation-tools github repo](https://github.com/aryeelab/dna-methylation-tools/issues)

Paper associated with the workspace can be found here: https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-2750-4

---

### License  

**Copyright Broad Institute, 2019 | BSD-3**
All code provided in this workspace is released under the WDL open source code license (BSD-3) [full license text here](https://github.com/openwdl/wdl/blob/master/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.

",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","aryee-lab/bisulfite-seq-tools-hg19"
321,"anvil-outreach","MaGIC Jamboree 2020","READER","https://app.terra.bio/#workspaces/anvil-outreach/MaGIC%20Jamboree%202020",TRUE,FALSE,NA,NA,NA,"## MaGIC Jamboree 2020
- Material for `06_exploratory_analysis_with_rstudio`
	- [Worksheet](https://docs.google.com/document/d/1vR555ldiejOaOzlM9TVYUlKzxvab5NrlD5jHzOmvutw)
	- [Rmd script](https://storage.googleapis.com/bioc_jamb_2020/tophitsGlaucOnly.Rmd)
	- RStudio custom environment container string:
			- us.gcr.io/anvil-gcr-public/anvil-rstudio-bioconductor:0.0.5

## About the Workspace

This workspace reproduces the fundamental steps in a genome wide association study (GWAS), using 1,000 Genomes Project¹ (phase 3) genotypes and simulated phenotypes.  

The analysis is structured in two parts:

1. Explore phenotypes and population structure (Jupyter Notebook - Hail/Python)         
2. Test for genetic associations using mixed-models and generate summary visualizations (WDL workflow)    

The output of the notebook (part 1) serves as the input to the workflow (part 2).

Instructions for applying the analyses presented in this workspace on your own data are provided in the penultimate section of this documentation. 


## Notes on data in this workspace  

To demonstrate an analysis that could be run on typical whole genome sequence data, this workspace provides mock phenotype data generated from publicly available 1000 Genomes phase 3 genotypes. Phenotypes have been simulated based on individual genotypes and known associated loci for multiple complex traits. The [GCTA software](https://cnsgenomics.com/software/gcta/#Overview)⁶ was used with lists of causal variants and an estimate of narrow sense heritability⁵ for each phenotype. 

**Traits and sources for causal variants**      
a. BMI: Giant-UKBB meta-analysis²  
b. Fasting glucose: MAGIC³  
c. Fasting insulin: MAGIC³  
d. Waist-to-hip ratio: GIANT-UKBB meta-analysis²  
e. Height: GIANT-UKBB meta-analysis²  
f. HDL: MVP⁴  
g. LDL: MVP⁴  
h. Total cholesterol: MVP⁴  
i. Triglycerides: MVP⁴  

**Generating the synthetic data**    
The scripts used to create the phenotype data, as well as intermediate data files and a readme file, are in a public Google bucket (`gs://terra-featured-workspaces/GWAS/data_processing/`). To browse the Google bucket, [click here](https://console.cloud.google.com/storage/browser/terra-featured-workspaces/GWAS?project=broad-dsde-outreach&organizationId=548622027621&pli=1).     

### Navigating the workspace DATA section
Sample phenotypes and related resources have been uploaded to tables in the DATA section. 

The *sample* table includes 2,504 individuals samples (each a row in the table). Columns correspond to phenotype values for each individual, and a unique sample identifier can be found in the *sample_id* column. This synthetic phenotypic data was added to the data table by uploading a tab-delimited file, and can be downloaded back to file. 

The *Workspace Data* table (under *OTHER DATA*) contains supporting resources used in the analysis in the form of genotype data in different file formats. We will use the VCF files (*vcf_files*) in **Analysis part 1**, and the GDS files (*gds_files*) to run the association workflow. Two different file formats are used because of differences in the requirements for the notebooks and workflows.
 
**Processed data** For reference, a completed sample set and analysis results can be found in the *sample_set* data tab. This represents possible outputs generated by running both the notebook and association workflow in this workspace.

For more information on working with data in the DATA table, see this article - [""Managing data with the data table""](https://support.terra.bio/hc/en-us/articles/360025758392). 

-----
## Analysis part 1: Explore phenotypes and population structure in the 1000 Genomes samples
You will explore the phenotype and genotype data in the **GWAS_initial_analysis** notebook. This notebook will also set up the data table for running a genome-wide association workflow.  Note that there are two versions of the analysis notebook: one that demonstrates a completed analysis (GWAS_initial_analysis_complete), and one with all output data cleared (GWAS_initial_analysis_cleared), which can be used to run the analysis interactively step-by-step.    

**What's in the notebook analysis?** .   
1. Importing phenotypic data from the sample table (in the DATA section) to the notebook environment
2. Exploring and processing these data to understand their underlying structure
3. Defining an outcome and a set of covariates to use when modeling genotype-phenotype associations
4. Importing, exploring, and performing quality control on genotype data
5. Understanding population structure within the 1000 Genomes samples
6. Preparing a full set of input parameters and data for a genome-wide association analysis workflow
7. Configuring the Terra data table to run a workflow

The genetic analyses in this notebook use the [Hail](https://hail.is/) software package. Hail is a framework for distributed computing with a focus on genetics. It is particularly relevant for whole genome sequence ([WGS](https://en.wikipedia.org/wiki/Whole_genome_sequencing)) analysis as it scales extremely efficiently with regard to variant callset and sample sizes.    

The code for these analyses is written in [Python](https://www.python.org/), making use of a few packages including [Pandas](https://pandas.pydata.org/), [Numpy](https://numpy.org/), [Matplotlib](https://matplotlib.org/), and [Hail](https://hail.is/). 
To facilitate downstream analysis, this notebook is set up to save output data to the workspace bucket and write the associated metadata to the data table using the [FireCloud Service Selector (FISS) package](https://github.com/broadinstitute/fiss).     

### Recommended runtime environment (i.e. cluster configuration)

| Option | Value |
| ------ | ------ |
| Environment | Hail: (Python 3.7.6, hail 0.2.30) |
| Profile | Custom |
| Master CPUs | 8 |
| Master Memory | 30 GB |
| Master Disk size | 100 |
| Startup Script | leave blank |
| Spark Cluster | Yes |
| Workers | 4 |
| Preemptible | 0 |
| Workers CPUs | 4 |
| Workers Memory | 15 GB |
| Workers Disk size | 50 GB |

You are able to adjust these VM parameters to fit your computation needs in the Terra interface.      

### Time and cost estimate  
Time to execute all the commands is ~28 minutes which currently costs ~$0.50 to complete (with the recommended cluster configuration). 

-----
-----
## Analysis part 2: Mixed-model association test workflows


### [vcfToGds](https://dockstore.org/workflows/github.com/manning-lab/vcfToGds)

This workflow converts genotype files from Variant Call Format ([VCF](https://en.wikipedia.org/wiki/Variant_Call_Format)) to Genomic Data Structure ([GDS](https://www.biostat.washington.edu/sites/default/files/modules/GDS_intro.pdf)), the input format required by the next workflow.          

**You do not need to run this workflow to apply the analysis on the data provided in this workspace**. GDS files for the 1000 Genomes data have been pre-generated and can be found under the workspace *Data* tab. We provide this tool so you can apply the analyses in this workspace to your own data. 

#### Time and cost estimates    
Note that actual time and cost may vary due to the use of preemptible instances. 

| Sample Size | # Variants | Time | Cost $ |
| -------- | -------- | -------- | ---------- |
| 2,500 samples | 22,000 | 8m | $0.47


### [genesis_GWAS](https://dockstore.org/workflows/github.com/AnalysisCommons/genesis_wdl/genesis_GWAS:v1.4.1?tab=info)

This workflow performs mixed-model association testing and generates statistics and visualizations, using output from the notebook described above as input.    
The workflow steps:
1. Generate a mixed model under the hypothesis of no variant effect using the GENESIS software
2. Generate per-variant association statistics with a phenotype, using the null model    
3. Generate quantile-quantile (QQ) and Manhattan (MH) plots (common descriptive figures for GWAS results). 

Note that output files are stored in the workspace bucket and metadata are populated back to the workspace data model.

#### Time and cost estimates    
Note that actual time and cost may vary due to the use of preemptible instances. 

| Sample Size | Time | Cost |
| -------- | -------- | ---------- |
| 2,500 samples | 26m | $0.57


----
----

## How to run on your own data  
While we have used the 1000 Genomes project genotypes and phenotypes in this workspace, both the notebook and workflow can be adapted to other genetic datasets. The steps for adapting these tools to another dataset are outline below:

**Update the data tables**
To use alternate phenotype data, you would need to upload you own [tab-delimited file to the sample data table](https://support.terra.bio/hc/en-us/articles/360025758392). Each row of this file should correspond to one sample in the genotype data, with a unique sample identifier also present in the genotype files (this is the *sample_id* column in this workspace). You can download the example sample data table to file and use that as a template.

Under the *Workspace Data* tab, you would need to update the VCF and GDS filepaths to point to the genotype files for your sample set. 

**Update the notebook**    
Accommodating other datasets may require modifying many  parts of this notebook. Inherently, the notebook is an interactive analysis where decisions are made as you go. It is **not** recommended that the notebook be applied to another dataset without careful thought. However, the notebook *should run* properly with one modification: in the section where *vcf_paths* is defined, you need to change the Google cloud storage link to point to the dataset you wish to work with. 

**Run an additional workflow**   
To generate GDS files (from the VCF files output by the notebook analysis), we have provided another workflow, **vcfToGds**. This workflow takes a list of VCF files and converts them to GDS files that can be input to the genome-wide association workflow. More details are provided further below. For step-by-step instructions on generating a list file, [see this article](https://support.terra.bio/hc/en-us/articles/360033353952).     

----
----

## Authors and contact information

This workspace is a product of the [Manning Lab](https://manning-lab.github.io/), in collaboration with the [Data Sciences Platform](https://www.broadinstitute.org/data-sciences-platform), at [The Broad Institute of MIT and Harvard](https://www.broadinstitute.org/). Contributing authors include:

* [Tim Majarian](mailto:tmajaria@broadinsitute.org) (Manning Lab)
* Alisa Manning (Manning Lab)
* [Allie Hajian](mailto:jhajian@broadinstitute.org) (DSP User Education team)



## Additional generally helpful resources

* **[Terra knowledge base](https://support.terra.bio/hc/en-us)**   
    Check here for information on setting up billing and security, how to do research on Terra, tutorials and forum.    
		
*  **[Jupyter Notebooks 101](https://app.terra.bio/#workspaces/help-gatk/Jupyter%20Notebooks%20101)**    
    This crash course will help you get started using Jupyter notebooks. It integrates hands-on practice to solidify concepts.    

* **[Hail tutorial notebooks](https://app.terra.bio/#workspaces/help-gatk/Hail-Notebook-Tutorials)**   
    Explore materials from Hail workshops given by the Hail team at the Broad Institute.
		
* **For helpful hints on controlling cloud costs**, see [this article (https://support.terra.bio/hc/en-
us/articles/360029748111)](https://support.terra.bio/hc/en-us/articles/360029748111).      
 
 

 ## License
**Copyright Broad Institute, 2019 | BSD-3**  
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at https://github.com/openwdl/wdl/blob/master/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.
 

 ## Citations
 
1. A global reference for human genetic variation, The 1000 Genomes Project Consortium, Nature 526, 68-74 (01 October 2015) doi:10.1038/nature15393  
2. Yengo L, Sidorenko J, Kemper KE, Zheng Z, Wood AR, Weedon MN, Frayling TM, Hirschhorn J, Yang J, Visscher PM, GIANT Consortium. (2018). Meta-analysis of genome-wide association studies for height and body mass index in ~700,000 individuals of European ancestry. Biorxiv  
3. Scott RA et al. Large-scale association analyses identify new loci influencing glycemic traits and provide insight into the underlying biological pathways. Nat. Genet. 2012;44;9;991-1005  
4. Klarin D, et al. Genetics of blood lipids among ~300,000 multi-ethnic participants of the Million Veteran Program. Nat. Genet. 2018;50:1514–1523. doi: 10.1038/s41588-018-0222-9.  
5. Zheng, et al. LD Hub: a centralized database and web interface to perform LD score regression that maximizes the potential of summary level GWAS data for SNP heritability and genetic correlation analysis. Bioinformatics. 2017 Jan 15;33(2):272-279.  
6. Yang et al. (2011) GCTA: a tool for Genome-wide Complex Trait Analysis. Am J Hum Genet. 88(1): 76-82.  
7. Purcell S, Neale B, Todd-Brown K, Thomas L, Ferreira MAR, Bender D, Maller J, Sklar P, de Bakker PIW, Daly MJ & Sham PC (2007) PLINK: a toolset for whole-genome association and population-based linkage analysis. American Journal of Human Genetics, 81.  
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","anvil-outreach/MaGIC Jamboree 2020"
324,"cell-imaging","cellpainting","READER","https://app.terra.bio/#workspaces/cell-imaging/cellpainting",TRUE,FALSE,NA,NA,NA,"# CellProfiler workflows on Terra

For this demonstration, we will use four plates of image data, metadata, and [CellProfiler](https://cellprofiler.org/) pipelines from:

> [Three million images and morphological profiles of cells treated with matched chemical and genetic perturbations](https://www.biorxiv.org/content/10.1101/2022.01.05.475090v1), Chandrasekaran et al., 2022

The workflows are published in [Dockstore](https://dockstore.org/search?organization=broadinstitute&entryType=workflows&search=cellprofiler), the code is in https://github.com/broadinstitute/cellprofiler-on-Terra, and for any feedback or issues please see [GitHub issues](https://github.com/broadinstitute/cellprofiler-on-Terra/issues).

# How do I get started?

1. Clone this workspace.
    * Need help? See the Terra workspace [video tutorial](https://www.youtube.com/watch?v=mYUNQyAJ6WI) and [docs](https://support.terra.bio/hc/en-us/articles/360026130851-Make-your-own-project-workspace).
2. Run notebook `create_terra_data_tables.ipynb` so that the data tables in your clone are updated to have output result paths in your clone's workspace bucket instead of the source workspace. Use the default environment when creating the Jupyter cloud enviroment.
    * Need help? See the Terra Jupyter notebook [video tutorial](https://www.youtube.com/watch?v=DO7idRZtWkA) and [docs](https://support.terra.bio/hc/en-us/articles/9612453133467).
3. Use Data Table ""plate"" to run the workflows; in this example we selected the 4 plates, but you can also select just one. Run the workflows in the following order:
    * Need help? See the Terra workflow [video tutorial](https://youtu.be/aHqp76vx5V8) and [docs](https://support.terra.bio/hc/en-us/articles/360034701991-Pipelining-with-workflows).
    * `0_create_load_data` with all parameters empty except the following
        * workflow input parameters
            * `create_load_data.config_yaml`: `this.config`
            * `create_load_data.destination_directory_gsurl`: `this.create_load_data_result_destination`
            * `create_load_data.images_directory_gsurl`: `this.images`
            * `create_load_data.plate_id`: `this.plate_id`
        * workflow output parameters
            * `create_load_data.load_data_csv`: `this.load_data_csv`
            * `create_load_data.load_data_with_illum_csv`: `this.load_data_with_illum_csv`
    * `2_cp_illumination_pipeline` with all parameters empty except the following
        * workflow input parameters
            * `cp_illumination_pipeline.cppipe_file`: `this.illum_cppipe`
            * `cp_illumination_pipeline.images_directory_gsurl`: `this.images`
            * `cp_illumination_pipeline.load_data`: `this.load_data_csv`
            * `cp_illumination_pipeline.output_illum_directory_gsurl`: `this.illumination_correction_result_destination`
    * `3_cpd_analysis_pipeline` with all parameters empty except the following
        * workflow input parameters
            * `cpd_analysis_pipeline.cppipe_file`: `this.analysis_cppipe`
            * `cpd_analysis_pipeline.illum_directory_gsurl`: `this.illumination_correction_result_destination`
            * `cpd_analysis_pipeline.images_directory_gsurl`: `this.images`
            * `cpd_analysis_pipeline.load_data_csv`: `this.load_data_with_illum_csv`
            * `cpd_analysis_pipeline.output_directory_gsurl`: `this.analysis_result_destination`
    * `4_cytomining` with all parameters empty except the following
        * workflow input parameters
            * `cytomining.cellprofiler_analysis_directory_gsurl`: `this.analysis_result_destination`
            * `cytomining.output_directory_gsurl`: `this.cytoming_result_destination`
            * `cytomining.plate_id`:`this.plate_id`
            * `cytomining.plate_map_file`: `this.plate_map`


## Estimated time and cost to run on sample data

Sample data consist of a set of 4x 384 well plates, 9 fields of views per well and 8 channels per image. 

Workflow name                 |Time |Batch Cost (Cost per plate) |View a completed run of this workflow
------------------------------|-----|----------------------------|-------------------------------------
0_create_load_data            | 15m | $0.01                      |[Feb 24, 2023, 3:49 PM](https://app.terra.bio/#workspaces/cell-imaging/cellpainting/job_history/79c579d9-d805-495d-998e-dad959195826)
1_cpd_max_projection_pipeline | N/A |                        N/A | max projection adjustment was not needed for this particular sample data
2_cp_illumination_pipeline.   | 10h | $2.68 (~$0.67 per plate)   |[Feb 24, 2023, 4:07 PM](https://app.terra.bio/#workspaces/cell-imaging/cellpainting/job_history/2eed6a96-2edd-4a60-b122-06447b97562e)
3_cpd_analysis_pipeline       |  3h | $51.67 (~$12.92 per plate) |[Feb 25, 2023, 8:27 AM](https://app.terra.bio/#workspaces/cell-imaging/cellpainting/job_history/3061b2c3-e9c0-401e-9cbf-29573ecb262b)
4_cytomining                  |  6h | $1.59 (~$0.40 per plate)   |[Feb 25, 2023, 2:30 PM](https://app.terra.bio/#workspaces/cell-imaging/cellpainting/job_history/d71e0793-c372-4f5b-8a83-f0d4a314139b)


**Notes:**
* Because these plates have been previously analyzed, your workflow run may complete immediately using results pulled from the cache. If you would like the results to be recomputed, uncheck [""Use call caching""](https://support.terra.bio/hc/en-us/articles/360047664872) when you run the workflows on this sample data.
* Check [""Delete intermediate outputs""](https://support.terra.bio/hc/en-us/articles/360039681632) to automatically delete the large intermediate files produced by these workflows.


---
# What's in this workspace?

---
## Workflows

### 0_create_load_data workflow

This workflow creates `load_data.csv` and `load_data_with_illum.csv` files that CellProfiler uses for loading each of the images for analysis.

- `load_data.csv` file contains the metadata of the images related to the acquisition as well as the location of the images. This file is required for running `cpd_max_proyection_pipeline` and `cp_illumination_pipeline` workflows.

- `load_data_with_illum.csv` same info than `load_data.csv` plus the information of the illumination correction images (.npy files) and their location. This file is required for running `cpd_max_proyection_pipeline` and `cpd_analysis_pipeline` workflows .

This workflow requires as inputs: 
   - `images_directory_gsurl`: the folder (bucket) where the images are, an XML file from the microscope that is expected to be in the same folder where the images are.
   - `config_yaml`: a config YAML file where the microscope channels information is specified. See [here](https://raw.githubusercontent.com/broadinstitute/pe2loaddata/master/config.yml) for an example YAML file. 
   - `destination_directory_gsurl`: bucket where the outputs (`load_data.csv` and `load_data_with_illum.csv`) will be saved. 

Users can skip this step if they opt to create those files either manually and then exporting image set listing using the cellprofiler GUI, or using other available resources i.e. [pe2loaddata](https://github.com/broadinstitute/pe2loaddata) 


### 1_cpd_max_projection_pipeline workflow

This workflow runs the CellProfiler maximum intensity projection pipeline distributed.
This workflow is optional, and it is meant to be used just when more than one plane of the fields ow views have being acquired.
The required inputs are:
- `cppipe_file`: CellProfiler pipeline `.cppipe` that performs the projection. 
- `images_directory_gsurl`: bucket where the inputs images are located.
- `load_data.csv`: file from `0_create_load_data` workflow.
- `load_data_with_illum.csv`: file from `0_create_load_data` workflow.
- `output_images_directory_gsurl`: bucket where the projected images are saved. 
  Additionally, in this folder there will be a new version of `load_data.csv` and `load_data_with_illum.csv` created to point to the new list of projected images.
  
Note: if this workflow is run, the images used for the subsequent analysis will be the outputs of this workflow, and the updated versions of the files `load_data.csv` and `load_data_with_illum.csv`. 

### 2_cp_illumination_pipeline workflow
This workflow runs the CellProfiler [illumination correction pipeline](https://cellprofiler.org/previous-examples#illumination-correction) in one VM (not distributed).
The required inputs are:
- `cppipe_file`: CellProfiler pipeline `.cppipe` that performs the illumination correction. 
- `images_directory_gsurl`: bucket where the inputs images are located.
- `load_data.csv`

Optional (but recommended):
- `output_illum_directory_gsurl`: bucket where the generated `.npy` images will be saved; by default they will be saved in the folder where the input images are located.  

### 3_cpd_analysis_pipeline workflow
This workflow runs the main CellProfiler pipeline distributed, which usually does the cell segmentation and measures the CellPainting features.
The required inputs are:
- `cppipe_file`: CellProfiler pipeline `.cppipe` that performs the analysis. 
- `images_directory_gsurl`: bucket where the inputs images are located.
- `load_data_csv`: if the `cp_illumination_pipeline` workflow was run, the input should be the file `load_data_with_illum.csv`.  
- `output_directory_gsurl`: bucket where the generated output from CellProfiler will be placed
  
Optionals:
- `cellprofiler_docker_image`: CellProfiler docker image the workflow uses. The version should match the version of CellProfiler your created your pipeline. By default, it uses: `cellprofiler/cellprofiler:4.2.1` 

### 4_cytomining workflow

This workflow runs the [cytominer-database](https://github.com/cytomining/cytominer-database) ingest step to create a SQLite database containing all the extracted features and the aggregation step from [pycytominer](https://github.com/cytomining/pycytominer) to create CSV files.
The required inputs are:

- `cellprofiler_analysis_directory_gsurl`: bucket where the inputs images are located.
- `output_directory_gsurl`: bucket where the generated output from CellProfiler will be placed
- `plate_id`: unique plate identifier given during the experimental image acquisition. 
- `plate_map_file`: additional metadata to be incorporated into the final outputs.  

---
## Data tables
[Data tables](https://support.terra.bio/hc/en-us/articles/360025758392-Managing-data-with-tables-) are used to define the collection of workflow instances to be run. **NOTE** Be sure to run notebook `create_terra_data_table.ipynb` so that the data tables in your clone are updated to have output result paths in your clone's workspace bucket instead of the source workspace.

### plate
Use Data Table ""plate"" to run workflows  `0_create_load_data`, `2_cp_illumination_pipeline`, `3_cpd_analysis_pipeline`, and `4_cytomining`.

---
## Files
Browse the files in the workspace bucket to see what is held in this workspace or your workspace clone. You can do this either via ""Open bucket in browser"" on the right hand side of this page or by clicking on ""Files"" in the lower left corner of the Data tab.

In this source workspace, you will see directories like:
```
0_create_load_data/                     # The resulting load_data.csv and load_data_with_illum.csv files resulting from the workflow run of pe2loaddata.
2_cp_illumination_pipeline/             # The resulting *.npy files resulting from the workflow run CellProfile illumination correction.
3_cpd_analysis_pipeline/                # The resulting CSV and PNG files from the workflow run of CellProfiler analysis.
4_cytomining/                           # The resulting SQLlite file from the workflow run of Cytomining.
cellprofiler_pipelines                  # CellProfiler pipeline definition files.
notebooks/                              # Jupyter notebooks
pe2loaddata_config/                     # pe2loaddata configuration file for all four plates.
plate_maps/                             # TSV plate maps and also the plate map catalog for the larger experiment.
source_4_images/                        # The Index.xml and *.tiff files for four plates.
submissions/                            # Execution directory of each of the submitted workflows; where you can find the stderr, stdout, and backend logs.
```

**Note:** In your cloned workspace, you will only see the `notebooks` directory at first. The data tables in your workspace clone will be referring to files stored in the source workspace.

---
## Notebooks

### `transfer_cell_profiler_inputs.ipynb`
This notebook was used to transfer metadata such as plate maps from GitHub and plate data to the source workspace bucket. There is no need to run this notebook in your clone. When you run the workflows in your clone, they can read input files from the source workspace bucket.

### `create_terra_data_table.ipynb`
This notebook created the Terra Data Table to provide the workflow input and output parameters. **Run this notebook in your clone so that the output destination paths are updated to be your clone's workspace bucket.** This notebook takes less than a minute to run.



---
# How to use the workflows on your own data

1. Place the input files in your Terra workspace bucket. See the documentation for the various tools for transferring data to Google Cloud Storage such as [gsutil cp](https://cloud.google.com/storage/docs/interoperability#gsutil_command_line) or the [Storage Transfer Service](https://cloud.google.com/storage-transfer/docs/overview).
2. Make the Data table for all the workflow instances you wish to run. You can use Excel, GoogleSheets, or a notebook like `create_terra_data_table.ipynb`. The key thing is that the column name for the first column must have prefix `entity:` and suffix `_id`.
3. Run the workflows!
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","cell-imaging/cellpainting"
325,"ucsc-idgc","UCSC_UShER","READER","https://app.terra.bio/#workspaces/ucsc-idgc/UCSC_UShER",TRUE,FALSE,NA,NA,NA,"Use the UShER and matUtils suite of tools to place your project's consensus fasta sequences on the publicly available SARS-CoV-2 Phylogeny

The UShER code base is publicly available on GitHub: https://github.com/yatisht/usher, where it is being actively developed.  The Snakemake workflow examples described in that repository were used as the concept to create the WDL workflow in this Terra Workspace.

A separate GitHub repository for this UShER WDL Workflow is available here as part of the University of California, Santa Cruz's Genomics Institute Pathogen Genomics Project:  https://github.com/pathogen-genomics/usher-public-workflow.

For convenience the UShER WDL workflow is also available on Dockstore: https://dockstore.org/workflows/github.com/pathogen-genomics/usher-public-workflow/publicTreeSamplePlacement:main?tab=info.

The UShER WDL workflow uses the UShER code via the latest version of the UShER Docker container.

**REQUIREMENTS:**
The input for UShER is a fasta file of concatenated consensus.fasta files from you samples.",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","ucsc-idgc/UCSC_UShER"
326,"warp-pipelines","BICAN-references","READER","https://app.terra.bio/#workspaces/warp-pipelines/BICAN-references",TRUE,FALSE,NA,NA,NA,"Public workspace for evaluating in-development references for the BICAN Sequencing Working Group.",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","warp-pipelines/BICAN-references"
327,"veme-training","VEME 2023 Pathogen Dashboards","READER","https://app.terra.bio/#workspaces/veme-training/VEME%202023%20Pathogen%20Dashboards",TRUE,FALSE,NA,NA,NA,"![](https://user-images.githubusercontent.com/48869631/261059152-e1c6b505-8f16-4881-9744-1900b4d7ae83.png)

# TRACKING PATHOGENS USING INTERACTIVE DASHBOARDS

This is the workspace of the Half day module at VEME 2023 on how to reproduce dashboards using the Genomic Dash Framework.
Genomic Dash Framework is dedicated to facilitating the development of interactive visualizations using Python Open Source Libraries. 

In this tutorial, we are going to prepare a Dengue virus 1 dataset for the framework. In order to make it simple we are going to focus on the African continent but the framework also works for general proposes. 

## DATA

The data used for this workshop was downloaded from Bacterial and Viral Bioinformatics Resource Center (BV-BRC) which includes the VIPR database (https://www.bv-brc.org/) using the following criterias: 

1. Search for pathogen (Dengue).
2. Select genomes (which provides metadata for the respective pathogen).
3. Download the global dataset as an excel file that can be fed into the pre-processing Jupyter Notebook.

## TUTORIAL

**Generating dataset to run on the framework**
To use Genomic Dash Framework you have to follow a template to produce your metadata. The metadata file has to have at least the following collumns: 

* category_1: This should be the pathogen of your dashboard. For this tutorial we are going to use Dengue 1 (DENV-1)
* category_2: The sub-categories you want to show, named as will be shown in the dashboard
* date: date formatted as YYYY-MM-DD
* country


**The first step to run the application is to download the files from the genomic-dash repository.**

1. Go to https://github.com/BIA-lab/genomic-dash/tree/main
2. Click the green ""Code"" button
3. Download the repository as a ZIP file.
4. Extract the ZIP file into the Desktop screen.
5. Once you have the repository extracted, you will be able to browse through the dashboard code files.


**The second step to run is providing the datasets with the pathogens to be displayed in the dashboard:**

1. Download the datasets you've generated using the Jupyter Notebook in the Terra workspace. In this case it is the ""dengue_1.csv"" file.

2. Browse to the climade-dash-main folder, then to the data folder, and create a folder called ""dengue"".

3. Copy the dataset file (dengue_1.csv), from the ""Downloads"" folder and paste it inside the ""dengue"" folder you've just created.


4. Edit the python files inside the ""climade-dash-main"" repository folder. To edit the code you can use any code editor or IDE. If you don't have any of them you could download and install one of them. Suggestions: Pycharm, VScode) 

* Change the dataset location (line 21) inside the ""pathogen_1.py"" file. ""The pathogen_1.py"" file is located insode the ""pages""        folder.

* If you wish, You can also change the page title on the ""header.py"" file (line 7), located inside the ""pages"" folder of the         repository. And also you could change the menu title on the ""pathogen_1.py"" file (line 29).

* In the ""dicts.py"" file, fill the ""dengue_categories"" list with the names of the different types of category within the                           ""category_2"" dataset column (unique category_2 values).

* Finally, to run the application follow the instructions given in the file README.md to build the docker image and to run the container.

* Once the dashboard is running, open your browser and type ""localhost:8511"" to check out the dashboard.

 ",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","veme-training/VEME 2023 Pathogen Dashboards"
328,"tidal-waves","Peat demo","READER","https://app.terra.bio/#workspaces/tidal-waves/Peat%20demo",TRUE,FALSE,NA,NA,NA,"Demo of how to use [Peat](https://broad.io/peat) to save overhead by grouping jobs into fewer WDL scatter branches.
To compare scatter with and without Peat, this workspace has two simple demo workflows using WDL scatter, one with and one without using Peat.


## Overview
WDL scatter with lots of branches can incur a large overhead, especially when each branch only has a small payload task. Each branch will involves starting up a VM, pulling a docker image and typically copying needed data to local disk.

The solution is to group jobs and run multiple jobs in the same WDL branch. Making sure that each job ends up being run once and only once in an ad hoc manner is cumbersome and error-prone, especially when trying to do another run with different numbers of jobs or groups.

Peat is designed to make this as easy as possible, especially when used from within a WDL.

## Data
No data in this workspace. The workflows run without input data.
## Workflows
There are two workflows to compare, ScatterWithoutPeat and ScatterWithPeat. They produce the same end result, but as the name suggests, the first uses WDL scatter without Peat, and the second wit Peat.

To make it as simple as possible, the workflows need no input data, just parameters. A ""job"" consists of a simple echo command that writes a line to a file, and then there is an extra reduce task that concatenates all the files into a single output file.

Workflow ScatterWithoutPeat does a scatter over all jobs, so each scatter branch is one job.

Workflow ScatterWithPeat groups jobs and does a scatter where each scatter branch runs a group of jobs managed by Peat.

The end result is the same.

Inputs for both workflows are the number of jobs and the name of the final output file. ScatterWithPeat in addition also takes the number of groups as input.

## Runtimes and costs

Both workflows run the same payload jobs and produce the same final output.

### ScatterWithoutPeat

Performs a simple job (writing a line to a file) many times via simple WDL scatter, then additionally concatenates all files into a single output file.

|   n_jobs  |  time  |  cost $  |  link  |
|-------------|---------|----------|-------|
| 1000 |  0:50  |  2.86  | [link](https://app.terra.bio/#workspaces/tidal-waves/Peat%20demo/job_history/b998a7af-f95d-4f26-b527-c8c20073e2e5) |
| 1200 |  0:56  |  3.50  | [link](https://app.terra.bio/#workspaces/tidal-waves/Peat%20demo/job_history/343fa865-4860-4070-bf4f-545728b395b6) |
| 1500  |  1:05   |  4.61  | [link](https://app.terra.bio/#workspaces/tidal-waves/Peat%20demo/job_history/3ed86f59-7973-40e7-9e3d-fdde61ea3cc4)  |
|  2000  |  1:22  |  6.71  | [link](https://app.terra.bio/#workspaces/tidal-waves/Peat%20demo/job_history/081fc2e5-00c4-44dc-8c58-423d742a38c9)  |

### ScatterWithPeat

Performs the same job, but using Peat to run multiple jobs on each WDL scatter branch, then additionally concatenates all files into a single output file.

|   n_jobs  | n_groups  |  time  |  cost $  |  job  |
|-------------|--------------|---------|----------|-------|
|  1000  |   50  |  0:30  |  0.16  |  [link](https://app.terra.bio/#workspaces/tidal-waves/Peat%20demo/job_history/21b9d7a0-d0dd-43c6-ae88-b07c91f429db)  |
|  1200  |   50  |  0:30  |  0.19  |  [link](https://app.terra.bio/#workspaces/tidal-waves/Peat%20demo/job_history/9733ac86-2308-4523-bd60-0b07628cc09a)  |
|  1500  |  50  |  0:11  |  0.15  |  [link](https://app.terra.bio/#workspaces/tidal-waves/Peat%20demo/job_history/bb7f8512-6693-4cb9-a948-f0a2b49b62f6)  |
|  2000  |  50  |  0:13  |  0.15  |  [link](https://app.terra.bio/#workspaces/tidal-waves/Peat%20demo/job_history/8c172bf5-47e3-4b7d-a2c6-e7de3199d666)  |

## Source and License

The WDL code for both workflows is also available in the [Peat GitHub repository](https://github.com/broadinstitute/peat/tree/main/wdl). License is BSD-3-clause.

## Contact
Oliver Ruebenacker <oliverr@broadinstitute.org>
## Workflow change log
| Date | Change | Author|
| --- | --- | --- |
| 2021-05-12  |  Adding benchmarks  |  Oliver Ruebenacker  |
| 2021-05-11 | Initial setup | Oliver Ruebenacker |


![Peat logo](https://github.com/broadinstitute/peat/raw/main/logo/peat.png)
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","tidal-waves/Peat demo"
329,"anvil-datastorage","AnVIL_T2T_CHRY","READER","https://app.terra.bio/#workspaces/anvil-datastorage/AnVIL_T2T_CHRY",TRUE,TRUE,"None","Population sampling","Globally diverse","# Telomere-to-Telomere (T2T) Consortium's AnVIL_T2T_CHRY Workspace

<p align=""center""><img src=""https://schatz-lab.org/images/t2tlogo.png"" width=""50%""></p>


The <a href=""https://sites.google.com/ucsc.edu/t2tworkinggroup"" target=""_blank"">Telomere-to-Telomere (T2T) consortium</a> is an open, community-based effort to de novo assemble the first complete reference human genome from the CHM13 hydatidiform mole. Using a combination of PacBio HiFi sequencing and Oxford Nanopore ultra long reads, the CHM13v1.1 reference genome was completed and released in 2022 as described in <a href=""https://www.science.org/doi/10.1126/science.abj6987"" target=""_blank"">""The complete sequence of a human genome"" (Nurk et al, Science, 2022)</a>. This assembly is the most accurate assembly ever produced, with an estimated sequence accuracy exceeding QV70 and 0 unresolved bases. The genome includes almost 200 Mbp of novel sequence compared to GRCh38, corrects many structural errors in the GRCh38 reference genome, and unlocks the most complex regions of the genome to clinical and functional study for the first time.<br>

Recently, the T2T consortium finished the first complete assembly of a human Y chromosome, from the Genome in a Bottle (GIAB) HG002/NA24385 sample. This chrY assembly adds over 30 Mbp of novel sequence relative to the GRCh38 chrY, and its addition completes the CHM13v2.0 (hs1) reference genome. The full details of the assembly are described in <a href=""https://www.biorxiv.org/content/10.1101/2022.12.01.518724v1"" target=""_blank"">""The complete sequence of a human Y chromosome"" (Rhie et al, bioRxiv, 2022)</a>

### Currently Available Data
Here we use the T2T-CHM13v2.0 reference genome to investigate how it improves short-read variant calling for individual samples and across populations. This includes all 3,202 samples from the recently extended 1000 Genomes Project (1KGP) collection and 279 open-access samples from the Simons Genome Diversity Project (SGDP) collection that we analyze using the GATK HaplotypeCaller for SNVs and indels on the NHGRI AnVIL Cloud Platform (1,599 and 175 XY samples respectively). To facilitate comparison between references, we perform alignment and variant calling for the SGDP samples using both the  T2T-CHM13v2.0 and GRCh38 references. Data for both references are included in this workspace in separate tables.  For the 1KGP samples, short-read alignment and variant calling was previously performed on T2T-CHM13v1.0 (which included the GRCh38-Y)—available <a href=""https://anvil.terra.bio/#workspaces/anvil-datastorage/AnVIL_T2T"" target=""_blank"">here</a>—and so only alignments and variant calls on T2T-CHM13v2.0 are provided for these samples in this workspace.

We demonstrate that the T2T-Y reference improves read mapping and variant calling across all samples in a number of major ways:
1. Adds almost 600 thousand base pairs of sequence that can be effectively used for variant calling with short reads
2. Corrects putative collapsed duplications on GRCh38-Y, reducing spurious variant calls within these regions
3. Improves short-read alignment across populations for a number of statistics, including reads mapped and per-base mismatch rate

#### A note on sex chromosome complement-aware alignment
In our analysis, we performed alignment separately for XX and XY samples using a sex chromosome complement-aware reference. For XX samples, the entire Y chromosome is masked, as there should be no reads originating from the Y chromosome. This reference is labeled `CHM13v2_XX_fasta` in the Workspace Data tab. For XY samples, the Y PAR is masked, thereby forcing any reads originating from the Y PAR to align instead to the homologous X PAR. This reference is labeled `CHM13v2_XY_fasta` in the Workspace Data tab.
During variant calling, we set the expected ploidy based on sample sex chromosome complement. For XX samples, all variants on the X chromosome are called as diploid. For XY samples, all variants in the non-PAR regions of the X chromosome and Y chromosome are called as haploid. Variants in the X PAR are called as diploid, as this region captures variation in both the X PAR and Y PAR.<br><br>
We provide short-read alignments in the compressed CRAM format, which requires the reference assembly to de-compress. As a result of how we performed alignment, users would need to use a separate reference for XX and XY samples (described above) when extracting reads from the sex chromosomes. To eliminate this need and to improve usability for the end user (you), we re-encoded the alignments to use a single un-masked T2T-CHM13v2.0 assembly as their reference (labeled `CHM13v2_fasta` in the Workspace Data tab). These re-encoded CRAMs are what are provided in this repository.<br><br>
If you plan on using these alignments for any downstream analyses (such as variant calling), we recommend that you analyze XX and XY samples separately, using the appropriate masked references described here.

The WDL-based workflows for aligning the samples and analyzing the variants are available at <a href=""https://github.com/schatzlab/t2t-chm13-chry"" target=""_blank"">https://github.com/schatzlab/t2t-chm13-chry</a>

### The Paper
For more information on this analysis and others performed on the complete Y chromosome assembly, please see our paper:

<a href=""https://doi.org/10.1101/2022.12.01.518724"" target=""_blank"">The complete sequence of a human Y chromosome</a><br>
Arang Rhie†, Sergey Nurk†, Monika Cechova†, Savannah J. Hoyt†, Dylan J. Taylor†, Nicolas Altemose, Paul W. Hook, Sergey Koren, Mikko Rautiainen, Ivan A. Alexandrov, Jamie Allen, Mobin Asri, Andrey V. Bzikadze, Nae-Chyun Chen, Chen-Shan Chin, Mark Diekhans, Paul Flicek, Giulio Formenti, Arkarachai Fungtammasan, Carlos Garcia Giron, Erik Garrison, Ariel Gershman, Jennifer Gerton, Patrick G.S. Grady, Andrea Guarracino, Leanne Haggerty, Reza Halabian, Nancy F. Hansen, Robert Harris, Gabrielle A. Hartley, William T. Harvey, Marina Haukness, Jakob Heinz, Thibaut Hourlier, Robert M. Hubley, Sarah E. Hunt, Stephen Hwang, Miten Jain, Rupesh K. Kesharwani, Alexandra P. Lewis, Heng Li, Glennis A. Logsdon, Julian K. Lucas, Wojciech Makalowski, Christopher Markovic, Fergal J. Martin, Ann M. Mc Cartney, Rajiv C. McCoy, Jennifer McDaniel, Brandy M. McNulty, Paul Medvedev, Alla Mikheenko, Katherine M. Munson, Terence D. Murphy, Hugh E. Olsen, Nathan D. Olson, Luis F. Paulin, David Porubsky, Tamara Potapova, Fedor Ryabov, Steven L. Salzberg, Michael E.G. Sauria, Fritz J. Sedlazeck, Kishwar Shafin, Valery A. Shepelev, Alaina Shumate, Jessica M. Storer, Likhitha Surapaneni, Angela M. Taravella Oill, Françoise Thibaud-Nissen, Winston Timp, Marta Tomaszkiewicz, Mitchell R. Vollger, Brian P. Walenz, Allison C. Watwood, Matthias H. Weissensteiner, Aaron M. Wenger, Melissa A. Wilson, Samantha Zarate, Yiming Zhu, Justin M. Zook, Evan E. Eichler, Rachel O'Neill, Michael C. Schatz, Karen H. Miga, Kateryna D. Makova, Adam M. Phillippy* (2023)",NA,"Michael Schatz","The workspace contains the reanalysis of all 3202 samples from the 1000 Genomes project (Byrska-Bishop et al, Cell, 2022) along with 279 open access genomes from the Simons Genome Diversity Project (Mallick et al, Nature, 2016) using the newly assembled CHM13v2.0 assembly, which contains a T2T sequence for chromosome Y","3481","N/A","T2T",NA,"NRES","CHM13v2.0","T2T_CHRY","Whole Genome","anvil-datastorage/AnVIL_T2T_CHRY"
330,"darwins-ark","Darwins Ark - Data Release 2022","READER","https://app.terra.bio/#workspaces/darwins-ark/Darwins%20Ark%20-%20Data%20Release%202022",TRUE,FALSE,NA,NA,NA,"# Darwin's Ark - Data Release 2022

## About

**[Darwin's Ark](https://darwinsark.org/)** is a collaborative scientific initiative that works with pet owners to understand the interaction of genetics and environment in behavior and health. The **Darwin's Dogs** project focuses on dog behavioral genomics.

## Contact

Please contact staff@darwinsark.org for general questions about the project and associated data.

For Terra workspace specific questions, please contact the workspace owner, Kathleen Morrill (kmorrill@broadinstitute.org)

## Information

### Data Release 2022: v.1.0, 2019-11-15

This workspace reflects a data freeze of the Darwin's Dogs project on November 15th, 2019.

The data for this release is filed under  `DarwinsArk_Dogs_Release_v.1.0_20191115` in the workspace bucket.

#### Citation

The data presented in this workspace reflects the data release for the following manuscript:

> Kathleen Morrill, Jessica Hekman, Xue Li, Jesse McClure, Brittney Logan, Linda Goodman, Mingshi Gao, Yinan Dong, Marjie Alonso, Elena Carmichael, Noah Snyder-Mackler, Jacob Alonso, Hyun Ji Noh, Jeremy Johnson, Michele Koltookian, Charlie Lieu, Kate Megquier, Ross Swofford, Jason Turner-Maier, Michelle E. White, Zhiping Weng, Andrés Colubri, Diane P. Genereux, Kathryn A. Lord, Elinor K. Karlsson. Ancestry-inclusive dog genomics challenges popular breed stereotypes. *Science*. 2022.

For questions about the study, please contact either corresponding author, Kathleen Morrill (kathleen.morrill@umassmed.edu) or Elinor Karlsson (elinor.karlsson@umassmed.edu), or reach out to staff@darwinsark.org for questions about the project and associated data.

Data files for this manuscript are also available through Data Dryad (DOI: [10.5061/dryad.g4f4qrfr0](https://doi.org/10.5061/dryad.g4f4qrfr0)) but only this workspace contains the archive of GWAS summary statistics. 

#### Download

In addition to copying this workspace and its bucket data, individual data files can be downloaded locally. Keep in mind that the data requestor pays the costs associated with downloading data.

#### Surveys

We presented owners with 22 surveys composed of 10-12 questions, for which any number or order can be answered. The majority offered response choices of agreement with statements presented or the frequency of behavior in question on a 5-point Likert scale. Among these surveys, 123 questions were sourced from published and validated canine behavioral and health surveys, including the Dog Personality Questionnaire (DPQ / DPQL), the Canine Health-related Quality of Life Survey (CHQLS), the Dog Impulsivity Assessment Scale (DIAS), the Canine Cognitive Dysfunction Rating scale (CCDR), and the Certified Dog Trainer Test (CDTT, International Association of Canine Professionals). The remainder of surveys were developed in collaboration with the International Association of Animal Behavior Consultants (IAABC) (MBT) or include original questions about personality (NEO Five-Factor Inventory). We also surveyed owners about their dog's physical characteristics.

#### Survey Data Files

##### Dogs

`DarwinsArk_20191115_dogs.csv` contains profile information on the dogs enrolled in the Darwin's Ark project.

##### Questions

`DarwinsArk_20191115_questions.csv` contains the first 110 behavioral survey questions and physical trait questions.

##### Answers

`DarwinsArk_20191115_answers.csv` contains the survey responses.

##### Factors and Factor Scores

We performed exploratory factor analysis on the first 110 survey questions for 10,253 dogs with complete survey data available. The first 8 factors explained a cumulative 24.26% of variance. Factor scores were generated for an additional 6,269 dogs having under 20% of missing data filled by random sample. We also calculated the mean age of each dog per factor from the ages at survey response to questions included in each factor.

**Factor 1 (“Human Sociability”)** captures questions about social interactions with people.

**Factor 2 (“Arousal Level”)** included questions about a dog’s reaction to excitement.

**Factor 3 (“Toy-directed Motor Patterns”)** included questions about engaging with toys and objects which may represent underlying differences in canine motor patterns.

**Factor 4 (“Biddability”)** represented ease of training and amenability to trained behaviors.

**Factor 5 (“Agonistic Threshold”)** described the conditions and contexts in which a dog may apply agonistic behavior.

**Factor 6 (“Dog Sociability”)** highlighted dog-directed social interactions.

**Factor 7 (“Environmental Engagement”)** covers questions about a dog’s responsiveness to their surroundings.

**Factor 8 (“Proximity Seeking”)** illustrates behaviors aimed at human contact and proximity.

#### Genetics

##### Genotyping Arrays

440 dogs underwent genotyping on the Axiom Canine Genotyping Array Set A & B for 1,268,920 variant call sites (1,267,416 SNPs and 1,504 indels). In the `sample` table, these samples are labeled ""array"" under data type.

##### Low-pass Sequencing

1,715 dogs enrolled were sequenced at coverages of 0.5x to 1.1x depth on the Gencove sequencing platform. In the `sample` table, these samples are labeled ""lowpass"" under data type.

##### Imputation

The imputation reference panel is a set of 32,438,672 SNPs and 13,910,371 indels jointly variant called from publicly available whole genome sequence data (mean coverage 22.9 ± 14.2x) for 435 canids and representing 287 dogs of known pure breed ancestry, 6 dogs of unknown ancestry, 100 worldwide indigenous or village dogs, 36 wolves, and 6 other wild canids.

Autosomal genotypes were imputed for all 2,155 dogs using Gencove's *loimpute* software, see:

>  K. Wasik, T. Berisa, J. K. Pickrell, J. H. Li, D. J. Fraser, K. King, C. Cox, Comparing low-pass sequencing and genotyping for trait mapping in pharmacogenetics. BMC Genomics. 22, 197 (2021).

#### Genetic Data Files

##### GWAS Data Set

For each sample processed by low-pass sequencing or genotyping array with imputation, genotypes with genotype probability below 70% were removed. Then, all VCFs were merged and converted to a PLINK data set. SNPs below a minor allele frequency of 2% and missing in over 20% of individuals were filtered out. Due to the diverse and admixed breed ancestry composition of the data set, only SNPs with extreme deviation from Hardy-Weinberg equilibrium, given p-values below 1e-20 in the exact test with mid-p adjustment and at observed/expected heterozygosity ratios under 0.25 or above 1.0, were excluded. After filtering data, 8,518,951 SNPs and 2,155 dogs remained with a total genotyping rate of 97.5%. Owner-reported sexes were encoded in the sample information file, confirmed by relative X-chromosome coverage for sequencing data and the autosomal genotypes of X for genotyping data; in total, 1084 males and 1071 females.

##### GWAS Summary Statistics

The results of genome-wide associations performed for this study using the data listed below are included in the workspace bucket in the bzip2-compressed archive, `DarwinsArk_20191115_GWAS_SummaryStatistics.tar.bz2`.

##### Breed Ancestry

We inferred global breed ancestry using the supervised mode of *ADMIXTURE* and a reference panel of 101 dog breeds with 12 dogs represented per breed. The results are given in `DarwinsArk_2019115_breedcalls.csv`.

",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","darwins-ark/Darwins Ark - Data Release 2022"
331,"broad-firecloud-tcga","TCGA_ACC_OpenAccess_V1-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_ACC_OpenAccess_V1-0_DATA",TRUE,TRUE,"Adrenocortical carcinoma","Tumor/Normal","USA","TCGA Adrenocortical carcinoma Open-Access Data Workspace","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients’ clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","92","Adrenal Gland","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh37/hg19","TCGA_ACC_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_ACC_OpenAccess_V1-0_DATA"
332,"fccredits-silver-tan-7621","CCLE_WGS","READER","https://app.terra.bio/#workspaces/fccredits-silver-tan-7621/CCLE_WGS",TRUE,FALSE,NA,NA,NA,"# CCLE WGS
This workspace contains the WGS data for the cell lines from [the CCLE2 paper](doi:10.1038/s41586-019-1186-3) which have been sequenced at the Broad Institute and have derivative data released publically. Specifically, some cell lines did not have WGS profiles at the time of this paper's publishing, but they have since been gapfilled and made available for download. As we continue to work on sequencing all cell lines using WGS, we will update this workspace as more become available.
(for access to the Sanger lines, please visit https://cellmodelpassports.sanger.ac.uk/)_


Next-generation characterization of the Cancer Cell Line Encyclopedia
Ghandi, M., Huang F. et al.
Nature doi:10.1038/s41586-019-1186-3 / May 8, 2019


> Maintained by Simone Zhang & Alvin Qin

## Using the data
To view the paths to the data you would need to **clone** the workspace using **your own billing account** and use it as a base for your own workspace (from which you can analyze the data).

Otherwise:
1. you can download any file from Google Cloud storage using the gs:// link and **gsutil** from your terminal (links can be accessed by downloading the data as csv). Since the bam and bai files are stored in requester-pays buckets, you need to provide a billing project to access the files using commands such as **gsutil -u [name of your billing project] cp gs://cclebams/rnasq_hg38/CDS-xxxxxx.bam .**
2. you can access the **CCLE_WGS** workspace programatically using python's firecloud-dalmatian

### Note: You might not be able to view file details when you click on file links in the data table in this workspace. This is expected, and does not suggest that the file paths are corrupted, since loading metadata from requester-pays buckets requires a billing project and this workspace isn't attached to any. However, you should be able to view them once you've cloned this workspace under your billing project.

## Remarks

1. Most of the bam files have been reprocessed and **might be slightly different from the original CCLE2 bam** files. We do not expect it to change any results obtained from the original bam files.
2. reprocessed bam files were reprocessed using the **PreProcessingForVariantDiscovery_GATK4** workflow version 8 with gatk:4.beta.3 version

More information in our **Broadinstitute/DepMap_Omics** github repo.",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","fccredits-silver-tan-7621/CCLE_WGS"
333,"featured-workspaces-hca","HCA Smart-seq2 Multi Sample Pipeline","READER","https://app.terra.bio/#workspaces/featured-workspaces-hca/HCA%20Smart-seq2%20Multi%20Sample%20Pipeline",TRUE,FALSE,NA,NA,NA,"## Deprecation notice 9/12/2024
We are deprecating the Smart-seq2 workflow. For an alternative, please see the [Smartseq2 Single Nucleus pipeline](https://app.terra.bio/#workspaces/featured-workspaces-azure-sc/Smart-seq2-Single-Nucleus-Multi-Sample).

# Smart-seq2 Pipeline for Analysis of Full-Length Single Cell Transcriptomic Data


The Smart-seq2 Multi-Sample pipeline, developed by the Data Coordination Platform (DCP) team of the Human Cell Atlas (HCA), processes full-length, single-cell transcriptome data generated with Smart-seq2 assays. 

The pipeline has been validated for analyzing both human and mouse datasets. This workspace currently describes `v2.2.21` of the Smart-seq2 Multi-Sample pipeline and provides a fully reproducible example of the workflow using both human and mouse example data. 

The current workflow processes multiple cells by importing and running the Smart-seq2 Single Sample workflow for each individual cell and then merging the resulting Loom output files. For more details about the pipeline, read the [Smart-seq2 Multi-Sample Overview ](https://broadinstitute.github.io/warp/docs/Pipelines/Smart-seq2_Multi_Sample_Pipeline/README) in the [WARP documentation](https://broadinstitute.github.io/warp/). A high-level overview of the Smart-seq2 pipeline can also be found on the [HCA Data Portal](https://data.humancellatlas.org/pipelines/smart-seq2-workflow).

Scroll down for details on the workflow, including example data, input, and output descriptions, estimated run times,  and costs.      

**Note that cost and time estimates will vary** with the use of [Preemptibles](https://cloud.google.com/compute/docs/instances/preemptible).  
You can also use [Google's cost calculator](https://cloud.google.com/products/calculator/) to estimate cost to run.  **For helpful hints on controlling Cloud costs**, see [Overview: Controlling Cloud costs on Terra](https://support.terra.bio/hc/en-us/articles/360029748111).  

---

##  Smart-seq2 

### What does it do?   

 The Smart-seq2 Multi-Sample pipeline is designed to process stranded or unstranded, paired- or single-end, scRNA-seq data from individual cells. It imports the Smartseq2 Single Sample workflow, which is divided into two parts that run in parallel. 
 
 In part one, the workflow uses HISAT2 to align reads to the genome and Picard to perform quality control on genome-aligned BAMs. 
 
 In part two, it uses HISAT2 to align reads to the transcriptome and RSEM to generate gene expression estimates from transcriptome-aligned BAMs. The Multi-Sample pipeline returns genome-aligned reads and expression estimates in BAM, and counts in a Loom file format.


### Running the workflow

This  Terra workspace is preloaded with  **two types of example sample data**: 
*  a human single-end sample (human_single_end) that is run using the single-end parameters
*  a mouse sample (mouse_single_end) that can be run **using either the paired-end or single-end** parameters 

To test these samples,  you can choose one of three workflow configurations (listed in the table below) from the Workflows tab. When you select a workflow, choose the sample_set from the table below that corresponds with the configuration. Remember! **The mouse_single_end samples can also be run with the paired-end configuration**.

| Workflow name | Sample_set to choose | Additional notes |
| --- | --- | --- |
| Smartseq2_Multisample_human_single | human_single_end | --- |
| Smartseq2_Multisample_mouse_single | mouse_single_end | --- |
| Smartseq2_Multisample_mouse_paired | mouse_single_end | When selecting the sample_set, you can also choose to create a new sample_set. Just select the data rows corresponding to mouse_single_end and give your sample_set a unique name (like mouse_paired_end) |

While we don't have example data for human paired-end in this workspace, you can bring your own! Just use the human single-end workflow, change the `paired-end` boolean to true, and specify the fastq2_input_files (as described in the inputs section below).

#### Save cost by using the reference disk
From the workflow configuration page, select the `Use reference disk` option to save time and cost on the workflow run. You'll need to do this each time you run the workflow. 


### What does the workflow require as input?

The following sections describe the necessary inputs for the Smart-seq2 Multi-Sample workflow.  


#### Sample Data Input: 

The  Multi-Sample workflow processes FASTQ files for both single- and paired-end samples; however, these samples can not be mixed in the same run.  

The following table describes the sample data inputs that are specifically necessary for running this workflow in Terra. These inputs are listed in the Sample Table of the workspace Data tab. For more information about the pipeline inputs, please read the [Smart-seq2 pipeline overview](https://broadinstitute.github.io/warp/docs/Pipelines/Smart-seq2_Multi_Sample_Pipeline/README).

| Workspace Sample Input Name | Input Description | Input Type |
| --- | --- | --- |
| fastq1_input_files | Link to the cloud locations for the READ1 FASTQ files | Array of strings |
| fastq2_input_files | Optional link to the cloud locations for the READ2 FASTQ files | Array of strings |
| input_ids | Unique identifiers for each sample  | Array of strings |
| batch_id | ID to link samples to a particular group or batch | String |

#### Additional Inputs:
The table below describes additional workflow inputs including reference data. In the Workspace Table of the Data tab, you can find both human and mouse options for these inputs. 

 | Reference name | Reference Description | Workflow Tool |
| --- | --- | --- |
| hisat2_ref_index | HISAT2 reference index file in tarball | HISAT2 |
| hisat2_ref_name | HISAT2 reference index name | HISAT2 |
| genome_ref_fasta | Genome reference in fasta format | Picard |
| gene_ref_flat | [RefFlat](https://software.broadinstitute.org/software/igv/genePred) file containing the location of RNA transcripts, exon start sites, etc.  | Picard |
| rrna_intervals | Ribosomal RNA intervals file | Picard |
| stranded | Library strand information for HISAT2; example values include FR (read corresponds to transcript), RF(read corresponds to reverse complement of transcript), or NONE | Picard |
| hisat2_ref_trans_index | HISAT2 transcriptome index file in tarball | HISAT2 |
| hisat2_ref_trans_name | HISAT2 transcriptome index file name | HISAT2 |
| rsem_ref_index | RSEM reference index file in tarball | RSEM |



#### Optional Boolean Inputs
There is a boolean parameter that can be changed in the Workflows inputs tab:

 * paired_end:  Boolean describing if the sample is paired-end


### What does it return as output?

 The following table lists the outputs of the Smart-seq2 Multi-Sample workflow.
 
 | Output file name | Output Description | Output Type |
 | --- | --- | --- |
 | bam_files | An array of genome-aligned BAM files (one for each sample) generated with HISAT2  | Array |
 | bam_index_files |  An array of BAM index files generated with HISAT2 | Array |
 | loom_output | Loom file containing counts and metrics for all samples  | File |

**Zarr Output Deprecation Notice:** Please note that the previously used Zarr array has been deprecated as of June 2020. The Loom file is the new default output. 

#### What's in the Loom output?

The Loom output contains RSEM expression values (both the TPMs and the expected_count) and cell metadata. The TPMs are contained in the Loom “matrix"", whereas expected_count is contained in the “layers/estimated_counts"".  You can read more about the Loom metrics and schema in the [Loom schema documentation](https://broadinstitute.github.io/warp/docs/Pipelines/Smart-seq2_Multi_Sample_Pipeline/Loom_schema).


### Genomic/Transcriptomic Reference data description  

The pipeline's required reference genomes (for human and mouse) and additional reference files are included in the Workspace Data table. New reference genomes can be generated using the auxiliary [build_indices pipeline](https://github.com/broadinstitute/warp/tree/develop/pipelines/skylab/build_indices). The human reference genome is hg38 (GRCh38) and the human primary assembly gene annotation list is GENCODE v27. The mouse reference genome is GRCm39 and the mouse annotation is GENCODE M21.

##### Enabling reference disks
The suggested workflow configuration (see ""Running the workflow"" above) uses [reference disks](https://support.terra.bio/hc/en-us/articles/360056384631-Reference-Disks-in-Terra). That means when Terra kicks off the workflow on a virtual computer, it attaches a portable disk (kind of like a flash-drive) that is preloaded with the reference files needed for the workflow. This saves time and cost running the workflow. 

### Estimated time and cost to run on sample data 

The following estimates are based on two sets of data, human and mouse, each containing different numbers of samples. All details of each set are listed to give insight into time and cost.
 
| Sample Set Name | Set Size (# of Cells) | Sample Set \_1.fastq Size | Sample Set \_2.fastq Size | Time | Cost $ | Per Cell Cost $ |
| :---:  | :---: | :---: | :---: | :---: | :---: | :---: |
| human_single_end | 2  | 46.9 MB | NA | 0 hr 27 min | 0.05 | 0.025 |
| mouse_single_end | 2  |  137.9 MB | 145.15 MB | 0 hr 27 min | 0.05 | 0.025 |

\
**For helpful hints on controlling Cloud costs, see [Overview: Controlling Cloud costs on Terra](https://support.terra.bio/hc/en-us/articles/360029748111).**   

### Versions

All versions listed here are available by cloning this workspace and selecting the version on the Smart-seq2 method. Other versions are listed as available, but only the versions below will be compatible with Terra. For a complete version list, please read the [Smartseq2 changelog in GitHub](https://github.com/broadinstitute/warp/blob/master/pipelines/skylab/smartseq2_multisample/MultiSampleSmartSeq2.changelog.md)

| Terra Compatible Version Name | Smart-seq2 Multi Sample Release Version | Smart-seq2 Single Sample Release Version | Date | Release Note | 
| :---:  | :---: | :---: | :---: | :---: |
| MultiSampleSmartSeq2_v2.2.21 | v2.2.21 | v5.1.20 | 8/16/2023 | Updated tool docker images. |
| MultiSampleSmartSeq2_v2.2.9 | v2.2.9 | v5.1.8 | 4/20/2022 |  Updated LoomUtils.wdl for a task in the Optimus pipeline. This change does not affect the MultiSampleSmartSeq2 pipeline.  |
| MultiSampleSmartSeq2_v2.2.6 | v2.2.6 | v5.1.5 | 10/13/2021 |  Updated Picard.wdl and LoomUtils.wdl for Single Nucleus SmartSeq2. These changes do not affect MultiSampleSmartSeq2.  |
| MultiSampleSmartSeq2_v2.2.4 | v2.2.4 | v5.1.3 | 8/05/2021 |  Added a task to Picard.wdl, updated the docker in LoomUtils.wdl task to 0.0.7, and updated the workflow to accommodate spaces in input_name.  |
| MultiSampleSmartSeq2_v2.2.1 | v2.2.1 | v5.1.1 | 1/05/2021 | Added Gene, CellID, library, species, and organ to Loom file; updated the docker in LoomUtils.wdl task to 0.0.6; updated merge_loom.py in the docker.    |
| MultiSampleSmartSeq2_v2.1.4 | v2.1.4 | v5.0.4 | 11/08/2020 | Added input checking code into HISAT tasks (called by the single sample workflow) in order to reduce number of DRS lookups   |
| MultiSampleSmartSeq2_v2.1.1 | v2.1.1 | v5.0.2 | 10/20/2020 | Changed input names to fastq1_input_files (for paired-end: fastq2_input_files), and added input_ids field or input names. Added new optional metadata fields.   |


---

### Contact Information

* For workspace questions and feedback, email the Broad pipelines team at warp-pipelines-help@broadinstitute.org or create a post on the [Featured Workspaces community forum](https://support.terra.bio/hc/en-us/community/topics/360001603491) (login required). Tag @Alexander Baumann in the **Details** section of your post.

* You can also Contact the Terra team for questions from the Terra main menu. When submitting a request, it can be helpful to include:
    * Your Project ID
    * Your workspace name
    * Your Bucket ID, Submission ID, and Workflow ID
    * Any relevant log information

---

### License
**Copyright Human Cell Atlas Authors, https://humancellatlas.org, 2023 | BSD-3**

All rights reserved. Full license text at https://github.com/broadinstitute/warp/blob/master/LICENSE. Note that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.

---",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","featured-workspaces-hca/HCA Smart-seq2 Multi Sample Pipeline"
334,"broad-firecloud-tcga","TCGA_PAAD_hg38_OpenAccess_GDCDR-12-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_PAAD_hg38_OpenAccess_GDCDR-12-0_DATA",TRUE,TRUE,"Pancreatic adenocarcinoma","Tumor/Normal","USA","TCGA Pancreatic adenocarcinoma Open-Access Data Workspace

*hg38 TCGA and TARGET workspaces reference files by their GDC UUIDs.  In order to run analyses on the referenced data files, you will need to run workflows that retrieve the files from the GDC and copy them to your workspace bucket.  See   [this forum post](https://gatkforums.broadinstitute.org/firecloud/discussion/10382/populating-hg38-tcga-and-target-workspaces-with-data-files#latest) for instructions on the running of these workflows.*","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients' clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","185","Pancreas","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh38/hg38","TCGA_PAAD_hg38_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_PAAD_hg38_OpenAccess_GDCDR-12-0_DATA"
336,"terra-kccg-production","nagim-demo-workspace","READER","https://app.terra.bio/#workspaces/terra-kccg-production/nagim-demo-workspace",TRUE,FALSE,NA,NA,NA,"Demo workspace for NAGIM.
This workspace contains data for 10 samples from the publicly available 1000 Genomes project. The samples have been processed from CRAM through the DRAGEN-GATK pipeline. 

The data is stored and processing took place in 'australia-southeas1' (Sydney) region. ",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","terra-kccg-production/nagim-demo-workspace"
337,"anvil-stage-demo","NHGRI AnVIL Notebooks Collection","READER","https://app.terra.bio/#workspaces/anvil-stage-demo/NHGRI%20AnVIL%20Notebooks%20Collection",TRUE,FALSE,NA,NA,NA,"# NHGRI AnVIL Notebooks Collection

This workspace is a collection of notebooks that are helpful for AnVIL users in Terra. The notebooks in this collection can be copied to a different workspace to aid users in their individual research needs. 

To copy a notebook to another workspace:
1. Navigate to the ""Notebooks"" section of this workspace. 
2. Click on the menu icon (circle with three dots) in the upper right.
3. Select ""copy to another workspace"".

----

## Data

These notebooks were designed to interact with data imported from Gen3.  You can learn how to search and export data from Gen3 and interact with this data model it in the Terra data tables using these resources:

- [Understanding and using Gen3 data in Terra documentation](https://support.terra.bio/hc/en-us/signin?return_to=https%3A%2F%2Fsupport.terra.bio%2Fhc%2Fen-us%2Farticles%2F360038087312) 


## Notebooks in this collection

### 1. Utilities Notebooks
These notebooks contain functions that are helpful for interacting with Gen3 data in Terra workspaces. 


**[Intro to FISS API in Python](https://app.terra.bio/#workspaces/biodata-catalyst/BioData%20Catalyst%20Collection/notebooks/launch/Intro%20to%20FISS%20API%20in%20Python.ipynb)**

- This notebook introduces users to the Firecloud API using a Python Jupyter notebook. 
- The example covers how the API communicates between the data table and notebook. The user loads an existing Terra data table into the notebook, subsets the dataframe, and saves the new dataframe as a tsv to the workspace bucket or as a new Terra data table.
- Note: a more scalable version of this process is available in the [terra\_data\_table_util](https://app.terra.bio/#workspaces/biodata-catalyst/BioData%20Catalyst%20Collection/notebooks/launch/terra_data_table_util.ipynb) notebook. 
- Suggested Notebook Runtime: 
	- Environment: Default
	- Compute Power: Default

**[Intro to FISS API in R](https://app.terra.bio/#workspaces/biodata-catalyst/BioData%20Catalyst%20Collection/notebooks/launch/Intro%20to%20Fiss%20API%20in%20R.ipynb)**

- This notebook introduces users to the Firecloud API using an R Jupyter notebook. The Reticulate R package is used to call python-based FISS commands in the R language. 
- The example covers how the API communicates between the data table and notebook. The user loads an existing Terra data table into the notebook, subsets the dataframe, and saves the new dataframe as a tsv to the workspace bucket or as a new Terra data table.
- Suggested Notebook Runtime: 
	- Environment: Legacy Python/R (released January 14: GATK4.1.4.1)
	- Compute Power: Default

**[Accessing GA4GH DRS URI data using TNU CLI](app.terra.bio/#workspaces/biodata-catalyst/BioData%20Catalyst%20Collection/notebooks/launch/Accessing%20GA4GH%20DRS%20URI%20data%20using%20TNU%20CLI.ipynb)**
	
- This Notebook provides a simple, brief introduction to accessing data identified by GA4GH Data Repository Service (DRS) URIs using the terra-notebook-utils command-line interface (CLI). This Notebook complements the Terra documentation article: Data Access with the GA4GH Data Repository Service (DRS), which is recommended reading.

**[Accessing GA4GH DRS URI data using TNU in Python](app.terra.bio/#workspaces/biodata-catalyst/BioData%20Catalyst%20Collection/notebooks/launch/Accessing%20GA4GH%20DRS%20URI%20data%20using%20TNU%20in%20Python.ipynb)**
	
- This Notebook provides a simple, brief introduction to accessing data identified by GA4GH Data Repository Service (DRS) URIs using the terra-notebook-utils Python API. This Notebook complements the Terra documentation article: Data Access with the GA4GH Data Repository Service (DRS), which is recommended reading.

**[Bring Your Own Data Tutorial](app.terra.bio/#workspaces/biodata-catalyst/BioData%20Catalyst%20Collection/notebooks/launch/Bring%20Your%20Own%20Data%20Tutorial.ipynb)**

- If you are planning to upload many files to your Terra workspace, we recommend you organize your data into a Terra data table. This is especially helpful if you plan to run workflows on your data. With data tables, you can avoid pasting in the ""gs://"" links to every file and instead use Terra's helpful UI features.
- In this example, we introduce tools to help you:
		- 	Programmatically upload data from your local machine to your Terra workspace using gsutil cp.
		- 	Programmatically generate a data table that contains your samples' CRAM and CRAI files.



### 2. VCF Tools

**[head-vcf-gz](https://app.terra.bio/#workspaces/biodata-catalyst/BioData%20Catalyst%20Collection/notebooks/launch/head-vcf-gz.ipynb)**

- This notebook allows you to easily view the header information of a gz compressed VCF file in a Terra workspace bucket or Google Cloud bucket. 
- Suggested Notebook Runtime: 
	- Environment: New Default (released January 14: GATK4.1.4.1)
	- Compute Power: Default

**[Merge and subsample VCFs](https://app.terra.bio/#workspaces/biodata-catalyst/BioData%20Catalyst%20Collection/notebooks/launch/VCF%20Merge%20and%20Subsample%20Tutorial.ipynb)**

- This notebook will produce a Terra data table that can be used as input to VCF merge workflows. There will be one workflow per chromosome, and they can be executed in parallel using the workflow tab in Terra, using the [xvcfmerge](https://dockstore.org/workflows/github.com/DataBiosphere/xvcfmerge:master?tab=info) workflow on Dockstore.
- After merging, users can use the subsample workflow to select the samples they want in the final VCF. 
- Suggested Notebook Runtime: 
	- Environment: Default
	- Compute Power: Default


## Contact Us
We are a group of developers that are working to improve research in the AnVIL ecosystem. We are happy to work with users to enhance and build resources. 

* [Michael Baumann](mailto:mbaumann@broadinstitute.org) (Broad Institute, Data Sciences Platform)
* [Beth Sheets](mailto:esheets@ucsc.edu) (UC Santa Cruz Genomics Institute)

## Workspace change log
| Date | Change | Author | 
| -------  | -------- | -------- |
| April 11, 2022 | archived BYOD, table transformer, IGV, workflow cost estimator to [attic](https://github.com/DataBiosphere/featured-notebooks/tree/master/attic) | Beth |
| Jan 5, 2021 | Collection created | Beth |",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","anvil-stage-demo/NHGRI AnVIL Notebooks Collection"
338,"gcc2022","GCC2022-Galaxy-in-AnVIL","READER","https://app.terra.bio/#workspaces/gcc2022/GCC2022-Galaxy-in-AnVIL",TRUE,FALSE,NA,NA,NA,"# GCC2022 Galaxy in AnVIL Training
Sched: https://gcc2022.sched.com/event/xXRo/anvil

Leads: Michael Schatz and Natalie Kucher

Welcome to Minneapolis! In this training, we will describe Galaxy and AnVIL, sequencing and alignments, and run a hands-on exercise to decode a secret message.


## Today's materials
Review the slides presented [here](https://drive.google.com/file/d/10iECQBfXqB4iLdSc440A6EvonC60mxRV/view?usp=sharing).

Follow step-by-step written instructions for the exercise [here](https://docs.google.com/document/d/1iOFSCIcBRQd9UfRJEalqnVUGmUjIlu9NK1pMQ4Ocj6M/edit?usp=sharing).

### Data

https://storage.googleapis.com/galaxy-anvil/ref.fa

https://storage.googleapis.com/galaxy-anvil/frag180.1.fq

https://storage.googleapis.com/galaxy-anvil/frag180.2.fq

https://storage.googleapis.com/galaxy-anvil/jump2k.1.fq

https://storage.googleapis.com/galaxy-anvil/jump2k.2.fq


## Stuck? Have questions?
Share your question to the AnVIL Help forum: [https://help.anvilproject.org/](https://help.anvilproject.org/) and we'll help you get an answer!


## Learn more!

#### Tutorial workspaces
* [Data Quickstart](https://app.terra.bio/#workspaces/fc-product-demo/Terra-Data-Tables-Quickstart)
* [Workflows Quickstart](https://app.terra.bio/#workspaces/fc-product-demo/Terra-Workflows-Quickstart)
* [Notebooks Quickstart](https://app.terra.bio/#workspaces/fc-product-demo/Terra-Notebooks-Quickstart)

#### Galaxy on Terra
* [Galaxy interactive environments](https://support.terra.bio/hc/en-us/articles/360050566271)
* [AnVIL: Galaxy on Terra FAQ](https://support.terra.bio/hc/en-us/articles/4402392877979)

#### Cloud data
* [Terra architecture and where your files live in it](https://support.terra.bio/hc/en-us/articles/360058163311)
* [Moving data to the Cloud Environment](https://support.terra.bio/hc/en-us/articles/360058268972)
* [Moving data from a Google bucket](https://support.terra.bio/hc/en-us/articles/4409101169051)
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","gcc2022/GCC2022-Galaxy-in-AnVIL"
339,"broad-firecloud-tcga","TCGA_HNSC_hg38_OpenAccess_GDCDR-12-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_HNSC_hg38_OpenAccess_GDCDR-12-0_DATA",TRUE,TRUE,"Head and Neck squamous cell carcinoma","Tumor/Normal","USA","TCGA Head and Neck squamous cell carcinoma Open-Access Data Workspace

*hg38 TCGA and TARGET workspaces reference files by their GDC UUIDs.  In order to run analyses on the referenced data files, you will need to run workflows that retrieve the files from the GDC and copy them to your workspace bucket.  See   [this forum post](https://gatkforums.broadinstitute.org/firecloud/discussion/10382/populating-hg38-tcga-and-target-workspaces-with-data-files#latest) for instructions on the running of these workflows.*","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients' clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","528","Head, Neck","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh38/hg38","TCGA_HNSC_hg38_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_HNSC_hg38_OpenAccess_GDCDR-12-0_DATA"
340,"broad-firecloud-cptac","PANOPLY_Production_Pipelines_v1_0","READER","https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_Production_Pipelines_v1_0",TRUE,FALSE,NA,NA,NA,"# PANOPLY: A cloud-based platform for automated and reproducible proteogenomic data analysis
#### Version 1.0

PANOPLY is a platform for applying state-of-the-art statistical and machine learning algorithms to transform multi-omic data from cancer samples into biologically meaningful and interpretable results. PANOPLY leverages [Terra](http://app.terra.bio)—a cloud-native platform for extreme-scale data analysis, sharing, and collaboration—to host proteogenomic workflows, and is designed to be flexible, automated, reproducible, scalable, and secure. A wide array of algorithms applicable to all cancer types have been implemented, and available in PANOPLY for analysis of cancer proteogenomic data.


![*Figure 1.* Overview of PANOPLY architecture and the various tasks that constitute the complete workflow. Tasks can also be run independently, or combined into custom workflows, including new tasks added by users. Inputs to PANOPLY consists of (i) externally characterized genomics and proteomics data (in gct format); (ii) sample phenotype and annotations (in csv format); and (iii) parameters settings (in yaml format). Panoply modules are grouped into Data Preparation Modules (green box), Data Analysis Modules (blue box) and Report Modules (red box). Data preparation modules perform quality checks on input data followed by optional normalization and filtering for proteomics data. Analysis ready data tables are then used as inputs to the data analysis modules. Results from the data analysis modules are summarized in interactive reports generated by appropriate report modules.](https://raw.githubusercontent.com/broadinstitute/PANOPLY/dev/panoply-overview.png)


PANOPLY v1.0 consists of the following components:

* A Terra production workspace on [PANOPLY_Production_Pipelines_v1_0](https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_Production_Pipelines_v1_0) with a preconfigured unified workflow to automatically run all analysis tasks on proteomics (global proteome, phosphoproteome, acetylome, ubiquitylome), transcriptome and copy number data; and an [additional workspace](https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_Production_Modules_v1_0) that includes separate methods for each analysis component. 
* An interactive Jupyter notebook (included in the Terra workspaces) that provides step-by-step instructions for uploading data, identifying data types, specifying parameters, and setting up the PANOPLY workspace for analyzing a new dataset.
* A GitHub [repository](https://github.com/broadinstitute/PANOPLY) that contains code for all PANOPLY tasks and workflows, including R code for implementing analysis algorithms, task module creation, and release management. A GitHub [wiki](https://github.com/broadinstitute/PANOPLY/wiki) includes documentation and description of algorithms. 
* A [tutorial](https://github.com/broadinstitute/PANOPLY/wiki/PANOPLY-Tutorial) illustrating the application of PANOPLY to a published breast cancer dataset and demonstrating the practical relevance of PANOPLY by regenerating many of the results described in (Mertins et al) (1) with minimal effort. The [PANOPLY_Tutorial](https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_Tutorial) workspace contains data and results from running the tuorial.
* Terra workspaces showing case studies of applying PANOPLY to the analysis of [CPTAC BRCA](https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_CPTAC_BRCA) (7) and [CPTAC LUAD](https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_CPTAC_LUAD) (8) datasets. The workspaces include all data, parameter settings and results.


PANOPLY provides a comprehensive collection of proteogenomic data analysis methods including sample QC (sample quality evaluation using profile plots and tumor purity scores (1), identify sample swaps, etc.), association analysis, RNA and copy number correlation (to proteome), connectivity map analysis (1 ,2), outlier analysis using BlackSheep (3), PTM-SEA (4), GSEA (5) and single-sample GSEA (6), consensus clustering, and multi-omic clustering using non-negative matrix factorization (NMF). Most analysis modules include a report generation task that outputs a HTML interactive report summarizing results from the respective analysis tasks. Mass spectrometry-based proteomics data amenable to analysis by PANOPLY includes isobaric label-based LC-MS/MS approaches like iTRAQ, TMT and TMTPro profiling the proteome and multiple PTM-omes including phospho-, acetyl-and ubiquitylomes.

Users can also [customize or create new pipelines and add their own tasks](https://github.com/broadinstitute/PANOPLY/wiki/Customizing-PANOPLY) and integrate them into the PANOPLY.


## Quick Start

* For a quick introduction and tour of PANOPLY, follow the [**tutorial**](https://github.com/broadinstitute/PANOPLY/wiki/PANOPLY-Tutorial). 
* Detailed **documentation** can be found [here](https://github.com/broadinstitute/PANOPLY/wiki).

### Contact

Email proteogenomics@broadinstitute.org with questions, comments or feedback.


### References

1. Mertins, P. et al. Proteogenomics connects somatic mutations to signalling in breast cancer. Nature 534, 55–62 (2016).
2. Subramanian, A. et al. A Next Generation Connectivity Map: L1000 Platform and the First 1,000,000 Profiles. Cell 171, 1437–1452.e17 (2017).
3. Blumenberg, L. et al. BlackSheep: A Bioconductor and Bioconda package for differential extreme value analysis. doi:10.1101/825067.
4.	Krug, K. et al. A Curated Resource for Phosphosite-specific Signature Analysis. Mol. Cell. Proteomics 18, 576–593 (2019).
5.	Subramanian, A. et al. Gene set enrichment analysis: a knowledge-based approach for interpreting genome-wide expression profiles. Proc. Natl. Acad. Sci. U. S. A. 102, 15545–15550 (2005).
6.	Barbie, D. A., Tamayo, P., Boehm, J. S., Kim, S. Y. & Moody, S. E. Systematic RNA interference reveals that oncogenic KRAS-driven cancers require TBK1. Nature (2009).
7. Gillette, M. A. et al. Proteogenomic Characterization Reveals Therapeutic Vulnerabilities in Lung Adenocarcinoma. Cell 182, 200–225.e35 (2020).
8. Krug, K., Jaehnig, E. J., Satpathy, S., Blumenberg, L., et al. Proteogenomic Landscape of Breast Cancer Tumorigenesis and Targeted Therapy. Cell 183, 1–21 (2020).
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","broad-firecloud-cptac/PANOPLY_Production_Pipelines_v1_0"
341,"help-gatk","Mitochondria-SNPs-Indels-hg38","READER","https://app.terra.bio/#workspaces/help-gatk/Mitochondria-SNPs-Indels-hg38",TRUE,FALSE,NA,NA,NA,"### GATK Best Practices for SNP/Indel Variant Calling in Mitochondria

A reproducible example of mitochondrial SNP and Indel variant calling (including low allele frequencies of 1-5%)  from whole-genome sequencing data. The pipeline takes into consideration inherent mitochondrial characteristics such as circular DNA structure and presence of NuMts (""nuclear mitochondrial DNA segment"" that transpose to the nuclear genome of eukaryotic organisms).

## Workflows

### 1-Mitochondria_Pipeline

**What does it do?**     
This WDL workflow performs mitochondia SNP/Indel variant calling on WGS hg38 input data.

**What data does it require as input?**  
This workflow accepts full WGS hg38 BAM or CRAM files as inputs. **Note: If using a CRAM file as input**, full reference files are necessary (BAM files do not need these files).    

Requirements/expectations:
 - WGS hg38 bam or cram 

**What does it output?** .    
A VCF file of SNP/Indel calls on mitochondria.         

**Sample data description and location**    
An example WGS hg38 cram input, NA12878, is provided in the workspace data model for testing.    

**Reference data description and location**  
The required and optional references and resources for the Tools are included in the Workspace Data table. The reference genome for this workspace is hg38.       

**Time and cost estimates**    

| Participant | Size | Time | Cost $ |
| :------------------: | :----------------: | :------: | :--------: |
| NA12878 | 19.51 GB | 0:36:00 | 0.01 |

Cost and Time will vary, view [Controlling-Cloud-costs-sample-use-cases](https://support.terra.bio/hc/en-us/articles/360029772212) for further details.  

**Software Version Notes**   
GATK 4.1  

---

### Contact Information  
For questions about this workspace please visit the Featured Workspaces Topic topic on the [Terra Community Forum](https://broadinstitute.zendesk.com/hc/en-us/community/topics). Use the search box to see if other users have asked the same question previously. If not, post and tag **@Samantha** so that we get notified.

This material is provided by the GATK Team. Please post any questions or concerns regarding the GATK tool to the [GATK forum](https://gatk.broadinstitute.org/hc/en-us/community/topics)

### Workspace Citation 
Details on citing Terra workspaces can be found here: [How to cite Terra](https://support.terra.bio/hc/en-us/articles/360035343652)

Data Sciences Platform, Broad Institute (*Year, Month Day that this workspace was last modified*) help-gatk/Mitochondria-SNPs-Indels-hg38 [workspace] Retrieved *Month Day, Year that workspace was retrieved*,  https://app.terra.bio/#workspaces/help-gatk/Mitochondria-SNPs-Indels-hg38

### License  
**Copyright Broad Institute, 2019 | BSD-3**  
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at https://github.com/openwdl/wdl/blob/master/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.

### Workspace Change Log
| Date | Change | Author |
| --- | --- | --- |
|  2019-26-02 | Initial Setup | Sushma Chaluvadi |
|  2020-29-04 | Updated workflow to point to GATK git repo. Workflow version ""ah_mt_fixspaces_wdl"" equivalent to 4.1.7.0 with additional bug fixes | Beri Shifaw |
|  2020-04-08 | Re-uploaded the workflow to fix execution bug. | Beri Shifaw |
|  2021-09-15 | Addition of Workspace Citation section | Beri Shifaw |

",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/Mitochondria-SNPs-Indels-hg38"
343,"biodata-catalyst","BDC-PIC-SURE API Python Examples","READER","https://app.terra.bio/#workspaces/biodata-catalyst/BDC-PIC-SURE%20API%20Python%20Examples",TRUE,FALSE,NA,NA,NA,"# NHLBI BioData Catalyst® (BDC) Powered by PIC-SURE Python API examples

This workspace contains Jupyter Notebook examples of BDC-PIC-SURE API use cases, using BDC studies. The PIC-SURE API is available in two languages: R and python. This workspace features the python PIC-SURE API example notebooks and requires python 3.6 or later.


## PIC-SURE API Overview
The main goal of the PICSURE API is to provide a simple and reliable way to work with data from studies that are part of BDC. Each individual study is accessible in a unique, easy to use, tabular format directly in an R or python environment. The API allows users to filter studies based on user-specified criteria, as well as to retrieve a cohort that has been created using the [PIC-SURE interface](https://picsure.biodatacatalyst.nhlbi.nih.gov). In addition to many heart, lung, blood, and sleep related datasets, there are 43 specific phenotype variables that have been harmonized across multiple TOPMed studies that are also accessible directly through the PIC-SURE API. 


## Workspace information
- Requirement : python 3.6 or higher. To select the appropriate runtime environment for your Terra Workspace, click on the gear wheel beside 'Cloud Environment' in the top right corner, and under Application Configuration select “Default: (GATK 4.1.4.1, Python 3.7.10, R 4.0.5)” or another appropriate configuration.
- Notebooks update information: the central repository for these notebooks is available on the [Access to Data using PIC-SURE API GitHub](https://github.com/hms-dbmi/Access-to-Data-using-PIC-SURE-API/tree/master/NHLBI_BioData_Catalyst).  Currently under active development, the repository is updated on a regular basis. Although the Terra public Workspace will be kept up-to-date as much as possible, there might be a difference between the version of the notebook you're using and the most recent one. So if you ran into an unexpected issue when running one of these example notebooks, it may be worth checking for a potential more up-to-date version available on GitHub.


## Available notebooks
The following example notebooks are available: 
  - Workspace_setup.ipynb: a notebook that helps to set up your workspace to use the PIC-SURE API. This includes saving your personal access token from PIC-SURE.
  - 0_Export_from_UI.ipynb: an interactive tutorial on how to export a dataframe built in the UI into your Terra workspace.
  - 1_PICSURE-API_101.ipynb: an illustration of the main functionalities of the PIC-SURE API.
  - 2_TOPMed_DCC_Harmonized_Variables_analysis.ipynb: an example of how to access and work with the ""harmonized variables"" across the TOPMed studies.
  - 3_PheWAS.ipynb: a straightforward PIC-SURE API use-case, using a PheWAS (Phenome-Wide Association Study) analysis as an illustration example.
  - 4_Genomic_Queries.ipynb: an illustration of how to use genomic variables to build queries.
  - 5_LongitudinalData.ipynb: an example of how to select longitudinal variables from PIC-SURE.
  - 6_Sickle_Cell.ipynb: an example illustrating how to select and view data related to Sickle Cell Disease (specifically the HCT for SCD dataset).
  - 7_Harmonization_with_PICSURE.ipynb: examples of harmonization across studies using (1) sex and BMI variables (including TOPMed Harmonized dataset and others) and (2) orthopnea and pneumonia variables.
  - ORCHID_COVID19_python.ipynb: the code accompanying the JAMA publication on November 7th 2021: ""[Effect of Hydroxychloroquine on Clinical Status at 14 Days in Hospitalized Patients With COVID-19](https://jamanetwork.com/journals/jama/fullarticle/2772922)"" 


## Data information
The data accessible through the PIC-SURE API are controlled-access data. Hence, if you're saving any data from the Jupyter notebooks directly into the Google Bucket associated with this workspace, you will need to set an Authorization Domain to protect this data. Instructions about how to proceed can be found here:

https://support.terra.bio/hc/en-us/articles/360039415171-Authorization-Domain-overview-for-BioData-Catalyst-users

## Contact
For bug report or additional information, please contact us using the [BioData Catalyst contact form](https://biodatacatalyst.nhlbi.nih.gov/contact/).

",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","biodata-catalyst/BDC-PIC-SURE API Python Examples"
344,"help-gatk","Somatic_variant_discovery_b37_v1","READER","https://app.terra.bio/#workspaces/help-gatk/Somatic_variant_discovery_b37_v1",TRUE,TRUE,NA,NA,NA,"### GATK Best Practices for somatic variant discovery (b37 reference)
The purpose of this workspace is to provide a fully reproducible example of the GATK Best Practices workflows for somatic variant discovery. 

#### Workspace attributes
All required and optional resources for the preconfigured methods included. The reference genome is b37, the Broad Institute's version of hg19. Some resource files may be named hg19 for historical reasons.

#### Data 
Full exomes of the HCC1143 cell line tumor and normal samples, pre-processed according to current GATK Best Practices. See BAM headers for additional details. 

#### Method configs
This workspace contains the following preset method configurations:

- **Somatic_SNV_Indel_Discovery_MC**: Somatic SNV and Indel discovery with Mutect2. Usage instructions: launch this on any qualifying Pair or Pair Set (each pair must include a tumor BAM + its matched normal and their BAM indices, aligned against the correct reference genome). By default the workflow will run the following tasks: GATK4 Mutect2 variant calling, estimation of cross-sample contamination, collection of pre-adapter artifact metrics, filtering on LOD scores and contamination, filtering on orientation bias, and finally functional annotation using Oncotator. Note that currently the Panel Of Normals (PON) must be provided by the user, ideally through a workspace attribute. Failure to provide a suitable PON will produce inferior results.",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/Somatic_variant_discovery_b37_v1"
345,"hacohen-immune","HLAthena-Demo","READER","https://app.terra.bio/#workspaces/hacohen-immune/HLAthena-Demo",TRUE,FALSE,NA,NA,NA,"
## Overview
This workspace demonstrates how to use the HLAthena_v1_external workflow. The HLAthena_v1_external workflow takes peptides and alleles as input and computes HLA presentation predictions. For more information about the underlying prediction algorithms and data please see:

Sarkizova S*, Klaeger S*, Le PM, Li WL., Oliveira G, Keshishian H, Hartigan CR, Zhang W, Braun DA, Ligon KL, Bachireddy P, Zervantonakis IK, Rosenbluth JM, Ouspenskaia T, Law T, Justesen S, Stevens J, Lane WJ, Eisenhaure T, Zhang GL, Clauser KR, Hacohen N#, Carr SA#, Wu CJ#, Keskin DB#. *A large peptidome dataset improves HLA class I epitope prediction across most of the human population*. Nature Biotechnology. 2019 Dec 16; doi: 10.1038/s41587-019-0322-9. [Epub ahead of print] ***[PubMed][1]***
 
[1]: <https://www.ncbi.nlm.nih.gov/pubmed/31844290> ""Pubmed""

Sample input and output files are provided along with detailed descriptions of the configuration parameters,  input file formats, and outputs files. The Terra environment provides a convenient way of scaling the workflow by parallelizing to multiple VMs.
	
	
___
## Terms of use
HLAthena is made available for academic, non-commercial, research use only under the terms of this license. Inquiries from commercial entities should be directed to softwarelicensing@broadinstitute.org. 

By using this software you agree to the terms of use detailed here: [HLAthena.tools][2] -> Predict ->'Terms of use' tab.

[2]: <http://www.HLAthena.tools> ""HLAthena.tools""

  
	
___

## Workflow: HLAthena_v1_external

### What does it do?
Runs HLAthena models for predicting peptide presentation by HLA class I molecules. 



### Inputs  

  *  Description of input parameters for the HLAthena_v1_external workflow 
  
   | Parameter name            	| Values                                                                                 	| Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            	|
|:--------------------------	|:---------------------------------------------------------------------------------------	|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------	|
| sampleName                	| this.name (part of the workspace data model)                                           	| A name of choice for the data being processed, e.g. ""MySample"". The <sampleName> will be used as a prefix in output file names , e.g. <sampleName>_predictions.txt                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     	|
| alleles_file              	| this.alleles (part of the workspace data model)                                        	| A file with allele(s) for which predictions will be run. The file should contain one allele per line. Multiple format options are accepted, for example: A0101, B40:01, HLA-C0102, HLA-A*02:01, etc.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   	|
| peptide_list              	| this.peptides (part of the workspace data model)                                       	| A file containing the input peptides that predictions should be computed for. This can be either in a peptide format (i.e. one peptide per line) or in a fasta format. Please see below for more details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              	|
| peptide_col_name          	| e.g. ""pep""                                                                             	| The name of the column with peptide sequences                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          	|
| exists_ctex               	| ""true"" or ""false""                                                                       	| Specifies whether peptide context data exists in the input file. See also the more detailed description of the input file format below                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 	|
| exists_expr               	| ""true"" or ""false""                                                                        	| Specifies whether peptide context data exists in the input file. See also the more detailed description of the input file format below                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 	|
| expr_col_name             	| e.g. ""TPM""                                                                             	| The name of the column containing peptide expression level. See also the more detailed description of the input file format below                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       	|
| logtransform_expr         	| ""true"" or ""false""                                                                        	| Should the expr_col_name  be log transformed or not. See also the more detailed description of the input file format below                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             	|
| aggregate_pep             	| ""true"" or ""false""                                                                        	| Should the input be aggregated in case the same peptide sequence appears multiple times. For example, the same peptide sequence could sometimes be derived from multiple source proteins and hence it can appear in the input file with different context sequences or with different expression values. Setting this parameter to ""true"" will aggregate the input on the peptide sequences and provide a single final scores for each peptide. String columns are semi-column concatenated, cleavability (and other numerical columns) is averaged, and expression is summed                                                                                                                                                          	|
| lens                      	| e.g. [""9""] or [""8"",""9"",""10"",""11""]                                                      	| Specifies the set of peptide lengths. Lengths 8, 9, 10, and 11 are supported. Used if the input file is in fasta format, all tiled peptides of the specified length[s] will be created.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                	|
| assign_by_ranks_or_scores 	| ""ranks"" or ""scores""                                                                     	| The workflow will provide both log-likelihood presentation scores ([0,1], 1 is good) as well as percentile ranks ([0, 100], 0 is good). If multiple alleles are provided, the workflow will pick the best allele for each peptide. The best allele will be determined either based on ""scores"" or based on ""ranks"" depending on the value of this input                                                                                                                                                                                                                                                                                                                                                                                	|
| assign_threshold          	| e.g. ""0.1""                                                                             	| Set the threshold value for assigning a peptide to an allele. For example if assign_by_ranks_or_scores is set to ""ranks"" assign_threshold can be set to ""0.1"" which means that a peptide will be assigned to an allele if it scores within the top 0.1 percentile amongst a large set of background decoys. Alternatively, if assign_by_ranks_or_scores is set to ""ranks"" assign_threshold can be set to ""0.98"".                                                                                                                                                                                                                                                                                                                       	|
| assign_colors             	| e.g. ""#8c0a82, #ab0c9f, #048edb, #00a0fa, #5F36A0, #ff6000, darkorange, #e05400, gray"" 	| A string of 9 colors. These colors are used to generate a plot of the fraction of peptides assigned to alleles for the current sample, the use case being a human complement of alleles with up to two HLA-A alleles, up to two HLA-B alleles and up to two HLA-C alleles. The first two colors are used for A alleles, the second two for B alleles, the fifth color is used for peptides that can be assigned to more than one A or B allele (no C), the sixth and seventh colors are used for C alleles, seventh color is used for peptides that can be assigned to multiple C alleles (no A or B) and the last color is used for peptides which cannot be assigned to any of the input alleles with the specified threshold criteria. 	|
	

	
* #### *alleles_file* input details
The alleles_file should contain one allele per row and no header line. Multiple formattings of the HLA alleles are accepted, e.g. A0101, B40:01, HLA-C0102, HLA-A*02:01, etc.
	
	Example:
	
  HLA-A01:01  
B40:01  
C0102
	
	
* #### *peptide_list* input details
A file containing the input peptides that predictions will be computed for. This can be either in peptide format (i.e. one peptide per line) or in fasta format:
	
  1. **peptide format**
	
	The peptide format should be tab-delimited and it should contain a header line. The file can contain peptides of different lengths, lengths 8, 9, 10, and 11 are supported. If a peptide or context sequence contains non-standard amino acid symbols the corresponding row will be excluded from the analysis. 
	
	Example:
	
	| pep       	| ctex_up                        	| ctex_dn                        	| TPM  	|
|-----------	|--------------------------------	|--------------------------------	|------	|
| AADIFYSRY 	| AAAAAAAAAGAGGGGFPHPAAAAAGGNFSV 	| AANQCRNLMAHPAPLAPGAASAYSSAPGEA 	| 1.5  	|
| AADLNLVLY 	| AAAAAAAAGAGGGGFPHPAAAAAGGNFSVA 	| ANQCRNLMAHPAPLAPGAASAYSSAPGEAP 	| 8.3  	|
| AADLVEALY 	| AAAAAAAGAGGGGFPHPAAAAAGGNFSVAA 	| NQCRNLMAHPAPLAPGAASAYSSA------ 	| 12.5 	|
| AIDEDVLRY 	| ---AAAALVSDSFSCGGSPGSSAFSLTSSS 	| AASSSPFANDYSVFQAPGVSGGSGGGGGGG 	| 0.5  	|
| IDLLKEIY  	| AAAAAALVSDSFSCGGSPGSSAFSLTSSSA 	| ASSSPFANDYSVFQAPGVSGGSGGGGGGGG 	| 30.2 	|
	
	The only required column is the peptide column, this is the column that contains peptide sequence (see peptide_col_name). If only this column is provided then only the MSintrinsic (or MSi) predictors will be ran. If *exists_ctex* is set to ""true"" and the file also contains *ctex_up* and *ctex_dn* columns then the MSiC predictor will also be ran. The *ctex_up* and *ctex_dn* columns should contain the 30 upstream and 30 downstream amino acid residues for each peptide. If the peptide is found at the boundary of a protein (i.e. N- or C-terminus) such that less than 30 residues are available in the source sequence, then *ctex_up* and *ctex_dn* should be padded with dashes '-'.
	
	Similarly, if *exists_expr* is set to true, the MSiCE models will be run as well. Expression should be provided as TPM (transcripts per million) values or log(TPM+1) values. The column name containing the expression information is specified by the  *expr_col_name* input. These values will be log-transformed if *logtransform_expr* is set to ""true"".
	
	If the input file contains any other columns they will be carried through to the final predictions output file, with the exception of a few column names that are used internally that may be removed.

  2. **fasta format**  
	
	The fasta format input is assumed to follow standard fasta formatting. In addition we assume amino acid sequence are provided (rather than codon sequences). Spaces within the sequence are accepted, and so are both uppercase and lowercase AA letters. 
	
	Example:
	
   |  	|
|-------------------------------------------------------------------------------------------------------------                                                      	 |
| >HPV-16 E7                                                                                                  	                                                                                            |
| mhgdtptlhe ymldlqpett dlycyeqlnd sseeedeidg pagqaepdra hynivtfcck cdstlrlcvq sthvdirtle dllmgtlgiv cpicsqkp 	|
| >KRAS_G12D                                                                                                  	                                                               	                          |
| MTEYKLVVVGADGVGKSALTIQLIQNHFVDEYDPTIEDSYRKQVVIDGETCLL                                                                               	|
	
	Based on the input sequences, all possible peptides of length lens will be tiled along each fasta sequences along with the corresponding upstream and downstream sequences (for MSiC model evaluation). 



	
	

	
### Outputs

* #### <sampleName>_predictions.txt
		 	
  | Column name                               	| Description                                                                                                                                                                                                                                                                 	|
|-------------------------------------------	|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------	|
| model_<allele>                              	| The type of model used for this {allele, length} combination, either allele-and-length specific or pan-allele-pan-length model                                                                                                                                              	|
| MSi_<allele>                                	| The predicted score for this allele [0,1], 1 is good                                                                                                                                                                                                                        	|
| pRank.MSi_<allele>                          	| The percentile ranks corresponding to the predicted score for this allele [0,100], 0 is good                                                                                                                                                                                	|
| MSiC_<allele>; MSiCE_<allele>                  	| Analogous as above, for the MSiC and MSiCE models if expression and sequence context data is available and the exists_ctex and exists_expr input options were set to true                                                                                                   	|
| pRank.MSiC_<allele>; pRank.MSiCE_<allele>      	| Analogous to above for the , for the MSiC and MSiCE models                                                                                                                                                                                                                  	|
| best.<model>                                	| The best model score across input alleles. model here is one of MSi, MSiC, MSiCE, where the most integrative model available is considered. Note that the best score does not necessarily satisfy the allele assignment threshold                                           	|
| best.<model>_allele                         	| The allele which corresponds to the best score                                                                                                                                                                                                                              	|
| assign.<model>_ranks OR assign.<model>_scores 	| Depending on the value of *assign_by_ranks_or_scores* one of these two columns will be present in the output file. The numerical values of the score or rank which satisfy the threshold criteria set for peptide-to-allele assignment, or ""NA"" if none of the input alleles do 	|
| assign.<model>_allele                       	| The allele(s) which satisfy the threshold criteria set for peptide-to-allele assignment, or ""unknown"" if none of the input alleles do                                                                                                                                         	|  	
	  
	
	

	
* #### <sampleName>_allele_assignment_counts.txt
	File containing a summary of how peptides were assigned to alleles. Information across all lengths is provided ('len' column = 'all') as well as information broken down by specific length ('len' = {8,9,10,11}).  
	
	

	
### Workflow method configurations
Please note that pre-populated method configurations have been provided for convenience and to serve as a starting point for configuring the workflow when exporting to other workspaces. The following table illustrated the differences between these workflow configurations (all are publicly available from the Method Repository):
	
   | workflow configuration        	| exists_expr 	| exists_ctex 	|
|--------------------------------	|-----------	|-----------	|
| HLAthena_v1_external_MSi      	| 0         	| 0         	|
| HLAthena_v1_external_MSi_MSiC 	| 0         	| 1         	|
| HLAthena_v1_external_MSi_MSiE 	| 1         	| 0         	|
| HLAthena_v1_external_MSi_MSiCE 	| 1         	| 1         	|
     		
.
		
Please note that configurations that include expression (HLAthena_v1_external_MSi_MSiE, HLAthena_v1_external_MSi_MSiCE) are not compatible with the fasta input format (i.e. sample_fasta). 
	

___	
## Sample data files

Sample data files for the workflow are provided in the google bucket associated with this workspace and incorporated into the data model. 

To view the files click on DATA -> Files
	
To view the data model click on DATA -> sample(2) (found under TABLES on the top left)


	
___	
## Example time and cost to run workflows


| Sample Size | Number of alleles  | ctex_expr | memoryGB | diskGB |  preemptible | Time | Cost $ |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| 10 peptides | 4 | yes_yes | 4 | 10 | 3 | 10-30min | 0.03 |


	
___	
## Questions

If you have further questions regarding the HLAthena workflow please reach out on the [HLAthena discussion group][3].
	
[3]:<https://groups.google.com/forum/#!forum/hlathena> ""HLAthena discussion group.""
	
	
	
	
	
	
	
	
	
	
	
	",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","hacohen-immune/HLAthena-Demo"
348,"mccarroll-genomestrip-terra","Genome STRiP 1000G","READER","https://app.terra.bio/#workspaces/mccarroll-genomestrip-terra/Genome%20STRiP%201000G",TRUE,FALSE,NA,NA,NA,"# Sample Genome STRiP workflows

## Overview

This workspace contains some sample workflows for running Genome STRiP.

Currently, the set of workflows is not complete and does not represent a complete analysis. The workflows do provide useful examples, however, for running Genome STRiP preprocessing at scale. Additional sample  workflows may be added in the future.

The data in this workspace is high-coverage whole-genome sequencing from the 1000 Genomes project.

## Workspace Data

The data used in this demonstration workspace is high-coverage whole-genome sequencing from the 1000 Genomes project. The underlying cram files are stored in the AnVIL.

## Genome STRiP Overview

The Genome STRiP software is a set of tools and analysis pipelines for analyzing structural variation in cohorts from whole-genome sequencing data. All analysis workflows begin by running Genome STRiP preprocessing, which gathers per-sample metadata that is then typically aggregated into processing batches for further analysis. The individual analysis pipelines then use this preprocessing data.

## Workflows

### SingleStep_Preprocessing / SingleStep_Preprocessing_RequesterPays

This workflow performs Genome STRiP preprocessing on a single input sample. There are two flavors of the workflow. The second version supports access to data and other resources that are requester-pays. This workflow can be run on a SampleSet to process all of the samples in the SampleSet. The output of the workflow is a metadata.zip file which is stored as an attribute on each sample.

### MergePerSampleMetadata

This workflow merges the preprocessing data (metadata.zip) from multiple samples together into a single metadata.zip file covering multiple samples. This workflow is designed to be run on a SampleSet. The output metadata.zip file is stored as an attribute on the SampleSet.

Typically, this workflow is designed to process the data from 100-200 samples together. The default memory and disk allocations are based on the typical requirements for sample sets of this size. If you are merging more samples together, you may need to allocate more disk and perhaps more memory.

Downstream analysis pipelines are generally designed to run on one or more merged metadata.zip files.

For best performance, it is recommended to group together samples with similar technical properties. For example, if you have some samples sequenced from whole blood and some samples sequenced from lymphoblastoid cell lines (LCLs) or other sources of DNA, is is best to group them separately so they can be analyzed separately downstream. Similarly if there are other technical differences, such as the read length.

### BuildReadDepthProfiles

These are optional workflows that generate ""depth profiles"" consisting of binned, normalized depth of coverage. These depth profiles are used internally by some downstream analyses in Genome STRiP. The output consists of compressed text files that can be processed by user-written scripts to analyze or visualize read depth as normalized by Genome STRiP.

Several versions of this workflow are provided that use bin sizes of 100bp, 1000bp and 10,000bp. In addition, normal preprocessing output contains 100Kb depth profiles as part of the standard output. These are used for analysis of sample sex and for QC.

## Workspace Reference Data

Under Workspace Data, this workspace also contains attributes referencing a Genome STRiP reference metadata bundle (referenceBundle) and some other files used by some of the Genome STRiP analysis pipelines (repeatTrackFile and segdupFile).

## Other Considerations

### Alt-aware alignments

Genome STRiP is designed to work with alt-aware alignments as generated by bwa-mem. If your data is not aligned in alt-aware mode, then GenomeSTRiP will give incorrect and misleading results in many areas of the genome, including many regions with known common copy number variants.




",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","mccarroll-genomestrip-terra/Genome STRiP 1000G"
349,"theiagen-validations","PHBG_Validation_v1-1-0","READER","https://app.terra.bio/#workspaces/theiagen-validations/PHBG_Validation_v1-1-0",TRUE,FALSE,NA,NA,NA,"This workspace was utilized to validate the functionality of PHBG workflows prior to the [PHBG v1.1.0 release](https://github.com/theiagen/public_health_bacterial_genomics/releases/tag/v1.1.0)

# PHBG v1.1.0 Release Notes

This minor release introduces multiple modules to the TheiaProk workflow series as well as a new workflow for performing core gene phylogenetic analysis (Core_Gene_SNP).

### Updates to the TheiaProk Workflow Series
<p align=""left"">
  <img src=""https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F44c250f9-8fd9-4321-8d97-22e8e8cb6108%2FTheiaProk_Illumina_PEv1.1_(1).png?id=fe8057e5-e880-46dd-a81f-ffd956effcb7&table=block&spaceId=be290196-9090-4f3c-a9ab-fe730ad213e0&width=2000&userId=b5cd57b7-fbf7-4ef0-a72e-937c08a7197e&cache=v2"" width=""800"" class=""left"">
</p>

**Taxon-specific modules added**:
- _Acinetobacter baumannii:_ [Kaptive](https://github.com/katholt/Kaptive) (detection of surface polysaccharide loci for _A. baumannii_) & [AcinetobacterPlasmid Typing](https://www.biorxiv.org/content/10.1101/2022.08.26.505409v1) (plasmid typing of _A. baumannii_ using abricate with the custom _A. baumannii_ plasmid typing database)
- *Pseudomonas aeruginosa*: [Pasty](https://github.com/rpetit3/pasty) (tool to identify the serogroup of _P. aeruginosa_ isolates)
- *Shigella* spp.: [ShigaTyper](https://github.com/CFSAN-Biostatistics/shigatyper) (tool designed to determine _Shigella_ serotype), [ShigEiFinder](https://github.com/LanLab/ShigEiFinder) (tool that is used to identify differentiate _Shigella_/EIEC using cluster-specific genes and identify the serotype using O-antigen/H-antigen genes), [SonneiTyper](https://github.com/katholt/sonneityping) (tool to identify input genomes as _S. sonnei_, assign those identified as _S. sonnei_ to hierarchical genotypes based on detection of single nucleotide variants)
- _Streptococcus pneuomniae_: [GPS unified workflow](https://www.pneumogen.net/gps/bioinformatics_training.html) ([PopPUNK](https://github.com/rpetit3/pbptyper) (tool for in silico Penicillin Binding Protein (PBP) typing), [SeroBA](https://github.com/sanger-pathogens/seroba) (tool for _S. pneumoniae_ serotyping), [PBPTyper](https://github.com/bacpop/PopPUNK) with [Global Pneumococcal Sequencing (GPS) database v6](https://www.pneumogen.net/gps/training_command_line.html) for GPS Cluster assignment

**QC and read processing modules added**:
- Option to quantify secondary genus abundance using the [MIDAS](https://github.com/snayfach/MIDAS)
- Option to utilize [fastp](https://github.com/OpenGene/fastp) rather than [trimmomatic](http://www.usadellab.org/cms/?page=trimmomatic) for read processing
- Option to utilize [bakta](https://github.com/oschwengers/bakta) rather than [prokka](https://github.com/tseemann/prokka) for genome annotation 
- Option to perform a QC check--i.e. determine QC Pass or QC Alert based on user-defined thresholds for multiple QC metrics 

**Column output updates**:
- `genome_length` renamed to `assembly_length`
- `est_coverage` renamed to `est_coverage_raw` (`est_coverage_clean` column output added)
	- _*Note*: Assembly length calculated by [quast](https://quast.sourceforge.net/) is used to calculate estimated coverage rather than the estimated genome length produced from the [mash](https://github.com/marbl/Mash) sketch_

### Core Gene SNP Workflow
<p align=""left"">
  <img src=""https://theiagen.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F13ed369b-81bd-4b79-a8ce-6b794ad24732%2FCore_Gene_SNP.png?id=afae86ca-8d4a-415f-8268-207bbd71d759&table=block&spaceId=be290196-9090-4f3c-a9ab-fe730ad213e0&width=2000&userId=&cache=v2"" width=""700"" class=""left"">
</p>


The Core_Gene_SNP workflow is a flexible workflow intended for core gene alignment and phylogenetic analysis of a set of samples. The workflow takes in gene sequence data in GFF3 format from a set of samples. It first produces a pangenome summary using [Pirate](https://github.com/SionBayliss/PIRATE), which clusters genes within the sample set into orthologous gene families. By default, the workflow also instructs Pirate to produce both core genome and pangenome alignments.

The workflow subsequently triggers the generation of a SNP distance matrix and a phylogenetic tree using the core genome alignment via [snp-dists](https://github.com/tseemann/snp-dists) and [iqtree](http://www.iqtree.org/), respectively. Optionally, the workflow will also run this analysis using the pangenome alignment.

### Other Modifications
- AMRFinderPlus task modifications:
    - Default docker image updated to v3.10.26 and output `database version`
    - Drug class outputs brought to Terra data table
- kSNP3 task/workflow modifications
    - tree Newick file output extensions changed to `.nwk`
- Gambit docker task modified to utilize GAMBIT v0.5.0
- TS_MLST task modified to utilize MLST v2.23.0

### New Documentation
[Detailed documentation](https://theiagen.notion.site/d8c9c887cc714600936a7c6304da9b39?v=7b2e523c881a4a5ab547bb18b97e1c88) has been created for all workflows in the PHBG v1.1.0 repository.


---

## What's Changed
* amrfinderplus task updates by @kapsakcj in https://github.com/theiagen/public_health_bacterial_genomics/pull/137
* Add Streptococcus pneumoniae subworkflow by @kapsakcj in https://github.com/theiagen/public_health_bacterial_genomics/pull/141
* Adds subworkflow for A. baumannii, includes Kaptive task (K & O typing) by @erikwolfsohn in https://github.com/theiagen/public_health_bacterial_genomics/pull/138
* Kleborate updates by @kapsakcj in https://github.com/theiagen/public_health_bacterial_genomics/pull/148
* kSNP3 task edit: changed file suffix from .tree to .nwk by @kapsakcj in https://github.com/theiagen/public_health_bacterial_genomics/pull/146
* Adds drug class output to TheiaProk by @michellescribner in https://github.com/theiagen/public_health_bacterial_genomics/pull/145
* update gambit task to v0.5.0 docker image by @michellescribner in https://github.com/theiagen/public_health_bacterial_genomics/pull/151
* Spneumo subworkflow enhancements: docker & GPS db version outputs and upgrade default pbptyper docker by @kapsakcj in https://github.com/theiagen/public_health_bacterial_genomics/pull/149
* Add midas as optional TheiaProk task by @michellescribner in https://github.com/theiagen/public_health_bacterial_genomics/pull/159
* Add option to hide point mutations from AMRFinderPlus output & update default amrfinderplus docker image by @michellescribner in https://github.com/theiagen/public_health_bacterial_genomics/pull/158
* Fix gambit parsing for next_taxon_rank is None by @michellescribner in https://github.com/theiagen/public_health_bacterial_genomics/pull/161
* add task for Abaum plasmid typing to TheiaProk_Illumina_PE and SE by @kapsakcj in https://github.com/theiagen/public_health_bacterial_genomics/pull/160
* Add option to kSNP3 to create maximum likelihood and neighbor joining trees by @michellescribner in https://github.com/theiagen/public_health_bacterial_genomics/pull/166
* update default mlst docker image to staphb/mlst:2.23.0 & fix CI env by @kapsakcj in https://github.com/theiagen/public_health_bacterial_genomics/pull/163
* Modify midas parsing by @michellescribner in https://github.com/theiagen/public_health_bacterial_genomics/pull/172
* Adds shigella subworkflow by @kapsakcj in https://github.com/theiagen/public_health_bacterial_genomics/pull/162
* Adds bakta task  by @michellescribner in https://github.com/theiagen/public_health_bacterial_genomics/pull/170
* Add fastp task, modify read trimming parameters, and modify estimated coverage calculations by @michellescribner in https://github.com/theiagen/public_health_bacterial_genomics/pull/169
* Fja tbprofiler update by @frankambrosio3 in https://github.com/theiagen/public_health_bacterial_genomics/pull/174
* Add Core_Gene_SNP workflow by @michellescribner in https://github.com/theiagen/public_health_bacterial_genomics/pull/178
* adds p. aeruginosa subworkflow and pasty for serogrouping by @jrotieno in https://github.com/theiagen/public_health_bacterial_genomics/pull/179
* update pasty_docker default; add `pasty_comment` string output for PE and SE wfs by @kapsakcj in https://github.com/theiagen/public_health_bacterial_genomics/pull/181
* Revert default read trimming parameters to v1.0 by @michellescribner in https://github.com/theiagen/public_health_bacterial_genomics/pull/184
* Eld docs dev by @emmadoughty in https://github.com/theiagen/public_health_bacterial_genomics/pull/180
* Fixed printf to convert sci notation to integers by @frankambrosio3 in https://github.com/theiagen/public_health_bacterial_genomics/pull/177
* Add qc_check task to TheiaProk by @michellescribner in https://github.com/theiagen/public_health_bacterial_genomics/pull/182
* Generate gene_presence_absence.csv with pirate task by @HNHalstead in https://github.com/theiagen/public_health_bacterial_genomics/pull/185
* MLST novel alleles by @emmadoughty in https://github.com/theiagen/public_health_bacterial_genomics/pull/186
* Export Taxon Table Fix and others by @sage-wright in https://github.com/theiagen/public_health_bacterial_genomics/pull/188
* fix file extension awareness cg_pipeline by @michellescribner in https://github.com/theiagen/public_health_bacterial_genomics/pull/189

## New Contributors
* @jrotieno made their first contribution in https://github.com/theiagen/public_health_bacterial_genomics/pull/179
* @emmadoughty made their first contribution in https://github.com/theiagen/public_health_bacterial_genomics/pull/180
* @HNHalstead made their first contribution in https://github.com/theiagen/public_health_bacterial_genomics/pull/185

**Full Changelog**: https://github.com/theiagen/public_health_bacterial_genomics/compare/v1.0.0...1.1.0",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","theiagen-validations/PHBG_Validation_v1-1-0"
350,"veme-training","VEME NGS 2023","READER","https://app.terra.bio/#workspaces/veme-training/VEME%20NGS%202023",TRUE,FALSE,NA,NA,NA,"Viral alignment, variant calling, assembly, and metagenomics.

Workshop tutorial: https://broadinstitute.github.io/viral-workshops",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","veme-training/VEME NGS 2023"
351,"help-gatk","GATKTutorials-Somatic-May2019","READER","https://app.terra.bio/#workspaces/help-gatk/GATKTutorials-Somatic-May2019",TRUE,FALSE,NA,NA,NA,"# What's in this workspace?
Welcome to Day 3 of the Genome Analysis Toolkit (GATK) workshop at the Haartman Institute in Helsinki, Finland! Today we will focus on Somatic Variant Discovery.

Earlier today you received introductions to GATK tools and Best Practices pipelines. In this workspace we will be going over two forms of Somatic Analysis: one comparing tumor and normal samples using Mutect2 workflow for variant differences, and another using the Copy Number Alterations (CNA) workflow for copy number variations. 

Your instructor will guide you through this workspace, but you should find the documentation within the notebooks sufficient to run through them again on your own or to share with a friend later (and we encourage you to do so!).

### Data
Data associated with this workspace is located in the [gs://gatk-tutorials/workshop_1903/3-somatic](https://console.cloud.google.com/storage/browser/gatk-tutorials/workshop_1903/3-somatic/?project=broad-dsde-outreach&organizationId=548622027621) google bucket. It contains both input and resource files for the Mutect2 and CNA workflows. Along with the inputs are precomputed outputs generated by each step in the tutorial. This can be used as input for any step in the workflow, in case your generated outputs are not correct or you are unable to complete a step in the workflow. 

### Tools
There are no tools in this workspace. The tutorials are notebook-based, allowing you to run each step manually and view the intermediate outputs of the workflow. This is a great way to understand each step in the workflow and give you the chance to manipulate the parameters to see what happens with the output.

If you are interested in a WDL based workflow of these analyses, check out the [Showcase](https://app.terra.bio/#library/showcase) area in Terra, which features many of our popular GATK workflows in workspaces ready to run your data.


### Notebooks
 **1-somatic-mutect2-tutorial :**
In this hands-on tutorial, we will call somatic short mutations, both single nucleotide and indels, using the GATK4 Mutect2 and FilterMutectCalls. If you need a primer on what somatic calling is about, see the following [GATK forum Article](https://software.broadinstitute.org/gatk/documentation/article?id=11127).


| Runtime Environments | Value |
| --- | --- |
| CPU Minimum| 4|
| Disksize Minimum | 100 GB |
| Memory Minimum | 15 GB |
| Startup Script | gs://gatk-tutorials/scripts/install_gatk_4110.sh |

**2-somatic-cna-tutorial :**
This hands-on tutorial outlines steps to detect alterations with high sensitivity in total and allelic copy ratios using GATK4's ModelSegments CNV workflow. The workflow is suitable for detecting somatic copy ratio alterations, more familiarly copy number alterations (CNAs), or copy number variants (CNVs) for whole genomes and targeted exomes.

| Runtime Environments | Value |
| --- | --- |
| CPU Minimum| 2|
| Disksize Minimum | 100 GB |
| Memory Minimum | 13 GB |
| Startup Script | gs://gatk-tutorials/scripts/install_gatk_4110.sh |

 
### Software versions
GATK4.1.1.0

### Time and cost 
This workspace focuses on the use of notebooks, thus the time and cost of completing the tutorial depends on the runtime environment of your notebook and the length of time you spend actively working in the notebook. With the given runtime environments above, users can expect the cost to be much less than a dollar an hour for each notebook. The specific cost will be displayed on the top right corner of your screen, next to the notebook machine options. 

## Appendix

### GATK @ BroadE 2019 in the Terra Support Center

Get yourself oriented with the entire workshop in the [Terra Support Center](https://broadinstitute.zendesk.com/hc/en-us/community/topics). (more on Terra Support below).

### GATK documentation and support

Detailed documentation, tutorials, and more are available at the [GATK web site](https://software.broadinstitute.org/gatk/). If you are still running into issues after consulting the User Guide, have a question, or just want to say hello, reach out to the GATK team via the [GATK Support Forum](https://gatkforums.broadinstitute.org/gatk)!

### Terra documentation and support

Documents and helpful guides to support your journey through Terra are available in the Terra Support Center. Navigate there by following the “Learn About Terra” link in the left-side menu. At the time of this workshop, we are still actively writing and publishing documents but we’ve got a good set of docs in the Quick Start Guide to get you going.


",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/GATKTutorials-Somatic-May2019"
352,"help-gatk","Pre-processing_b37_v1","READER","https://app.terra.bio/#workspaces/help-gatk/Pre-processing_b37_v1",TRUE,TRUE,NA,NA,NA,"### GATK Best Practices for data pre-processing (b37 reference)
The purpose of this workspace is to provide a fully reproducible example of the GATK Best Practices workflow for pre-processing data (intended for downstream variant discovery analysis). 

#### Workspace attributes
All required and optional resources for the preconfigured methods included. The reference genome is b37 (aka GRCh37), the Broad Institute's version of hg19. Some resource files may be named hg19 for historical reasons. 

#### Data 
Two sets of 24 unmapped BAMs of NA12878 WGS data (one per read group) downsampled to 20% (med) and 5% (small), respectively. 

#### Method configs
This workspace contains the following preset method configurations:

- **PreProcessingForVariantDiscovery_GATK4**
Generic version of the pre-processing portion (read group uBAMs to analysis-ready BAM) of the single-sample pipeline used in production at the Broad Institute, running GATK4 according to GATK Best Practices (June 2016). Usage instructions: launch this on any qualifying Sample or Set of Samples (each sample must reference a list of unmapped BAMs, one per read group). See workflow for additional input requirements and version notes.",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/Pre-processing_b37_v1"
353,"broad-firecloud-tcga","TCGA_KIRP_OpenAccess_V1-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_KIRP_OpenAccess_V1-0_DATA",TRUE,TRUE,"Kidney Renal Papillary Cell Carcinoma","Tumor/Normal","USA","TCGA Kidney renal papillary cell carcinoma Open-Access Data Workspace","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients’ clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","291","Kidney","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh37/hg19","TCGA_KIRP_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_KIRP_OpenAccess_V1-0_DATA"
354,"broad-firecloud-dsde","gatk-sv-single-sample-example","READER","https://app.terra.bio/#workspaces/broad-firecloud-dsde/gatk-sv-single-sample-example",TRUE,FALSE,NA,NA,NA,"# GATK-SV Single Sample Pipeline

The GATK-SV Single Sample Pipeline is an integrated structural variation detection and resolution pipeline designed to call
many forms of structural variation in whole genome sequencing data obtained from a single sample. The pipeline will identify,
genotype, and annotate structural variation from the following variant types: copy number variants, including deletions and
duplications; insertions; inversions; reciprocal chromosomal translocations; and several forms of complex structural variation. 
The Single Sample pipeline is based upon the GATK-SV cohort pipeline, which jointly analyzes WGS data from large research
cohorts and has been used to create SV call sets for [gnomAD-SV](https://www.nature.com/articles/s41586-020-2287-8) and the
[SFARI SSC autism research study](http://dx.doi.org/10.1038/s41588-018-0107-y). The Single Sample pipeline in this workspace
is designed to facilitate running the methods developed for the cohort-more GATK-SV pipeline on small data sets or in 
clinical contexts where batching large numbers of samples is not an option. To do so, it uses precomputed data, SV calls,
and model parameters computed by the cohort pipeline on a reference panel composed of similar samples. The pipeline integrates this
precomputed information with signals extracted from the input CRAM file to produce a call set similar in quality to results
computed by the cohort pipeline in a computationally tractable and reproducible manner.

GATK-SV uses Manta, WHAM, GATK gCNV, and cn.MOPS as raw calling algorithms, and then integrates, filters, refines, and annotates 
the calls from these tools to produce a final output. Please note that most large published joint call sets produced by
GATK-SV, including gnomAD-SV, included the tool MELT, a state-of-the-art mobile element insertion (MEI) detector, as part of the pipeline. 
Due to licensing restrictions, we cannot provide a public docker image or reference panel VCFs for this algorithm. The 
version of the pipeline configured in this workspace does not run MELT or include MELT calls for the reference panel. Therefore, 
the output will be less sensitive to MEI calls that might appear in gnomAD or other joint call sets. 

Source code for all GATK-SV pipelines is stored and documented in the [GATK-SV GitHub repository](https://github.com/broadinstitute/gatk-sv).

Please cite the [gnomAD-SV](https://www.nature.com/articles/s41586-020-2287-8) paper when referring to these methods.

## Data Used

### Case Sample

This workspace contains configurations for generating SV calls for a NA12878 input WGS CRAM file. This file is part of the
high coverage (30X) [WGS data](https://www.internationalgenome.org/data-portal/data-collection/30x-grch38) for the 1000
Genomes Project samples generated by the New York Genome Center and hosted in
[AnVIL](https://app.terra.bio/#workspaces/anvil-datastorage/1000G-high-coverage-2019). 

### Reference Panel

The reference panel configured in this workspace consists of data and calls computed from 156 publicly available samples
chosen from the NYGC/AnVIL 1000 Genomes high coverage data linked above. Inputs to the pipeline for the reference panel include
a precomputed SV callset VCF; raw calls for the reference panel samples from Manta and WHAM; trained models for
calling copy number variation in GATK gCNV case mode; and parameters learned by the cohort mode pipeline in machine learning
models executed on the reference panel samples.

### Reference Resources

The pipeline uses a number of resource and data files computed for the hg38 reference. These include reference
sequences and indices, genome tracks such as segmental duplication and RepeatMasker tracks, and data used for annotation of
called variants, including GenCode gene tracks and gnomAD site allele frequencies. 

## Workflows

### GATKSVSingleSample

#### What does it do?

Run the GATK-SV Single Sample Pipeline end-to-end on an input CRAM file from the case sample.

#### What does it require as input?

Case data is configured by the following inputs
to the pipeline:

|Input Type|Input Name|Description|
|---------|--------|--------------|
|`String`|`sample_id`|Sample identifier to use for the case sample|
|`File`|`bam_or_cram_file`|Path to the GCS location of the input CRAM or BAM file|
|`String`|`batch`|Arbitrary name to be assigned to the run|
|`Boolean`|`requester_pays_cram`|Set to `true` if the case data is stored in a requester-pays GCS bucket|

Other inputs to the workflow represent the reference resources for hg38, the input data for the reference panel, and the
set of docker images used in the pipeline. Please contact GATK-SV developers if you are interested in customizing these
inputs beyond their defaults.

#### What does it return as output?

|Output Type|Output Name|Description|
|---------|--------|--------------|
|`File`|`final_vcf`|SV VCF output for the pipeline. Includes all sites genotyped as variant in the case sample and genotypes for the reference panel. Annotated with overlap of functional genome elements and allele frequencies of matching variants in gnomAD|
|`File`|`final_vcf_idx`|Index file for `final_vcf`|
|`File`|`final_bed`|Final output in BED format. Filter status, list of variant samples, and all VCF INFO fields are reported as additional columns.|
|`File`|`metrics_file`|Metrics computed from the input data and intermediate and final VCFs. Includes metrics on the SV evidence, and on the number of variants called, broken down by type and size range.|
|`File`|`qc_file`|Quality-control check file. This extracts several key metrics from the `metrics_file` and compares them to pre-specified threshold values. If any QC checks evaluate to FAIL, further diagnostics may be required.|
|`File`|`ploidy_matrix`|Matrix of contig ploidy estimates computed by GATK gCNV.|
|`File`|`ploidy_plots`|Plots of contig ploidy generated from `ploidy_matrix`|
|`File`|`non_genotyped_unique_depth_calls`|This VCF file contains any depth based calls made in the case sample that did not pass genotyping checks and do not match a depth-based call from the reference panel. If very high sensitivity is required, examine this file for additional large CNV calls.|
|`File`|`non_genotyped_unique_depth_calls_idx`|Index file for `non_genotyped_unique_depth_calls`|
|`File`|`pre_cleanup_vcf`|VCF output in a representation used internally in the pipeline. This file is less compliant with the VCF spec and is intended for debugging purposes.|
|`File`|`pre_cleanup_vcf_idx`|Index file for `pre_cleanup_vcf`|

#### Example time and cost run on sample data

|Sample Name|Sample Size|Time|Cost $|
|-----------|-----------|----|------|
|NA12878|18.17 GiB|23hrs|~$8.00|

##### License 

Copyright (c) 2009-2020, Broad Institute, Inc. All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are met:

* Redistributions of source code must retain the above copyright notice, this
  list of conditions and the following disclaimer.

* Redistributions in binary form must reproduce the above copyright notice,
  this list of conditions and the following disclaimer in the documentation
  and/or other materials provided with the distribution.

* Neither the name Broad Institute, Inc. nor the names of its
  contributors may be used to endorse or promote products derived from
  this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""
AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.



",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","broad-firecloud-dsde/gatk-sv-single-sample-example"
356,"broad-firecloud-tcga","TCGA_CESC_OpenAccess_V1-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_CESC_OpenAccess_V1-0_DATA",TRUE,TRUE,"Cervical Squamous Cell Carcinoma and Endocervical Adenocarcinoma","Tumor/Normal","USA","TCGA Cervical squamous cell carcinoma and endocervical adenocarcinoma Open-Access Data Workspace","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients’ clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","308","Cervix","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh37/hg19","TCGA_CESC_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_CESC_OpenAccess_V1-0_DATA"
357,"broad-firecloud-tcga","TCGA_STAD_hg38_OpenAccess_GDCDR-12-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_STAD_hg38_OpenAccess_GDCDR-12-0_DATA",TRUE,TRUE,"Stomach adenocarcinoma","Tumor/Normal","USA","TCGA Stomach adenocarcinoma Open-Access Data Workspace

*hg38 TCGA and TARGET workspaces reference files by their GDC UUIDs.  In order to run analyses on the referenced data files, you will need to run workflows that retrieve the files from the GDC and copy them to your workspace bucket.  See   [this forum post](https://gatkforums.broadinstitute.org/firecloud/discussion/10382/populating-hg38-tcga-and-target-workspaces-with-data-files#latest) for instructions on the running of these workflows.*","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients' clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","478","Stomach","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh38/hg38","TCGA_STAD_hg38_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_STAD_hg38_OpenAccess_GDCDR-12-0_DATA"
358,"warp-pipelines","Exome-Analysis-Pipeline","READER","https://app.terra.bio/#workspaces/warp-pipelines/Exome-Analysis-Pipeline",TRUE,TRUE,NA,NA,NA,"### Exome Germline Variant Discovery used by Broad Genomics
This workspace contains fully reproducible example workflows for exome sequence data pre-processing, germline short variant discovery, and joint variant calling, as used for production by the Genomics Platform at the Broad Institute and recommended for research purposes.

Descriptions of the workflows are available in GATK's Best Practices Documents for [preprocessing](https://gatk.broadinstitute.org/hc/en-us/articles/360035535912) and [Germline short variant discovery](https://gatk.broadinstitute.org/hc/en-us/articles/360035535932).  

#### This workspace is an updated version of the previously featured [Exome-Analysis-Pipeline workspace](https://app.terra.bio/#workspaces/help-gatk/Exome-Analysis-Pipeline).

The materials in this workspace were developed by the Data Sciences Platform at the Broad Institute. 

## Workflows Overview

![](https://storage.googleapis.com/terra-featured-workspaces/Exome-Analysis-Pipeline/Exome_overview.png)

This workspace has three example workflows that you can run using the “Running the workflow” instructions below. The workflows are designed to run in the order presented, but we have provided sample inputs for each workflow so that you can try each one independently.  

1. **ExomeGermlineSingleSample**: performs preprocessing on a set of unmapped BAM files (multiple read groups per single sample) to produce single CRAM and reblocked GVCF files per sample.

2.  **Generate-Sample-Map**:  takes as input several (reblocked) GVCF files (i.e. output from ExomeGermlineSingleSample) and generates a sample map file, a text file where the first column is the name of the sample and the second column contains the path to the samples GVCF file.

3.  **JointGenotyping**: takes a sample map file listing the (reblocked) GVCFs produced with ExomeGermlineSingleSample and performs variant calling on all the provided GVCF files. It then filters to produce a multi-sample VCF (a minimum of 50 samples is required).   

## Workflows

### 1. Exome Germline Single Sample

**What does it do?**     

This WDL pipeline implements data pre-processing and initial variant calling (GVCF generation) according to the GATK best practices for germline SNP and indel discovery in human exome sequencing data. 

The workflow takes as input an array of unmapped BAM files (all belonging to the same sample) to perform preprocessing tasks such as mapping, marking duplicates, and base recalibration then uses Haplotypecaller and ReblockGVCF to generate a reblocked GVCF or VCF. 

For the latest version of the workflow please visit the WARP repository [ExomeGermlineSingleSample](https://github.com/broadinstitute/warp/tree/master/pipelines/broad/dna_seq/germline/single_sample/exome). You can also read the pipeline's [documentation](https://broadinstitute.github.io/warp/docs/Pipelines/Exome_Germline_Single_Sample_Pipeline/README). 

**What data does it require as input?**    

- Human exome sequencing data in unmapped BAM (uBAM) format
     - If your sequence files are not in unmapped BAM format, please review the [Sequence-Format-Conversion](https://app.terra.bio/#workspaces/help-gatk/Sequence-Format-Conversion) workspace for file conversion workflows. 
- One or more read groups, one per uBAM file, all belonging to a single sample (SM)
- Input uBAM files must additionally comply with the following requirements:
  - Filenames all have the same suffix (we use "".unmapped.bam"")
  - Files must pass validation by ValidateSamFile
  - Reads are provided in query-sorted order
  - All reads must have an RG tag
- Reblocked GVCF output names must end in "".rb.g.vcf.gz""
- Reference genome must be Hg38 with ALT contigs
- Unique exome calling, target, and bait .interval_list obtained from sequencing provider. Generally, the calling, target, and bait files will not be the same.

**What does it return as output?**

The following files are stored in the workspace Google bucket and links to the files are written to the `read_group_set` data table:    
- CRAM file, CRAM index, and CRAM md5 
- [Reblocked](https://gatk.broadinstitute.org/hc/en-us/articles/4414594365467) GVCF and its GVCF index (read more about reblocking in the [Exome pipeline documentation](https://broadinstitute.github.io/warp/docs/Pipelines/Exome_Germline_Single_Sample_Pipeline/README#reblocking) and [WARP blog](https://broadinstitute.github.io/warp/blog/Nov21_ReblockedGVCF))
- BQSR report
- Several summary metrics       

**Running the workflow**

The workflow in this workspace is pre-configured to use the `NA12878` sample listed in the `read_group_set` data table in the workspace Data tab. This example is a single sample with two read groups. The unmapped BAM for each `NA12878` read group is listed in the `read_group` data table. 

The workflow is configured to call this input from the data table. To run:
1. Select the workflow from the Workflows tab. 
2. In the configuration page, select the `read_group_set` entity in Step 1.
3.  Choose the `NA12878` dataset in Step 2.
4.  Select the checkbox next to `Use reference disks`.
5.  Run the workflow.

If you want to use this workflow on your own samples, you can download the Input JSON file from the workflow configuration page and use it as a template for setting up your own data.      

- Optional inputs, like the `fingerprint_genotypes_file`, will need to match your samples. This workspace is set up to check fingerprints for the `NA12878` sample optionally.
- For the CheckFingerprint task, the sample name specified in the `sample_and_unmapped_bams` variable must match the sample name in the `fingerprint_genoptyes_file`(VCF format).

**Important configuration notes** 
- The workflow is written in WDL1.0 and imports structs to organize and use inputs. 

- If you run the workflow on a sample that only has one uBAM (i.e., one read group), you need to update the config attributes for  `sample_and_unmapped_bams` to include `[]` around the `flowcell_unmapped_bams` as shown below:

`{ ""sample_name"": this.read_group_set_id, ""base_file_name"": this.read_group_set_id, ""flowcell_unmapped_bams"": [this.read_groups.flowcell_unmapped_bams], ""final_gvcf_base_name"": this.read_group_set_id, ""unmapped_bam_suffix"": "".bam"" }`

-By default, the workflow produces a reblocked GVCF to be used in [joint calling](https://gatk.broadinstitute.org/hc/en-us/articles/360035890431), but can be set to directly output a VCF instead of a GVCF. 

-The reblocking step is optional; to turn it off, change the `skip_reblocking`parameter  to `true`.

**Reference data description and location**     

The required and optional references and resources for the workflows are set in the workflow configurations. The reference genome is hg38 (aka GRCh38).

**Enabling reference disks**

The suggested workflow configuration (see ""Running the workflow"" above) uses [reference disks](https://support.terra.bio/hc/en-us/articles/360056384631). That means when Terra kicks off the workflow on a virtual computer, it attaches a portable disk (kind of like a flash-drive) that is preloaded with the reference files needed for the workflow. This saves time and cost running the workflow. 

**Time and cost estimates** 
Below is an example of the time and cost for running the workflow with the `Use reference disk` option enabled.

| Sample Name | Number of Entities | Sample Size | Time | Cost $ |
| ---  | --- | --- | --- | --- |
| NA12878 | 2 | 8.08 GB | 06:21:00 | ~$0.61 |

**Note:** 

For more information about controlling Cloud costs, see [Understanding and controlling Cloud costs on Terra](https://support.terra.bio/hc/en-us/articles/360029748111).

------------
### 2. Generate Sample Map

**What does it do?**    
This WDL generates a sample_map file, a tab-delimited text file of 2 columns; 1. the name of the sample and 2. the file path (in this case, the Google bucket path of the file). 

The sample map can be used for downstream tools, like the JointGenotyping workflow, that require the sample names in the GVCF header. It allows the downstream tools to determine the sample names without downloading the GVCF headers.

**What does it require as input?**  
- An array of file names
- An array of file paths
- Name of output sample_map 

**What does it return as output?**    
- Sample map file

**Running the workflow**

Example paths to GVCF files are listed in the `samples` data table of the workspace Data tab. To run the workflow, select the workflow from the Workflows tab. Select the `sample_set` data table as the root entity and choose the `Example_JointGenotyping` set. 

**Reference data description and location**  
Reference files are not used in this workflow. 

**Time and cost estimates**         
Below is an example of the time and cost for running the workflow.

| Sample Name | Number of Entities | Sample Size | Time | Cost $ |
| -------  | -------- | -------- | ---------- | --- |
| Example_JointGenotyping | 51 | NA | 00:02:00 | ~$0.01 |

------------
### 3. Joint Genotyping    
**What does it do?**    
This WDL implements the joint calling and variant quality score recalibration (VQSR) filtering portion of the GATK Best Practices.

**What does it require as input?**  
- A sample_map file listing the GVCFs produced by HaplotypeCaller in GVCF mode.
- Bare minimum: 50 samples. Gene panels are not supported.

**What does it return as output?**     
A VCF file and its index, filtered using VQSR, with genotypes for all samples present in the input VCF. All sites that are present in the input VCF are retained. Filtered sites are annotated as such in the FILTER field.      
	
**Running the workflow**     
The JointGenotyping workflow accepts a sample_map file that lists GVCFs produced by haplotypecaller.  The map file is provided in the `sample_map` column of the `sample_set` data table. 

To run, select the workflow from the Workflows tab and choose the `sample_set` table as the root entity. Then, select the `Example_JointGenotyping` data and launch the workflow.

**Reference data description and location**  
Required and optional references and resources for the workflows are included in the workflow configuration. **JointGenotyping** is configured with hg38 references.

**Estimated time and cost to run on sample data with reference disks enabled**     

| Sample Name | Time | Cost $ |
| :---:  | :---: | :---: |
| Example_JointGenotyping | 02:45:00 | $2.88 |  
 
---

### Additional Resources
As the workflows highlighted here are used for production, they contain unique quality measures that differ from other GATK-related workflows for short variant discovery. 
- For additional examples of applying the GATK Best Practices, including Jupyter Notebooks with step-by-step GATK workflow details, try the [GATK4-Germline-Preprocessing-VariantCalling-JointCalling Workspace](https://app.terra.bio/#workspaces/help-gatk/GATK4-Germline-Preprocessing-VariantCalling-JointCalling).
- This workspace showcases whole exome analysis, but the Generate-Sample-Map and  JointGenotyping workflows can similarly be applied to whole genome analysis. See the [Whole-Genome-Anaysis-Pipeline workspace](https://app.terra.bio/#workspaces/warp-pipelines/Whole-Genome-Analysis-Pipeline) for an example genome sequencing workflow.


* For questions regarding GATK-related tools and Best Practices, see the [GATK website](https://gatk.broadinstitute.org/hc/en-us).
* For Terra-specific documentation and support, see the [Terra Support](https://support.terra.bio/hc/en-us).

### Contact information  
* For workspace questions and feedback, email the Broad pipelines team at warp-pipelines-help@broadinstitute.org or create a post on the [Featured Workspaces community forum](https://support.terra.bio/hc/en-us/community/topics/360001603491) (login required). Tag @Alexander Baumann in the **Details** section of your post.

* You can also Contact the Terra team for questions from the Terra main menu. When submitting a request, it can be helpful to include:
    * Your Project ID
    * Your workspace name
    * Your Bucket ID, Submission ID, and Workflow ID
    * Any relevant log information

* Please post any questions or concerns regarding the GATK tools to the [GATK forum](https://gatk.broadinstitute.org/hc/en-us/community/topics)

### Workspace Citation 
Details on citing Terra workspaces can be found here: [How to cite Terra](https://support.terra.bio/hc/en-us/articles/360035343652)

Data Sciences Platform, Broad Institute (*Year, Month Day that this workspace was last modified*) warp-pipelines/Exome-Analysis-Pipeline [workspace] Retrieved *Month Day, Year that workspace was retrieved*,  https://app.terra.bio/#workspaces/warp-pipelines/Exome-Analysis-Pipeline

### License  
**Copyright Broad Institute, 2023 | BSD-3**  
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at https://github.com/broadinstitute/warp/blob/develop/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.

### Workspace Change Log
| Date | Change | Author |
| --- | --- | --- |
| 2023-08-16 | Updated Exome and JointGenotyping workflows to versions 3.1.10 and 1.6.8, respectively; updated tool versions and Docker images to address vulnerabilities. | Kaylee Mathews |
| 2022-09-12 | Updated Exome to v3.1.5; more memory added to several tasks. | Liz Kiernan |
| 2022-08-28 | Updated contact information. | Kaylee Mathews |
| 2022-05-04 | Updated workflows to latest version. Updated to Picard version 2.26.10 and GATK version 4.2.6.1 to address log4j vulnerabilities. Added workspace overview diagram. | Kaylee Mathews |
| 2022-02-09 | Updated workflows to latest version and added reblocked GVCF outputs. | Kaylee Mathews |
| 2021-09-01 | Added configuration note for running samples with only one uBAM. | Liz Kiernan |
|  2020-10-20 | Initial feature of workspace. | Sophie Crennan |
|  2021-09-15 | Addition of Workspace Citation section. | Beri Shifaw |",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","warp-pipelines/Exome-Analysis-Pipeline"
359,"terra-outreach","Viral_Insertion_Detection","READER","https://app.terra.bio/#workspaces/terra-outreach/Viral_Insertion_Detection",TRUE,FALSE,NA,NA,NA,"### Prototype Overview
**This workspace demonstrates a proof-of-concept for an approach to viral insertion detection.** It includes a pipeline for identifying viral reads found in a host organism, and detects potential insertion sites in a host's genome. It uses popular GATK tools like Pathseq for viral read detection and BWASpark for alignment, as well as bedtools and Integrative Genomics Viewer (IGV) for providing final ""insertion snapshots"" that a researcher can use to further investigate detected sites. 

Users may find value in the WDL code for filtering, realignment, and IGV screenshot capturing and may use it in their own workflows. 
**Note that this is not an officially maintained GATK tutorial.**

Pre-reading:
1. [Blog post about this workspace](https://support.terra.bio/hc/en-us/articles/360052481131)
2. [Walker MA, Pedamallu CS, et al. Bioinformatics. 2018;34(24):4287-4289.
](https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/bty501)
3. [Kostic AD, et al. Nat Biotechnol. 2011;29(5):393-396.](https://www.nature.com/articles/nbt.1868)


### Methods
We prototyped a simple method for identifying potential viral insertion sites in a host genome. Users are encouraged to take this workspace and iterate on this method with their own data. Workflows are also provided for generating samples in silico with a specified insertion site. The method is designed as follows:

1. Run PathSeq on a sample BAM against a comprehensive virus reference to identify virus reads. Here PathSeq parameters have been optimized for detecting chimeric reads.
2. Align viral reads found by Pathseq to the host reference using BWASpark (insertion sites will have chimeric reads that align to both host and the viral genome).
3. Scan the genome for clusters of chimeric reads and take IGV snapshots of these loci for manual review.

### Improvements
We acknowledge that our approach could be improved, for example by looking for read pairs where one read aligned to human and the other to virus, but ultimately decided to limit the scope of the project to simple soft-clipped read detection, which performed reasonably well.

### Benchmarks on cervical cancer samples

We tested the method on a set of 4 cervical cancer tumor samples from [Hu et al.](http://dx.doi.org/10.1038/ng.3178) containing HPV insertion sites validated using whole-genome, virus capture, and Sanger sequencing. Sample sequencing data (10x mean coverage) was transferred from the NCBI SRA into Terra using the SRA-to-uBAM workflow and then run through workflows 4 and 5. 

The table below summarizes the results. The method identified the same validated insertion sites reported by the study in 3 of the 4 samples (T6050, T4931, SIHA). In one sample (HELA), an insertion site was located in a long tandem repeat (LTR) element within the FHIT gene on chromosome 3, whereas Hu et al. reported finding an insertion site in the POU5F1B gene on chromosome 8. There were supporting two reads at this site, but BLAT returned no alignments to the region reported in the study. However, 11 integration sites located in FHIT from 9 other samples were detected by Sanger sequencing.

**Summary of detected insertion sites in cervical cancer samples**

| Sample Name | Insertion Locus | Gene | Validated |
| :---:  | :---: | :---: | :---: |
| T6050 | 11:132514827-132515827; 13:74230267-74231938 | - | Y |
| HELA | 3:60579865-60580877 | FHIT | N |
| T4931 | 2:121669666-121670666; 2:121686584-121687605; 2:121687681-121688681 | GLI2 | Y |
| SIHA | 13:73788366-73789366; 13:74087010-74088017 | - | Y |



**Caveats:** This prototype will not do a good job calling insertions near sites of microhomology because this approach is looking for chimeric reads and the aligner favors aligning to sites without these. We are not doing paired end alignment so there is less accurate mapping with this approach.  

---
---
 
## Data 
### Synthetic sample
In this example workspace, we create a synthetic sample which is a subset of hg19 with 30x coverage over chromosome 17 and insert **HPV16 in the p53 gene at chr17:7,572,926** with a megabase on each side of the site.

Workflows 1 and 2 are used to create the synthetic sample with all generated output shown in the participant table.

If you would like to create your own synthetic data using these pipelines replace the following input files in the configuration of workflows 1 & 2: 

### 1-Generate_synthetic_reads

**Synopsis:** 
*Generates synthetic read data based on intervals and a VCF of variants.*

**Input files can be found [here](https://console.cloud.google.com/storage/browser/terra-featured-workspaces/Viral-Insertion-Detection)**
- vcf (ex. is hpv_16.vcf)
- interval where you want the insertion to occur (ex. intervals_exome_b37_chr17_6.5-8.5mbp.bed)
- text file pointing to the intervals you want to scatter over (ex. intervals_scattered_list.txt)

NEAT is a toolkit written by Zachary Stephens et al. and the documentation and source code is available on Github at https://github.com/zstephens/neat-genreads

### 2-Mutate-Reads-with-BAMSurgeon-all-variants

**Synopsis:**
*Adds SNVs, Indels, or SVs to BAM alignments.*

**Input files can be found [here](https://console.cloud.google.com/storage/browser/terra-featured-workspaces/Viral-Insertion-Detection)**
- input sv variant (hpv16indel.txt)

*Note: Virus-only fasta was obtained from Refseq: ftp://ftp.ncbi.nlm.nih.gov/refseq/release/viral/ (use files ending in .fna.gz)*

**Output synthetic bams can be found [here](https://console.cloud.google.com/storage/browser/terra-featured-workspaces/Viral-Insertion-Detection/input_synth_bams;tab=objects?pageState=(%22StorageObjectListTable%22:(%22f%22:%22%255B%255D%22))&forceOnObjectsSortingFiltering=false)**

BAMSurgeon is a toolkit written by Adam Ewing et al. and the documentation and source code is available on Github at https://github.com/adamewing/bamsurgeon . Direct link to [manuel](https://github.com/adamewing/bamsurgeon/blob/master/doc/Manual.pdf)

If you would like to use your own sample, ignore workflows 1 & 2 and the participant table.

### Optional workflow


**Optional-SRA-to-UBAM** - this workflow is provided if you'd like to use SRA data for your analysis. The configuration is filled in so if you'd like to use it you simply need to update the sample data table to include a column labeled ""accession"" and provide the accession code of interest. 

---
---

## Workflows for Analysis
Since workflows 1 & 2 are described above in the data section, this section will focus on workflows 3-5 and the reasons you may need to use the workflow marked optional. 

**Reference/Resource data description and location**  
The reference genome for this workspace is hg19. Required and optional references and resources for the methods are included in the Workspace Data table and the [gatk-featured-workspaces/Viral-Insertion-Detection](https://console.cloud.google.com/storage/browser/terra-featured-workspaces/Viral-Insertion-Detection) bucket

### 3-Pathseq-build-microbe-reference

**What does it do?**  
*Builds a microbe reference for use in Pathseq.* You only need to run this workflow if you want to get the latest RefSeq accession catalog/ taxonomy data files. For convenience, the outputs of this workflow are already produced and configured to be used in workflow 4.    

**Input:**       
FASTA file containing microbe sequences from NCBI RefSeq

**Output:**  
- FASTA index and dictionary files (written to the sample table)
- GATK BWA-MEM index image
- PathSeq taxonomy file

**Estimated time and cost to run on sample data**

| Sample Name | Sample Size | Time | Cost $ |
| :---:  | :---: | :---: | :---: |
| Hpv16_microbe_fasta | 295 MB | 0:26 | 0.02 | 

### 4-Pathseq-pipeline-WGS-downsampled

**What does it do?**  
*Runs GATK PathSeq pipeline for detecting pathogens*     

**Input:**       
- BAM or CRAM
- Pathseq host image: BWA-MEM index image of the reference
- Pathseq microbe image: BWA-MEM index image of the virus
- Pathseq microbe fasta dictionary
- Pathseq host.bfi: Bloom filter of host reference k-mers
The microbe image and fasta are provided by workflow 3. The host image and bfi are provided as a part of the Pathseq resource bundle (we recommend using these files instead of creating your own) in the gatk-best-practices bucket: https://console.cloud.google.com/storage/browser/gatk-best-practices/pathseq/resources

**Output:**  
- BAM containing all high-quality non-host reads aligned to a microbe reference 
- Score metrics file
- Taxonomy score file
- Total read count
There are several outputs generated that can be found in the Google bucket of the submission, but these are the only files that will be written back to the sample table. You can change the configuration if you'd like these other files to also be written back to the table. 

**Estimated time and cost to run on sample data**  

| Sample Name | Sample Size | Time | Cost $ |
| :---:  | :---: | :---: | :---: |
| 1 | 753 KB | 0:21 | 0.03 | 

### 5-Pathseq-virus-insertion

**What does it do?**  
*Generates IGV snapshots of potential viral insertion sites*     

**Input:** 
- BAM (aligned to microbe)
- Host reference files
- hg19 reference image (hg38 can be found [here](https://console.cloud.google.com/storage/browser/terra-featured-workspaces/Viral-Insertion-Detection))
- Taxonomy id for virus ([find here](https://www.ncbi.nlm.nih.gov/Taxonomy/Browser/wwwtax.cgi))

The input BAM for this workflow is generated by workflow 4. The reference image can be created using the Optional-BWAIndexImage. Here the hg_19 image is provided in the configuration.

**Output:** 
- IGV snapshots
- Viral aligned sorted BAM & BAI to host reference

These outputs are written to the sample table and the other outputs can be found in the Google bucket under the submission-id or viewed in the Job History under Outputs.

**Estimated time and cost to run on sample data**  

| Sample Name | Sample Size | Time | Cost $ |
| :---:  | :---: | :---: | :---: |
| 1 | 60 KB | 4:15 | 0.01 | 
    


---

### Workspace Change Log
| Date | Change | Author |
| --- | --- | --- |
|  2020-10-20 | Reviewed | Jason Nomburg |
|  2020-08-17 | Workspace drafted | Tiffany Miller |

### Acknowledgements

Many thanks to Jason Nomburg for being our scientific collaborator and reviewing this workspace.

### Contact information

Please direct all questions to the [Terra forum](https://support.terra.bio/hc/en-us/community/topics/360001603491-Featured-Workspaces) and tag @Mark Walker and @Tiffany Miller
 ",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","terra-outreach/Viral_Insertion_Detection"
361,"ASHG2021","2021-ASHG-Galaxy-SV-Discovery","READER","https://app.terra.bio/#workspaces/ASHG2021/2021-ASHG-Galaxy-SV-Discovery",TRUE,FALSE,NA,NA,NA,"# Structural variant discovery from long-read sequencing data on the cloud with Galaxy in Terra
#### Wednesday January 19, 2022 at 12:00 pm - 1:30 pm EST

This workshop guides you through an end-to-end SV identification journey using Galaxy, a platform designed to facilitate access to computational methods for researchers without a programming background.

The accompanying tutorial workspace includes a step-by-step guide for analyzing data in Galaxy. 

Using this workspace, you will:

* Launch a Galaxy instance in AnVIL powered by Terra
* Upload data
* Check the quality of sequencing reads with FastQC
* Align sequences to a reference genome with minimap2
* Generate mismatched and deleted reference bases (MD) tags with CalMD
* Call structural variants with sniffles
* Merge structural variants across samples with JasmineSV
* Sort a VCF dataset by coordinates with VCFsort
* Extract a field from a VCF file with bcftools query
* Visualize the results and identify potentially pathogenic variants

## Getting started — setting up the tutorial workspace

Before you begin, create your own editable copy (clone) of this WORKSPACE. Click the round circle with three dots in the upper right corner of this page and choose ""Clone"".


![](https://storage.googleapis.com/terra-featured-workspaces/ASHG_January_2022/clone_image.png)

![]()


## Work from your copy of the workspace following these step-by-step instructions
|![doc icon](https://storage.googleapis.com/terra-featured-workspaces/QuickStart/icon_read_more%401x.png) | Follow step-by-step written instructions [here](https://drive.google.com/file/d/1A6AE-QPEbjebsh8XogQ6Eds-gkFnV3sV/view?usp=sharing)|
| --------| ------------------| 

![white space](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/white-space.jpg)

|![doc icon](https://storage.googleapis.com/terra-featured-workspaces/QuickStart/icon_read_more%401x.png) | Follow step-by-step instructions with screenshots [here](https://drive.google.com/file/d/1-LeNBqTd3RXWoYXns8WPN5KgGlOeYaVT/view?usp=sharing) |  
| --------| ------------------|   

![white space](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/white-space.jpg)  


You can download the data we'll be using for this exercise here: [hg002.fq.gz](https://schatz-lab.org/anvil/2022.01.19.ASHG.AnVIL.Galaxy.SV/hg002.fq.gz), [hg003.fq.gz](https://schatz-lab.org/anvil/2022.01.19.ASHG.AnVIL.Galaxy.SV/hg003.fq.gz), [hg004.fq.gz](https://schatz-lab.org/anvil/2022.01.19.ASHG.AnVIL.Galaxy.SV/hg004.fq.gz).

### Have questions?
Post your question to the AnVIL Help forum: [https://help.anvilproject.org/](https://help.anvilproject.org/).

## Workshop Description
This workshop guides you through an end-to-end SV identification journey using Galaxy, a platform designed to facilitate access to computational methods for researchers without a programming background. Specifically, we will use Galaxy in Terra, in the context of the NHGRI Genomic Data Science Analysis, Visualization and Informatics Lab-space (AnVIL). This cloud-based environment enables you to analyze large genomic datasets with familiar tools and reproducible workflows securely.

Through live demonstrations and interactive exercises, you will learn how to:

* Bring data into a project workspace in Terra
* Launch a Galaxy instance in Terra and run a complete workflow to identify SVs
* Visualize results and identify potentially pathogenic variants
* The skills you will learn in this workshop will extend to other scientific use cases, datasets and tools beyond the examples shown.

## Background
Growing evidence that structural variants (SVs) are responsible for many types of diseases and traits is fueling interest in taking a fresh look at different disease types using long-read sequencing. Although short-read technologies have long been cheaper and more readily available, long-read sequencing produces data that can yield significantly more accurate results for identifying SVs.

However, the large amounts of data and complexity of the computational methods involved can make it difficult for newcomers to access this exciting area of research, particularly in the context of the traditional computing environments that are provided by default to academic researchers.

## Audience
Researchers and clinicians are interested in exploring SV calling with long-read sequencing data. This workshop will also appeal to anyone more broadly interested in practical ways to access and analyze data in the cloud - with or without advanced computing training.

## Prerequisites
The ideal audience member will have a basic familiarity with genomics terminology and standard high-throughput sequencing data formats.

## Agenda-at-a-glance
#### 12:00 pm - 12:05 pm | Welcome and overview of workshop agenda and logistics 

#### 12:05 pm - 12:15 pm | Overview of structural variants and methods for SV discovery

#### 12:15 pm - 12:25 pm | AnVIL Powered by Terra Overview

An overview of AnVIL Powered by Terra platform and its capabilities, focusing on the Cloud Environment and its architecture. 


#### 12:25 pm - 12:55 pm | Hands-on with workspaces and data in AnVIL Powered by Terra

Cloning a Terra workspace for downstream analysis in Galaxy.


#### 12:55 pm - 01:25 pm | Structural variant discovery with Galaxy in AnVIL Powered by Terra

Learn an end-to-end SV identification journey using Galaxy to visualize results and identify potentially pathogenic variants.


#### 01:25 pm - 01:30 pm | Closing Remarks, Q&A

## Resources
#### Tutorial workspaces
* [Data Quickstart](https://app.terra.bio/#workspaces/fc-product-demo/Terra-Data-Tables-Quickstart)
* [Workflows Quickstart](https://app.terra.bio/#workspaces/fc-product-demo/Terra-Workflows-Quickstart)
* [Notebooks Quickstart](https://app.terra.bio/#workspaces/fc-product-demo/Terra-Notebooks-Quickstart)

#### Galaxy on Terra
* [Galaxy interactive environments](https://support.terra.bio/hc/en-us/articles/360050566271)
* [AnVIL: Galaxy on Terra FAQ](https://support.terra.bio/hc/en-us/articles/4402392877979)

#### Cloud data
* [Terra architecture and where your files live in it](https://support.terra.bio/hc/en-us/articles/360058163311)
* [Moving data to the Cloud Environment](https://support.terra.bio/hc/en-us/articles/360058268972)
* [Moving data from a Google bucket](https://support.terra.bio/hc/en-us/articles/4409101169051)
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","ASHG2021/2021-ASHG-Galaxy-SV-Discovery"
362,"help-gatk","Somatic_variant_discovery_b37","READER","https://app.terra.bio/#workspaces/help-gatk/Somatic_variant_discovery_b37",TRUE,TRUE,NA,NA,NA,"### GATK Best Practices for somatic variant discovery (b37 reference)
The purpose of this workspace is to provide a fully reproducible example of the GATK Best Practices workflows for somatic variant discovery. 

#### Workspace attributes
All required and optional resources for the preconfigured methods included. The reference genome is b37, the Broad Institute's version of hg19. Some resource files may be named hg19 for historical reasons.

#### Data 
Full exomes of the HCC1143 cell line tumor and normal samples, pre-processed according to current GATK Best Practices. See BAM headers for additional details. 

#### Method configs
This workspace contains the following preset method configurations:

- **Somatic_SNV_Indel_Discovery_MC**: Somatic SNV and Indel discovery with Mutect2. Usage instructions: launch this on any qualifying Pair or Pair Set (each pair must include a tumor BAM + its matched normal and their BAM indices, aligned against the correct reference genome). By default the workflow will run the following tasks: GATK4 Mutect2 variant calling, estimation of cross-sample contamination, collection of pre-adapter artifact metrics, filtering on LOD scores and contamination, filtering on orientation bias, and finally functional annotation using Oncotator. Note that currently the Panel Of Normals (PON) must be provided by the user, ideally through a workspace attribute. Failure to provide a suitable PON will produce inferior results.",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/Somatic_variant_discovery_b37"
363,"terra-outreach","Molecular-Oncology-Almanac","READER","https://app.terra.bio/#workspaces/terra-outreach/Molecular-Oncology-Almanac",TRUE,TRUE,NA,NA,NA,"### Introduction

The purpose of this workspace is to highlight and demonstrate the Molecular Oncology Almanac WDL Workflow, a wrapper for the tool’s algorithm, allowing for a simple user interface to launch an analysis and review results.

**What is the Molecular Ontology Almanac?**

The Molecular Oncology Almanac (MOAlmanac) is a clinical interpretation algorithm for cancer genomics that annotates and evaluates whole exome and transcriptome genomic data from individual patient samples. The MOAlmanac can specifically:

* Identifies/Isolates -->
			- mutations and genomic features related to therapeutic sensitivity and resistance
			- mutations of prognostic relevance
			- genomic features that may be related to one another
			- overlaps between somatic variants observed from both DNA and RNA - or any other source of validation sequencing.
			- somatic and germline variants related to microsatellite stability.
			- somatic mutations from single nucleotide variants (SNVs), insertions and deletions, copy number alterations/variations (CNVs), and fusions based on clinical and biological relevance.
			- germline mutations relevant to adult and hereditary cancers.
* Annotates -->
			- somatic and germline variants that are present in several variant databases/data sources.*
* Calculates/Evaluates -->
			- coding mutational burdens and compares the sample/patient with TCGA data.
			- contribution of known COSMIC mutational signatures with deconstructsigs.
			- which genes have been altered in both somatic and germline settings by integrating data types
* Creates -->
			- portable web-based actionability reports that summarize clinically relevant findings.


* Find the full list [here](https://github.com/vanallenlab/moalmanac/tree/main/moalmanac/datasources).

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

#### Get Started

For first time Terra users, the following Quickstart videos are recommended for a high-level introduction to the platform:

1. [Introduction to Data Tables](https://www.youtube.com/watch?v=IeLywroCNNA)
2. [Workflows in Terra - Quickstart Workspace Demo](https://www.youtube.com/watch?v=HObb_J9fPc0)

For a full list of tutorial videos to learn more about the platform, please refer to this [playlist](https://www.youtube.com/playlist?list=PLh_zJaZ9uQ7P0w6bMLWgL8oDul2EiNlv6).



To actively participate in this workspace, including making edits, running an analysis, etc, a few prerequisite tasks are required. If you do not want to run your own analysis, skip these steps:

1. [Set up your billing](https://support.terra.bio/hc/en-us/articles/360026182251-How-to-set-up-billing-in-Terra)
2. [Clone the workspace](https://support.terra.bio/hc/en-us/articles/360026130851-How-to-clone-a-workspace)

Cloning a workspace (1) creates a workspace copy that can be edited and allows for analysis actions (such as running the `moalmanac` workflow) which will be billed to the above designated GCP billing source (2). Continue to the Dashboard in the cloned workspace and follow the steps below to run the `moalmanac` workflow with the provided example data.

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

#### Data

**What is the example data?**

The example data provided in the workspace to illustrate set-up of a Workspace and its data tables to be able to successfully  launch the workflow. 
There is a single participant (“Example”) and one sample (“Example-Profile”) belonging to the participant. This data can be viewed in the Data tab as individual data tables - combined, data tables are referred to as the “Data Model”.

**What is the Data Model?**

The Data Model** refers to the set of data tables listed in the Data tab of a Workspace; it is a Terra-centric method to organize, cross-reference, and access your data within the workspace. This workspace has 2 pre-defined data tables: 

					1. `participant`: a single entry, or participant, named “Example”.
					2. `sample`: a single entry, or sample, named “Example-Profile”, that references the associated participant + additionally relevant input files for `moalmanac` Workflow.

** Full documentation (and video) on how to set up your own data tables with your own data can be found [here](https://support.terra.bio/hc/en-us/articles/360025758392-Managing-data-with-in-app-tables-). Additional information on understanding the types of tables (entities) and how they relate to each other and workflow set-up can be found [here](https://support.terra.bio/hc/en-us/articles/360033913771-Understanding-Entity-Types).

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

#### Workflows

This workspace contains a single Workflow, named `moalmanac` - accessible from the Workflows tab. Clicking into the workflow will open the “workflow configuration” page from which various required and optional definitions can be set prior to launching a workflow***.

Outlined below are general steps to launch a workflow - full details on setting up a method configuration and launching a workflow submission can be found [here](https://support.terra.bio/hc/en-us/articles/360026521831-How-to-set-up-a-workflow-analysis-):

					1. Select the data table containing your desired input data (“Step 1”).
					2. Select the specific rows/data from the input data table (“Step 2”).
					3. Set the input variables from the “Inputs'' tab. Definitions of all `moalmanac` inputs can be found here.
					4. Set the outputs from the “Outputs” tab. Definitions of all `moalmanac` outputs can be found here.
					5. Click Run Analysis.

Listed in the Github wiki pages are detailed descriptions of all possible [inputs](https://github.com/vanallenlab/moalmanac/blob/main/docs/description-of-inputs.md) and [outputs](https://github.com/vanallenlab/moalmanac/blob/main/docs/description-of-outputs.md).


Once your workflow has been launched, monitor its status from the Job History tab. This [document](https://support.terra.bio/hc/en-us/articles/360037096272-Monitor-and-troubleshoot-in-the-Job-History-tab) outlines how to navigate the Job History tab and learn about the information it provides (workflow status, error logs, etc).

When the workflow has completed, you can view the output files from the `sample` table - visible as hyperlinks in columns denoted with the output variable name.

*** In this example, all the definitions have been pre-set as a saved configuration so no modifications are required to run the workflow as is.

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

#### License + Contact Information

Copyright Broad Institute, 2020 | BSD-3
All code provided in this workspace is released under the WDL open source code license [BSD-3](https://github.com/openwdl/wdl/blob/master/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.

To cite this workspace/workflow:

	Reardon, B. et al. (2020). Clinical interpretation of integrative molecular profiles to guide precision cancer medicine. bioRxiv 2020.09.22.308833 doi:10.1101/2020.09.22.308833

For general questions about getting started and working in Terra, please contact:

support@terra.bio, post on our [Community Forum](https://support.terra.bio/hc/en-us/community/topics), or use our Contact Us in-app feature (accessible from the main menu).

For specific questions about the algorithm or science, please contact:

breardon@broadinstitute.org or post to the [GitHub Issues](https://github.com/vanallenlab/moalmanac/issues) page.

For more information please visit the [Github repository](https://github.com/vanallenlab/moalmanac), browse our database at [moalmanac.org](https://moalmanac.org/), or browse other [Github repositories in the Molecular Oncology Almanac ecosystem](https://github.com/topics/molecular-oncology-almanac).",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","terra-outreach/Molecular-Oncology-Almanac"
364,"warp-pipelines","Imputation","READER","https://app.terra.bio/#workspaces/warp-pipelines/Imputation",TRUE,FALSE,NA,NA,NA,"# Imputation Pipeline
The Imputation pipeline was developed by the Broad Methods and Pipelines teams to impute missing genotypes from initial variant calls (VCFs) using a large genomic reference panel. 


This workspace describes `v1.1.11` of the pipeline. For more details about the pipeline, see the [Imputation Overview](https://broadinstitute.github.io/warp/docs/Pipelines/Imputation_Pipeline/README) on the WARP documentation site.

Scroll down for details on the workflow, including input and output descriptions and requirements, estimated run times and costs, and information on sample data.       

**Note that cost and time estimates will vary** with the use of [Preemptibles](https://cloud.google.com/compute/docs/instances/preemptible).  
You can also use [Google's cost calculator](https://cloud.google.com/products/calculator/) to estimate the cost to run. 

**For helpful hints on controlling Cloud costs**, see [Overview: Controlling Cloud costs on Terra](https://support.terra.bio/hc/en-us/articles/360029748111).  


##  Imputation 

### What does it do?   
The Imputation workflow is based on the [Michigan Imputation Server pipeline](https://imputationserver.readthedocs.io/en/latest/pipeline/). Overall, it filters, phases, and performs imputation on a multi-sample VCF produced with an array-processing pipeline. It outputs the imputed VCF along with key imputation metrics.

![]()


>**To try an array analysis pipeline preceding the Imputation workflow, see the [Illumina-Genotyping-Array](https://app.terra.bio/#workspaces/warp-pipelines/Illumina-Genotyping-Array) workspace.**


![]()


### What does it require as input?
A full description of the pipeline’s required and optional inputs can be found in the [Imputation Overview](https://broadinstitute.github.io/warp/docs/Pipelines/Imputation_Pipeline/README).

The pipeline takes in **either** an array of GVCF files (one GVCF per sample) **OR** a multi-sample single GVCF file. The workflow in this workspace is set up to use an array of VCF files, which are listed in the `Imputation_sample` data table.  These samples must be from the same species and genotyping chip.

**You must have two or more samples to run the pipeline.** 

However, the pipeline is cost-optimized for between 100 and 1,000 samples. After 1,000 samples, the cost per sample no longer decreases (see section ""Additional price estimates"" below). 

| Input name |  Description |
| --- | --- |
| single_sample_vcfs | Array of VCFs, one for each sample; can be used in lieu of a merged VCF containing all samples. Must have two or more samples. |
| single_sample_vcf_indices | Array of indices, one for each sample; can be used in lieu of a merged index for a multi-sample VCF. Must have two or more samples. |
| multi_sample_vcf | Merged VCF containing multiple samples; can also use an array of individual VCFs. Must have two or more samples. |
| multi_sample_vcf_index | Merged index for the merged VCF; can also use an array of index files if using an array of VCFs. Must have two or more samples. |
| contigs | Array of strings defining which contigs (chromosomes) should be used for the reference panel. |
| genetic_maps_eagle | Genetic map file for phasing.	 |
| haplotype_database | Haplotype database; only used when checking fingerprints. |
| output_callset_name | Output callset name. |
| ref_dict | Reference dictionary. |
| reference_panel_path | Path to the cloud storage containing the reference panel files for all contigs. This pipeline uses a modified version of the 1000 Genomes reference panel. Read more in the [Imputation Overview](https://broadinstitute.github.io/warp/docs/Pipelines/Imputation_Pipeline/README).	 |



### What does it return as output?

All outputs are fully detailed in the [Imputation Overview](https://broadinstitute.github.io/warp/docs/Pipelines/Imputation_Pipeline/README#workflow-outputs).  The workflow is set up to output files to the `Imputation_sample_set` data table.

| Name |  Description |
| --- | --- |
| imputed_multisample_vcf | VCF from the InterleaveVariants task; contains imputed variants as well as missing variants from the input VCF.	 |
| imputed_multisample_vcf_index | Index file for VCF from the InterleaveVariants task. |
| aggregated_imputation_metrics | Aggregated QC metrics from the MergeImputationQcMetrics task; reports the fraction of sites well-imputed and outputs to TXT file; the fraction of ""well-imputed"" is based on the minimac reported R2 metric, with R2>0.3 being ""well-imputed."" Since homomorphic sites lead to an R2 value of 0, we report the fraction of sites with any variation which are well-imputed in addition to the fraction of total sites. |
| chunks_info | TSV from StoreChunksInfo task; contains the chunk intervals as well as the number of variants in the array. |
| failed_chunks | File with the failed chunks from the StoreChunksInfo task. |
| imputed_single_sample_vcfs| Array of imputed single sample VCFs from the SplitMultiSampleVcf task. |
| imputed_single_sample_vcf_indices | Array of indices for the imputed VCFs from the SplitMultiSampleVcf task. |
| n_failed_chunks | File with the number of failed chunks from the StoreChunksInfo task. |


### Reference data description and location  
All required reference files are listed in the Workspace data table. For details on the reference panel generation, see the [Imputation Overview](https://broadinstitute.github.io/warp/docs/Pipelines/Imputation_Pipeline/README)



### Running the workflow
The workflow is set up to use the callset (Set1) listed in the `Imputation_sample_set` table. The individual GVCF files for this callset are in the `Imputation_sample` table.

To run the workflow:

1. Go to the Workflows page.
2. Select the Imputation workflow.
3. Make sure the `Imputation_sample_set` table is selected as the root entity in Step 1.
4. In Step 2, select the `Set1` callset. 
5. Save the workflow setup.
6. Select `Run Analysis`.
7. Select `Launch`.

#### Customizing the workflow for your data
If you have individual GVCFs for each sample in your callset, you can add them to the `Imputation_sample` data table. The workflow is already set up to pull an array of GVCF files. You'll also need to add the set of GVCFs to the `Imputation_sample_set` data table.

If you want to use a single file containing variant calls for multiple samples, you can add this file to the `Imputation_sample` data.  On the workflow setup page, change the following:
1. In Step 1 on the workflow setup page, make the `Imputation_sample` data table the root entity.
2. In the input setup, remove the attributes for `single_sample_vcfs` and its index.
3. In the input setup, adjust the `output_callset_name` attribute to reflect the new callset name/ID. 

#### Modifying contigs
To save time and cost, the workflow is currently setup to only run on chromosomes 21 and 22 of the example dataset. To modify to more chromosomes, adjust the `contigs` input parameter accordingly.


### Estimated time and cost to run on sample data 

The following table provides time and cost estimates for running the preconfigured workspace dataset. 
 
| Sample set name | VCF file size | Time | Cost $ | Number of contigs (chromosomes) | Additional details |
| ---  | --- | --- | --- | --- | --- |
| Set1 | ~100 MB |  2 h 00 min | 2.80 | 2 | The workflow is only set to run on two chromosomes. If running all chromosomes, the cost is ~$10 per run. See additional prices estimates below. |


#### Additional price estimates
The pipeline is cost-optimized for between 100 and 1,000 samples, where the cost per sample continues to decrease until 1,000 samples are run.

| Cohort size ( # samples) | Cost $ |
| --- | --- | 
| 1 | 8 |
| 10 | 0.8 | 
| 100 | 0.11 | 
| 1000 | 0.024 |
| 13.5 K | 0.025 |

**For helpful hints on controlling Cloud costs, see [Overview: Controlling Cloud costs on Terra](https://support.terra.bio/hc/en-us/articles/360029748111).**   

### Contact Information

* For workspace questions and feedback, email the Broad pipelines team at warp-pipelines-help@broadinstitute.org or create a post on the [Featured Workspaces community forum](https://support.terra.bio/hc/en-us/community/topics/360001603491) (login required). Tag @Alexander Baumann in the **Details** section of your post.

* You can also Contact the Terra team for questions from the Terra main menu. When submitting a request, it can be helpful to include:
    * Your Project ID
    * Your workspace name
    * Your Bucket ID, Submission ID, and Workflow ID
    * Any relevant log information

### Versions

All versions listed here are available by cloning this workspace and selecting the version on the Imputation workflow. For a complete version list, please read the [Imputation changelog](https://github.com/broadinstitute/warp/blob/master/pipelines/broad/arrays/imputation/Imputation.changelog.md) in GitHub.

| Release Version | Date | Release Note | 
| :---: | :---: | :--- |
| v1.1.11 | August 2023 | Updated BCFTools/VCFTools and Minimac4 Docker images to fix vulnerabilities, updated FormatImputationOutputs, FormatImputationWideOutputs, and IngestOutputsToTDR tasks with GCR images instead of Dockerhub, added meta section to allowNestedInputs, updated GATK to version 4.3.0.0, and adjusted disk size calculation in SplitMultiSampleVcf, and moved ReplaceHeader to its own scatter to remove dependency between the two nested scatters to help with wall clock time. |
| v1.1.1 | September 2022 | Renamed the CompareVCFs task in VerifyImputation.wdl to CompareVcfsAllowingQualityDifferences, this update has no effect on this pipeline. |
| v1.1.0 | April 2022 | Updated to Picard version 2.26.10 and GATK version 4.2.6.1 to address log4j vulnerabilities. Updated QC metrics calculation for Imputation pipeline to only evaluate sites with MAF.|
| v1.0.0 | October 2021 | Initial public release of the Imputation pipeline. |",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","warp-pipelines/Imputation"
365,"anvil-outreach","demos-single-cell-bioconductor","READER","https://app.terra.bio/#workspaces/anvil-outreach/demos-single-cell-bioconductor",TRUE,FALSE,NA,NA,NA,"## Visualize Single Cell Expression with Bioconductor

![](https://drive.google.com/uc?id=1t8qlRe2jucQXQG8h6Lk2I9KDglbPQ0He)
Learn how to use the Bioconductor tool `iSEE` to visualize single-cell RNA sequencing (scRNA-seq) data [[slides](https://docs.google.com/presentation/d/18J1x5rkWJisiv3EOMVZOma52XWxAVKANTg0Q2FXbOWU)]

## Original Workspace

https://anvil.terra.bio/#workspaces/anvil-outreach/demos-single-cell-bioconductor

## References

- [AnVIL Support Forum](https://help.anvilproject.org)
- [AnVIL in 2 minutes playlist](https://www.youtube.com/playlist?list=PL6aYJ_0zJ4uCABkMngSYjPo_3c-nUUmio)
- [AnVIL_Book_Getting_Started](https://jhudatascience.org/AnVIL_Book_Getting_Started)
- [AnVIL Collection](https://hutchdatascience.org/AnVIL_Collection) (more activities)",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","anvil-outreach/demos-single-cell-bioconductor"
367,"ctat-firecloud","TRUST4","READER","https://app.terra.bio/#workspaces/ctat-firecloud/TRUST4",TRUE,FALSE,NA,NA,NA,"# TRUST4

A fully reproducible example workflow for immune repertoire reconstruction.

Complete documentation for TRUST4 is available on the [TRUST4 repository](https://github.com/liulab-dfci/TRUST4).

## Workflow Description

Tcr Receptor Utilities for Solid Tissue (TRUST4), a component of the [Trinity Cancer Transcriptome Analysis Toolkit (CTAT)](https://github.com/NCIP/Trinity_CTAT/wiki), is used for reconstructing immune receptor repertoires in T-cells and B-cells using unselected RNA sequencing data, profiled from solid tissues, including tumors. TRUST4 performs de novo assembly on V, J, C genes including the hypervariable complementarity-determining region 3 (CDR3) and reports consensus of BCR/TCR sequences.  TRUST4 then realigns the contigs to IMGT reference gene sequences to report the corresponding information. TRUST4 supports both single-end and paired-end sequencing data with any read length.

## Input

- RNA-seq data in single or paired-end FASTQ format or aligned BAM format.
- scRNA-seq data in SMART-seq style FASTQ format (single or paired-end) or 10x Genomics style BAM format.
- Fasta file coordinate and sequence of V/D/J/C genes (OPTIONAL, Default: hg38_bcrtcr.fa).
- Detailed V/D/J/C gene reference file, such as from IMGT database (OPTIONAL, Default: human_IMGT+C.fa).

TRUST4 WDL example input JSONs and optional input parameters are available on the [TRUST4-WDL repository](https://github.com/d-s-cohen/trust4-wdl/tree/main/example_json). 

## Output

- example_annot.fa.gz - Fasta containing annotation of the consensus assemblies
- example_cdr3.out.gz - CDR1,2,3 and gene information for the consensus assemblies - Not in SMART-seq
- example_report.tsv.gz - Simple report - Detailed below

## Example data

This workspace contains a small set of paired FASTQ RNA-seq data (example_1.fq, example_2.fq), BAM RNA-seq data (example.bam), and SMART-seq style data (cell\*_1.fastq.gz, cell\*_2.fastq.gz) used for testing purposes.

## Time and cost estimates

Below is an example of the time and cost for running the workflow.

|Sample|Format|Read Pairs|Cost|Time|
|-|-|-|-|-|
|FZ-116|BAM|86M|$0.05|47m|
|FZ-116|FASTQ|86M|$0.09|1h 22m |


Note: Cost and time will vary with the use of preemptible instances.

## Output example

example_report.tsv.gz:

|#count|frequency|CDR3nt|CDR3aa|V|D|J|C|cid|cid_full_length|
|-|-|-|-|-|-|-|-|-|-|
|1680|2.097552e-02|TGTCAACAGTATGATACTGTCCCTCCGTCTTTC|CQQYDTVPPSF|IGKV1-33*01|.|IGKJ4*01|IGKC|assemble43990|1|
|1222|1.631500e-02|TGCGCGAATGGGCCCCCTGGCCTCTCTCGGGGAGTTACTCGGCGACACTACTACTTCGGTATGGACGTCTGG|CANGPPGLSRGVTRRHYYFGMDVW|IGHV3-23*05|IGHD3-10*01|IGHJ6*02|IGHA1|assemble1|1|
|838|1.119508e-02|TGTGCGAGAAGCGAAGGGAATCGGGGAGTTTTACGCTTTGACTCCTGG|CARSEGNRGVLRFDSW|IGHV4-59*01|IGHD3-10*01|IGHJ4*02|IGHA1|assemble43985|1|
|767|1.023983e-02|TGTGTGAAAGATGGTGACACCAACACCTGGACGAGTGTTTATTATTCGTACTGG|CVKDGDTNTWTSVYYSYW|IGHV3-23*01|IGHD3-3*01|IGHJ4*02|IGHA1|assemble43992|1|

- The count column is composed of the abundance of clonotypes.
- The frequency column is composed of the abundances of clonotypes divided by the sum of all abundances in the repertoire.
- The CDR3nt column is composed of the nucleotide sequences of the clonotypes.
- The CDR3aa is composed of the amino acid sequences of the clonotypes. 
- The V,J,D,C columns are composed of the V,J,D,C-gene segment usages.
- The cid column is composed of the contig/consensus identifiers.

## Contact information

Questions can be directed to the Trinity CTAT email list: trinity_ctat_users@googlegroups.com

## License

Copyright Broad Institute, 2021 | BSD-3 All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at https://raw.githubusercontent.com/NCIP/ctat-mutations/master/LICENSE.txt).",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","ctat-firecloud/TRUST4"
368,"broad-firecloud-cptac","CMAP_Data","READER","https://app.terra.bio/#workspaces/broad-firecloud-cptac/CMAP_Data",TRUE,FALSE,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","broad-firecloud-cptac/CMAP_Data"
369,"help-gatk","GATKTutorials-BigQuery-June2019","READER","https://app.terra.bio/#workspaces/help-gatk/GATKTutorials-BigQuery-June2019",TRUE,FALSE,NA,NA,NA,"## What's in this workspace?

Welcome to Day 4 of the Genome Analysis Toolkit (GATK) workshop at Newcastle University in Newcastle upon Tyne, U.K.!  This afternoon, we will walk through a hands on data analysis exercise in real time using BiGQuery and Jupyter notebooks.    

### Data
Metadata from the publicly available 1000 Genomes [project](http://www.internationalgenome.org/data).

![image](https://storage.googleapis.com/terra-featured-workspaces/BigQuery-Tutorial/1000GenomesDataPortal.png)


### Tools
There are no tools in this workspace. The tutorials are notebook-based, allowing you to run each step manually and view the intermediate outputs of the workflow. This is a great way to understand each step in the workflow and gives you the chance to manipulate the parameters to see what happens with the output.

If you are interested in a WDL based workflow of these analyses, check out the [Showcase](https://app.terra.bio/#library/showcase) area in Terra, which features many of our popular GATK workflows in workspaces ready to run your data.


### Notebooks

This practice workspace is the foundation for Big Query workshops, and the notebooks are ordered based on dependencies.

	1. Intro to Jupyter Notebooks
	2. R_Environment_Setup
	3. R_Retrieve Data from BigQuery
	4. R_Retrieve a Cohort with BigQuery 

Your instructor will guide you through this workspace, but you should find the documentation within the notebooks sufficient to run through them again on your own or to share with a friend later (and we encourage you to do so!).

| Runtime Environments | Value |
| --- | --- |
| CPU Minimum| 4|
| Disksize Minimum | 100 GB |
| Memory Minimum | 15 GB |
 
### Software versions
R: latest

R packages: bigrquery, dplyr, skimr, ggplot2, reticulate

### Time and cost 
This workspace focuses on the use of notebooks, thus the time and cost of completing the tutorial depends on the runtime environment of your notebook and the length of time you spend actively working in the notebook. With the given runtime environments above, users can expect the cost to be much less than a dollar an hour for each notebook. The specific cost will be displayed on the top right corner of your screen, next to the notebook machine options. 

## Appendix

### GATK @ Newcastle 2019 in the Terra Support Center

Get yourself oriented with the entire workshop in the [Terra Support Center](https://broadinstitute.zendesk.com/hc/en-us/community/topics). (more on Terra Support below).

### GATK documentation and support

Detailed documentation, tutorials, and more are available at the [GATK web site](https://software.broadinstitute.org/gatk/). If you are still running into issues after consulting the User Guide, have a question, or just want to say hello, reach out to the GATK team via the [GATK Support Forum](https://gatkforums.broadinstitute.org/gatk)!

### Terra documentation and support

Documents and helpful guides to support your journey through Terra are available in the Terra Support Center. Navigate there by following the “Learn About Terra” link in the left-side menu. At the time of this workshop, we are still actively writing and publishing documents but we’ve got a good set of docs in the Quick Start Guide to get you going.
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/GATKTutorials-BigQuery-June2019"
370,"theiagen-demos","MGen-Theiagen-2023","READER","https://app.terra.bio/#workspaces/theiagen-demos/MGen-Theiagen-2023",TRUE,FALSE,NA,NA,NA,"# Terra for Public Health Pathogen Genomics
This featured workspace was used to perform the case study described in a recent manuscript submitted for review to the Microbial Genomics special collection: [Implementation of Genomics in Clinical and Public Health Microbiology](https://www.microbiologyresearch.org/content/implementation-of-genomics-in-clinical-and-public-health-microbiology). 

## Experimental Design
We have undertaken example bioinformatics analyses via Terra with Theiagen workflows. Sequence reads representing bacterial isolates were selected from the [Genomics and Food Safety (Gen-FS) Salmonella enterica ser. Bareilly whole-genome standards dataset](https://pubmed.ncbi.nlm.nih.gov/29372115/), [Global Pneumococcal Sequencing (GPS) study dataset](https://pubmed.ncbi.nlm.nih.gov/31003929/)  (NCBI BioProject PRJEB3084), and CDC Division of Healthcare Quality Promotion (DHQP) dataset of [OXA-48-like producing Carbapenem Resistant Organisms](https://pubmed.ncbi.nlm.nih.gov/29553324/) (NCBI BioProject PRJNA296771). Sequence Read Archive (SRA) accessions are provided in Table S3. Together, this set of Illumina paired-end reads represents multiple taxa of public health concern.

Sequence reads were brought into a Terra workspace using the SRA_Fetch v1.4.1 workflow. This workflow downloads read data from NCBI’s Sequence Read Archive (SRA), requiring only the SRA or ENA run accession, and a user-defined sample name. All sequence data were analysed with the TheiaProk_Illumina_PE v1.1.1 workflow under default settings. TheiaProk_Illumina_PE performs quality control, genome assembly, taxon assignment, and genome characterisation of Illumina paired-end reads from bacterial isolates. TheiaProk_Illumina_PE automatically activates taxon-specific analyses, e.g. Salmonella serotyping, based on in-silico taxon identification. The Terra-compatible kSNP3 workflow was used to generate a core-genome maximum parsimony phylogeny, pairwise SNP distance matrix, and a summary of antibiotic-resistance genes (ARGs) and plasmids for the *S. enterica* ser. Bareilly samples.

## Data
Sequence data used in this workspace are available in the NCBI Sequence Read Archive under the accession numbers listed in case_study_sample data table. 

## Workflows
All workflows imported in this workspace, SRA_Fetch, kSNP3, and TheiaProk_Illumina_PE are available on publicly-accessible [Theiagen GitHub repositories](https://github.com/theiagen) in tagged version releases and were imported directly from [Dockstore](https://dockstore.org/organizations/Theiagen). 

### SRA_Fetch
The SRA_Fetch workflow downloads sequence data from NCBI’s Sequence Read Archive (SRA). It requires an SRA run accession to populate the associated read files to a Terra data table. 

#### Inputs
The only input for the SRA_Fetch workflow is an SRA run accession, which begin with “SRR”, or an ENA run accession, which begin with “ERR”. Please see the NCBI Metadata and Submission Overview for assistance with identifying accessions: https://www.ncbi.nlm.nih.gov/sra/docs/submitmeta/.

#### Outputs
This workflow produces output columns for the read data. For paired-end data, these are read1 and read2 columns (for single-end data, only the read1 column). 


### TheiaProk_Illumina_PE
The TheiaProk workflows are for the assembly, quality assessment, and characterization of bacterial genomes.

All input reads are processed through “[core tasks](https://www.notion.so/TheiaProk-Workflow-Series-89b9c08406094ec78d08a578fe861626)” in each workflow. These undertake read trimming and assembly, quality assessment, species identification, and some genome characterization. For some taxa identified, “taxa-specific sub-workflows” will be automatically activated, undertaking additional taxa-specific characterization steps. When setting up each workflow, users may choose to use “optional tasks” as additions or alternatives to tasks run in the workflow by default.

<p align=""center"">
	<a href=""https://www.theiagen.com"">
  <img src=""https://storage.googleapis.com/theiagen-public-files/terra/MGEN_2023/TheiaProk_Illumina_PE.png"" width=""550"" background=""transparent"" class=""center"" alt=""TheiaProk_kSNP3_Workflows"">
	</a>
</p>

#### Inputs
The TheiaProk_Illumina_PE workflow takes in a sample name and Illumina paired-end read data. Read file names should end with `.fastq` or `.fq`, with the optional addition of `.gz`. When possible, Theiagen recommends zipping files with [gzip](https://www.gnu.org/software/gzip/) prior to Terra upload to minimize data upload time.

*By default, the workflow anticipates 2 x 150bp reads (i.e. the input reads were generated using a 300-cycle sequencing kit). Modifications to the optional parameter for `trimminlen` may be required to accommodate shorter read data, such as the 2 x 75bp reads generated using a 150-cycle sequencing kit.
*

#### Outputs
Please refer to our [technical documentation](https://theiagen.notion.site/31f66cc07d674b67a06da1eba9e6ed40?v=e5a81c9d2f534f61907d13d92c8211c0) for a full list of TheiaProk_Illumina_PE outputs.

### kSNP3
The kSNP3 workflow is for phylogenetic analysis of bacterial genomes using single nucleotide polymorphisms (SNPs). The kSNP3 workflow identifies SNPs amongst a set of genome assemblies, then calculates a number of phylogenetic trees based on those SNPs:

- Pan-genome phylogenetic trees: The term “pan-genome” is used here to describe the collective genetic content amongst the set of genomes, including regions outside of genes and other coding sequences.  Outputs based on the pan-genome are labeled with `_pan`.
- Core-genome phylogenetic trees: The kSNP3 workflow will also generate phylogenetic trees based on the core genome (genetic content that is present in all members of the set of genomes). Outputs based on the core-genome are labeled with `_core`.

This workflow also features an optional module, `summarize_data` that creates a presence/absence matrix for the analyzed samples from a list of indicated columns (such as AMR genes, plasmid types etc.). If the `phandango_coloring` variable is set to `true`, this will be formatted for visualization in [Phandango](https://jameshadfield.github.io/phandango/#/), else it can be viewed in Excel.

<p align=""center"">
	<a href=""https://www.theiagen.com"">
  <img src=""https://storage.googleapis.com/theiagen-public-files/terra/MGEN_2023/kSNP3.png"" width=""550"" background=""transparent"" class=""center"" alt=""TheiaProk_kSNP3_Workflows"">
	</a>
</p>

#### Inputs
The TheiaProk_Illumina_PE workflow requires an array of assembly files, a cluster name , and a set of sample names. 

#### Outputs
Please refer to our [technical documentation](https://theiagen.notion.site/kSNP3-78251e57ea2e450e957ef26d93131351) for a full list of kSNP3 outputs.


## Contact Information
We are actively seeking feedback on the workflow! Please contact us with any feedback on what can be improved, changed, or added, or for any support requests by emailing support@terrapublichealth.zendesk.com or support@theiagen.com.
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","theiagen-demos/MGen-Theiagen-2023"
371,"help-gatk","Pre-processing_b37_v2","READER","https://app.terra.bio/#workspaces/help-gatk/Pre-processing_b37_v2",TRUE,TRUE,NA,NA,NA,"### GATK Best Practices for data pre-processing (b37 reference)
The purpose of this workspace is to provide a fully reproducible example of the GATK Best Practices workflow for pre-processing data (intended for downstream variant discovery analysis). 

#### Workspace attributes
All required and optional resources for the preconfigured methods included. The reference genome is b37 (aka GRCh37), the Broad Institute's version of hg19. Some resource files may be named hg19 for historical reasons. 

#### Data 
Two sets of 24 unmapped BAMs of NA12878 WGS data (one per read group) downsampled to 20% (med) and 5% (small), respectively. 

#### Method configs
This workspace contains the following preset method configurations:

- **PreProcessingForVariantDiscovery_GATK4**
Generic version of the pre-processing portion (read group uBAMs to analysis-ready BAM) of the single-sample pipeline used in production at the Broad Institute, running GATK4 according to GATK Best Practices (June 2016). Usage instructions: launch this on any qualifying Sample or Set of Samples (each sample must reference a list of unmapped BAMs, one per read group). See workflow WDL for additional input requirements. IMPORTANT CAVEAT: while this method is in principle applicable for data that will be used for somatic variant detection, some of its hardcoded parameters (the base quality score binning strategy) have not yet been fully vetted for somatic analysis. We intend to update the method to make those parameters customizable and produce separate method configs for the two use cases. 

#### Version notes

v1 -> v2 updates the reference files to use https://console.cloud.google.com/storage/browser/broad-references/hg19/v0/ as source, for the purpose of consolidating resource versions across multiple workspaces.",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/Pre-processing_b37_v2"
372,"broad-firecloud-cptac","PANOPLY_Production_Pipelines_v1_2","READER","https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_Production_Pipelines_v1_2",TRUE,FALSE,NA,NA,NA,"# PANOPLY: A cloud-based platform for automated and reproducible proteogenomic data analysis
#### Version 1.2

PANOPLY is a platform for applying state-of-the-art statistical and machine learning algorithms to transform multi-omic data from cancer samples into biologically meaningful and interpretable results. PANOPLY leverages [Terra](http://app.terra.bio)—a cloud-native platform for extreme-scale data analysis, sharing, and collaboration—to host proteogenomic workflows, and is designed to be flexible, automated, reproducible, scalable, and secure. A wide array of algorithms applicable to all cancer types have been implemented, and available in PANOPLY for analysis of cancer proteogenomic data.


![*Figure 1.* Overview of PANOPLY architecture and the various tasks that constitute the complete workflow. Tasks can also be run independently, or combined into custom workflows, including new tasks added by users. Inputs to PANOPLY consists of (i) externally characterized genomics and proteomics data (in gct format); (ii) sample phenotype and annotations (in csv format); and (iii) parameters settings (in yaml format). Panoply modules are grouped into Data Preparation Modules (green box), Data Analysis Modules (blue box) and Report Modules (red box). Data preparation modules perform quality checks on input data followed by optional normalization and filtering for proteomics data. Analysis ready data tables are then used as inputs to the data analysis modules. Results from the data analysis modules are summarized in interactive reports generated by appropriate report modules.](https://raw.githubusercontent.com/broadinstitute/PANOPLY/dev/panoply-overview.png)


PANOPLY v1.2 consists of the following components:

* A Terra production workspace on [PANOPLY_Production_Pipelines_v1_1](https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_Production_Pipelines_v1_1) with a preconfigured unified workflow to automatically run all analysis tasks on proteomics (global proteome, phosphoproteome, acetylome, ubiquitylome), transcriptome and copy number data; and an [additional workspace](https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_Production_Modules_v1_1) that includes separate methods for each analysis component. 
* An interactive Jupyter notebook (included in the Terra workspaces) that provides step-by-step instructions for uploading data, identifying data types, specifying parameters, and setting up the PANOPLY workspace for analyzing a new dataset.
* A GitHub [repository](https://github.com/broadinstitute/PANOPLY) that contains code for all PANOPLY tasks and workflows, including R code for implementing analysis algorithms, task module creation, and release management. A GitHub [wiki](https://github.com/broadinstitute/PANOPLY/wiki) includes documentation and description of algorithms. 
* A [tutorial](https://github.com/broadinstitute/PANOPLY/wiki/PANOPLY-Tutorial) illustrating the application of PANOPLY to a published breast cancer dataset and demonstrating the practical relevance of PANOPLY by regenerating many of the results described in (Mertins et al) (1) with minimal effort. The [PANOPLY_Tutorial](https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_Tutorial) workspace contains data and results from running the tuorial.
* Terra workspaces showing case studies of applying PANOPLY to the analysis of [CPTAC BRCA](https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_CPTAC_BRCA) (7) and [CPTAC LUAD](https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_CPTAC_LUAD) (8) datasets. The workspaces include all data, parameter settings and results.


PANOPLY provides a comprehensive collection of proteogenomic data analysis methods including sample QC (sample quality evaluation using profile plots and tumor purity scores (1), identify sample swaps, etc.), association analysis, RNA and copy number correlation (to proteome), connectivity map analysis (1 ,2), outlier analysis using BlackSheep (3), PTM-SEA (4), GSEA (5) and single-sample GSEA (6), consensus clustering, and multi-omic clustering using non-negative matrix factorization (NMF). Most analysis modules include a report generation task that outputs a HTML interactive report summarizing results from the respective analysis tasks. Mass spectrometry-based proteomics data amenable to analysis by PANOPLY includes isobaric label-based LC-MS/MS approaches like iTRAQ, TMT and TMTPro profiling the proteome and multiple PTM-omes including phospho-, acetyl-and ubiquitylomes.

Users can also [customize or create new pipelines and add their own tasks](https://github.com/broadinstitute/PANOPLY/wiki/Customizing-PANOPLY) and integrate them into the PANOPLY.


## Quick Start

* For a quick introduction and tour of PANOPLY, follow the [**tutorial**](https://github.com/broadinstitute/PANOPLY/wiki/PANOPLY-Tutorial). 
* Detailed **documentation** can be found [here](https://github.com/broadinstitute/PANOPLY/wiki).

### Contact

Email proteogenomics@broadinstitute.org with questions, comments or feedback.


### Citation

Mani, D. R. et al. PANOPLY: a cloud-based platform for automated and reproducible proteogenomic data analysis. *Nature Methods* 1–3 (2021) doi:10.1038/s41592-021-01176-6.
  

### References

1. Mertins, P. et al. Proteogenomics connects somatic mutations to signalling in breast cancer. *Nature* 534, 55–62 (2016).
2. Subramanian, A. et al. A Next Generation Connectivity Map: L1000 Platform and the First 1,000,000 Profiles. *Cell* 171, 1437–1452.e17 (2017).
3. Blumenberg, L. et al. BlackSheep: A Bioconductor and Bioconda package for differential extreme value analysis. *J of Proteome Research* 20(7), 3767-3773 (2021).
4.	Krug, K. et al. A Curated Resource for Phosphosite-specific Signature Analysis. *Mol. Cell. Proteomics* 18, 576–593 (2019).
5.	Subramanian, A. et al. Gene set enrichment analysis: a knowledge-based approach for interpreting genome-wide expression profiles. *Proc. Natl. Acad. Sci.* 102, 15545–15550 (2005).
6.	Barbie, D. A., Tamayo, P., Boehm, J. S., Kim, S. Y. & Moody, S. E. Systematic RNA interference reveals that oncogenic KRAS-driven cancers require TBK1. *Nature* (2009).
7. Gillette, M. A. et al. Proteogenomic Characterization Reveals Therapeutic Vulnerabilities in Lung Adenocarcinoma. *Cell* 182, 200–225.e35 (2020).
8. Krug, K., Jaehnig, E. J., Satpathy, S., Blumenberg, L., et al. Proteogenomic Landscape of Breast Cancer Tumorigenesis and Targeted Therapy. *Cell* 183, 1–21 (2020).
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","broad-firecloud-cptac/PANOPLY_Production_Pipelines_v1_2"
373,"kco-tech","wot","READER","https://app.terra.bio/#workspaces/kco-tech/wot",TRUE,FALSE,NA,NA,NA,"### Overview
This tutorial provides a practical, hands-on introduction to inferring developmental trajectories with [Waddington-OT](https://broadinstitute.github.io/wot/).

Single cell RNA-sequencing allows us to profile the diversity of cells along a developmental time-course by recording static snapshots at different time points. However, we cannot directly observe the progression of any individual cell over time because the measurement process is destructive.

Waddington-OT is designed to infer the temporal couplings of a developmental stochastic process ℙt from samples collected independently at various time-points. We represent a developing population of cells with a time-varying distribution ℙt on gene expression space. The temporal couplings describe the flow of mass as the population develops and grows. For a pair of time-points (t&#7522;,t&#11388;), the coupling γt&#7522;,t&#11388; tells us: What descendants does cell x from time t&#7522; give rise to at time t&#11388;?

In this tutorial, we explore the Waddington-OT workflow, starting with inferring temporal couplings with optimal transport. We then go through numerous downstream analyses including visualizing cell fates, interpolating the distribution of cells at held-out time points, and inferring gene regulatory networks. The tutorial guides the reader through the concepts, each of which is illustrated with an interactive Jupyter notebook using data from a time-course of iPS reprogramming ([Schiebinger et al. 2019](https://www.cell.com/cell/fulltext/S0092-8674(19)30039-X)).

### Tutorial
A step by step tutorial that accompanies this workspace is available at https://broadinstitute.github.io/wot/tutorial/.

### Data
Single-cell RNA sequencing from a time-course of iPS reprogramming ([Schiebinger et al. 2019](https://www.cell.com/cell/fulltext/S0092-8674(19)30039-X)).

### Notebooks
Nine notebooks are provided:

* Notebook 0: Install wot and download data
* Notebook 1: Visualizing and exploring the data
* Notebook 2: Computing transport matrices
* Notebook 3: Inferring long-range temporal couplings
* Notebook 4: Ancestors descendants, and trajectories
* Notebook 5: Fate Matrices
* Notebook 6: Transition tables
* Notebook 7: Validation by Geodesic Interpolation
* Notebook 8: Predictive TFs 


###  Contact information

* **Website:** https://broadinstitute.github.io/wot/
* **Email:** wot@broadinstitute.org
* **Github:** https://github.com/broadinstitute/wot

### Time and cost
It takes ~25 minutes and costs ~25 cents to run the notebooks using the data provided in the tutorial.


### Software versions
This workspace requires wot version 1 (installed in Notebook 0) and Python 3


### License
[BSD 3-Clause](https://github.com/broadinstitute/wot/blob/master/LICENSE)

###  Evaluation form
To rate this workspace as part of the Terra Open Science Contest, please fill out this [seven question survey](https://forms.gle/ma4AembkSLkyAaTE9).
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","kco-tech/wot"
374,"broad-firecloud-cptac","PANOPLY_Production_Modules_v1_4","READER","https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_Production_Modules_v1_4",TRUE,FALSE,NA,NA,NA,"# PANOPLY: A cloud-based platform for automated and reproducible proteogenomic data analysis
#### Version 1.4

PANOPLY is a platform for applying state-of-the-art statistical and machine learning algorithms to transform multi-omic data from cancer samples into biologically meaningful and interpretable results. PANOPLY leverages [Terra](http://app.terra.bio)—a cloud-native platform for extreme-scale data analysis, sharing, and collaboration—to host proteogenomic workflows, and is designed to be flexible, automated, reproducible, scalable, and secure. A wide array of algorithms applicable to all cancer types have been implemented, and available in PANOPLY for analysis of cancer proteogenomic data.


| ![PANOPLY Overview ](https://raw.githubusercontent.com/broadinstitute/PANOPLY/dev/panoply-overview-v2.png) |
|:--:|
| *Figure 1. Overview of PANOPLY architecture and the various tasks that constitute the complete workflow. Tasks can also be run independently, or combined into custom workflows, including new tasks added by users. Inputs to PANOPLY consists of (i) externally characterized genomics and proteomics data (in gct format); (ii) sample phenotype and annotations (in csv format); and (iii) parameters settings (in yaml format). Panoply modules are grouped into Data Preparation Modules (green box), Data Analysis Modules (blue box) and Report Modules (red box). Data preparation modules perform quality checks on input data followed by optional normalization and filtering for proteomics data. Analysis ready data tables are then used as inputs to the data analysis modules. Results from the data analysis modules are summarized in interactive reports generated by appropriate report modules. Modules under development are listed in grey text.* |


PANOPLY consists of the following components:

* A Terra production workspace on [PANOPLY_Production_Pipelines_v1_4](https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_Production_Pipelines_v1_4) with a preconfigured unified workflow to automatically run all analysis tasks on proteomics (global proteome, phosphoproteome, acetylome, ubiquitylome), transcriptome and copy number data; and an [additional workspace](https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_Production_Modules_v1_4) that includes separate methods for each analysis component. 
* An interactive Jupyter notebook (included in the Terra workspaces) that provides step-by-step instructions for uploading data, identifying data types, specifying parameters, and setting up the PANOPLY workspace for analyzing a new dataset.
* A GitHub [repository](https://github.com/broadinstitute/PANOPLY) that contains code for all PANOPLY tasks and workflows, including R code for implementing analysis algorithms, task module creation, and release management. A GitHub [wiki](https://github.com/broadinstitute/PANOPLY/wiki) includes documentation and description of algorithms. 
* A [tutorial](https://github.com/broadinstitute/PANOPLY/wiki/PANOPLY-Tutorial) illustrating the application of PANOPLY to a published breast cancer dataset and demonstrating the practical relevance of PANOPLY by regenerating many of the results described in (Mertins et al) (1) with minimal effort. The [PANOPLY_Tutorial](https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_Tutorial) workspace contains data and results from running the tuorial.
* Terra workspaces showing case studies of applying PANOPLY to the analysis of [CPTAC BRCA](https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_CPTAC_BRCA) (7) and [CPTAC LUAD](https://app.terra.bio/#workspaces/broad-firecloud-cptac/PANOPLY_CPTAC_LUAD) (8) datasets. The workspaces include all data, parameter settings and results.


PANOPLY provides a comprehensive collection of proteogenomic data analysis methods including sample QC (sample quality evaluation using profile plots and tumor purity scores (1), identify sample swaps, etc.), association analysis, RNA and copy number correlation (to proteome), connectivity map analysis (1 ,2), outlier analysis using BlackSheep (3), PTM-SEA (4), GSEA (5) and single-sample GSEA (6), consensus clustering, and multi-omic clustering using non-negative matrix factorization (NMF). Most analysis modules include a report generation task that outputs a HTML interactive report summarizing results from the respective analysis tasks. Mass spectrometry-based proteomics data amenable to analysis by PANOPLY includes isobaric label-based LC-MS/MS approaches like iTRAQ, TMT and TMTPro profiling the proteome and multiple PTM-omes including phospho-, acetyl-and ubiquitylomes.

Users can also [customize or create new pipelines and add their own tasks](https://github.com/broadinstitute/PANOPLY/wiki/Customizing-PANOPLY) and integrate them into the PANOPLY.


## Quick Start

* For a quick introduction and tour of PANOPLY, follow the [**tutorial**](https://github.com/broadinstitute/PANOPLY/wiki/PANOPLY-Tutorial). 
* Detailed **documentation** can be found [here](https://github.com/broadinstitute/PANOPLY/wiki).

### Contact

Email proteogenomics@broadinstitute.org with questions, comments or feedback.


### Citation

Mani, D. R. et al. PANOPLY: a cloud-based platform for automated and reproducible proteogenomic data analysis. *Nature Methods* 1–3 (2021) doi:10.1038/s41592-021-01176-6.
  

### References

1. Mertins, P. et al. Proteogenomics connects somatic mutations to signalling in breast cancer. *Nature* 534, 55–62 (2016).
2. Subramanian, A. et al. A Next Generation Connectivity Map: L1000 Platform and the First 1,000,000 Profiles. *Cell* 171, 1437–1452.e17 (2017).
3. Blumenberg, L. et al. BlackSheep: A Bioconductor and Bioconda package for differential extreme value analysis. *J of Proteome Research* 20(7), 3767-3773 (2021).
4.	Krug, K. et al. A Curated Resource for Phosphosite-specific Signature Analysis. *Mol. Cell. Proteomics* 18, 576–593 (2019).
5.	Subramanian, A. et al. Gene set enrichment analysis: a knowledge-based approach for interpreting genome-wide expression profiles. *Proc. Natl. Acad. Sci.* 102, 15545–15550 (2005).
6.	Barbie, D. A., Tamayo, P., Boehm, J. S., Kim, S. Y. & Moody, S. E. Systematic RNA interference reveals that oncogenic KRAS-driven cancers require TBK1. *Nature* (2009).
7. Gillette, M. A. et al. Proteogenomic Characterization Reveals Therapeutic Vulnerabilities in Lung Adenocarcinoma. *Cell* 182, 200–225.e35 (2020).
8. Krug, K., Jaehnig, E. J., Satpathy, S., Blumenberg, L., et al. Proteogenomic Landscape of Breast Cancer Tumorigenesis and Targeted Therapy. *Cell* 183, 1–21 (2020).
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","broad-firecloud-cptac/PANOPLY_Production_Modules_v1_4"
375,"broad-firecloud-tcga","CPTAC_OV_OpenAccess_V1_DRAFT","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/CPTAC_OV_OpenAccess_V1_DRAFT",TRUE,TRUE,NA,NA,NA,"Draft CPTAC workspace for Ovarian project.  Two PCCs participated in the CPTAC Ovarian project: JHU and PNNL.  Both JHU and PNNL conducted experiments with on the whole proteome, JHU conducted experiments on the Glycoproteome, PNNL conducted experiments on the Phosphoproteome.  

Experimental sample sets have been created for each itraq experiment.  The PSM files are referenced through attributes on their respective experiment sample set.

 Protein reports are aggregated across all experiments done at a specific PCC and on a specific analyte.Hence, the workspace contains sample sets for each pairing of PCC and analyte type and attributes on these sample sets reference the protein reports.",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","broad-firecloud-tcga/CPTAC_OV_OpenAccess_V1_DRAFT"
376,"biodata-catalyst","BioData Catalyst GWAS blood pressure trait","READER","https://app.terra.bio/#workspaces/biodata-catalyst/BioData%20Catalyst%20GWAS%20blood%20pressure%20trait",TRUE,FALSE,NA,NA,NA,"![Warning for users of Freeze 8 data. Please continue reading in the section below for more information.](https://raw.githubusercontent.com/aofarrel/verbose-fiesta/master/Terra/Images/freeze8warning.png)

_February 2021 Update_: This template workspace was developed and tested using TOPMed Freeze 5b data. Initial scale testing with Freeze 8 data has raised scalability issues related to LD pruning and generating a Genetic Relatedness Matrix within the Hail Jupyter notebook, which is the third and final notebook in this workspace and is also known as the genomic data preparation notebook. We do not suggest using Freeze 8 data with the Hail notebook, and we are looking into alternative tools. Some researchers may be able to use kinship matrices provided through the dbGAP TOPMed Combined Exchange Area to bypass the Hail notebook. The current GENESIS WDL used for association testing in this Template Workspace has been updated for use with Freeze 8; it is only the Hail notebook which has issues with Freeze 8.
# Template Blood Pressure Trait GWAS in NHLBI's BioData Catalyst

This template workspace was created to offer example tools for conducting a single variant, mixed-models GWAS focusing on a blood pressure trait from start to finish using the [NHLBI BioData Catalyst](https://biodatacatalyst.nhlbi.nih.gov/) ecosystem. We have created a set of documents [to get you started in the BioData Catalyst system](https://support.terra.bio/hc/en-us/sections/360008068731-BDC-category). If you're ready to conduct an analysis, proceed with this dashboard: 

## Data Model

This template was set up to work with the NHLBI BioData Catalyst Gen3 data model. In this dashboard, you will learn how to import data from the Gen3 platform into this Terra template and conduct an association test using this particular data model.

### TOPMed Data

Currently, BioData Catalyst's Gen3 hosts the [TOPMed](https://www.nhlbi.nih.gov/science/trans-omics-precision-medicine-topmed-program) program, which is controlled access. If you do not already have access to a TOPMed project through dbGAP, this template workspace may not yet be helpful to you. To apply for access to TOPMed, submit an application within [dbGAP](https://www.nhlbiwgs.org/topmed-data-access-scientific-community). 

If you already have access to a TOPMEd project and have been onboarded to the BioData Catalyst platform, you should be able to access your data through BioData Catalyst powered by Gen3 and use your data with this template workspace. We focused this template on analyzing a blood pressure trait, but not all TOPMed projects may contain blood pressure data. You will need to carefully consider how to update this analysis for the dataset you bring and how this may affect the scientific accuracy of the question you are asking. 

### A note about TOPMed metadata
Some types of metadata will always be present:  GUID, Case ID, Project Name, Number of Samples, Study, Gender, Age at Index, Race, Ethnicity, Number of Aliquots, SNP Array Files, Unaligned Read Files, Aligned Read Files, Germline Variation Files.

Other metadata depend on the analysis plan submitted when applying for TOPMed access. Examples include BMI, Years Smoked, years smoked greater than 89, hypertension, hypertension medications, diastolic blood pressure, systolic blood pressure, etc.

The TOPMed Data Coordinating Center (DCC) is currently harmonizing select phenotypes across TOPMed, which will also be deposited into the TOPMed accessions. The progress of phenotype metadata harmonization can [be tracked here](https://topmedphenotypes.org/). The data in the Gen3 graph model in this tutorial are harmonized phenotypes. You can find the unharmonized phenotypic and environmental data in the ""Reference File"" node of the Gen3 graph. Documentation about how to interact with unharmonized data in Terra is coming soon.

## Outline of this template

***Part 1: Navigate the BioData Catalyst environment***
Learn how to search and export data from Gen3 and workflows from Dockstore into a Terra workspace. Each cloud-based platform interoperates with one another for fast and secure research.  The template we have created here can be cloned for you to walk through as suggested, or you can use the basics you learn here to perform your own analysis. 

***Part 2: Explore TOPMed data in an interactive Jupyter notebook***
In this Terra workspace, you can find a series of interactive notebooks to explore TOPMed data. 

First, you will use the notebook **1-unarchive-vcf-tar-file-to-workspace** to extract the contents of tar bundles to your workspace for use in the GWAS. These tar bundles were generated by dbGAP and contain TOPMed multi-sample VCFs per consent code and chromosome. 

Next, the **2-GWAS-preliminary-analysis** notebook will lead you through a series of steps to explore the phenotypic data. It will allow you to examine phenotypic distributions and call the functions in the companion **terra-data-util** notebook to consolidate your clinical data from Gen3 into a single data table that can be imported into the third Jupyter notebook.  Then you will explore genetic relatedness using the [HAIL genomic data analysis tool](https://hail.is/) in **3-GWAS-genomic-data-preparation**, the third notebook in this workspace.  

***Part 3: Perform mixed-model association tests using workflows***
Next, perform mixed models genetic association tests (run as a series of batch workflows using GCP Compute engine). For details on the four workflows and what they do, scroll down to **Perform mixed-model association test workflows**.  The workflows are publicly available in [Dockstore](https://dockstore.org/) in this [collection](https://dockstore.org/organizations/bdcatalyst/collections/GWAS).       

Mixed models require two steps within the [GENESIS](https://bioconductor.org/packages/release/bioc/html/GENESIS.html) package in [R](https://www.r-project.org/about.html):     
1) Fitting a null model assuming that each genetic variant has no effect on phenotype and 2) Testing each genetic variant for association with the outcome, using the fitted null model.
    
-----

# Part 1: Navigate the NHLBI BioData Catalyst multi-platform ecosystem

## 1a) Link your Terra account to external services
Before you're able to access genomic data from Gen3 in the Terra data table,  you need to link your Terra account to external services. Link your profile [by following these instructions](https://support.terra.bio/hc/en-us/articles/360038086332?flash_digest=2f492682b688b21da27c701af68656ac095d5803).

## 1b) Create an Authorization Domain to protect your controlled-access data

Because this workspace was created to be used with controlled access data, it should be registered under an Authorization Domain that limits its access to only researchers with the appropriate approvals. Learn how to set up an Authorization Domain [here](https://support.terra.bio/hc/en-us/articles/360039415171) before proceeding.

## 1c) Export a TOPMed project with blood pressure data from Gen3

1. Start by learning about Gen3's graph-structured data model for NHLBI's BioData Catalyst using this [orientation document](https://support.terra.bio/hc/en-us/articles/360038087312).
2. Once you better understand the graph, log into [Gen3](https://gen3.biodatacatalyst.nhlbi.nih.gov/) through the NIH portal using your eRA Commons username and password. 
3. Navigate to the [Gen3 Explorer](https://gen3.biodatacatalyst.nhlbi.nih.gov/explorer) view to see what datasets you currently have and do not have access to. On the left-hand side, you can use the faceted search tool to narrow your results to specific projects. 
4. First, under ""Files"" and ""Access"", select ""Data with Access"" to filter through projects that you currently have access to.
5. Next, under ""Filters"", you can select phenotypic or environmental data to narrow your results. Here, select the ""Diagnosis"" tab and under ""BP Diastolic"" move the left hand side of the sliding bar from 0 to 35.  This will make your search range 35 - 163. This will only show the TOPMed projects that contain that trait data. 
6. In all of TOPMed, there are 23 studies with diastolic blood pressure data. You may see anywhere from 0 to 23, depending on what projects you have applied for and received access to. 
12. Next, click on the ""Subject"" tab. If you have access to a TOPMed project with blood pressure data, this will list all of the project names. Select only a single project to use in this template. 
14. Once selected, click the button ""Export all to Terra"", wait until the Terra window appears, and add your data to your copy of this template workspace. 

## 1d) Select a workspace for your data
Once the new Terra window appears, you are given a few options for where to place your data. 

1) ""Start with a template""
This feature allows you to import data directly into a template workspace that has everything set up for you to do an analysis but does not contain any data. In fact, this workspace right here is set up as a template, with workflows and notebooks already in place. Once you select a template workspace to import into, you will need to enter:
- Workspace name: Enter a name that is meaningful for your records.
- Billing Project: Select the billing projects available to you. If you are a new user, you can use the [$300 of free credits offered](https://support.terra.bio/hc/en-us/articles/360027940952-Free-credits-FAQs).
- Authorization Domain: Assign the authorization domain that you generated above to protect your data. This is important for working with controlled access data. 

2) ""Start with an existing workspace"" 
If you have already created a workspace, you can import your data directly to it. 

3) ""Start a new workspace""
This will create an empty workspace. You can individually copy notebooks and workflows from other workspaces, import workflows from Dockstore, or start fresh. Like the template workspace option, you will need to include a name, billing project, and authorization domain.

-----

# Part 2: Explore TOPMed data in Jupyter Notebooks  
There are three notebooks in this workspace. The first and second one use similiar computational resources, but are seperated as you may want to re-run your preliminary GWAS analysis (the focus of the second notebook) more than once, without unarchiving VCFs more than once (the focus of the first notebook). The third notebook is seperate because it requires a much larger compute cluster than the other two notebooks. Overall, this splitting of notebooks into three saves on computation costs.
## 2a)  Extract multi-sample VCFs to your workspace 
Gen3 uploaded tar compressed bundles, as they are provided by dbGAP, into cloud buckets owned by BioData Catalyst. To make these tar files actionable and ready for use in analyses, users will need to unarchive these tar bundlers to their workspace. 

First, open the ***1-unarchive-vcf-tar-file-to-workspace*** notebook and follow the steps to select which tar bundle(s) to extract to your workspace for use in the GWAS. Please understand that this step may be time consuming since TOPMed multi-sample VCF files are several hundred gigabytes in size.

## 2b) Process and visualize your phenotypic data
Now that you can interact with the Gen3 structured data more easily, you will use an interactive notebook to explore your phenotypic and environmental data.

1. Open the **2-GWAS-preliminary-analysis** notebook.
2. From within this **2-GWAS-preliminary-analysis** notebook you can call functions from the companion **terra_data_table_util** notebook to reformat multiple data tables into a single data table that can be loaded as a dataframe in the notebook.
3. Subset the dataframe to include only your traits of interest and remove any individuals that lack data for these traits.
4. Visualize phenotype and environmental variable distributions in a series of plots.
5. Copy your data into your workspace bucket.

## 2c) Prepare your genomic data for input into association test workflows
Continue your GWAS analysis in this notebook, which uses Hail alongside a powerful compute cluster. Further information on how Hail works is given in the notebook itself.
1. [Learn how to customize your interactive analysis compute](https://support.terra.bio/hc/en-us/articles/360038125912) to work with the data you imported. 
2. Import the multi-sample VCF from the ""Reference File"" data table using DRS. You can learn more about GA4GH's Data Repository Service [here](https://support.terra.bio/hc/en-us/articles/360039330211).
3. Filter your VCF to only common variants to increase statistical power. Genetic analyses in this notebook utilize the [Hail software](https://hail.is/). Hail is a framework for distributed computing with a focus on genetics. Particularly relevant for whole genome sequence ([WGS](https://en.wikipedia.org/wiki/Whole_genome_sequencing)) analysis, Hail allows for efficient, nearly boundless computing (in terms of variant and sample size).    
4. Perform a principal component analysis ([PCA](https://en.wikipedia.org/wiki/Principal_component_analysis)) to assess population stratification. Genetic stratification can strongly affect association tests and should be accounted for.
5. Generate a genetic relatedness matrix ([GRM](https://hail.is/docs/0.2/methods/genetics.html?highlight=pc_rel#hail.methods.genetic_relatedness_matrix)) to account for closely related individuals in your association testing workflows.
6. Generate a new ""sample_set"" data table that holds the derived files we created in the steps above using the [FireCloud Service Selector (FISS) package](https://github.com/broadinstitute/fiss).  The files in this data table will be used in the workflows we run in Part 3.     

### Time and cost estimate
You can adjust the runtime configuration to fit your computational needs in each Jupyter notebook. We suggest using the recommended default configuration for the first two notebooks. The third notebook should use the latest Hail image and the associated runtime environment must be a Spark cluster. However, the number of workers, CPUs, and memory will vary drastically based on your input files and cost/time needs. Within the notebook itself, we provide some rule of thumb examples for you to consider for your Spark cluster configuration. Terra will provided a per-hour estimate of cost as you adjust settings of your compute environment. 

The first two notebooks can usually be completed in about 30 minutes total. Depending on your compute configuration and your data, the third notebook is very difficult to predict reliably, but you can assume it will take at least a few hours if running on Freeze 8 data, n>1000 samples.

When working in a notebook with computing times over 30 minutes, learn more about Terra's [auto-pause feature](https://support.terra.bio/hc/en-us/articles/360029761352) and [how to adjust auto-pause](https://support.terra.bio/hc/en-us/articles/360027020412) for your needs. Please carefully consider how adjusting auto-pause can remove protections that help you from accidentally accumulating cloud costs that you did not need.

-----

# Part 3: Perform mixed-model association tests using workflows
In Part 2, we explored the data we imported from Gen3 and performed a few important steps for preparing our data for association testing. We generated a new ""sample_set"" data table that holds the files we created in the interactive notebook. These files will be used in our batch workflows that will perform the association tests. Below, we describe the four workflows in this workspace and their cost estimates for running on the sample set we create in this tutorial.  

The workflows used in this template were imported from [Dockstore](https://www.dockstore.org) and their parameters were configured to work with Terra's data model.  If you're interested in searching other Docker-based workflows, [learn more about how they can easily be launched in Terra](https://support.terra.bio/hc/en-us/articles/360038137292).

## Notes on how attributes are set in workflows
We have set the input and output attributes for each workflow in this template. Before running the first workflow, you can look through the inputs and outputs of each workflow and see that outputs from the first workflow feed into the second workflow, and so on. 

In the 2-GWAS-preliminary-analysis notebook, we created a Sample Set data table that holds a row called ""systolicbp"" which contains the input files for the following workflows. You can check this data table out in the Data tab of this workspace. When you open a workflow, make sure that ""Sample Set"" is set and the ""systolicbp"" (or whatever you named your run) is selected before running a workflow. 

#### [1-vcfToGds](https://dockstore.org/workflows/github.com/manning-lab/vcfToGds)

This workflow converts genotype files from Variant Call Format ([VCF](https://en.wikipedia.org/wiki/Variant_Call_Format)) to Genomic Data Structure ([GDS](http://si.biostat.washington.edu/sites/default/files/modules/GDS_intro.pdf)), the input format required by the R package GENESIS.       

##### Time and cost estimates    

| Sample Set Name | Sample Size | # Variants | Time | Cost $ |
| -------  | --------  | -------- | -------- | ---------- |
| systolicbp | 1,052 samples | 6,429,788 | 15m | $1.01

Inputs:
* VCF  genotype file (or chunks of VCF files)

Outputs:
* GDS genotype file

#### [2-genesis_GWAS](https://dockstore.org/workflows/github.com/AnalysisCommons/genesis_wdl/genesis_GWAS)

This workflow creates a null model from phenotype data with the GENESIS biostatistical package. This null model can then be used for association testing. This workflow also runs single variant and aggregate test for genetic data. Implements Single-variant, Burden, SKAT, SKAT-O and, SMMAT tests for Continuous or Dichotomous outcomes. All tests account for familiar relatedness through kinship matrixes. Underlying functions adapted from: Conomos MP and Thornton T (2016). GENESIS: GENetic EStimation and Inference in Structured samples (GENESIS): Statistical methods for analyzing genetic data from samples with population structure and/or relatedness. R package version 2.3.4.        

##### Time and cost estimates    

| Sample Set Name | Sample Size | Time | Cost |
| -------  | -------- | -------- | ---------- |
|  systolicbp | 1,052 samples | 26m | $0.94

Inputs:
* GDS genotype file
* Genetic Relatedness Matrix
* Trait outcome name
* Trait outcome type
* CSV file of covariate traits
* Sample ID list

Outputs:
* A null model as an RData file
* Compressed CSV file(s) containing raw results
* CSV file containing all associations
* CSV file containing top associations
* PNG file of Quantile-Quantile and Manhattan plots


# Cost Examples
Below are reported costs from using 1,000 and 10,000 samples to conduct a GWAS using the BioData Catalyst GWAS Blood Pressure Trait template workspace. The costs were derived from single variant tests that used Freeze 5b VCF files that were filtered for common variants (MAF <0.05) for input into workflows. The way these steps scale will vary with the number of variants, individuals, and parameters chosen. TOPMed Freeze 5b VCF files contain 582 million variants and Freeze 8 increases to ~1.2 billion. For GWAS analyses with Freeze 8 data, computational resources and costs are expected to be significantly higher.

| Analysis Step | Cost (n=1,000; Freeze5b) | Cost (n=10,000; Freeze 5b) |
| -- | -- | -- |
| GWAS Preliminary Analysis Notebook | $29.34 ($19.56/hr for 1.5 hours) | $336 ($56/hr for 6 hours) |
| vcfTogds workflow | $1.01 | $5.01 |  
| genesis_GWAS workflow | $0.94 | $6.67 |
| TOTAL | $32.29 | $347.68 | 

These costs were derived from running these analyses in Terra in June 2020.

-----
# Optional: Bring your own data
Both the notebook and workflow can be adapted to other genetic datasets. The steps for adapting these tools to another dataset are outlined below:

***Update the data tables***  Learn more about uploading data to Terra [here](https://support.terra.bio/hc/en-us/articles/360025758392-Managing-data-with-the-data-table-).  You can use functions available from the terra_data_table_util companion notebook to consolidate new data tables you generate.

***Update the notebook***
Accommodating other datasets may require modifying many parts of this notebook. Inherently, the notebook is an interactive analysis where decisions are made as you go. It is not recommended that the notebook be applied to another dataset without careful thought. 

***Run an additional workflow***
You can search [Dockstore](https://www.dockstore.org) for available workflows and export them to Terra following [this method](https://docs.dockstore.org/en/develop/launch-with/terra-launch-with.html). 


### Helpful resources to master this tutorial

* If you are new to BioData Catalyst powered by Terra, we have created an [onboarding syllabus](https://bdcatalyst.gitbook.io/biodata-catalyst-documentation/analyze-data/terra/onboarding-syllabus-using-gen3-terra-dockstore-and-pic-sure) that includes several introductory webinars.

* [Controlling cloud costs](https://support.terra.bio/hc/en-us/articles/360029748111)

* [Intro to Jupyter notebooks in Terra](https://app.terra.bio/#workspaces/help-gatk/Jupyter%20Notebooks%20101)

* [Intro to Hail using a Terra workspace](https://app.terra.bio/#workspaces/help-gatk/Hail-Notebook-Tutorials)

* [GWAS tutorial using open data from the 1000 Genomes Project](https://app.terra.bio/#workspaces/broad-t2d-dev/Running_GWAS_in_Terra)

### Authors, contact information, and funding

This template was created for the [NHLBI's BioData Catalyst](https://biodatacatalyst.nhlbi.nih.gov/) project in collaboration with the [Computational Genomics Platform](https://cgpgenomics.ucsc.edu/) at [UCSC Genomics Institute](https://ucscgenomics.soe.ucsc.edu/) and the [Data Sciences Platform](https://www.broadinstitute.org/data-sciences-platform) at [The Broad Institute](https://www.broadinstitute.org/). The association analysis tools were contributed by the [Manning Lab](https://manning-lab.github.io/).

Contributing authors include:
* [Beth Sheets](mailto:esheets@ucsc.edu) (UC Santa Cruz Genomics Institute)
* Michael Baumann (Broad Institute, Data Sciences Platform)
* Brian Hannafious (UC Santa Cruz Genomics Institute)
* [Tim Majarian](mailto:tmajaria@broadinsitute.org) (Manning Lab)
* Alisa Manning (Manning Lab)
* Ash O'Farrell (UC Santa Cruz Genomics Institute)

----

### Workspace change log 

| Date | Change | Author | 
| -------  | -------- | -------- |
| Feb 3, 2020 | Added Freeze 8 disclaimer | Ash |
| Jan 8, 2020 | Split second notebook into two (for a total of three) in the interest of clarity & cost-savings by minimizing the time spent using Hail. Also, general notebook & dashboard updates | Ash |
| Dec 9, 2020 | Update notebooks, workflows, and workspace markdown | Ash |
| June 26, 2020 | terra_data_table_util updates | Beth |
| Feb 26, 2020 | Added notebook to copy/extract VCF | Beth |
| Jan 31, 2020 | Replaced text with new Broad documentation | Beth |
| Jan 30, 2020 | Template updates | Beth |
| Jan 3, 2020 | Updates from BDC F2F | Beth |
| Dec 3, 2019 | Gen3 updates | Beth |
| Nov 22, 2019 | Updates from Alisa | Beth |
| Oct 22, 2019 | User experience edits from Beri | Beth|",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","biodata-catalyst/BioData Catalyst GWAS blood pressure trait"
377,"help-terra","T101_Quickstart","READER","https://app.terra.bio/#workspaces/help-terra/T101_Quickstart",TRUE,TRUE,NA,NA,NA,"Hands on practice working in Terra. This Quickstart walks through a mock study of the correlation between height and GPA for a cohort of 7th, 8th, and 9th graders.   

## Quickstart learning objectives 
After working through this tutorial, you should know how to   
1. Interpret and modify a workspace data table of input data and metadata   
2. Set up and run a workflow on data from a table   
3. Set up and launch a Jupyter Cloud Environment VM  
4. Run a notebook to plot and visualize processed data 

## How long will it take? How much will it cost?    
**Time**     
You should be able to complete the Quickstart tutorial in about an hour.    

**Cost**      
Running the tutorial will cost  much less than $1 (Google Cloud data storage and VM costs). You will need to have a Terra Billing project and your own copy of the Quickstart workspace to complete the tutorial.     

## Quickstart summary    

The question being asked is ""Is there a relationship between a student's height and their total GPA across three subjects. The Quickstart has four steps (scroll down for more details) that mimic the typical steps in a research journey you might do in Terra.      

<img alt=""Diagram of steps in Quickstart"" src=""https://storage.googleapis.com/terra-featured-workspaces/QuickStart/Unified-Quickstart-flow.png"" width=""900"">     

1. Explore mock data in a `student` table    
2. Set up and run a workflow to calculate the total GPA for each student
3. Set up a Jupyter Cloud Environment to run a notebook analysis
4. Analyze student data by running the `Analyze-data-from-table` notebook    
5. (optional) Run the Jupyter-101 notebook

### 1. Data exploration  
Explore and manipulate synthetic data for a mock study of 86 middle school students.  Study data includes  student **names**, **height**, and **language**, **math**, and **science grades**.   

### 2. Data processing 
Get hands-on practice setting up and running a workflow (data processing) on data from the student table. The workflow calculates the average GPA for each student from the three subject grades. . You'll plot the results - `Cumulative_GPA` - in a notebook next.

### 3. Data visualizing part 1  
Set up a Cloud Environment (software + VM + persistent disk) to run your interactive Jupyter notebook analysis. Cloud Environments in Terra can be customized to fit your analysis needs. 

### 4. Data visualizing part 2

In the `Analyze-data-from-table.jpynb` notebook, you'll import the height and average GPA from the student table and generate plots to see if there is a correlation. Comparing results from the small and full datasets demonstrates how small number statistics can skew your results, and why it's important to use large datasets. 
   
**Analyze-data-from-table.jpynb**     
This notebook pulls in height and Cumulative_GPA data from a `student` table subset (eight students) as well as from the full dataset in the `student` table (86 students). After importing the data from the workspace table, you'll plot and compare the Cumulative GPA for both the subset and full dataset.    

**Jupyter 101.jpynb**    
If you're new to notebooks, explore this introduction to Jupyter notebooks to learn more about:
- Why notebooks are used in biomedical research.
- The relationship between the notebook and the workspace.
- Jupyter Notebook basics: how to use a notebook, install packages, and import modules.
- Common libraries in data analysis and popular tutorial notebooks.


## Contact information  
This material is provided by the Terra Team. Please post any questions or concerns to our forum site: <a href=""https://support.terra.bio/hc/en-us/community/topics/360001603491-Featured-Workspaces"" target=""blank"">Terra community forum</a>. 
<br>
<br>

## Workspace Citation 
Details on citing Terra workspaces can be found in the article <a href=""https://support.terra.bio/hc/en-us/articles/360035343652"" target=""blank"">How to cite Terra</a>.

Data Sciences Platform, Broad Institute (*Year, Month Day that this workspace was last modified*) help-terra/T101-Data-Tables-Quickstart [workspace] Retrieved *Month Day, Year that workspace was retrieved*,  https://app.terra.bio/#workspaces/help-terra/T101-Quickstart.
<br>
<br>

## License  
**Copyright Broad Institute, 2020 | BSD-3**  
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at <a href=""https://github.com/openwdl/wdl/blob/master/LICENSE"" target=""blank"">github.com/openwdl/wdl/blob/master/LICENSE</a>). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-terra/T101_Quickstart"
378,"kco-tech","TOSC19-Waddington-OT","READER","https://app.terra.bio/#workspaces/kco-tech/TOSC19-Waddington-OT",TRUE,FALSE,NA,NA,NA,"### Overview
This tutorial provides a practical, hands-on introduction to inferring developmental trajectories with [Waddington-OT](https://broadinstitute.github.io/wot/).

Single cell RNA-sequencing allows us to profile the diversity of cells along a developmental time-course by recording static snapshots at different time points. However, we cannot directly observe the progression of any individual cell over time because the measurement process is destructive.

Waddington-OT is designed to infer the temporal couplings of a developmental stochastic process ℙt from samples collected independently at various time-points. We represent a developing population of cells with a time-varying distribution ℙt on gene expression space. The temporal couplings describe the flow of mass as the population develops and grows. For a pair of time-points (t&#7522;,t&#11388;), the coupling γt&#7522;,t&#11388; tells us: What descendants does cell x from time t&#7522; give rise to at time t&#11388;?

In this tutorial, we explore the Waddington-OT workflow, starting with inferring temporal couplings with optimal transport. We then go through numerous downstream analyses including visualizing cell fates, interpolating the distribution of cells at held-out time points, and inferring gene regulatory networks. The tutorial guides the reader through the concepts, each of which is illustrated with an interactive Jupyter notebook using data from a time-course of iPS reprogramming ([Schiebinger et al. 2019](https://www.cell.com/cell/fulltext/S0092-8674(19)30039-X)).

### Tutorial
A step by step tutorial that accompanies this workspace is available at https://broadinstitute.github.io/wot/tutorial/.

### Data
Single-cell RNA sequencing from a time-course of iPS reprogramming ([Schiebinger et al. 2019](https://www.cell.com/cell/fulltext/S0092-8674(19)30039-X)).

### Notebooks
Nine notebooks are provided:

* Notebook 0: Install wot and download data
* Notebook 1: Visualizing and exploring the data
* Notebook 2: Computing transport matrices
* Notebook 3: Inferring long-range temporal couplings
* Notebook 4: Ancestors descendants, and trajectories
* Notebook 5: Fate Matrices
* Notebook 6: Transition tables
* Notebook 7: Validation by Geodesic Interpolation
* Notebook 8: Predictive TFs 


###  Contact information

* **Website:** https://broadinstitute.github.io/wot/
* **Email:** wot@broadinstitute.org
* **Github:** https://github.com/broadinstitute/wot

### Time and cost
It takes ~25 minutes and costs ~25 cents to run the notebooks using the data provided in the tutorial.


### Software versions
This workspace requires wot version 1 (installed in Notebook 0) and Python 3


### License
[BSD 3-Clause](https://github.com/broadinstitute/wot/blob/master/LICENSE)

###  Evaluation form
To rate this workspace as part of the Terra Open Science Contest, please fill out this [seven question survey](https://forms.gle/ma4AembkSLkyAaTE9).
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","kco-tech/TOSC19-Waddington-OT"
379,"encode-tutorial","ENCODE-Tutorial-May-2019","READER","https://app.terra.bio/#workspaces/encode-tutorial/ENCODE-Tutorial-May-2019",TRUE,TRUE,NA,NA,NA,"Learn how to search, analyze, and visualize ENCyclopedia Of DNA Elements (ENCODE) data. The resources in this workspace cover binning ENCODE ChIP-seq datasets into non-overlapping 5 kB bins and determining the signal enrichment in each bin.  More information about the ENCODE project can be found here: https://www.encodeproject.org/.  

The contents in this tutorial come from the ENCODE Terra Tutorial given May, 2019 and include the following steps:

1. How to access and import selected ENCODE data from the Data Explorer
2. How to use a workflow tool to calculate the Probability of Being Signal (PBS) to indicate the presence of H3K27ac histone marks
3. How to identify regions of interest by plotting  a comparison between BED files generated by the PBS Tool in a Jupyter Notebook
4. How to zero in on regions of interest by visualizing tracks in IGV in the [web browser](http://igv.org/app/)


# How to use this workspace
[Follow the step-by-step instructions in this pdf.](https://storage.googleapis.com/terra-featured-workspaces/encode-tutorial-2019/Encode%20Tutorial%20Handout%20May%202019.pdf). The step-by-step guide will walk you through the following:   

* Step 1 - Find and select ENCODE data and export to a cloned version of this workspace	       
* Step 2 - Verify data in the workspace data table
* Step 3 - Set up and run the Probability of Being Signal (PBS) workflow on data in the data table
* Step 4 - Monitor the status of your PBS submissions 
* Step 5 - Analyze PBS workflow output in a Jupyter Notebook to identify regions of interest   
* Step 6 - Visualize bigWig tracks in IGV	8      


## Accessing ENCODE data using the Data Explorer

ENCODE data is hosted on Terra and is available through the Terra [Data Explorer](https://broad-gdr-encode.appspot.com/) , in addition to the ENCODE platform. Follow Step 1 on the [pdf tutorial guide](https://storage.googleapis.com/terra-featured-workspaces/encode-tutorial-2019/Encode%20Tutorial%20Handout%20May%202019.pdf) to navigate to the ENCODE Data Explorer and create a custom cohort. 

The ENCODE data hosted in Terra's Data Explorer is not an exact mirror of the data that is publicly hosted on AWS.  Here is a description of what is hosted on the integrated Data Explorer as of May, 2019.

For tabular data / metadata, the bucket contains experiment information for these assay types:

* ATAC-seq
* ChIA-PET
* ChIP-seq
* DNase-seq
* Hi-C
* microRNA-seq
* polyA RNA-seq
* RRBS
* total RNA-seq
* WGBS

The following filters were applied to the references to pull metadata for linked entities:

* Human data
* Status = released
* Files that can trace back to a replicate

From that set of data, the following files were transfered from S3 to Google:

* BAMs with output type “alignments”
* bigBeds with output type “peaks”
* bigWigs with output type “fold change over control”

The integrated Data Explorer in Terra contains tabular rows for 659 donors and 159,884 “files” (the file rows contain the metadata of everything except donors.)  It's 98.23 TB in size.
For questions about data access and any information related to the ENCODE data hosted in Terra, please contact Jeff Korte: at jkorte@broadinstitute.org

## Preloaded Data
Steps 1-2 of the tutorial walk you through the process of accessing, selecting, and linking ENCODE data using the Data Explorer. If you don't want to generate your own cohorts, we include metadata for three BAM data files that are preloaded in the workspace data table that you can use for analysis (steps 3 - 6). 

#### Cohort name: heart_bam

* ChIP-seq data 
* Selected tissues (heart left ventricle and right atrium auricular region)
* Donor accession  ENCDO793LXB, ENCDO845WKR, ENCDO271OUW, ENCDO451RUA
* Target H3K27ac
* File format is BAM


##  Workspace Attributes 
This workspace has no attributes that are loaded automatically.

-----------
## Overview of the analysis tool (""method"")   

We provide preconfigured tools for two workflows that calculate Probability of Being Signal (PBS) values. One accepts bigWig files as input, the other accepts BAM files. PBS values can be used to easily identify 5 KB regions of interest and compare multiple datasets regardless of read depth and peak calling. 

* **PBS-bam** - takes a BAM file as input .   

The **PBS-bam** WDL will bin the reads into 5 KB windows, fit a gamma distribution to the binned data, and calculate the probability of the bin containing a signal (PBS) for the H3K27ac modification. The WDL outputs two files: (1) a BED file named **pbs.bed** with bins and their scores and (2) a PDF file named **fit.pdf** containing a plot illustrating the empircal distribution, background distribution, and probability of being signal.  

* **Assigning the bin count value**:  After binning the data, the PBS tool assigns a count value to each 5 KB region, based on the ratio between the number of base pairs covered by reads in each bin and the total bin size.    

*  **Calculating the probability of being signal (PBS)**: The tool first estimates the background distribution of the data with a gamma distribution, then calculates the PBS as the fraction of bins at each count value that are signal.  This signal correlates to the right tail distribution - the signal is the measure of the difference between the bin and the expected gamma distribution curve.
 
* All alignments use the human reference genome **hg19** (also referred to as GRCh37, or b37).

* The PBS-bam tool has been tested on data sets containing the target histone acetylation site ***H3K27ac.***  The tool may work differently on alternative targets. 

The workflow is summarized in the graphic below.

![image](https://storage.googleapis.com/terra-featured-workspaces/encode-tutorial-2019/Binning_PBS.png)

### Software Version

| Tool Name | Version  |
|---|---|
| PBS_bam| snapshot 14 |  

### Time and cost 

| Workflow Name                  	| 1 file (range) 	| 100 files 	| Time to Run 1 file 	| Time to Run 100 files 	|
|--------------------------------	|----------------	|-----------	|--------------------	|-----------------------	|
| PBS-bam     	| $0.03 	| < $3.00   	| 10-15 minutes          	| 15-30 minutes              	|***

The notebook is set to the default ""Medium"" configuration which costs $0.19 per hour.

* Cost and Time will vary due to the use of pre-emptibles and the size of the files.

* Users can also use [Google's cost calculator](https://cloud.google.com/products/calculator/).


------------
## Licensing

**Copyright Broad Institute, 2019 | BSD-3**
All code provided in this workspace is released under the WDL open source code license (BSD-3) [full license text here](https://github.com/openwdl/wdl/blob/master/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.

-----------
## Contact information

For questions about this tutorial and using Encode Data in general, please contact Kevin Dong (kdong@broadinstitute.org)
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","encode-tutorial/ENCODE-Tutorial-May-2019"
380,"fc-product-demo","Terra-Data-Tables-Quickstart","READER","https://app.terra.bio/#workspaces/fc-product-demo/Terra-Data-Tables-Quickstart",TRUE,FALSE,NA,NA,NA,"How to use workspace data tables to organize, access and analyze data - including sets of data - in the cloud.                  

![Data QuickStart exercise flow ](https://storage.googleapis.com/terra-featured-workspaces/QuickStart/Data-Quickstart_Diagram-of-flow.png)      

![white space](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/white-space.jpg)    

### How long will it take to run? How much will it cost?    

| Time to complete | Total cost |     
|-------|-------|     
|~15-20 minutes per part (x4 parts)   | <<$1.00 ( GCP fees paid through the workspace billing project) |       
 




 **Note that you don't have to do all parts at once** (though you do need to do them in order!)    
 
![white space](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/white-space.jpg)  

### First: Create your own editable copy (clone) of this WORKSPACE to work from       

To create your own copy, click on the round circle with three dots in the upper right corner of this page:   
    ![Clone_ScreenShot](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/Clone_workspace_Screen%20Shot.png)

| ![tip icon](https://storage.googleapis.com/terra-featured-workspaces/QuickStart/video-icon-final_scaled-2.png) |[**Click for a video walkthrough**](https://youtu.be/mYUNQyAJ6WI)  ![white space](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/white-space.jpg)  |     
|-------|-------|    

       
![white space](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/white-space.jpg)     


 
## Part 1: Run a workflow on a single specimen in a table     
  
### Learning objectives
By the end of this section, you should understand the basics of workspace tables; what information they include and where to find them, how they keep data organized, and how to use them to link to input for a workflow analysis. By running on a single specimen, you'll get familiar with how tables work with hands-on practice.          


### Time and cost to complete
The exercise should take about 20 minutes and cost $0.02 (paid to GCP through the workspace billing project you used).         

   
|![white space](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/white-space.jpg)    ![read more icon](https://storage.googleapis.com/terra-featured-workspaces/QuickStart/icon_read_more%401x.png) | [**Open step-by-step instructions**](https://support.terra.bio/hc/en-us/articles/360047046131) |  
|---| ---------|         


![white space](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/white-space.jpg)     

## Part 2: Make a basic workspace table from scratch        


### Learning objectives
In this section, you'll make your own workspace table by creating a ""load file"" in a spreadsheet editor and upload to Terra. You'll learn the formatting requirements of load files and experiment with how changing the load file changes the table in your workspace .           
        
### Time and cost estimates
Expect to spend 10-15 minutes reading about this and making your own tables. These exercises won't cost anything!               
   
|![white space](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/white-space.jpg)    ![read more icon](https://storage.googleapis.com/terra-featured-workspaces/QuickStart/icon_read_more%401x.png) | [**Open step-by-step instructions**](https://support.terra.bio/hc/en-us/articles/360047102512) |  
|---| ---------|         
    
![white space](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/white-space.jpg)

## Part 3: Run on sets of single specimens       

### Learning objectives
In this section, you'll how to group entities together to analyze as a set, as well as how to create entity_set tables. Sets let you run a workrflow or workflows on the same group of entities efficiently - again and again.     

![diagram of using sets of single entities to process groups efficiently](https://storage.googleapis.com/terra-featured-workspaces/QuickStart/Data-Quickstart_Part3_Sets-of-single-entities-as-input.png)        

### Time and cost estimates
This exercise should take about 15 minutes and cost less than $0.02 to run. 
  
   
|![white space](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/white-space.jpg)    ![read more icon](https://storage.googleapis.com/terra-featured-workspaces/QuickStart/icon_read_more%401x.png) | [**Open step-by-step instructions**](https://support.terra.bio/hc/en-us/articles/360047611871) |  
|---| ---------|            

    
![white space](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/white-space.jpg)


## Part 4: Run a WDL with a set (array) as input       
    

### Learning objectives
Learn how to set up and run a workflow that takes multiple entities and output a single file.   

![Diagram of running a workflow on sets as arrays of input with single output file](https://storage.googleapis.com/terra-featured-workspaces/QuickStart/Data-Quickstart_Part4_Sets-as-input-arrays.png)      

### Time and cost estimates
This will take about 15 minutes and cost less than $0.02 to run.      
 
   
| ![white space](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/white-space.jpg)   ![read more icon](https://storage.googleapis.com/terra-featured-workspaces/QuickStart/icon_read_more%401x.png) | [**Open step-by-step instructions**](https://support.terra.bio/hc/en-us/articles/360047621171) |  
|---| ---------|            

    
![white space](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/white-space.jpg)




## How can I learn more?
Now that you know the basics, you are well on the way to using the Terra platform to do your own analysis. Below are some more advanced resources for next steps.     

* To learn more about Terra and its functionality, see our [user guide](https://support.terra.bio/hc/en-us)       
* [Getting started on Terra](https://support.terra.bio/hc/en-us/categories/360005881492)      
* [Doing research on Terra](https://support.terra.bio/hc/en-us/categories/360001399872)    
* [Pipelining with workflows](https://support.terra.bio/hc/en-us/sections/360004147011)      
* To learn more about cloning workspaces, see [this article](https://support.terra.bio/hc/en-us/articles/360026130851)     
* For more information about how to manage data with a data table, see [this article](https://support.terra.bio/hc/en-us/articles/360025758392)      
* To learn more about troubleshooting and monitoring your workflow submission on Terra, see [this article](https://support.terra.bio/hc/en-us/articles/360027920592)          
* [Understanding Cloud costs and tips and tricks for reducing costs](https://support.terra.bio/hc/en-us/articles/360029748111)          
* [Sample use cases Cloud costs](https://support.terra.bio/hc/en-us/articles/360029772212)         
       
![white space](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/white-space.jpg)     



## Contact information  
This material is provided by the Terra Team. Please post any questions or concerns to our [Terra forum site.](https://broadinstitute.zendesk.com/hc/en-us/community/topics/360000500432-General-Discussion) 

Help us improve! Let us know what you think of the workflows QuickStart by filling out this [feedback form](https://docs.google.com/forms/d/e/1FAIpQLSeD5gI2CIEgblJ16DdgamE6w4yl3zlt_2xIkgONDaKfRxQxCw/viewform). We value your input.
       
![white space](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/white-space.jpg)


## License  
**Copyright Broad Institute, 2020 | BSD-3**  
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at https://github.com/openwdl/wdl/blob/master/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.

### Workspace Citation 
Details on citing Terra workspaces can be found here: [How to cite Terra](https://support.terra.bio/hc/en-us/articles/360035343652).

Data Sciences Platform, Broad Institute (*7/26/2021*), fc-product-demo/Terra-Data-Tables-Quickstart [workspace] Retrieved *Month Day, Year that workspace was retrieved*,  https://app.terra.bio/#workspaces/fc-product-demo/Terra-Data-Tables-Quickstart",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","fc-product-demo/Terra-Data-Tables-Quickstart"
381,"help-gatk","GATKTutorials-Pipelining-August2019","READER","https://app.terra.bio/#workspaces/help-gatk/GATKTutorials-Pipelining-August2019",TRUE,FALSE,NA,NA,NA,"# What's in this workspace?
*This workspace was originally developed for the Genome Analysis Toolkit (GATK) workshop in São Paulo, Brazil in August 2019. Feel free to run it on your own, even if you were unable to attend that workshop!*

Welcome to Day 4 of the Genome Analysis Toolkit (GATK) workshop!

Earlier today, you learned about WDL and Cromwell. Using this empty workspace, we will practice starting a workspace from scratch. Your instructor will guide you through this workspace, but you should find the documentation on this page sufficient to run through them again on your own or to share with a friend later (and we encourage you to do so!).

### Data
You will need to download the data bundle located [here](https://broad.io/GATK1908) if you haven't already. This bundle contains several folders but the one you will need for this workspace is labelled **Terra**. It provides the WDL script, workspace and sample metadata, and an inputs file to run.

### Tools
There are no tools in this workspace yet. But we will be putting a tool in that runs GATK's HaplotypeCaller, using the instructions below.

**Add a new Tool**
1. In a new tab, navigate to Code & Tools. On the right hand side of the page, click on the `Broad Methods Repository` link. This currently will take you to our legacy application to upload a new Method, which is another name for Tool. In the near future, you will be able to upload your new tool directly in Terra. 
2. Click the blue `Create New Method` button in the upper right corner.
3. Fill out the window that pops up as follows:
>**Namespace:** your own name or abbreviation of your name. 

The namespace is essentially the publisher of the method. You can publish it as yourself, as we are doing here, or sometimes you may want to publish it under your lab or institution's name. 
>**Name:** `HelloGATK`

This refers to the name of your tool. You can call it whatever you want, but we've chosen to match it to the WDL script we will be using. Lastly:
>**WDL:** Click the blue `load from file...` link, and load the `hello_gatk_terra.wdl` script from your data bundle, under the Terra folder

4. Click `Upload`
5. Click the blue `Export to Workspace` button in the upper right corner, then in the dialog that pops up, select `Use Blank Configuration`
6. Under `Destination Workspace`, select your clone of this workspace, then click `Export to Workspace`
7. The page will ask you `Go to edit page now?` Click Yes, and you will be redirected back to your workspace!

**Add Data**

For our tutorial purposes, we've already uploaded your sample files to a public google bucket so that you do not need to incur storage costs. If you did want to upload your own samples to run this workflow on later, you can do so by navigating to the Data tab in your workspace, and clicking on the `Files` section in the lefthand menu. From there, you can upload whatever files you need.

**Run your new Tool!**
1. Navigate to the `Tools` tab in your workspace
2. Click on the HelloGATK tool you recently added.
3. Drag the `hello_gatk_fc.inputs.json` file from your Terra folder in the data bundle to upload it and populate the inputs section. You'll notice there are a number of `gs://` links, which represent files stored in google buckets. 
4. Click the green `Save` button to save the inputs you just uploaded
5. At the top of the page, select the radio button next to `Process single workflow from files`
6. Click the green `Run Analysis` button

**Optional: Parameterize your inputs**

You don't want to go in and change these `gs://` links for each individual sample you want to run on, especially if you're running on a large number of samples. You can run your workflow on many samples with a single click by setting up your Data tables.
1. Go to the `Data` tab in your workspace
2. Upload `participant.tsv` and `sample.tsv` from the Terra folder in your data bundle by clicking the `+` icon next to Tables in the lefthand menu
3. Click on the Workspace Data section in the lefthand menu, and upload `workspace_attributes.tsv` from the Terra folder. 
4. Navigate back to the `Tools` tab. 
5. Select the radio button for `Process multiple workflows from` and choose `Sample` from the dropdown. 
6. In the inputs table at the bottom, we now need to replace the `gs://` links with references to the data table. For the `inputBAM` and `bamIndex`, try typing `this` and see what Terra auto-fills for you as options. The keyword `this` refers to whatever data table you selected in the dropdown at the top. Since we picked `Sample`, it fills in options that exist in your `Sample` table. Click `Save` when you are done.
7. Click the green `Run Analysis` button

### Software versions
GATK4.1.1.0

## Appendix

### GATK @ Brazil 2019 in the Terra Support Center

Following the conclusion of the workshop, the workshop materials will be made available in the Terra Support Center. For now, you can find these materials in a google drive folder at [broad.io/GATK1908](https://broad.io/GATK1908)

### GATK documentation and support

Detailed documentation, tutorials, and more are available at the [GATK web site](https://software.broadinstitute.org/gatk/). If you are still running into issues after consulting the User Guide, have a question, or just want to say hello, reach out to the GATK team via the [GATK Support Forum](https://gatkforums.broadinstitute.org/gatk)!

### Terra documentation and support

Documents and helpful guides to support your journey through Terra are available in the Terra Support Center. Navigate there by following the “Learn About Terra” link in the left-side menu. At the time of this workshop, we are still actively writing and publishing documents but we’ve got a good set of docs in the Quick Start Guide to get you going.


",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/GATKTutorials-Pipelining-August2019"
382,"help-gatk","BroadE-Day2-Germline","READER","https://app.terra.bio/#workspaces/help-gatk/BroadE-Day2-Germline",TRUE,FALSE,NA,NA,NA,"# What's in this workspace?
Welcome to Day 2 of the Genome Analysis Toolkit (GATK) workshop for the BroadE workshop series at Broad! Yesterday you got an overview of the various tools and pipelines, but today will focus on Germline Variant Discovery. 

Here you will be running four different notebooks. In the morning we will cover variant discovery, and in the afternoon we will look at different methods for variant filtering. This workspace is read-only, so clone your own unique copy to work with it.

Your instructor will guide you through this workspace, but you should find the documentation within the notebooks sufficient to run through them again on your own or to share with a friend later (and we encourage you to do so!).

The notebook(s) in this workspace are:
* 1-germline-variant-discovery-tutorial
* 2-gatk-hard-filtering-tutorial-python
* 3-gatk-hard-filtering-tutorial-r-plotting
* 4-gatk-cnn-tutorial-python

All notebooks in this workspace can use the following runtime settings:

| Runtime Environments | Value |
| --- | --- |
| CPU Minimum| 4|
| Disksize Minimum | 100 GB |
| Memory Minimum | 15 GB |
| Startup Script | gs://gatk-tutorials/scripts/install_gatk_4100_with_condaenv.sh |

## Appendix

### GATK @ BroadE 2019 in the Terra Support Center

Following the conclusion of the workshop, the workshop materials will be made available in the Terra Support Center. For now, you can find these materials in a google drive folder at [broad.io/GATK1903](broad.io/GATK1903)

### GATK documentation and support

Detailed documentation, tutorials, and more are available at the [GATK web site](https://software.broadinstitute.org/gatk/). If you are still running into issues after consulting the User Guide, have a question, or just want to say hello, reach out to the GATK team via the [GATK Support Forum](https://gatkforums.broadinstitute.org/gatk)!

### Terra documentation and support

Documents and helpful guides to support your journey through Terra are available in the Terra Support Center. Navigate there by following the “Learn About Terra” link in the left-side menu. At the time of this workshop, we are still actively writing and publishing documents but we’ve got a good set of docs in the Quick Start Guide to get you going.



",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/BroadE-Day2-Germline"
383,"broad-firecloud-tcga","CPTAC_BRCA_OpenAccess_V1_DRAFT","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/CPTAC_BRCA_OpenAccess_V1_DRAFT",TRUE,TRUE,NA,NA,NA,"Draft CPTAC workspace for BRCA project.  One PCC, Broad Institute (BI), participated in the CPTAC BRCA project.  BI conducted Proteome and Phosphoproteome experiments.

Experimental sample sets have been created for each itraq experiment.  The PSM files are referenced through attributes on their respective experiment sample set.

 Protein reports are aggregated across all experiments done at a specific PCC and on a specific analyte. Hence, the workspace contains sample sets for each pairing of PCC and analyte type and attributes on these sample sets reference the protein reports.",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","broad-firecloud-tcga/CPTAC_BRCA_OpenAccess_V1_DRAFT"
384,"help-gatk","Jupyter Notebooks 101","READER","https://app.terra.bio/#workspaces/help-gatk/Jupyter%20Notebooks%20101",TRUE,TRUE,NA,NA,NA,"
A crash course for getting started using Jupyter notebooks for interactive visualization and statistical analyses.

**An updated version of this workspace is available, please visit [Terra-Notebooks-Quickstart](https://app.terra.bio/#workspaces/fc-product-demo/Terra-Notebooks-Quickstart)**

Maybe you have heard of Jupyter notebooks and you're interested in what they are and how to start using them to do interactive analysis on large amounts of data. If so, we're glad you're here! This notebook will help you understand

* Why notebooks are used in biomedical research    
* The relationship between the notebook and the workspace    
* Jupyter Notebook basics: how to use a notebook, install packages, and import modules    
* Common libraries in data analysis and popular tutorial notebooks    

To run the notebook, you will need to clone this workspace to your own Billing Project. Then head over to the **Notebooks** tab of your cloned workspace to get started.

**If you are already familiar with Jupyter notebooks**, check out the notebooks in the [Terra Notebooks Playgound](https://app.terra.bio/#workspaces/help-gatk/Terra%20Notebooks%20Playground), which contains a set of Jupyter Notebooks that allow users to play with notebook functionality. These include both R and Python Setup notebooks and template notebooks for accessing and analyzing data. 

---

### Data

No data here!! We'll be focusing on the basic aspects of using Notebooks. However, if you are interested in working with data and bringing data from the Data tab visit the [Terra Notebooks Playgound](https://app.terra.bio/#workspaces/help-gatk/Terra%20Notebooks%20Playground) workspace. 

---

### Workflows

No tools here!! This is a Notebook-focused workspace. 

---

### Notebooks

 **Intro to Jupyter Notebooks :**

| Runtime Environments | Value |
| --- | --- |
| CPU Minimum| 1 |
| Disksize Minimum | 100 GB |
| Memory Minimum | 3.75 GB |
| Startup Script | - |

---

### Contact information  
The following material is provided by the GATK Team. Please post any questions or concerns to our forum site : [Terra](https://broadinstitute.zendesk.com/hc/en-us/community/topics/360000500432-General-Discussion) , [WDL/Cromwell](https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team).

---

### License  
**Copyright Broad Institute, 2020 | BSD-3**  
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at https://github.com/openwdl/wdl/blob/master/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.

",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/Jupyter Notebooks 101"
385,"help-gatk","GATKTutorials-Somatic-June2019","READER","https://app.terra.bio/#workspaces/help-gatk/GATKTutorials-Somatic-June2019",TRUE,FALSE,NA,NA,NA,"# What's in this workspace?
Welcome to Day 3 of the Genome Analysis Toolkit (GATK) workshop at Newcastle University in Newcastle upon Tyne, U.K.! Today we will focus on Somatic Variant Discovery.

Earlier today you received introductions to GATK tools and Best Practices pipelines. In this workspace we will be going over two forms of Somatic Analysis: one comparing tumor and normal samples using Mutect2 workflow for variant differences, and another using the Copy Number Alterations (CNA) workflow for copy number variations. 

Your instructor will guide you through this workspace, but you should find the documentation within the notebooks sufficient to run through them again on your own or to share with a friend later (and we encourage you to do so!).

### Data
Data associated with this workspace is located in the [gs://gatk-tutorials/workshop_1906/3-somatic](https://console.cloud.google.com/storage/browser/gatk-tutorials/workshop_1906/3-somatic/?project=broad-dsde-outreach&organizationId=548622027621) google bucket. It contains both input and resource files for the Mutect2 and CNA workflows. Along with the inputs are precomputed outputs generated by each step in the tutorial. This can be used as input for any step in the workflow, in case your generated outputs are not correct or you are unable to complete a step in the workflow. 

### Tools
There are no tools in this workspace. The tutorials are notebook-based, allowing you to run each step manually and view the intermediate outputs of the workflow. This is a great way to understand each step in the workflow and give you the chance to manipulate the parameters to see what happens with the output.

If you are interested in a WDL based workflow of these analyses, check out the [Showcase](https://app.terra.bio/#library/showcase) area in Terra, which features many of our popular GATK workflows in workspaces ready to run your data.


### Notebooks
 **1-somatic-mutect2-tutorial :**
In this hands-on tutorial, we will call somatic short mutations, both single nucleotide and indels, using the GATK4 Mutect2 and FilterMutectCalls. If you need a primer on what somatic calling is about, see the following [GATK forum Article](https://software.broadinstitute.org/gatk/documentation/article?id=11127).


| Runtime Environments | Value |
| --- | --- |
| CPU Minimum| 4|
| Disksize Minimum | 100 GB |
| Memory Minimum | 15 GB |
| Startup Script | gs://gatk-tutorials/scripts/install_gatk_4110_with_condaenv.sh |

**2-somatic-cna-tutorial :**
This hands-on tutorial outlines steps to detect alterations with high sensitivity in total and allelic copy ratios using GATK4's ModelSegments CNV workflow. The workflow is suitable for detecting somatic copy ratio alterations, more familiarly copy number alterations (CNAs), or copy number variants (CNVs) for whole genomes and targeted exomes.

| Runtime Environments | Value |
| --- | --- |
| CPU Minimum| 2|
| Disksize Minimum | 100 GB |
| Memory Minimum | 13 GB |
| Startup Script | gs://gatk-tutorials/scripts/install_gatk_4110_with_condaenv.sh |

 
### Software versions
GATK4.1.1.0

### Time and cost 
This workspace focuses on the use of notebooks, thus the time and cost of completing the tutorial depends on the runtime environment of your notebook and the length of time you spend actively working in the notebook. With the given runtime environments above, users can expect the cost to be much less than a dollar an hour for each notebook. The specific cost will be displayed on the top right corner of your screen, next to the notebook machine options. 

## Appendix

### GATK @ Newcastle 2019 in the Terra Support Center

Get yourself oriented with the entire workshop in the [Terra Support Center](https://broadinstitute.zendesk.com/hc/en-us/community/topics). (more on Terra Support below).

### GATK documentation and support

Detailed documentation, tutorials, and more are available at the [GATK web site](https://software.broadinstitute.org/gatk/). If you are still running into issues after consulting the User Guide, have a question, or just want to say hello, reach out to the GATK team via the [GATK Support Forum](https://gatkforums.broadinstitute.org/gatk)!

### Terra documentation and support

Documents and helpful guides to support your journey through Terra are available in the Terra Support Center. Navigate there by following the “Learn About Terra” link in the left-side menu. At the time of this workshop, we are still actively writing and publishing documents but we’ve got a good set of docs in the Quick Start Guide to get you going.


",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/GATKTutorials-Somatic-June2019"
386,"broad-firecloud-tcga","TCGA_GBM_OpenAccess_V1-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_GBM_OpenAccess_V1-0_DATA",TRUE,TRUE,"Glioblastoma multiforme","Tumor/Normal","USA","TCGA Glioblastoma multiforme Open-Access Data Workspace","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients’ clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","617","Brian","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh37/hg19","TCGA_GBM_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_GBM_OpenAccess_V1-0_DATA"
387,"broad-firecloud-tcga","TCGA_LUSC_hg38_OpenAccess_GDCDR-12-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_LUSC_hg38_OpenAccess_GDCDR-12-0_DATA",TRUE,TRUE,"Lung squamous cell carcinoma","Tumor/Normal","USA","TCGA Lung squamous cell carcinoma Open-Access Data Workspace

*hg38 TCGA and TARGET workspaces reference files by their GDC UUIDs.  In order to run analyses on the referenced data files, you will need to run workflows that retrieve the files from the GDC and copy them to your workspace bucket.  See   [this forum post](https://gatkforums.broadinstitute.org/firecloud/discussion/10382/populating-hg38-tcga-and-target-workspaces-with-data-files#latest) for instructions on the running of these workflows.*","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients' clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","504","Lung","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh38/hg38","TCGA_LUSC_hg38_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_LUSC_hg38_OpenAccess_GDCDR-12-0_DATA"
388,"anvil-datastorage","AnVIL_GTEx_public_data","READER","https://app.terra.bio/#workspaces/anvil-datastorage/AnVIL_GTEx_public_data",TRUE,TRUE,"Unspecified","TBD","","**Release notes:** Update on August 10, 2023: on August 10, 2023, the following directory was moved to here from workspace AnVIL_GTEx_V8_hg38: AnVIL_GTEx_V8_hg38/gtex_histology_svs_files/

This workspace is an archive of GTEx public data.  The primary location for the GTEx public data is on the [GTEx Portal](https://gtexportal.org).  Descriptions of the public data files can be found [here](https://gtexportal.org/home/datasets)

Please note that this workspace only contains GTEx public data.  Access to GTEx protected data requires dbGaP approval, as described [here](https://gtexportal.org/home/protectedDataAccess). 

The following data are available in the workspace-associated bucket:

* [GTEx V8 public data ](https://console.cloud.google.com/storage/browser/fc-ed391d18-3c0a-4499-a292-35ca51ebf381/gtex_analysis_v8?pageState=(%22StorageObjectListTable%22:(%22f%22:%22%255B%255D%22))&prefix=&forceOnObjectsSortingFiltering=false)
* [GTEx v9 public data](https://console.cloud.google.com/storage/browser/fc-ed391d18-3c0a-4499-a292-35ca51ebf381/gtex_analysis_v9?pageState=(%22StorageObjectListTable%22:(%22f%22:%22%255B%255D%22))&prefix=&forceOnObjectsSortingFiltering=false).  Please note that V9 does not include new RNA-seq or QTL data.
* [eGTEx data](https://console.cloud.google.com/storage/browser/fc-ed391d18-3c0a-4499-a292-35ca51ebf381/gtex_egtex?pageState=(%22StorageObjectListTable%22:(%22f%22:%22%255B%255D%22))&prefix=&forceOnObjectsSortingFiltering=false)
* [Additional datasets](https://console.cloud.google.com/storage/browser/fc-ed391d18-3c0a-4499-a292-35ca51ebf381/gtex_additional_datasets?pageState=(%22StorageObjectListTable%22:(%22f%22:%22%255B%255D%22))&prefix=&forceOnObjectsSortingFiltering=false)
* [External datasets](https://console.cloud.google.com/storage/browser/fc-ed391d18-3c0a-4499-a292-35ca51ebf381/gtex_external_datasets?pageState=(%22StorageObjectListTable%22:(%22f%22:%22%255B%255D%22))&prefix=&forceOnObjectsSortingFiltering=false)
* [Histology images](https://console.cloud.google.com/storage/browser/fc-ed391d18-3c0a-4499-a292-35ca51ebf381/gtex_histology_svs_files;tab=objects?pageState=(%22StorageObjectListTable%22:(%22f%22:%22%255B%255D%22))&prefix=&forceOnObjectsSortingFiltering=false) in [Aperio SVS format](https://openslide.org/formats/aperio/)  

The following participant-level annotations are provided in the GTEx V8 public data:
* Sex
* Age-bracket, in 10-year intervals
* Hardy Scale death classification.

Protected data available in other workspaces (with an approved dbGaP application) include:

*    BAM files for RNA-Seq, Whole Exome Seq, and Whole Genome Seq
*    Genotype Calls (.vcf) for OMNI SNP Arrays, WES, and WGS
*    OMNI SNP Array Intensity files (.idat and .gtc)
*    Affymetrix Expression Array intensity files (.cel)
*    Allele Specific Expression (ASE) tables
*    All expression matrices from the Portal, including samples that did not pass the Analysis Freeze QC
*    Sample Attributes
*    Subject Phenotypes

For documentation on analysis and data production pipelines, please see the [documentation on the GTEx Portal](https://gtexportal.org/home/documentationPage) and [https://github.com/broadinstitute/gtex-pipeline](https://github.com/broadinstitute/gtex-pipeline).  For questions about the data, please consult the [FAQs](https://gtexportal.org/home/faq), or submit new questions [here](https://gtexportal.org/home/contact).
","Kristin Ardlie","NHGRI","Common Fund (CF) Genotype-Tissue Expression Project","","","",NA,"NRES","","GTEx","TBD","anvil-datastorage/AnVIL_GTEx_public_data"
389,"broad-firecloud-tcga","TCGA_TGCT_OpenAccess_V1-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_TGCT_OpenAccess_V1-0_DATA",TRUE,TRUE,"Testicular Germ Cell Tumors","Tumor/Normal","USA","TCGA Testicular Germ Cell Tumors Open-Access Data Workspace","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients’ clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","150","Testes","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh37/hg19","TCGA_TGCT_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_TGCT_OpenAccess_V1-0_DATA"
390,"broad-firecloud-tcga","TCGA_KIRC_OpenAccess_V1-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_KIRC_OpenAccess_V1-0_DATA",TRUE,TRUE,"Kidney Renal Clear Cell Carcinoma","Tumor/Normal","USA","TCGA Kidney renal clear cell carcinoma Open-Access Data Workspace","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients’ clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","504","Kidney","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh37/hg19","TCGA_KIRC_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_KIRC_OpenAccess_V1-0_DATA"
391,"help-gatk","five-dollar-genome-analysis-pipeline","READER","https://app.terra.bio/#workspaces/help-gatk/five-dollar-genome-analysis-pipeline",TRUE,FALSE,NA,NA,NA,"### GATK Best Practices for Germline SNPs and Indels as used at the Broad Institute

A fully reproducible example of data pre-processing and germline short variant discovery. The ""$5 Genome Analysis Pipeline"" name refers to the cost of running the full pipeline (with all options turned to do the maximum amount of work) on a typical whole genome dataset, on the Google Cloud Platform, as explained on the Broad site in a [Gatk blog post](https://software.broadinstitute.org/gatk/blog?id=11415).   

A scientific description of the workflow is available in [Gatk's Best Practices Document](https://software.broadinstitute.org/gatk/best-practices/workflow?id=11145).

**Note: This workspace is superseded by the [Whole-Genome-Analysis-Pipeline](https://app.terra.bio/#workspaces/help-gatk/Whole-Genome-Analysis-Pipeline) workspace and is no longer being updated.**


-----
-----

### Germline_Single_Sample_Workflow

**What does it do?**     
This WDL pipeline implements data pre-processing and initial variant calling (GVCFgeneration) according to the GATK Best Practices for germline SNP and Indel discovery in human whole-genome sequencing data. For the latest version of the workflow please visit the git repository [five-dollar-genome-analysis-pipeline](https://github.com/gatk-workflows/five-dollar-genome-analysis-pipeline).

**What data does it require as input?**    
This workflow accespts a sample of human, whole-genome paired-end sequencing data in unmapped BAM (uBAM) format:    
- One or more read groups, one per uBAM file, all belonging to a single sample (SM)
- Input uBAM files must additionally comply with the following requirements:
- - filenames all have the same suffix (we use "".unmapped.bam"")
- - files must pass validation by ValidateSamFile
- - reads are provided in query-sorted order
- - all reads must have an RG tag
- Reference genome must be Hg38 with ALT contigs

**What does it return as output?**     
The following files are stored in the workspace Google bucket and metedata are written to the workspace data table.    
- CRAM file, CRAM index, and CRAM md5 
- GVCF and its gvcf index 
- BQSR Report
- Several Summary Metrics       

**Sample data description and location**    

Links to the expected input types are provided in the workspace data model for testing. The Pre-Germline_Single_Sample_Workflow accepts a file containing a list of unaligned bams. This workspace data model contains both a full sized and downsampled version of NA12878 of unaligned bam list file under the column flowcell_unmapped_bams_list.

* Small unaligned bam list of NA12878
* Unaligned bam list of NA12878

**Reference data description and location**     
Required and optional references and resources for the workflow are set in the workflow configurations. The reference genome for this workspace is hg38 (aka GRCh38).     

**Time and cost estimates**    

Below is an example of the time and cost for running the workflow.

| Sample Name | Sample Size | Time | Cost $ |
| ---  | :---: | :---: | :---: |
| NA12878_24RG_small | 3.11 GB | 2:35:00 | 0.66 |
| NA12878 | 64.89 GB | 21:12:00 | 4.15 |

**Note:** Cost and time will vary with the use of [Preemptibles](https://gatkforums.broadinstitute.org/firecloud/discussion/11786/preemptibles#latest).  
Users can also use [Google's BigQuery](https://software.broadinstitute.org/firecloud/documentation/article?id=11788) for task level calculation.     
For more information about controlling Cloud costs, see [this article](https://support.terra.bio/hc/en-us/articles/360029748111).


**Software Version**      
- GATK 4 or later
- BWA 0.7.15-r1140
- Picard 2.16.0-SNAPSHOT
- Samtools 1.3.1 (using htslib 1.3.1)
- Python 2.7

---


---

### Contact information  
The following material is provided by the GATK Team. Please post any questions or concerns to one of our forum sites : [GATK](https://gatkforums.broadinstitute.org/gatk/categories/ask-the-team/) ,  [Terra](https://broadinstitute.zendesk.com/hc/en-us/community/topics/360000500432-General-Discussion) , or [WDL/Cromwell](https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team).

---
### License  
**Copyright Broad Institute, 2019 | BSD-3**  
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at https://github.com/openwdl/wdl/blob/master/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/five-dollar-genome-analysis-pipeline"
393,"help-gatk","Pre-processing_hg38_v1","READER","https://app.terra.bio/#workspaces/help-gatk/Pre-processing_hg38_v1",TRUE,TRUE,NA,NA,NA,"### GATK Best Practices for data pre-processing (hg38 reference)
The purpose of this workspace is to provide a fully reproducible example of the GATK Best Practices workflow for pre-processing data (intended for downstream variant discovery analysis). 

#### Workspace attributes
All required and optional resources for the preconfigured methods included. The reference genome is hg38 (aka GRCh38).

#### Data 
Two sets of 24 unmapped BAMs of NA12878 WGS data (one per read group) downsampled to 20% (med) and 5% (small), respectively. 

#### Method configs
This workspace contains the following preset method configurations:

- **PreProcessingForVariantDiscovery_GATK4**
Generic version of the pre-processing portion (read group uBAMs to analysis-ready BAM) of the single-sample pipeline used in production at the Broad Institute, running GATK4 according to GATK Best Practices (June 2016). Usage instructions: launch this on any qualifying Sample or Set of Samples (each sample must reference a list of unmapped BAMs, one per read group). See workflow for additional input requirements and version notes. IMPORTANT CAVEAT: while this workflow is in principle applicable for somatic variant detection, some of its hardcoded parameters (the base quality score binning strategy) have not yet been fully vetted for somatic analysis. We intend to update the method to make those parameters customizable and produce separae method configs for the two use cases. ",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/Pre-processing_hg38_v1"
394,"broad-firecloud-tcga","TCGA_KIRC_hg38_OpenAccess_GDCDR-12-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_KIRC_hg38_OpenAccess_GDCDR-12-0_DATA",TRUE,TRUE,"Kidney Renal Clear Cell Carcinoma","Tumor/Normal","USA","TCGA Kidney renal clear cell carcinoma Open-Access Data Workspace

*hg38 TCGA and TARGET workspaces reference files by their GDC UUIDs.  In order to run analyses on the referenced data files, you will need to run workflows that retrieve the files from the GDC and copy them to your workspace bucket.  See   [this forum post](https://gatkforums.broadinstitute.org/firecloud/discussion/10382/populating-hg38-tcga-and-target-workspaces-with-data-files#latest) for instructions on the running of these workflows.*","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients' clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","504","Kidney","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh38/hg38","TCGA_KIRC_hg38_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_KIRC_hg38_OpenAccess_GDCDR-12-0_DATA"
395,"broad-firecloud-tcga","TCGA_KICH_OpenAccess_V1-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_KICH_OpenAccess_V1-0_DATA",TRUE,TRUE,"Kidney Chromophobe","Tumor/Normal","USA","TCGA Kidney Chromophobe Open-Access Data Workspace","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients’ clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","113","Kidney","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh37/hg19","TCGA_KICH_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_KICH_OpenAccess_V1-0_DATA"
396,"brain-initiative-bcdc","scATAC","READER","https://app.terra.bio/#workspaces/brain-initiative-bcdc/scATAC",TRUE,FALSE,NA,NA,NA,"## Deprecation notice 9/12/2024
We are deprecating the scATAC pipeline. For an alternative workflow, please see the [ATAC Pipeline from Dockstore](https://dockstore.org/workflows/github.com/broadinstitute/warp/atac:atac_v2.3.0?tab=versions). 

# scATAC: Single Cell ATAC-seq Analysis Pipeline


The scATAC Pipeline (previously referred to as snap-atac) was developed by the Broad Pipelines team to process single cell/nucleus ATAC-seq datasets. The pipeline is based on the [SnapATAC pipeline](https://github.com/r3fang/SnapATAC) described by [Fang et al. (2019)](https://www.biorxiv.org/content/10.1101/615179v2.full). Overall, the pipeline uses the python module [SnapTools](https://github.com/r3fang/SnapTools) to align and process paired reads in the form of FASTQ files. It produces an hdf5-structured Snap file that includes a cell-by-bin count matrix. In addition to the Snap file, the final outputs include a GA4GH compliant aligned BAM and QC metrics.

This workspace describes the pipeline and provides a fully reproducible example of the workflow, which is described in detail in the [scATAC Overview](https://broadinstitute.github.io/warp/docs/Pipelines/Single_Cell_ATAC_Seq_Pipeline/README) in the [WARP documentation](https://broadinstitute.github.io/warp/). 

We'd like to acknowledge and thank Rongxin Fang and Yang Li at Ludwig Cancer Research, Bing Ren at UC San Diego, and Sebastian Preissl at the Center for Epigenomics, UC San Diego for their work on this pipeline.

---

## Sample Data
### Test Data
Links to the expected input types are available in the workspace data model for testing. The scATAC v1.3.1 workflow accepts an R1_fastq.gz and an R2_fastq.gz. This workspace data model contains a demo sized R1 and R2 FASTQ file under the columns `fastq1` and `fastq2`, respectively.      

### Workspace Data
The reference genome for this workspace is mm10.  A pre-built human reference can be found in the Files section of this workspace or at `'gs://fc-f41be401-ca1c-4cbc-8f19-d326a0f7aa97/input_files/mm10.tar`.

## Tools 
scATAC.wdl (v1.3.1):  This WDL pipeline takes sequencing data in FASTQ format (R1 and R2) and outputs Snap files.  The pipeline is composed of five steps:

| step name        | step description                                                                         |
|------------------|------------------------------------------------------------------------------------------|
| AlignPairEnd     | Align the FASTQ files to the genome                                                      |
| SnapPre          | Initial generation of Snap file                                                          |
| SnapCellByBin    | Binning of data by genomic bins                                                          |
| MakeCompliantBAM | Generation of a GA4GH compliant BAM                                                      |
| BreakoutSnap     | Extraction of tables from Snap file into text format (for testing and user availability) |

## Requirements/Expectations

**Inputs**
The scATAC_v1.3.1 workflow accepts two FASTQ files and a reference file. The input data are samples:

* Pair-end sequencing data in FASTQ format - R1 and R2 fastq files.
* A reference bundle file

The pipeline accepts paired reads in the form of FASTQ files. The current version of the pipeline requires that cellular barcodes for reads be appended to the FASTQ read names (for both R1 and R2 FASTQ files). The following is an example of the format of the expected input. The full cell barcode must form the first part of the read name (for both R1 and R2 files) and be separated from the rest of the line by a colon.

```
@CAGTTGCACGTATAGAACAAGGATAGGATAAC:7001113:915:HJ535BCX2:1:1106:1139:1926 1:N:0:0
ACCCTCCGTGTGCCAGGAGATACCATGAATATGCCATAGAACCTGTCTCT
+
DDDDDIIIIIIIIIIIIIIHHIIIIIIIIIIIIIIIIIIIIIIIIIIIII
```

The following table outlines the inputs to the pipeline in more detail.

| input name       | input type    | description                                                                |
|------------------|---------------|----------------------------------------------------------------------------|
| input_fastq1     | File          | FASTQ file of the first reads (R1)                                         |
| input_fastq2     | File          | FASTQ file of the second reads (R2)                                        |
| input_id | String | A unique identifier for the sample that will be used name the output files |
| genome_name      | String        | name of the genomic reference used                                         |
| input_reference  | File          | reference bundle that is generated with bwa-mk-index-wdl                   |
| output_bam       | String        | name of the output bam to generate                                         |
| bin_size_list | String | optional list of bin sizes used to generate the cell-by-bin matrix; default size is 10000 (bp) |

`input_reference` is an input reference bundle that has been generated with the bwa-mk-index-wdl accessory pipeline. This accessory pipeline can be found [here](https://github.com/HumanCellAtlas/skylab/tree/master/library/accessory_workflows/build_bwa_reference).

If your sequencing data is not in FASTQ format, check out this file conversion workspace, [https://app.terra.bio/#workspaces/help-gatk/Sequence-Format-Conversion](https://app.terra.bio/#workspaces/help-gatk/Sequence-Format-Conversion) for prepared workflows to assist in file conversion.

**Outputs**  

The pipeline outputs a Snap file that contains all the output information as well as snap_qc  file containing quality control metrics. Additionally, the pipeline extracts information from the Snap file and outputs it in text  files for easier viewing. The pipeline also returns a GA4GH compliant BAM file, where the cell identifiers are extracted from the read names and into the CB tags.

For more details on working with Snap files please refer to the [SnapATAC pipeline documentation] (external link)](https://github.com/r3fang/SnapTools). The format of the SNAP file, as well as what the different section contains, is described in more detail [here (external link)](https://github.com/r3fang/SnapTools) and [here](https://github.com/r3fang/SnapTools/blob/master/docs/snap_format.docx). 

The pipeline outputs the following files:

| output file name              | description            |
|-------------------------------|------------------------|
| output_snap_qc                | Quality control metrics corresponding to the Snap file |
| output_snap                   | Output Snap file (in hdf5 container format) |
| output_aligned_bam            | Output BAM file, compliant with GA4GH |
| breakout_barcodes             | Text file containing the 'Fragments session' barcodeLen, barcodePos fields           |
| breakout_fragments            | Text file containing the 'Fragments session' fragChrom, fragLen and fragStart fields |
| breakout_binCoordinates       | Text file with the AM section ('Cell x bin accessibility' matrix), binChrom and binStart fields |
| breakout_binCounts            | Text file with the AM section ('Cell x bin accessibility' matrix), idx, idy and count fields |
| breakout_barcodesSection      | Text file with the data from the BD section ('Barcode session' table) |
| output_pipeline_version | Version of the processing pipeline |

The output bins for summarization of the data are of size 10 kb in the Snap file.
 
## Workflow
The source code for this workflow is found on GitHub in the [WARP Repository](https://github.com/broadinstitute/warp/tree/master/pipelines/skylab/scATAC). The pipeline is tagged with versioned releases, and this workspace will be updated periodically as the pipeline updates.

---

## Time/Cost Analysis

| Sample Name | R1.fastq Size | R2.fastq Size | Time | Cost $ |
| :---:  | :---: | :---: | :---: | :---: |
| demo1 | 20.47 MB | 21.54 MB |0:03:00 | 0.10 |   

**For helpful hints on controlling Cloud costs**, see this article ([click here to view](https://support.terra.bio/hc/en-us/articles/360029748111)).       

---
## Versions
All versions of the pipeline are detailed in the [scATAC changelog](https://github.com/broadinstitute/warp/blob/master/pipelines/skylab/scATAC/scATAC.changelog.md).

Updates related to this workspace are detailed in the table below:

| scATAC Release Version | Date | Release Note | 
| --- | --- | --- |
| scATAC v1.3.1 (current version) | 6/2023 | Added `output_pipeline_version` as workflow output |
| scATAC v1.2.1 | 4/2022 | Updated breakoutSnap.py to use python3 instead of python2 |
| scATAC v1.2.0 | 1/2021 | Added input_id as workflow input; updated the example R1 and R2 FASTQ files  |
| scATAC v1.1.0 | 8/2020 | Added bin_size_list as an optional input to the scATAC workflow with a default of 10,000 bp |

---
## Contact Information

* For workspace questions and feedback, email the Broad pipelines team at warp-pipelines-help@broadinstitute.org or create a post on the [Featured Workspaces community forum](https://support.terra.bio/hc/en-us/community/topics/360001603491) (login required). Tag @Alexander Baumann in the **Details** section of your post.

* You can also Contact the Terra team for questions from the Terra main menu. When submitting a request, it can be helpful to include:
    * Your Project ID
    * Your workspace name
    * Your Bucket ID, Submission ID, and Workflow ID
    * Any relevant log information 

---

## License  

**Copyright Broad Institute, 2023 | BSD-3**  
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at https://github.com/broadinstitute/warp/blob/master/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","brain-initiative-bcdc/scATAC"
397,"verily-terra-solutions","analysis-of-felis-catus-behavior","READER","https://app.terra.bio/#workspaces/verily-terra-solutions/analysis-of-felis-catus-behavior",TRUE,TRUE,NA,NA,NA,"# Impactful research demonstrates new plotting capabilities

These new plotting capabilities will be available on this platform on April 1, 2022.
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","verily-terra-solutions/analysis-of-felis-catus-behavior"
398,"help-gatk","GATKTutorials-Pipelining-May2019","READER","https://app.terra.bio/#workspaces/help-gatk/GATKTutorials-Pipelining-May2019",TRUE,FALSE,NA,NA,NA,"# What's in this workspace?
Welcome to Day 4 of the Genome Analysis Toolkit (GATK) workshop at the Haartman Institute in Helsinki, Finland! 

Earlier today, you learned about WDL and Cromwell, and using this empty workspace, we will practice starting a workspace from scratch. Your instructor will guide you through this workspace, but you should find the documentation on this page sufficient to run through them again on your own or to share with a friend later (and we encourage you to do so!).

### Data
You will need to download the data bundle located [here](broad.io/GATK1905) if you haven't already. This bundle contains several folders but the one you will need for this workspace is labelled **Terra**. It provides the WDL script, workspace and sample metadata, and an inputs file to run.

### Tools
There are no tools in this workspace yet. But we will be putting a tool in that runs GATK's HaplotypeCaller, using the instructions below.

**Add a new Tool**
1. In a new tab, navigate to Code & Tools. On the right hand side of the page, click on the `Broad Methods Repository` link. This currently will take you to our legacy application to upload a new Method, which is another name for Tool. In the near future, you will be able to upload your new tool directly in Terra. 
2. Click the blue `Create New Method` button in the upper right corner.
3. Fill out the window that pops up as follows:
>**Namespace:** your own name or abbreviation of your name. 

The namespace is essentially the publisher of the method. You can publish it as yourself, as we are doing here, or sometimes you may want to publish it under your lab or institution's name. 
>**Name:** `HelloGATK`

This refers to the name of your tool. You can call it whatever you want, but we've chosen to match it to the WDL script we will be using. Lastly:
>**WDL:** Click the blue `load from file...` link, and load the `hello_gatk_terra.wdl` script from your data bundle, under the Terra folder

4. Click `Upload`
5. Click the blue `Export to Workspace` button in the upper right corner, then in the dialog that pops up, select `Use Blank Configuration`
6. Under `Destination Workspace`, select your clone of this workspace, then click `Export to Workspace`
7. The page will ask you `Go to edit page now?` Click Yes, and you will be redirected back to your workspace!

**Add Data**

For our tutorial purposes, we've already uploaded your sample files to a public google bucket so that you do not need to incur storage costs. If you did want to upload your own samples to run this workflow on later, you can do so by navigating to the Data tab in your workspace, and clicking on the `Files` section in the lefthand menu. From there, you can upload whatever files you need.

**Run your new Tool!**
1. Navigate to the `Tools` tab in your workspace
2. Click on the HelloGATK tool you recently added.
3. Drag the `hello_gatk_fc.inputs.json` file from your Terra folder in the data bundle to upload it and populate the inputs section. You'll notice there are a number of `gs://` links, which represent files stored in google buckets. 
4. Click the green `Save` button to save the inputs you just uploaded
5. At the top of the page, select the radio button next to `Process single workflow from files`
6. Click the green `Run Analysis` button

**Optional: Parameterize your inputs**

You don't want to go in and change these `gs://` links for each individual sample you want to run on, especially if you're running on a large number of samples. You can run your workflow on many samples with a single click by setting up your Data tables.
1. Go to the `Data` tab in your workspace
2. Upload `participant.tsv` and `sample.tsv` from the Terra folder in your data bundle by clicking the `+` icon next to Tables in the lefthand menu
3. Click on the Workspace Data section in the lefthand menu, and upload `workspace_attributes.tsv` from the Terra folder. 
4. Navigate back to the `Tools` tab. 
5. Select the radio button for `Process multiple workflows from` and choose `Sample` from the dropdown. 
6. In the inputs table at the bottom, we now need to replace the `gs://` links with references to the data table. For the `inputBAM` and `bamIndex`, try typing `this` and see what Terra auto-fills for you as options. The keyword `this` refers to whatever data table you selected in the dropdown at the top. Since we picked `Sample`, it fills in options that exist in your `Sample` table. Click `Save` when you are done.
7. Click the green `Run Analysis` button

### Software versions
GATK4.1.1.0

## Appendix

### GATK @ BroadE 2019 in the Terra Support Center

Get yourself oriented with the entire workshop in the [Terra Support Center](https://broadinstitute.zendesk.com/hc/en-us/community/topics). (more on Terra Support below).

### GATK documentation and support

Detailed documentation, tutorials, and more are available at the [GATK web site](https://software.broadinstitute.org/gatk/). If you are still running into issues after consulting the User Guide, have a question, or just want to say hello, reach out to the GATK team via the [GATK Support Forum](https://gatkforums.broadinstitute.org/gatk)!

### Terra documentation and support

Documents and helpful guides to support your journey through Terra are available in the Terra Support Center. Navigate there by following the “Learn About Terra” link in the left-side menu. At the time of this workshop, we are still actively writing and publishing documents but we’ve got a good set of docs in the Quick Start Guide to get you going.


",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/GATKTutorials-Pipelining-May2019"
399,"help-gatk","GATKTutorials-Pipelining","READER","https://app.terra.bio/#workspaces/help-gatk/GATKTutorials-Pipelining",TRUE,TRUE,NA,NA,NA,"# What's in this workspace?
Welcome to Day 4 of the Genome Analysis Toolkit (GATK) workshop! Earlier today, you learned about WDL and Cromwell. Using this empty workspace, we will practice starting a workspace from scratch. Your instructor will guide you through this workspace, but you should find the documentation on this page sufficient to run through them again on your own or to share with a friend later (and we encourage you to do so!).

*This workspace was originally developed for the Genome Analysis Toolkit (GATK) workshop in San Jose, Costa Rica in February 2020. Feel free to run it on your own, even if you were unable to attend that workshop!*

## Data
You will need to download the data bundle located [here](https://broad.io/GATK2002) if you haven't already. This bundle contains several folders but the one you will need for this workspace is labelled **Terra**. It provides the WDL script, workspace and sample metadata, and an inputs file to run.

## Workflows
There are no workflows in this workspace yet. But we will be putting a workflow in that runs GATK's HaplotypeCaller, using the instructions below.

### Add a new Workflow
1. Open a new tab in Terra. Click on the hamburger menu in the upper left of the page, and navigate to Workflows, which will be under the Library menu. 
2. On the right hand side of the page, click on the `Broad Methods Repository` link. This currently will take you to our legacy application to upload a new Method, which what our legacy application called Workflows. In the near future, you will be able to upload your new workflow directly in Terra. 
3. Click the blue `Create New Method` button in the upper right corner.
4. Fill out the window that pops up as follows:

**Namespace:** your own name or abbreviation of your name. 
The namespace is essentially the publisher of the method. You can publish it as yourself, as we are doing here, or sometimes you may want to publish it under your lab or institution's name. 

**Name:** `HelloGATK`
This refers to the name of your workflow. You can call it whatever you want, but we've chosen to match it to the WDL script we will be using. Lastly:

**WDL:** Click the blue `load from file...` link, and load the `hello_gatk_terra.wdl` script from your data bundle, under the Terra folder

5. Click `Upload`
6. Click the blue `Export to Workspace` button in the upper right corner, then in the dialog that pops up, select `Use Blank Configuration`
7. Under `Destination Workspace`, select your clone of this workspace, then click `Export to Workspace`
8. The page will ask you `Go to edit page now?` Click Yes, and you will be redirected back to your workspace!

### Add Data
For our tutorial purposes, we've already uploaded your sample files to a public google bucket so that you do not need to incur storage costs. If you did want to upload your own samples to run this workflow on later, you can do so by navigating to the Data tab in your workspace, and clicking on the `Files` section in the lefthand menu. The _Optional: Parameterize your inputs_ section below will walk you through the process of adding data with some premade files, which can be adapted to include your own samples.

### Run your new Workflow!
1. Navigate to the `Workflows` tab in your workspace
2. Click on the HelloGATK workflow you recently added.
3. Drag the `hello_gatk_terra.inputs.json` file from your Terra folder in the data bundle to upload it and populate the inputs section. You'll notice there are a number of `gs://` links, which represent files stored in google buckets. 
4. Click the blue `Save` button to save the inputs you just uploaded
5. At the top of the page, select the radio button next to `Process single workflow from files`, then hit `Save` again.
6. Click the blue `Run Analysis` button, then click `Launch` in the dialog box that pops up.

### Optional: Parameterize your inputs
When you run a workflow regularly, it is helpful to set up your Data tables to manage your files. For example, you don't want to go in and change these `gs://` links for each individual sample you want to run on, especially if you're running on a large number of samples. You can run your workflow on many samples with a single click by setting up your Data tables.

1. Go to the `Data` tab in your workspace
2. Upload `participant.tsv` and `sample.tsv` from the Terra folder in your data bundle by clicking the `+` icon next to Tables in the lefthand menu. You will upload the participant file first, and check the box for `Create participant, sample, and pair associations`. Terra allows you to upload whatever tables you'd like, but it does have some built in cross-referencing between these three table types. Thus, when you tick that checkbox, you will tell Terra to connect your participant and sample tables.
3. Click on the Workspace Data section in the lefthand menu, and upload `workspace_attributes.tsv` from the Terra folder. 
4. Navigate back to the `Workflows` tab and select the HelloGATK workflow. 
5. Select the radio button for `Process multiple workflows from` and choose `Sample` from the dropdown. 
6. In the inputs table at the bottom, we now need to replace the `gs://` links with references to the data table. For the `inputBAM` and `bamIndex`, try typing `this` and see what Terra auto-fills for you as options. The keyword `this` refers to whatever data table you selected in the dropdown at the top. Since we picked `Sample`, it fills in options that exist in your `Sample` table. Click `Save` when you are done.
7. Click the blue `Run Analysis` button, then click `Launch` in the dialog box that pops up.

### Software versions
GATK4.1.4.1

## Appendix

### GATK @ Costa Rica 2020 in the Terra Support Center

Following the conclusion of the workshop, the workshop materials will be made available in the Terra Support Center. For now, you can find these materials in a google drive folder at [broad.io/GATK2002](https://broad.io/GATK2002)

### GATK documentation and support

Detailed documentation, tutorials, and more are available at the [GATK web site](https://gatk.broadinstitute.org/hc/en-us). If you are still running into issues after consulting the User Guide, have a question, or just want to say hello, reach out to the GATK team via the [GATK Support Forum](https://gatk.broadinstitute.org/hc/en-us/community/topics)!

### Terra documentation and support

Documents and helpful guides to support your journey through Terra are available in the Terra Support Center. Navigate there by following the “Learn About Terra” link in the left-side menu. At the time of this workshop, we are still actively writing and publishing documents but we’ve got a good set of docs in the Quick Start Guide to get you going.

### Workspace Citation 
Details on citing Terra workspaces can be found here: [How to cite Terra](https://support.terra.bio/hc/en-us/articles/360035343652)

Data Sciences Platform, Broad Institute (*Year, Month Day that this workspace was last modified*) help-gatk/GATKTutorials-Pipelining [workspace] Retrieved *Month Day, Year that workspace was retrieved*,  https://app.terra.bio/#workspaces/help-gatk/GATKTutorials-Pipelining
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/GATKTutorials-Pipelining"
400,"broad-firecloud-tcga","CCLE_hg19_GDCLegacy_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/CCLE_hg19_GDCLegacy_DATA",TRUE,TRUE,"cancer","Cell line",NA,"The [Cancer Cell Line Encyclopedia](https://portals.broadinstitute.org/ccle) (CCLE) is a collaboration between the Broad Institute, the Novartis Institutes for Biomedical Research, and the Genomics Institute of the Novartis Research Foundation. CCLE researchers conduct detailed genetic and pharmacologic characterizations of a large panel of human cancer models, develop integrated computational analyses that link distinct pharmacologic vulnerabilities to genomic patterns, and translate cell line integrative genomics into cancer patient stratification.  The data in this workspace is drawn from the GDC's Legacy archive.","NCI","Chet Birger","The Cancer Cell Line Encyclopedia (CCLE) project is a collaboration between the Broad Institute, and the Novartis Institutes for Biomedical Research and its Genomics Institute of the Novartis Research Foundation to conduct a detailed genetic and pharmacologic characterization of a large panel of human cancer models, to develop integrated computational analyses that link distinct pharmacologic vulnerabilities to genomic patterns and to translate cell line integrative genomics into cancer patient stratification. The CCLE provides public access to genomic data, analysis and visualization for about 1000 cell lines.","948",NA,"CCLE",NA,"GRU","GRCh37/hg19","Broad-Novartis Cancer Cell Line Encyclopedia","Whole Exome;RNA-Seq","broad-firecloud-tcga/CCLE_hg19_GDCLegacy_DATA"
401,"broad-firecloud-tcga","TCGA_COAD_hg38_OpenAccess_GDCDR-12-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_COAD_hg38_OpenAccess_GDCDR-12-0_DATA",TRUE,TRUE,"Colon adenocarcinoma","Tumor/Normal","USA","TCGA Colon adenocarcinoma Open-Access Data Workspace

*hg38 TCGA and TARGET workspaces reference files by their GDC UUIDs.  In order to run analyses on the referenced data files, you will need to run workflows that retrieve the files from the GDC and copy them to your workspace bucket.  See   [this forum post](https://gatkforums.broadinstitute.org/firecloud/discussion/10382/populating-hg38-tcga-and-target-workspaces-with-data-files#latest) for instructions on the running of these workflows.*","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients' clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","460","Colon","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh38/hg38","TCGA_COAD_hg38_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_COAD_hg38_OpenAccess_GDCDR-12-0_DATA"
402,"broad-firecloud-tcga","TCGA_PCPG_hg38_OpenAccess_GDCDR-12-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_PCPG_hg38_OpenAccess_GDCDR-12-0_DATA",TRUE,TRUE,"Pheochromocytoma and Paraganglioma","Tumor/Normal","USA","TCGA Pheochromocytoma and Paraganglioma Open-Access Data Workspace

*hg38 TCGA and TARGET workspaces reference files by their GDC UUIDs.  In order to run analyses on the referenced data files, you will need to run workflows that retrieve the files from the GDC and copy them to your workspace bucket.  See   [this forum post](https://gatkforums.broadinstitute.org/firecloud/discussion/10382/populating-hg38-tcga-and-target-workspaces-with-data-files#latest) for instructions on the running of these workflows.*","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients' clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","179","Adrenal gland","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh38/hg38","TCGA_PCPG_hg38_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_PCPG_hg38_OpenAccess_GDCDR-12-0_DATA"
403,"rare-x_gg","RAREX_Researcher_Dashboard","READER","https://app.terra.bio/#workspaces/rare-x_gg/RAREX_Researcher_Dashboard",TRUE,FALSE,NA,NA,NA,"RARE-X believes….
* Patients’ data is critical to rare disease innovation — but does little to help advance progress if it is not inclusive and widely accessible for research.
* Researchers need access to more and better data to transform rare disease drug development. Providing a way to break down the silos of these data sets is essential to progress.
* Rare patient communities want their data shared, securely but openly — where it can drive better and faster progress.
* Meaningful, equitable advancement in rare disease innovation requires global collaboration and sharing of data.
* If patients are empowered and become partners in the data ownership process, it benefits all involved in rare disease diagnosis, research, discovery, drug development, and treatment.
* Patients’ data should only be shared with their consent and for their primary benefit. RARE-X does not sell patients’ data.

Below is a snapshot of the available data via RARE-X Data Access program.  
To request access to the full data please see [HERE][1]

[1]: <https://forms.monday.com/forms/02fc5ad2b4b1a75e1d3520b913d73ecb?r=use1>


![whitespace](https://storage.cloud.google.com/terra-featured-workspaces/RARE-X/Dashboard_Survey_Completion_06262024.png)
![whitespace](https://storage.cloud.google.com/terra-featured-workspaces/RARE-X/Monthly_DCP_Enrollment_06262024.png)
![whitespace](https://storage.cloud.google.com/terra-featured-workspaces/RARE-X/Diagnosis_And_Counts_01302024.png)",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","rare-x_gg/RAREX_Researcher_Dashboard"
405,"broad-epigenomics-prod","pbs-analysis","READER","https://app.terra.bio/#workspaces/broad-epigenomics-prod/pbs-analysis",TRUE,FALSE,NA,NA,NA,"## Overview
This workspace contains a workflow for binning, estimating the background distribution, and calculating the probability of being signal (PBS) for ChIP-seq and related datatypes.  

### Overall inputs:
- BAM filename
- Genome assembly
- Bin size (minimum 5 kB)

### Overall outputs:
- BED file with binned reads that are adjusted for mappability and copy number variations.
- BED file with  a value at each bin indicating the ploidy, and which will be used in subsequently rescaling epitope files. (Input control only)
- Parameters file with estimated values defining a gamma distribution describing the background of the bed file.  This file can be used to calculate per-bin p-values relative to the background distribution.
- BED file with probability of being signal (PBS) values for each bin.

## Loading Data Tables
Uploading a TSV to model your data is crucial to using Terra. You can learn more about the process through these helpful articles: [I](https://support.terra.bio/hc/en-us/articles/360025758392-Managing-data-with-tables-), [II](https://support.terra.bio/hc/en-us/articles/360051043031-Overview-How-to-add-a-Table-to-a-Terra-workspace), [III](https://support.terra.bio/hc/en-us/articles/360033913771-Understanding-Entity-Types), [IV](https://support.terra.bio/hc/en-us/articles/4416217042203)

### Basic Example
All tables require a column of unique identifiers. The header for this column must begin with ""entity:"" followed by the table name (in this case: ""file"") followed by ""\_id"". To run this workflow in Terra, you will also need to provide a Google Cloud Platform path to a ChIP-seq BAM file. Other metadata columns such as the reference genome and the assay target are useful for keeping track of the data. All columns except for the first may be named however you see fit. (**N.B. without a matching input control, the workflow will not be able to detect CNVs and therefore will skip rescaling by CNVs**).

![Example Sample TSV](https://drive.google.com/u/0/uc?id=1-r-hNB7i-vihUyQu62LcQ09JyFKMqekT&export=download)\
_Example file TSV with ENCODE data_

### Using Matching Input Controls
A slightly more complicated use case involves adding another table for matching input controls. Terra supports linked ""participant"" and ""sample"" tables (these are reserved keywords). Adding a second table reduces any redundancies in the case that multiple experiments share an input control.  In those cases, it is much faster to detect CNVs once given that the process can be time-intensive. This table is very similar to the ""file"" table above except that the first column header needs to be ""entity:participant_id"" for the other references to work.\
![Example Participant TSV](https://drive.google.com/u/0/uc?id=1-sct5WGr58VYbwE-Ts29lBG5TACJ9xir&export=download)\
_Example participant TSV with ENCODE data_

The original ""file"" table will also need to be slightly modified. The first header must be ""entity:sample_id"" and an additional ""participant"" column must be added containing the identifiers that are used in the ""participant"" table. Thus the two tables will be linked.\
![Example Sample TSV with Participant Column](https://drive.google.com/u/0/uc?id=1-nCvVlykaJ_pI9HZ5cqHsBQC7QzLenFZ&export=download)\
_Example sample TSV with ENCODE data_

**When uploading the tables, you must first upload the ""participant"" table then the ""sample"" table. Make sure to check ""Create participant, sample, and pair associations"" in the Import Table Data form when uploading the ""sample"" table**

## Running the Workflow
![Example Workflow](https://drive.google.com/u/0/uc?id=1-tKX80t4Wm1Gz2AgZuUqu2WbYOeE_1Lb&export=download)\
_Example workflow parameters using linked participant/sample tables_

## Notes
### ENCODE Data
The data in this workspace comes from ENCODE which is hosted here on the Terra [Data Explorer](https://broad-gdr-encode.appspot.com/), in addition to the ENCODE platform.  All the files came from ChIP-seq experiments aligned to hg19. The control experiments had a ""Target of Assay"" equal to ""Control"". Other experiments targeted common histone marks: H3K27ac, H3K27me3, H3K36me3, HeK4me1, H3K4me2, H3K4me3, H3K79me2, H3K9ac, H3K9me3.",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","broad-epigenomics-prod/pbs-analysis"
406,"BRAIN-RF1-billing-project","BRAIN-integrative-analysis","READER","https://app.terra.bio/#workspaces/BRAIN-RF1-billing-project/BRAIN-integrative-analysis",TRUE,FALSE,NA,NA,NA,"**Note: This workspace is continuously evolving.**

### Introduction
Computing platforms are largely modality-specific, serving scientists by specializing in certain sub-domains. To enable research across the sub-domains, we aim to develop a data ecosystem that allows integration of data of multiple modalities from multiple sources. The BRAIN Initiative has established archives well positioned to house and maintain specific data types. In this project sponsored by the BRAIN Initiative ([RF1MH133777](https://reporter.nih.gov/search/ZN2DaV-aVEKM9-dekSriOg/project-details/10725550)), we establish access to single neuron morphology housed at the [Brain Image Library (BIL)](https://api.brainimagelibrary.org/web/index.html) and electrophysiology data from the [DANDI Archive](https://dandiarchive.org/), in addition to Terra's existing capability of data handoff from [NeMO Archive](https://nemoarchive.org/).

This workspace contains Jupyter Notebooks for streaming data from the federated BRAIN Initiative data repositories, including the [DANDI Archive](https://dandiarchive.org/) and the [Brain Image Library (BIL)](https://api.brainimagelibrary.org/web/index.html).

### Jupyter Notebook: BIL_get_samples.ipynb
This notebook downloads electrophysiology data from the Brain Image Library (BIL) for a given sample or samples and writes the sample ID and file path to the data model.

Required user inputs:
* BIL ID for the study
* Sample ID (filename in the manifest without extension)

How to run this notebook:
1. Open this notebook in your clone of the workspace and create a cloud environment using the Default image.
2. Run all the cells in the *Notebook Setup* section.
3. Run `get_bil()` in the *Get BIL Sample(s)* section and input the BIL ID and Sample ID(s) when prompted.

```
get_bil()
Enter the BIL ID, e.g. ace-aim-now: ace-aim-now
Enter one sample ID or comma-separated list of multiple sample IDs: 20190830_sample_2, 20180501_sample_5
```

### Jupyter Notebook: DANDI_get_samples.ipynb
This notebook queries the DANDI Archive and gets the file paths for a given cell's morphology data and writes the sample IDs and file paths to the data model.

Required user inputs:
* Dandiset ID
* Version
* Subject ID(s)

How to run this notebook:
1. Open this notebook in your clone of the workspace and create a cloud environment using the Default image.
2. Run all the cells in the *Notebook Setup* section.
3. Run `get_dandi()` in the *Get DANDI Sample(s)* section and input the Dandiset ID, Version, and Subject ID(s) when prompted.

```
get_dandi()
Enter the Dandiset ID: 000008
Enter version: 0.211014.0809
Enter one or multiple subject IDs: sub-mouse-CAASK, sub-mouse-CSHED
```

### Jupter Notebook: DANDI_functions.ipynb
This notebook contains functions that are imported into the DANDI_get_samples.ipynb notebook. ",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","BRAIN-RF1-billing-project/BRAIN-integrative-analysis"
407,"broad-firecloud-tcga","TCGA_UVM_hg38_OpenAccess_GDCDR-12-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_UVM_hg38_OpenAccess_GDCDR-12-0_DATA",TRUE,TRUE,"Uveal Melanoma","Tumor/Normal","USA","TCGA Uveal Melanoma Open-Access Data Workspace

*hg38 TCGA and TARGET workspaces reference files by their GDC UUIDs.  In order to run analyses on the referenced data files, you will need to run workflows that retrieve the files from the GDC and copy them to your workspace bucket.  See   [this forum post](https://gatkforums.broadinstitute.org/firecloud/discussion/10382/populating-hg38-tcga-and-target-workspaces-with-data-files#latest) for instructions on the running of these workflows.*","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients' clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","80","Eye","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh38/hg38","TCGA_UVM_hg38_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_UVM_hg38_OpenAccess_GDCDR-12-0_DATA"
408,"broad-firecloud-tcga","TCGA_BLCA_hg38_OpenAccess_GDCDR-12-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_BLCA_hg38_OpenAccess_GDCDR-12-0_DATA",TRUE,TRUE,"Bladder Urothelial Carcinoma","Tumor/Normal","USA","TCGA Bladder Urothelial Carcinoma Open-Access Data Workspace

*hg38 TCGA and TARGET workspaces reference files by their GDC UUIDs.  In order to run analyses on the referenced data files, you will need to run workflows that retrieve the files from the GDC and copy them to your workspace bucket.  See   [this forum post](https://gatkforums.broadinstitute.org/firecloud/discussion/10382/populating-hg38-tcga-and-target-workspaces-with-data-files#latest) for instructions on the running of these workflows.*","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients' clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","412","Bladder ","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh38/hg38","TCGA_BLCA_hg38_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_BLCA_hg38_OpenAccess_GDCDR-12-0_DATA"
410,"broad-firecloud-tcga","TCGA_LGG_hg38_OpenAccess_GDCDR-12-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_LGG_hg38_OpenAccess_GDCDR-12-0_DATA",TRUE,TRUE,"Brain Lower Grade Glioma","Tumor/Normal","USA","TCGA Brain Lower Grade Glioma Open-Access Data Workspace

*hg38 TCGA and TARGET workspaces reference files by their GDC UUIDs.  In order to run analyses on the referenced data files, you will need to run workflows that retrieve the files from the GDC and copy them to your workspace bucket.  See   [this forum post](https://gatkforums.broadinstitute.org/firecloud/discussion/10382/populating-hg38-tcga-and-target-workspaces-with-data-files#latest) for instructions on the running of these workflows.*","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients' clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","516","Brain","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh38/hg38","TCGA_LGG_hg38_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_LGG_hg38_OpenAccess_GDCDR-12-0_DATA"
411,"help-gatk","Pre-processing_hg38_v2","READER","https://app.terra.bio/#workspaces/help-gatk/Pre-processing_hg38_v2",TRUE,FALSE,NA,NA,NA,"### GATK Best Practices for Germline SNPs Indels
The workflow takes unmapped pair-end sequencing data ([unmapped BAM format](https://software.broadinstitute.org/gatk/documentation/article?id=11008)) and returns a CRAM, a GVCF, and a BAI. The workspace is a fully reproducible example of the data pre-processing portion of the GATK Best Practices for Germline SNP & Indel Discovery on human whole-genome sequence data.         

Scroll down for details on the workflow, including input and output descriptions and requirements, estimated run times and costs, and  sample data details and location.  A detailed description of the workflow is available in [Gatk's Best Practices Document](https://software.broadinstitute.org/gatk/best-practices/workflow?id=11145).       

**Note:** This workspace is superseded by the [Whole-Genome-Analysis-Pipeline](https://app.terra.bio/#workspaces/help-gatk/Whole-Genome-Analysis-Pipeline) workspace and is no longer being updated. 

Cost and time estimates will vary with the use of [Preemptibles](https://gatkforums.broadinstitute.org/firecloud/discussion/11786/preemptibles#latest).  
You can also use [Google's cost calculator](https://cloud.google.com/products/calculator/) to estimate cost to run. For helpful hints on controlling Cloud costs, see [this article](https://support.terra.bio/hc/en-us/articles/360029748111).        

The following material is provided by the GATK Team. Please post any questions or concerns to one of our forum sites : [GATK](https://gatkforums.broadinstitute.org/gatk/categories/ask-the-team/) , [Terra](https://broadinstitute.zendesk.com/hc/en-us/community/topics/360000500432-General-Discussion) , or [WDL/Cromwell](https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team).

## The Pre-Processing_HG38 workflow
**What does it do?**   
The workflow takes an unmapped pair-end sequencing data ([unmapped BAM format](https://software.broadinstitute.org/gatk/documentation/article?id=11008)) and returns a CRAM file, a GVCF file and BAI (index).    

**What are input data requirements/expectations?**     
The Pre-Processing_B38 workflow accepts a file containing a list of human whole-genome pair-end sequencing data in unmapped BAM (uBAM) format. In particular:    
- One or more read groups, one per uBAM file, all belonging to a single sample (SM)
- Input uBAM files must comply with the following requirements:
  - Filenames all have the same suffix (we use "".unmapped.bam"")
  - Files must pass validation by ValidateSamFile
  - Reads are provided in query-sorted order
  - All reads must have an RG tag
- Reference genome must be B38

**If your sequencing data is not in uBAM format**, check out this file conversion workspace, [https://app.terra.bio/#workspaces/help-gatk/Sequence-Format-Conversion](https://app.terra.bio/#workspaces/help-gatk/Sequence-Format-Conversion) for workflows to convert:    

1. Interleaved FASTQ to paired FASTQ
2. Paired FASTQ to unmapped BAM
3. BAM to unmapped BAM
4. CRAM to BAM files from sequencer output for use in GATK analysis tools     

**What outputs does the workflow return?**       
Metadata for all outputs are written to the workspace data table, and include:     
- CRAM, CRAM index, and CRAM md5     
- GVCF and its gvcf index      
- BQSR report      
- Several summary metrics     

**Sample data description and location**  
This workspace data sample table contains metadata for both a full sized and downsampled version of NA12878's of unaligned BAM list file under the column flowcell_unmapped_bams_list. Links to the expected input types are available in the workspace data model for testing.      
 
**Reference data description and location**    
Required and optional references and resources for the methods are included in the Workspace Data table (""Workspace Attributes"" in FireCloud). The reference genome for this workspace is B38.      

**Example time and cost to run**     
Below is an example of the time and cost for running the workflow. Note that cost and time will vary with the use of [preemptibles](https://gatkforums.broadinstitute.org/firecloud/discussion/11786/preemptibles#latest).  
You can also use [Google's cost calculator](https://cloud.google.com/products/calculator/). 

| Sample Name | Sample Size | Time | Cost $ |      
| ---  | :---: | :---: | :---: |     
| NA12878_24RG_small | 3.11 GB | 4:18:00 | 0.77 |
| NA12878 | 64.89 GB | 47:16:00 | 7.23 |      

**For more information on understanding and controlling Cloud costs**, see [this article](https://support.terra.bio/hc/en-us/articles/360029748111)).

---
---

### Software Versions  
- GATK 4.0.11.0
- BWA 0.7.15-r1140
- Picard 2.16.0-SNAPSHOT
- Samtools 1.3.1 (using htslib 1.3.1)
- Python 2.7

----

### Contact information  
This material is provided by the GATK Team. Please post any questions or concerns to one of our forum sites : [GATK](https://gatkforums.broadinstitute.org/gatk/categories/ask-the-team/) , [Terra](https://broadinstitute.zendesk.com/hc/en-us/community/topics/360000500432-General-Discussion) , or [WDL/Cromwell](https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team).

----

### License  
**Copyright Broad Institute, 2019 | BSD-3**  
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at https://github.com/openwdl/wdl/blob/master/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/Pre-processing_hg38_v2"
412,"theiagen-validations","theiagen-public-resources","READER","https://app.terra.bio/#workspaces/theiagen-validations/theiagen-public-resources",TRUE,FALSE,NA,NA,NA,"",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","theiagen-validations/theiagen-public-resources"
414,"broad-firecloud-tcga","TCGA_KIRP_hg38_OpenAccess_GDCDR-12-0_DATA","READER","https://app.terra.bio/#workspaces/broad-firecloud-tcga/TCGA_KIRP_hg38_OpenAccess_GDCDR-12-0_DATA",TRUE,TRUE,"Kidney Renal Papillary Cell Carcinoma","Tumor/Normal","USA","TCGA Kidney renal papillary cell carcinoma Open-Access Data Workspace

*hg38 TCGA and TARGET workspaces reference files by their GDC UUIDs.  In order to run analyses on the referenced data files, you will need to run workflows that retrieve the files from the GDC and copy them to your workspace bucket.  See   [this forum post](https://gatkforums.broadinstitute.org/firecloud/discussion/10382/populating-hg38-tcga-and-target-workspaces-with-data-files#latest) for instructions on the running of these workflows.*","NCI","dbGAP","This cohort is part of The Cancer Genome Atlas project (https://cancergenome.nih.gov/abouttcga/overview).  This cohort includes raw data and analysis of cancer patients samples by genomic DNA copy number arrays, DNA methylation, exome sequencing, mRNA arrays, microRNA sequencing and reverse phase protein arrays. De-identified patients' clinical phenotypes and metadata are also included. For more information see the full TCGA cohorts publication list at: https://cancergenome.nih.gov/publications; Data description is also summarized at : https://TCGA_data.nci.nih.gov/docs/publications//tcga/datatype.html","291","Kidney","TCGA","Primary tumor cell, Whole blood","General Research Use","GRCh38/hg38","TCGA_KIRP_hg38_OpenAccess","Whole Exome;Genotyping Array;RNA-Seq;miRNA-Seq;Methylation Array;Protein Expression Array","broad-firecloud-tcga/TCGA_KIRP_hg38_OpenAccess_GDCDR-12-0_DATA"
415,"help-terra","Documentation-Best-Practices_template","READER","https://app.terra.bio/#workspaces/help-terra/Documentation-Best-Practices_template",TRUE,TRUE,NA,NA,NA,"## Short and sweet descriptive title here 

In this first paragraph, include a concise summary (bullet points are helpful) of the workspace. Remember, this is all that will show in the workspace card on the Showcase Workspaces page!     
* **Step 1:** Should mirror the graphic, and the high-level content below       
* **Step 2:** This makes it obvious what to expect           
* **Step 3:** Bullet points can be very. helpful     
* **Step 4:**  Note that after each section, I've included an invisible ""white space"" that. separates things, since Terra dashboards don't have any other way to do that.           

<br>

![Notebooks_QuickStart_flow](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/Notebooks_QuickStart_flow.png)
  
<br>

### How long will it take to run? How much will it cost?     
**Time:** Users really want to know this, so it's right up front!     

**Cost:** This is probably even more important than time...       
     
<br>

### What are the components of this Terra Quickstart Example Workspace?   

For tutorials and templates, it's useful to outline the components of Terra that people are going to use:
![image1](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/Notebooks-QuickStart_symbol_flow.png)            

|1. Terra Data Library ![white space](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/white-space.jpg) ![white space](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/white-space.jpg) |  2. Workspace DATA table  ![white space](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/white-space.jpg) |  3. Three Jupyter notebooks ![white space](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/white-space.jpg) ![white space](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/white-space.jpg) |     
| --------| ----------------| --------|       

<br><br>    

## How do I get started? Step-by-step instructions

### Before you begin: Create your own editable copy (clone) of this WORKSPACE     
In order to run the workflows/notebooks in this tutorial, you need to have edit and execute permissions in the workspace. Cloning the workspace makes an exact copy with you as owner. 

**1.** Click on the round circle with three dots in the upper right corner of this page: 
    ![Clone_ScreenShot](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/Clone_workspace_Screen%20Shot.png)
		
**2.** Select ""Clone"" from the dropdown menu     

**3.** Rename your new workspace something memorable     

**Note:**  It may help to write down or memorize the name of your workspace          

**4.**  Choose your billing project (could be free credit) from the dropdown menu          

**5.** Do not select an authorization domain as this workspace uses open-access data      

**6.** Click the ""Clone Workspace"" button to make your own copy    

<br>   

### Work from your copy of the workspace with these step-by-step instructions    
An overview of each step is given below. Find step-by-step instructions on [this page](https://support.terra.bio/hc/en-us/articles/360043454592), or download a clickable pdf [here](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/Notebooks%20QuickStart%20worksheet.pdf).       
   
|![PDF icon](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/PDF-icon_scaled.png) | Download a clickable PDF of step-by-step instructions [here](https://storage.cloud.google.com/terra-featured-workspaces/QuickStart/Notebooks%20QuickStart%20worksheet.pdf) |  
| --------| ------------------|    

<br>

###  Step 1. Same name as above
**Objectives**     
Define what you expect people to learn or understand from this step    

**Time and cost to complete**    
Again, important information!!   

<br>

### Step  2. Same name as above
**Objectives**     
Define what you expect people to learn or understand from this step    

**Time and cost to complete**    
Again, important information!! Note the white space to separate sections.  

<br>

### Step 3. Same name as above
**Objectives**     
Define what you expect people to learn or understand from this step    

**Time and cost to complete**    
Again, important information!!   

<br>

### Step 4: Same name as above
**Objectives**     
Define what you expect people to learn or understand from this step    

**Time and cost to complete**    
Again, important information!!   

<br>

## Additional Resources  

### How can I learn more about *this topic*?
* Include bullet points and link to references         
* [Getting started on Terra](https://support.terra.bio/hc/en-us/categories/360001728852)      
* [Doing research on Terra](https://support.terra.bio/hc/en-us/categories/360001399872)      


### How can I learn more about *another topic*?      
* For an example of doing the first, exploratory part of a GWAS in a notebook, see this [Featured Workspace](https://app.terra.bio/#workspaces/fc-product-demo/2019_ASHG_Reproducible_GWAS).
* For a real-world example of using a Jupyter notebook in Terra to reproduce a cluster analysis from a publication, see [this Featured Workspace](https://app.terra.bio/#workspaces/help-gatk/Reproducibility_Case_Study_Tetralogy_of_Fallot). 


### How can I learn more about *this. third topic*?

<br>
## Contact information (required)  
This material is provided by the Terra Team. Please post any questions or concerns to our forum site : [Terra](https://support.terra.bio/hc/en-us/community/topics/360001603491-Featured-Workspaces) 

<br>

## License  
**Copyright Broad Institute, 2023 | BSD-3**  
All code provided in this workspace is released under the WDL open source code license (BSD-3) (full license text at https://github.com/openwdl/wdl/blob/master/LICENSE). Note however that the programs called by the scripts may be subject to different licenses. Users are responsible for checking that they are authorized to run all programs before running these tools.",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-terra/Documentation-Best-Practices_template"
416,"help-gatk","BroadE-Day3-Somatic","READER","https://app.terra.bio/#workspaces/help-gatk/BroadE-Day3-Somatic",TRUE,TRUE,NA,NA,NA,"# What's in this workspace?
Welcome to Day 3 of the Genome Analysis Toolkit (GATK) workshop for the BroadE workshop series at Broad!

Earlier today you received introductions to GATK tools and Best Practices pipelines. In this workspace we will be going over two forms of Somatic Analysis: one comparing tumor and normal samples using Mutect2 workflow for variant differences, and another using the Copy Number Alterations (CNA) workflow for copy number variations. 

Your instructor will guide you through this workspace, but you should find the documentation within the notebooks sufficient to run through them again on your own or to share with a friend later (and we encourage you to do so!).

### Data
Data associated with this workspace is located in the [gs://gatk-tutorials/workshop_1903/3-somatic](https://console.cloud.google.com/storage/browser/gatk-tutorials/workshop_1903/3-somatic/?project=broad-dsde-outreach&organizationId=548622027621) google bucket. It contains both input and resource files for the Mutect2 and CNA workflows. Along with the inputs are precomputed outputs generated by each step in the tutorial. This can be used as input for any step in the workflow, in case your generated outputs are not correct or you are unable to complete a step in the workflow. 

### Tools
There are no tools in this workspace. The tutorials are notebook-based, allowing you to run each step manually and view the intermediate outputs of the workflow. This is a great way to understand each step in the workflow and give you the chance to manipulate the parameters to see what happens with the output.

If you are interested in a WDL based workflow of these analyses, check out the [Showcase](https://app.terra.bio/#library/showcase) area in Terra, which features many of our popular GATK workflows in workspaces ready to run your data.


### Notebooks
 **1-somatic-mutect2-tutorial :**
In this hands-on tutorial, we will call somatic short mutations, both single nucleotide and indels, using the GATK4 Mutect2 and FilterMutectCalls. If you need a primer on what somatic calling is about, see the following [GATK forum Article](https://software.broadinstitute.org/gatk/documentation/article?id=11127).


| Runtime Environments | Value |
| --- | --- |
| CPU Minimum| 4|
| Disksize Minimum | 100 GB |
| Memory Minimum | 15 GB |
| Startup Script | gs://gatk-tutorials/scripts/install_gatk_4100.sh |

**2-somatic-cna-tutorial :**
This hands-on tutorial outlines steps to detect alterations with high sensitivity in total and allelic copy ratios using GATK4's ModelSegments CNV workflow. The workflow is suitable for detecting somatic copy ratio alterations, more familiarly copy number alterations (CNAs), or copy number variants (CNVs) for whole genomes and targeted exomes.

| Runtime Environments | Value |
| --- | --- |
| CPU Minimum| 2|
| Disksize Minimum | 100 GB |
| Memory Minimum | 13 GB |
| Startup Script | gs://gatk-tutorials/scripts/install_gatk_4100.sh |

 
### Software versions
GATK4.1

### Time and cost 
This workspace focuses on the use of notebooks, thus the time and cost of completing the tutorial depends on the runtime environment of your notebook and the length of time you spend actively working in the notebook. With the given runtime environments above, users can expect the cost to be much less than a dollar an hour for each notebook. The specific cost will be displayed on the top right corner of your screen, next to the notebook machine options. 

## Appendix

### GATK @ BroadE 2019 in the Terra Support Center

Get yourself oriented with the entire workshop in the [Terra Support Center](https://broadinstitute.zendesk.com/hc/en-us/community/topics). (more on Terra Support below).

### GATK documentation and support

Detailed documentation, tutorials, and more are available at the [GATK web site](https://software.broadinstitute.org/gatk/). If you are still running into issues after consulting the User Guide, have a question, or just want to say hello, reach out to the GATK team via the [GATK Support Forum](https://gatkforums.broadinstitute.org/gatk)!

### Terra documentation and support

Documents and helpful guides to support your journey through Terra are available in the Terra Support Center. Navigate there by following the “Learn About Terra” link in the left-side menu. At the time of this workshop, we are still actively writing and publishing documents but we’ve got a good set of docs in the Quick Start Guide to get you going.


",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","help-gatk/BroadE-Day3-Somatic"
417,"malaria-featured-workspaces","MalariaGEN_Pf7_Data_Resource","READER","https://app.terra.bio/#workspaces/malaria-featured-workspaces/MalariaGEN_Pf7_Data_Resource",TRUE,FALSE,NA,NA,NA,"![MalariaGEN](https://storage.googleapis.com/broad-malaria-public/pf7_workspace/resources/malariagen_logo.png) 
# Pf7 data release
- Date: 2022-10-21


***
## Description

The MalariaGEN Pf7 data release was produced by the Parasite Surveillance Team at the Genomic Surveillance Unit, Wellcome Sanger Institute. It includes 20,864 parasite samples collected in 33 different countries in Africa, Asia, America and Oceania. From these, 16,203 are high-quality samples with complete metadata. In total, it identifies more than 10 million variant positions (SNPs and indels).

This workspace includes metadata, genotyping data and drug-resistance inference for samples contributed to MalariaGEN. To encourage specialised analyses, we additionally include the csp c-terminal haplotypes and full gene haplotypes for crt for all the samples in the release. Additional data can be found on the MalariaGEN Pf7 Home Page.

Potential data users are asked to respect the legitimate interest of all parties that contributed to its development. Information about contributing partners, including contact details to facilitate equitable collaboration are available here:

For more information on the studies on the P. falciparum Community Project that generated these data, please visit: https://www.malariagen.net/wp-content/uploads/2023/11/Pf7-contributing-studies.pdf

The methods used to generate the data are described in detail in ""Pf7: an open dataset of Plasmodium falciparum genome variation in 20,000 worldwide samples"", MalariaGEN, Wellcome Open Research 2023, 8:22, https://doi.org/10.12688/wellcomeopenres.18681.1

Funding to support the generation of Pf7 data was granted by Wellcome, Bill and Melinda Gates Foundation and the Genomic Surveillance Unit.  For questions related to the original data release please contact support@malariagen.net

![Pf7 Summary Statistics](https://storage.googleapis.com/broad-malaria-public/pf7_workspace/resources/summary_stats_figure.png)

Figure extracted from Pf7 app https://apps.malariagen.net/apps/pf7/

***
## Citation information

Publications using these data should acknowledge and cite the source of the data using the following format: ""This publication uses data from MalariaGEN as described in ([https://doi.org/10.12688/wellcomeopenres.18681.1](https://doi.org/10.12688/wellcomeopenres.18681.1)).""


***


## Table Field Descriptions:

### Pf7_samples

This table includes sample metadata for all 20,864 samples collected from partners and details of sequence read data available at the European Nucleotide Archive (ENA, https://www.ebi.ac.uk/ena). It contains the following columns:

    - Sample                         Unique ID of each sample (which can be used to link to other sample information and genotypes)
    - Study                          Code of the partner study which collected this sample
    - Country                        Country in which the sample was collected (in the case of returning travellers this is the country visited)
    - Admin level 1                  First Administration level in which the sample was collected
    - Country latitude               GPS coordinate of the latitude of the Country
    - Country longitude              GPS coordinate of the longitude of the Country
    - Admin level 1 latitude         GPS coordinate of the latitude of the Admin level 1
    - Admin level 1 longitude        GPS coordinate of the longitude of the Admin level 1 
    - Year                           Year in which the sample was collected
    - ENA                            ENA run accession(s) for the sequencing read data. In some cases multiple runs of sequencing data were merged
    - All samples same case    Identifies all the sample in the data set which were collected from the same individual, either at the same of different time points
    - Population                     Population to which the sample was assigned to (SA = South America, AF-W = Africa - West, AF-C = Africa - Central, AF-NE = Africa - Northeast, AF-E = Africa - East, AS-S-E = Asia - South - East, AS-S-FE = Asia - South - Far East, AS-SE-W = Asia - Southeast - West, AS-SE-E = Asia - Southeast - East, OC-NG = Oceania - New Guinea)
    - Percent callable                     Percentage of the genome which has coverage of at least 5 reads and less than 10% of reads with mapping quality 0
    - QC pass                        Flag indicating whether the sample passed QC (True=passed QC, False=failed QC)
    - Exclusion reason               Reason samples failed QC (High_num_singletons, Low_coverage, Lower_covered_duplicate, Mixed_species, Unverified_identity). Note that Unverified_identity samples, we do not include spatial and temporal metadata.
    - Sample type                    Amplification technology used on the sample (MDA, gDNA or sWGA)
    - Sample was in Pf6              Flag indicating whether the sample was on the previous (Pf6) release (True=in previous release, False=not in previous release i.e. new sample)

*** 
### Pf7_drug_resistance_marker_genotypes


This file contains genotypes at drug resistance markers for all QC pass 16,203 samples derived from analysis of sequence data. It contains the following columns:

    - Sample                         Unique ID of each sample (which can be used to link to other sample information and genotypes)
    - crt_76[K]                      Amino acid at crt position 76. For explanation see below.
    - crt_72-76[CVMNK]               Amino acids at crt positions 72 to 76. For explanation see below.
    - dhfr_51[N]                     Amino acid at dhfr position 51. For explanation see below.
    - dhfr_59[C]                     Amino acid at dhfr position 59. For explanation see below.
    - dhfr_108[S]                    Amino acid at dhfr position 108. For explanation see below.
    - dhfr_164[I]                    Amino acid at dhfr position 164. For explanation see below.
    - dhps_437[G]                    Amino acid at dhps position 437. For explanation see below.
    - dhps_540[K]                    Amino acid at dhps position 540. For explanation see below.
    - dhps_581[A]                    Amino acid at dhps position 581. For explanation see below.
    - dhps_613[A]                    Amino acid at dhps position 613. For explanation see below.
    - kelch13_349-726_ns_changes     Non-synonymous mutations at Kelch13 positions 349-726. For explanation see below.  
    - mdr1_dup_call                  1.0=mdr1 duplicated, 0.0=mdr1 not duplicated, -1.0=duplication status of mdr1 undetermined
    - mdr1_breakpoint                Tandem duplication breakpoints around mdr1.
    - pm2_dup_call                   1.0=plasmepsin 2-3 duplicated, 0.0=plasmepsin 2-3 not duplicated, -1.0=duplication status of plasmepsin 2-3 undetermined
    - pm2_breakpoint                 Tandem duplication breakpoints around plasmepsin 2-3.

Explanation of amino acid columns in crt, dhfr and dhps:

Each value can have a single haplotype if homozygous or two haplotypes separated by a comma if heterozygous
It is possible to have heterozygous calls where both amino acid haplotypes are the same. The heterozygosity here is at the nucleotide level. These could perhaps be considered homozygous alt.
- represents missing (missing genotype in at least one of the positions)
* represents an unphased het followed by another het. Because hets are unphased it is not possible to resolve the two haplotypes. These are perhaps best considered missing.
! represents a frame-shift in the haplotype. These are perhaps best considered missing.

Explanation of non-synonymous changes (ns_changes): 
Non-synonymous mutations are shown in the form: <REF><POS><ALT>. Homozygous mutations are shown in upper case and heterozygous in lower case. The nomenclature for amino acids described above is also used on this field. 

*** 
### Pf7_inferred_resistance_status_classification


This file includes sample phenotype data for 16,203 samples that passed QC derived from the data in Pf_7_drug_resistance_marker_genotypes.txt, using the rules outlined in ""Pf7 mapping genetic markers to inferred resistance status classification.docx"", together with deletion genotypes for HRP2 and HRP3 that can be used to determine resitance to parid diagnostic tests (RDTs). It contains the following columns:

	  - Sample                         Unique ID of each sample (which can be used to link to other sample information and genotypes)
    - Chloroquine                    Chloroquine resistance status. Resistant/Sensitive/Undetermined
    - Pyrimethamine                  Pyrimethamine resistance status. Resistant/Sensitive/Undetermined
    - Sulfadoxine                    Sulfadoxine resistance status. Resistant/Sensitive/Undetermined
    - Mefloquine                     Mefloquine resistance status. Resistant/Sensitive/Undetermined
    - Artemisinin                    Artemisinin resistance status. Resistant/Sensitive/Undetermined
    - Piperaquine                    Piperaquine resistance status. Resistant/Sensitive/Undetermined
    - SP (uncomplicated)             Sulfadoxine-Pyrimethamine treatment resistance status. Samples carrying the dhfr triple mutant, which is strongly associated with SP failure. Resistant/Sensitive/Undetermined
    - SP (IPTp)                      Sulfadoxine-Pyrimethamine intermittent preventive treatment in pregnancy resistance status. Samples carrying the dhfr/dhps sextuple mutant, which confers a higher level of SP resistance. Resistant/Sensitive/Undetermined
    - AS-MQ                          Artesunate-mefloquine resistance status. Resistant/Sensitive/Undetermined
    - DHA-PPQ                        Dihydroartemisinin-piperaquine resistance status. Resistant/Sensitive/Undetermined
    - HRP2                           Deletions at HRP2 associated with failure of rapid diagnostic tests. del=HRP2 deleted, nodel=HRP2 not deleted, uncallable
    - HRP3                           Deletions at HRP3 associated with failure of rapid diagnostic tests. del=HRP3 deleted, nodel=HRP3 not deleted
    - HRP2 and HRP3                  Deletions at HRP2 and HRP3 associated with failure of rapid diagnostic tests. del=both HRP2 and HRP3 deleted, nodel=either HRP2, HRP3 or both not deleted, uncallable


***
	
### Pf7_fws

This file includes Fws values for 16,203 samples that passed QC. It contains the following columns:

    - Fws                            Fws value


***
### Pf7_crt_haplotypes.txt

This file includes the full gene crt haplotypes for 16,203 QC-pass samples. This includes the following columns:

    - crt_aa_haplotype               Amino acid haplotype. Please note the following:
                                     - If the nucleotide haplotype is homozygous (i.e. there are no heterozygous variants), there will be a single amino acid haplotype
                                     - If the nucleotide haplotype is heterozygous (i.e. there is at least one heterozygous variants), there will be two amino acid haplotypes, separated by a comma
                                     - Note that in some cases, there could be a heterozygous nucleotide haplotype, both alleles of which give the same amino acid haplotype. In cases such as this, the amino acid haplotype will be repeated, separated by a comma
                                     - If there are multiple mutations that are heterozygous, and cannot be phased, an asterisk (*) will be appended to the aa_haplotype call
                                     - If any variant is missing, value will be '-'.
									 
    - crt_nucleotide_haplotype       Nucleotide haplotype. Please note the following:
                                     - If the nucleotide haplotype is homozygous (i.e. there are no heterozygous variants), there will be a single haplotype
                                     - If the nucleotide haplotype is heterozygous (i.e. there is at least one heterozygous variants), there will be two haplotypes, separated by a comma
                                     - Nucleotides which match the reference sequence are shown in lower case, whereas nucleotides that differ (SNPs) are shown in upper case
                                     - If any variant is missing, nucleotide_haplotype will be '-'.
									 
    - crt_ns_changes                 Non-synonymous mutations, listed in genomic position order. Please note the following:
	                                 - All mutations will be shown in the form ""<REF AA><AA number><ALT AA>"", e.g. ""K76T""
	                                 - Homozygous mutations are shown in upper case and heterozygous in lower case.
	                                 - If there are two distinct amino acid haplotypes, these will be separated by a comma.
	                                 - If multiple non-synonymous mutations are found within an amino acid haplotype, these will be separated by a forward slash (/).
	                                 - If there are multiple mutations that are heterozygous, and cannot be phased, an asterisk (*) will be appended to the end
	                                 - If there are no non-synonymous mutations, this column will be empty
	                                 - Missing variants will be ignored, but any non-synoymous mutation elsewhere will still be included. Note the consequence of this is that it is possible that crt_aa_haplotype will be '-' but crt_ns_changes will have one or more non-synonymous mutations reported.
	                                 - If any mutations result in a premature stop codon, value will be ""!""


***
### Pf7_csp_c_terminal_haplotypes

This file includes the csp c-terminal (amino acids 277-397) haplotypes for 16,203 QC-pass samples plus 6 lab strain samples (7G8/PG0563-C, GB4/PG0564-C, HB3/PG0565-C, Dd2/PG0566-C, 3D7/PG0567-C, IT/PG0568-C). This includes the following columns:

    - csp_277-397_aa_haplotype       Amino acid haplotype. Please note the following:
                                     - If the nucleotide haplotype is homozygous (i.e. there are no heterozygous variants), there will be a single amino acid haplotype
                                     - If the nucleotide haplotype is heterozygous (i.e. there is at least one heterozygous variants), there will be two amino acid haplotypes, separated by a comma
                                     - Note that in some cases, there could be a heterozygous nucleotide haplotype, both alleles of which give the same amino acid haplotype. In cases such as this, the amino acid haplotype will be repeated, separated by a comma
                                     - If there are multiple mutations that are heterozygous, and cannot be phased, an asterisk (*) will be appended to the aa_haplotype call
                                     - If any variant is missing, value will be '-'.
									 
    - csp_277-397_nucleotide_haplotype      Nucleotide haplotype. Please note the following:
                                     - If the nucleotide haplotype is homozygous (i.e. there are no heterozygous variants), there will be a single haplotype
                                     - If the nucleotide haplotype is heterozygous (i.e. there is at least one heterozygous variants), there will be two haplotypes, separated by a comma
                                     - Nucleotides which match the reference sequence are shown in lower case, whereas nucleotides that differ (SNPs) are shown in upper case
                                     - If any variant is missing, nucleotide_haplotype will be '-'.
									 
    - csp_277-397_ns_changes         Non-synonymous mutations, listed in genomic position order. Please note the following:
	                                 - All mutations will be shown in the form ""<REF AA><AA number><ALT AA>"", e.g. ""S301N""
	                                 - Homozygous mutations are shown in upper case and heterozygous in lower case.
	                                 - If there are two distinct amino acid haplotypes, these will be separated by a comma.
	                                 - If multiple non-synonymous mutations are found within an amino acid haplotype, these will be separated by a forward slash (/).
	                                 - If there are multiple mutations that are heterozygous, and cannot be phased, an asterisk (*) will be appended to the end
	                                 - If there are no non-synonymous mutations, this column will be empty
	                                 - Missing variants will be ignored, but any non-synoymous mutation elsewhere will still be included. Note the consequence of this is that it is possible that csp_277-397_aa_haplotype will be '-' but csp_277-397_ns_changes will have one or more non-synonymous mutations reported.
	                                 - If any mutations result in a premature stop codon, values will be ""!""


***
### Pf7_eba175_callset


This file includes eba175 allelic type calls for 16,203 QC-pass samples.  This includes the following columns

    - Sample                         Unique ID of each sample (which can be used to link to other sample information and genotypes)
    - f_frac                         Fraction of reads containing F-type kmer
    - c_frac                         Fraction of reads containing C-type kmer
	- eba175_call                    eba175 allelic type call. F/C/Mixed/No Call


***
### Pf7_vcf


This directory contains vcf files, one per chromosome. Each file is in bgzip format (.vcf.gz) and has an associated tabix index file (.vcf.gz.tbi). There are sixteen files in total, fourteen for each of the autosomes (Pf3D7_01_v3 - Pf3D7_14_v3), one for the mitochondrial sequence (Pf_M76611) and one for the apicoplast sequence (Pf3D7_API_v3).

The files, once unzipped, are tab-separated text files, but may be too large to open in Excel.

The VCF format is described in https://github.com/samtools/hts-specs

Tools to assist in handling VCF files are freely available from
http://samtools.github.io/bcftools/

The VCF files contains details of 10,145,661 discovered variant genome positions.
These variants were discovered amongst all samples from the release.
4,397,801 of these variant positions are SNPs, with the remainder being either
short insertion/deletions (indels), or a combination of SNPs and indels. It is
important to note that many of these variants are considered low quality. Only
the variants for which the FILTER column is set to PASS should be considered of
reasonable quality. There are 5,868,659 such PASS variants of which 3,125,721
are SNPs and 2,742,938 indels.

The FILTER column is based on two types of information. Firstly certain regions
of the genome are considered ""non-core"". This includes sub-telomeric regions,
centromeres and internal VAR gene regions on chromosomes 4, 6, 7, 8 and 12. All
variants within non-core regions are considered to be low quality, and hence
will not have the FILTER column set to PASS. The regions which are core and
non-core can be found in the file
ftp://ngs.sanger.ac.uk/production/malaria/pf-crosses/1.0/regions-20130225.onebased.txt.

Secondly, variants are filtered out based on a quality score called VQSLOD. All
variants with a VQSLOD score below 0 are filtered out, i.e. will have a value of
Low_VQSLOD in the FILTER column, rather than PASS. The VQSLOD score for each
variant can be found in the INFO field of the VCF file. It is possible to use
the VQSLOD score to define a more or less stringent set of variants (see next
section for further details).

It is also important to note that many variants have more than two alleles. For
example, amongst the 5,868,659 PASS variants, 2,513,888 are biallelic. The
remaining 611,833 PASS variants have 3 or more alleles. The maximum number of
alternative alleles represented is 6. Note that some positions can in truth have
more than 6 alternative alleles, particularly those at the start of short tandem
repeats. In such cases, some true alternative alleles will be missing.

In addition to alleles representing SNPs and indels, some variants have an
alternative allele denoted by the * symbol. This is used to denote a ""spanning
deletion"". For samples that have this allele, the base at this position has been
deleted. Note that this is not the same as a missing call - the * denotes that
there are reads spanning across this position, but that the reads have this
position deleted yet map on either side of the deletion. For further details see
https://software.broadinstitute.org/gatk/guide/article?id=6926

In addition to the VQSLOD score mentioned above, The INFO field contains many
other variant-level metrics. The metrics QD, FS, SOR, DP are all measures
related to the quality of the variant. The VQSLOD score is derived from these
five metrics.

AC contains the number of non-reference alleles amongst the samples in the file.
Because the file contains diploid genotype calls, homozygous non-reference calls
will be counted as two non-reference alleles, whereas heterozygous calls will be
counted as one non-reference allele. Where a variant position has more than one
one non-reference allele, counts of each different non-reference allele are
given. AN contains the total number of called alleles, including reference
alleles. A simple non-reference allele frequency can be calculated as AC/AN.
AC and AN values are all specfic to the samples in the study the VCF was created
for.

Various functional annotations are held in the the SNPEFF variables of the INFO
field. Where appropriate, the amino acid change caused by the variant can be
found in SNPEFF_AMINO_ACID_CHANGE. Note that for multi-allelic variants, only
one annotation is given, and therefore this should not be relied on for non-
biallelic variants. SNPEFF_AMINO_ACID_CHANGE also does not take account of
nearby variants, so if two SNPs are present in the same codon, the
amino acid change given is likely to be wrong. Similarly, if two coding indels
are found in the same exon, the SNPEFF annotations are likely to be wrong. This
situation occurs at the CRT locus (see next section for further details).

Coding variants are identified using the CDS flag in the INFO field.

Columns 10 and onwards of the VCF contain the information for each sample.
The first component of this (GT) is always the diploid genotype call as
determined by GATK. A value of 0/0 indicates a homozygous reference call. A
value of 1/1 indicates a homozygous alternative allele call. 0/1 indicates a
heterozygous call. A value of 2 indicates the sample has the second alternative
allele, i.e. the second value in the ALT column. For example 2/2 would mean the
sample is homozygous the the second alternative allele, 0/2 would mean the
sample is heterozygous for the reference and second alternative alleles, and 1/2
would mean the sample is heterozygous for the first and second alternative
alleles. A value of ./. indicates a missing genotype call, usually because there
are no reads mapping at this position in that sample.


#### Recommendations regarding sets of variants to use in analyses


Variants are filtered using the VQSLOD metric. VQSLOD is log(p/q) where p is the
probability of being true and q is the probability of being false. Theoretically,
when VQSLOD > 0, p is greater than q, and therefore the variant is more likely
true than false. Conversely, when VQSLOD < 0, the variant is theoretically more
like false than true. This is why we have chosen 0 as the threshold to use to
declare that variants have passed the filters: all PASS variants are
theoretically more likely true than false. Of course, for variants where VQSLOD
is only slightly above 0, there is only a slightly greater probability of being
true than of being false. Therefore, for example, many of the variants with
values between 0 and 1 are likely to be false.

Empirically we have found that SNPs tend to be more accurate than indels, coding
variants tend to be more accurate than non-coding variants, and bi-allelic
variants tend to be more accurate than multi-allelic variants. If you require a
very reliable set of variants for genome-wide analysis, and don't mind if you
miss some real variants, we recommend using only bi-allelic coding SNPs in the
core genome with a VQSLOD score > 6. There are 83,168 such stringent SNPs in the
call set. We include a command below to create such a set of variants.

If instead you would like to know of all likely variation within a certain
region, even if this means including a few false variants, we recommend using
all PASS variants. Finally, if you want to ensure you miss as little as possible
of the true variation, at the risk of including large numbers of false positives,
you could ignore the FILTER column and use all variants in the VCF.

In general, we recommend caution in analysing indels. For any given sample, the
majority of differences from the reference genome are likely to be due to indels
in low-complexity non-coding regions, e.g. in length polymorphisms of short
tandem repeats (STRs), such as homopolymer runs or AT repeats. In general, it is
difficult to map short reads reliably in such regions, and this is compounded by
the fact that these regions tend to have high AT content, and in general we
typically have much lower coverage in high AT regions. Indels also tend to be
multi-allelic, making analysis much more challenging than for (typically
bi-allelic) SNPs.

Despite what is written above, it may often be important to analyse indels in
order to determine the true nature of variation at a locus. An example of this
is analysis of haplotypes of codons 72-76 of the chloroquine resistance
transporter gene PfCRT. These codons are translated from bases 403,612-403,626
on chromosome 7 (Pf3D7_07_v3:403612-403626). The vcf contains two indels here,
an insertion of a T after position 403,618, and a deletion of T after position
403,622. However, it turns out that every sample that has the insertion after
403,618 also has the deletion after 403,622. If we write out the full sequence
of the 3D7 reference (Ref), and a sample which has both the insertion and
deletion (Alt), we see the following:
Ref: TGTGTAAT-GAATAAA = TGTGTAATGAATAAA (amino acid sequence CVMNK)
Alt: TGTGTAATTGAA-AAA = TGTGTAATTGAAAAA (amino acid sequence CVIEK)
As can be seen from the above, the single base insertion and deletion are
equivalent to three SNPs at positions 403,620 (G/T), 403,621 (A/G) and
403,623 (T/A). If we had only chosen to analyse SNPs at this locus, we would
not have seen the CVIEK haplotype.


#### Zarr format
Variants in Zarr format are also available in this workspace.
	
***
## External Notebooks
Additional notebooks for working with Pf7 data exist outside this workspace:
- https://malariagen.github.io/parasite-data/landing-page.html
- https://malariagen.github.io/parasite-data/pf7/Data_access.html

	
***
	
## Data excluded from release:
Sequence read data on samples collected in Indonesia cannot be made publically available because of national export restrictions.
	
![Sanger Institute](https://storage.cloud.google.com/broad-malaria-public/pf7_workspace/resources/Wellcome_Sanger_Institute_Logo_Landscape_Digital_RGB_Full_Colour_300px.png)
	
![MalariaGEN](https://storage.googleapis.com/broad-malaria-public/pf7_workspace/resources/malariagen_logo_300px.png)
",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","malaria-featured-workspaces/MalariaGEN_Pf7_Data_Resource"
418,"biodata-catalyst","BDC-PIC-SURE API R Examples","READER","https://app.terra.bio/#workspaces/biodata-catalyst/BDC-PIC-SURE%20API%20R%20Examples",TRUE,FALSE,NA,NA,NA,"# NHLBI BioData Catalyst® (BDC) Powered by PIC-SURE R API examples

This workspace contains Jupyter Notebook examples of BDC-PIC-SURE API use cases, using BDC studies. The PIC-SURE API is available in two languages: R and python. This workspace features the R PIC-SURE API example notebooks and requires R 3.4 or higher.


## PIC-SURE API Overview
The main goal of the PICSURE API is to provide a simple and reliable way to work with data from studies that are part of BDC. Each individual study is accessible in a unique, easy to use, tabular format directly in an R or python environment. The API allows users to filter studies based on user-specified criteria, as well as to retrieve a cohort that has been created using the [PIC-SURE interface](https://picsure.biodatacatalyst.nhlbi.nih.gov). In addition to many heart, lung, blood, and sleep related datasets, there are 43 specific phenotype variables that have been harmonized across multiple TOPMed studies that are also accessible directly through the PIC-SURE API. 


## Workspace information
- Requirement : R 3.4 or higher. To select the appropriate runtime environment for your Terra Workspace, click on the gear wheel beside 'Cloud Environment' in the top right corner, and under Application Configuration select “Default: (GATK 4.1.4.1, Python 3.7.10, R 4.0.5)” or another appropriate configuration.
- Notebooks update information: the central repository for these notebooks is available on the [Access to Data using PIC-SURE API GitHub](https://github.com/hms-dbmi/Access-to-Data-using-PIC-SURE-API/tree/master/NHLBI_BioData_Catalyst).  Currently under active development, the repository is updated on a regular basis. Although the Terra public Workspace will be kept up-to-date as much as possible, there might be a difference between the version of the notebook you're using and the most recent one. So if you ran into an unexpected issue when running one of these example notebooks, it may be worth checking for a potential more up-to-date version available on GitHub.


## Available notebooks
The following example notebooks are available: 
  - Workspace_setup.ipynb: a notebook that helps to set up your workspace to use the PIC-SURE API. This includes saving your personal access token from PIC-SURE.
  - 0_Export_from_UI.ipynb: an interactive tutorial on how to export a dataframe built in the UI into your Terra workspace.
  - 1_PICSURE-API_101.ipynb: an illustration of the main functionalities of the PIC-SURE API.
  - 2_TOPMed_DCC_Harmonized_Variables_analysis.ipynb: an example of how to access and work with the ""harmonized variables"" across the TOPMed studies.
  - 3_PheWAS.ipynb: a straightforward PIC-SURE API use-case, using a PheWAS (Phenome-Wide Association Study) analysis as an illustration example.
  - 4_Genomic_Queries.ipynb: an illustration of how to use genomic variables to build queries.
  - 5_LongitudinalData.ipynb: an example of how to select longitudinal variables from PIC-SURE.
  - 6_Sickle_Cell.ipynb: an example illustrating how to select and view data related to Sickle Cell Disease (specifically the HCT for SCD dataset).
  - 7_Harmonization_with_PICSURE.ipynb: examples of harmonization across studies using (1) sex and BMI variables (including TOPMed Harmonized dataset and others) and (2) orthopnea and pneumonia variables.
  - ORCHID_COVID19_python.ipynb: the code accompanying the JAMA publication on November 7th 2021: ""[Effect of Hydroxychloroquine on Clinical Status at 14 Days in Hospitalized Patients With COVID-19](https://jamanetwork.com/journals/jama/fullarticle/2772922)"" 


## Data information
The data accessible through the PIC-SURE API are controlled-access data. Hence, if you're saving any data from the Jupyter notebooks directly into the Google Bucket associated with this workspace, you will need to set an Authorization Domain to protect this data. Instructions about how to proceed can be found here:

https://support.terra.bio/hc/en-us/articles/360039415171-Authorization-Domain-overview-for-BioData-Catalyst-users

## Contact
For bug report or additional information, please contact us using the [BioData Catalyst contact form](https://biodatacatalyst.nhlbi.nih.gov/contact/).

",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,"","biodata-catalyst/BDC-PIC-SURE API R Examples"
